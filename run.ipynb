{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_validate, ShuffleSplit\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "from os.path import join\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "from build_features import load_moons, load_concentric\n",
    "from utils import euclidian, plot_plain_separator\n",
    "from model_selection import run_model_selection, load_model\n",
    "\n",
    "use_tunned = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a sample dataset with few superposition for test. This dataset is supposed to be the correct one, without error on label acquisition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = load_concentric(noise=0.00, samples=1000, factor=0.1)\n",
    "x_test = np.vstack([x_train, x_test])\n",
    "y_test = np.concatenate([y_train, y_test])\n",
    "data_test = pd.DataFrame(x_test, columns=['x1', 'x2'])\n",
    "data_test['class'] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set()\n",
    "ax_1 = sns.scatterplot(x=x_test[y_test == 0,0], y=x_test[y_test == 0,1])\n",
    "ax_2 = sns.scatterplot(x=x_test[y_test == 1,0], y=x_test[y_test == 1,1])\n",
    "plt.title(\"Dataset de Teste\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a noisy dataset for training. The noise is to simmulate an error on the label acquisition process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = load_concentric(noise=0.16, samples=200, factor=0.3)\n",
    "x_train = np.vstack([x_train, x_test])\n",
    "y_train = np.concatenate([y_train, y_test])\n",
    "y_backup = y_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_train = pd.DataFrame(x_train, columns=['x1', 'x2'])\n",
    "data_train['class'] = y_train\n",
    "\n",
    "sns.set()\n",
    "sns.scatterplot(x='x1', y='x2', hue='class', data=data_train)\n",
    "plt.title('Distribuição de Classes em Meia Lua - Treino')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Run hyperparameter tuning using grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    cls = run_model_selection(x_train, y_train)\n",
    "    best = cls.best_params_\n",
    "    use_tunned = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run training on noisy dataset without relabeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.69974918\n",
      "Iteration 2, loss = 0.69780402\n",
      "Iteration 3, loss = 0.69590741\n",
      "Iteration 4, loss = 0.69405713\n",
      "Iteration 5, loss = 0.69225429\n",
      "Iteration 6, loss = 0.69049819\n",
      "Iteration 7, loss = 0.68878573\n",
      "Iteration 8, loss = 0.68711764\n",
      "Iteration 9, loss = 0.68549476\n",
      "Iteration 10, loss = 0.68391352\n",
      "Iteration 11, loss = 0.68236866\n",
      "Iteration 12, loss = 0.68086061\n",
      "Iteration 13, loss = 0.67938761\n",
      "Iteration 14, loss = 0.67794522\n",
      "Iteration 15, loss = 0.67653183\n",
      "Iteration 16, loss = 0.67514779\n",
      "Iteration 17, loss = 0.67379243\n",
      "Iteration 18, loss = 0.67246523\n",
      "Iteration 19, loss = 0.67116183\n",
      "Iteration 20, loss = 0.66988266\n",
      "Iteration 21, loss = 0.66863330\n",
      "Iteration 22, loss = 0.66740369\n",
      "Iteration 23, loss = 0.66619402\n",
      "Iteration 24, loss = 0.66500140\n",
      "Iteration 25, loss = 0.66382753\n",
      "Iteration 26, loss = 0.66266329\n",
      "Iteration 27, loss = 0.66150781\n",
      "Iteration 28, loss = 0.66036268\n",
      "Iteration 29, loss = 0.65922756\n",
      "Iteration 30, loss = 0.65809827\n",
      "Iteration 31, loss = 0.65697393\n",
      "Iteration 32, loss = 0.65585337\n",
      "Iteration 33, loss = 0.65473192\n",
      "Iteration 34, loss = 0.65361019\n",
      "Iteration 35, loss = 0.65248754\n",
      "Iteration 36, loss = 0.65136265\n",
      "Iteration 37, loss = 0.65023719\n",
      "Iteration 38, loss = 0.64911366\n",
      "Iteration 39, loss = 0.64798335\n",
      "Iteration 40, loss = 0.64684919\n",
      "Iteration 41, loss = 0.64570797\n",
      "Iteration 42, loss = 0.64456640\n",
      "Iteration 43, loss = 0.64342035\n",
      "Iteration 44, loss = 0.64226809\n",
      "Iteration 45, loss = 0.64111336\n",
      "Iteration 46, loss = 0.63995381\n",
      "Iteration 47, loss = 0.63878404\n",
      "Iteration 48, loss = 0.63760258\n",
      "Iteration 49, loss = 0.63641241\n",
      "Iteration 50, loss = 0.63521104\n",
      "Iteration 51, loss = 0.63400513\n",
      "Iteration 52, loss = 0.63279444\n",
      "Iteration 53, loss = 0.63158041\n",
      "Iteration 54, loss = 0.63035875\n",
      "Iteration 55, loss = 0.62912928\n",
      "Iteration 56, loss = 0.62789187\n",
      "Iteration 57, loss = 0.62664554\n",
      "Iteration 58, loss = 0.62538908\n",
      "Iteration 59, loss = 0.62412497\n",
      "Iteration 60, loss = 0.62285320\n",
      "Iteration 61, loss = 0.62157807\n",
      "Iteration 62, loss = 0.62030186\n",
      "Iteration 63, loss = 0.61901550\n",
      "Iteration 64, loss = 0.61772630\n",
      "Iteration 65, loss = 0.61643164\n",
      "Iteration 66, loss = 0.61513250\n",
      "Iteration 67, loss = 0.61382914\n",
      "Iteration 68, loss = 0.61251955\n",
      "Iteration 69, loss = 0.61120540\n",
      "Iteration 70, loss = 0.60988794\n",
      "Iteration 71, loss = 0.60856052\n",
      "Iteration 72, loss = 0.60723092\n",
      "Iteration 73, loss = 0.60589967\n",
      "Iteration 74, loss = 0.60455952\n",
      "Iteration 75, loss = 0.60321315\n",
      "Iteration 76, loss = 0.60186574\n",
      "Iteration 77, loss = 0.60051799\n",
      "Iteration 78, loss = 0.59916688\n",
      "Iteration 79, loss = 0.59781056\n",
      "Iteration 80, loss = 0.59644779\n",
      "Iteration 81, loss = 0.59508367\n",
      "Iteration 82, loss = 0.59371878\n",
      "Iteration 83, loss = 0.59235330\n",
      "Iteration 84, loss = 0.59098591\n",
      "Iteration 85, loss = 0.58961476\n",
      "Iteration 86, loss = 0.58824402\n",
      "Iteration 87, loss = 0.58687174\n",
      "Iteration 88, loss = 0.58549448\n",
      "Iteration 89, loss = 0.58411705\n",
      "Iteration 90, loss = 0.58273943\n",
      "Iteration 91, loss = 0.58136079\n",
      "Iteration 92, loss = 0.57998365\n",
      "Iteration 93, loss = 0.57861334\n",
      "Iteration 94, loss = 0.57724179\n",
      "Iteration 95, loss = 0.57586614\n",
      "Iteration 96, loss = 0.57449013\n",
      "Iteration 97, loss = 0.57311737\n",
      "Iteration 98, loss = 0.57174188\n",
      "Iteration 99, loss = 0.57036641\n",
      "Iteration 100, loss = 0.56898566\n",
      "Iteration 101, loss = 0.56760073\n",
      "Iteration 102, loss = 0.56621378\n",
      "Iteration 103, loss = 0.56482415\n",
      "Iteration 104, loss = 0.56343330\n",
      "Iteration 105, loss = 0.56203898\n",
      "Iteration 106, loss = 0.56064112\n",
      "Iteration 107, loss = 0.55924394\n",
      "Iteration 108, loss = 0.55784709\n",
      "Iteration 109, loss = 0.55644809\n",
      "Iteration 110, loss = 0.55504949\n",
      "Iteration 111, loss = 0.55365074\n",
      "Iteration 112, loss = 0.55225072\n",
      "Iteration 113, loss = 0.55084841\n",
      "Iteration 114, loss = 0.54944406\n",
      "Iteration 115, loss = 0.54803515\n",
      "Iteration 116, loss = 0.54662511\n",
      "Iteration 117, loss = 0.54521488\n",
      "Iteration 118, loss = 0.54380103\n",
      "Iteration 119, loss = 0.54238445\n",
      "Iteration 120, loss = 0.54096805\n",
      "Iteration 121, loss = 0.53955148\n",
      "Iteration 122, loss = 0.53813230\n",
      "Iteration 123, loss = 0.53671038\n",
      "Iteration 124, loss = 0.53528758\n",
      "Iteration 125, loss = 0.53386147\n",
      "Iteration 126, loss = 0.53243250\n",
      "Iteration 127, loss = 0.53100358\n",
      "Iteration 128, loss = 0.52957394\n",
      "Iteration 129, loss = 0.52814344\n",
      "Iteration 130, loss = 0.52671311\n",
      "Iteration 131, loss = 0.52528255\n",
      "Iteration 132, loss = 0.52385084\n",
      "Iteration 133, loss = 0.52242482\n",
      "Iteration 134, loss = 0.52099927\n",
      "Iteration 135, loss = 0.51957543\n",
      "Iteration 136, loss = 0.51815439\n",
      "Iteration 137, loss = 0.51673359\n",
      "Iteration 138, loss = 0.51531177\n",
      "Iteration 139, loss = 0.51389201\n",
      "Iteration 140, loss = 0.51247186\n",
      "Iteration 141, loss = 0.51105230\n",
      "Iteration 142, loss = 0.50963556\n",
      "Iteration 143, loss = 0.50821920\n",
      "Iteration 144, loss = 0.50680530\n",
      "Iteration 145, loss = 0.50539494\n",
      "Iteration 146, loss = 0.50398754\n",
      "Iteration 147, loss = 0.50258380\n",
      "Iteration 148, loss = 0.50118014\n",
      "Iteration 149, loss = 0.49977746\n",
      "Iteration 150, loss = 0.49837692\n",
      "Iteration 151, loss = 0.49697995\n",
      "Iteration 152, loss = 0.49558658\n",
      "Iteration 153, loss = 0.49419560\n",
      "Iteration 154, loss = 0.49280587\n",
      "Iteration 155, loss = 0.49141676\n",
      "Iteration 156, loss = 0.49003047\n",
      "Iteration 157, loss = 0.48864790\n",
      "Iteration 158, loss = 0.48726766\n",
      "Iteration 159, loss = 0.48588942\n",
      "Iteration 160, loss = 0.48451449\n",
      "Iteration 161, loss = 0.48314250\n",
      "Iteration 162, loss = 0.48177049\n",
      "Iteration 163, loss = 0.48040068\n",
      "Iteration 164, loss = 0.47903413\n",
      "Iteration 165, loss = 0.47766908\n",
      "Iteration 166, loss = 0.47630697\n",
      "Iteration 167, loss = 0.47494815\n",
      "Iteration 168, loss = 0.47359187\n",
      "Iteration 169, loss = 0.47223889\n",
      "Iteration 170, loss = 0.47088714\n",
      "Iteration 171, loss = 0.46953787\n",
      "Iteration 172, loss = 0.46819197\n",
      "Iteration 173, loss = 0.46684800\n",
      "Iteration 174, loss = 0.46550659\n",
      "Iteration 175, loss = 0.46416829\n",
      "Iteration 176, loss = 0.46283219\n",
      "Iteration 177, loss = 0.46149945\n",
      "Iteration 178, loss = 0.46016958\n",
      "Iteration 179, loss = 0.45884309\n",
      "Iteration 180, loss = 0.45752059\n",
      "Iteration 181, loss = 0.45620279\n",
      "Iteration 182, loss = 0.45488948\n",
      "Iteration 183, loss = 0.45357918\n",
      "Iteration 184, loss = 0.45227306\n",
      "Iteration 185, loss = 0.45097147\n",
      "Iteration 186, loss = 0.44967424\n",
      "Iteration 187, loss = 0.44838179\n",
      "Iteration 188, loss = 0.44709362\n",
      "Iteration 189, loss = 0.44580972\n",
      "Iteration 190, loss = 0.44453039\n",
      "Iteration 191, loss = 0.44325454\n",
      "Iteration 192, loss = 0.44198274\n",
      "Iteration 193, loss = 0.44071551\n",
      "Iteration 194, loss = 0.43945437\n",
      "Iteration 195, loss = 0.43819878\n",
      "Iteration 196, loss = 0.43694868\n",
      "Iteration 197, loss = 0.43570424\n",
      "Iteration 198, loss = 0.43446460\n",
      "Iteration 199, loss = 0.43323029\n",
      "Iteration 200, loss = 0.43200149\n",
      "Iteration 201, loss = 0.43077823\n",
      "Iteration 202, loss = 0.42956014\n",
      "Iteration 203, loss = 0.42834632\n",
      "Iteration 204, loss = 0.42713680\n",
      "Iteration 205, loss = 0.42593169\n",
      "Iteration 206, loss = 0.42473128\n",
      "Iteration 207, loss = 0.42353604\n",
      "Iteration 208, loss = 0.42234559\n",
      "Iteration 209, loss = 0.42116012\n",
      "Iteration 210, loss = 0.41998028\n",
      "Iteration 211, loss = 0.41880616\n",
      "Iteration 212, loss = 0.41763602\n",
      "Iteration 213, loss = 0.41647112\n",
      "Iteration 214, loss = 0.41531206\n",
      "Iteration 215, loss = 0.41415885\n",
      "Iteration 216, loss = 0.41301142\n",
      "Iteration 217, loss = 0.41186900\n",
      "Iteration 218, loss = 0.41073198\n",
      "Iteration 219, loss = 0.40960035\n",
      "Iteration 220, loss = 0.40847470\n",
      "Iteration 221, loss = 0.40735583\n",
      "Iteration 222, loss = 0.40624285\n",
      "Iteration 223, loss = 0.40513518\n",
      "Iteration 224, loss = 0.40403357\n",
      "Iteration 225, loss = 0.40293742\n",
      "Iteration 226, loss = 0.40184663\n",
      "Iteration 227, loss = 0.40076114\n",
      "Iteration 228, loss = 0.39968131\n",
      "Iteration 229, loss = 0.39860712\n",
      "Iteration 230, loss = 0.39753849\n",
      "Iteration 231, loss = 0.39647537\n",
      "Iteration 232, loss = 0.39541731\n",
      "Iteration 233, loss = 0.39436441\n",
      "Iteration 234, loss = 0.39331704\n",
      "Iteration 235, loss = 0.39227456\n",
      "Iteration 236, loss = 0.39123724\n",
      "Iteration 237, loss = 0.39020522\n",
      "Iteration 238, loss = 0.38917855\n",
      "Iteration 239, loss = 0.38815776\n",
      "Iteration 240, loss = 0.38714214\n",
      "Iteration 241, loss = 0.38613209\n",
      "Iteration 242, loss = 0.38512699\n",
      "Iteration 243, loss = 0.38412673\n",
      "Iteration 244, loss = 0.38313132\n",
      "Iteration 245, loss = 0.38214110\n",
      "Iteration 246, loss = 0.38115652\n",
      "Iteration 247, loss = 0.38017759\n",
      "Iteration 248, loss = 0.37920479\n",
      "Iteration 249, loss = 0.37823751\n",
      "Iteration 250, loss = 0.37727493\n",
      "Iteration 251, loss = 0.37631622\n",
      "Iteration 252, loss = 0.37536363\n",
      "Iteration 253, loss = 0.37441659\n",
      "Iteration 254, loss = 0.37347448\n",
      "Iteration 255, loss = 0.37253798\n",
      "Iteration 256, loss = 0.37160718\n",
      "Iteration 257, loss = 0.37068163\n",
      "Iteration 258, loss = 0.36976112\n",
      "Iteration 259, loss = 0.36884462\n",
      "Iteration 260, loss = 0.36793349\n",
      "Iteration 261, loss = 0.36702852\n",
      "Iteration 262, loss = 0.36612851\n",
      "Iteration 263, loss = 0.36523330\n",
      "Iteration 264, loss = 0.36434215\n",
      "Iteration 265, loss = 0.36345564\n",
      "Iteration 266, loss = 0.36257442\n",
      "Iteration 267, loss = 0.36169791\n",
      "Iteration 268, loss = 0.36082624\n",
      "Iteration 269, loss = 0.35995986\n",
      "Iteration 270, loss = 0.35909864\n",
      "Iteration 271, loss = 0.35824289\n",
      "Iteration 272, loss = 0.35739214\n",
      "Iteration 273, loss = 0.35654690\n",
      "Iteration 274, loss = 0.35570755\n",
      "Iteration 275, loss = 0.35487206\n",
      "Iteration 276, loss = 0.35404170\n",
      "Iteration 277, loss = 0.35321753\n",
      "Iteration 278, loss = 0.35239981\n",
      "Iteration 279, loss = 0.35158749\n",
      "Iteration 280, loss = 0.35078020\n",
      "Iteration 281, loss = 0.34997867\n",
      "Iteration 282, loss = 0.34918292\n",
      "Iteration 283, loss = 0.34839275\n",
      "Iteration 284, loss = 0.34760842\n",
      "Iteration 285, loss = 0.34682977\n",
      "Iteration 286, loss = 0.34605641\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 287, loss = 0.34528832\n",
      "Iteration 288, loss = 0.34452622\n",
      "Iteration 289, loss = 0.34376903\n",
      "Iteration 290, loss = 0.34301720\n",
      "Iteration 291, loss = 0.34227086\n",
      "Iteration 292, loss = 0.34152919\n",
      "Iteration 293, loss = 0.34079280\n",
      "Iteration 294, loss = 0.34006160\n",
      "Iteration 295, loss = 0.33933542\n",
      "Iteration 296, loss = 0.33861445\n",
      "Iteration 297, loss = 0.33789828\n",
      "Iteration 298, loss = 0.33718682\n",
      "Iteration 299, loss = 0.33648017\n",
      "Iteration 300, loss = 0.33577776\n",
      "Iteration 301, loss = 0.33508020\n",
      "Iteration 302, loss = 0.33438678\n",
      "Iteration 303, loss = 0.33369755\n",
      "Iteration 304, loss = 0.33301334\n",
      "Iteration 305, loss = 0.33233394\n",
      "Iteration 306, loss = 0.33165885\n",
      "Iteration 307, loss = 0.33098851\n",
      "Iteration 308, loss = 0.33032207\n",
      "Iteration 309, loss = 0.32966043\n",
      "Iteration 310, loss = 0.32900322\n",
      "Iteration 311, loss = 0.32835022\n",
      "Iteration 312, loss = 0.32770164\n",
      "Iteration 313, loss = 0.32705780\n",
      "Iteration 314, loss = 0.32641821\n",
      "Iteration 315, loss = 0.32578308\n",
      "Iteration 316, loss = 0.32515291\n",
      "Iteration 317, loss = 0.32452690\n",
      "Iteration 318, loss = 0.32390511\n",
      "Iteration 319, loss = 0.32328819\n",
      "Iteration 320, loss = 0.32267560\n",
      "Iteration 321, loss = 0.32206693\n",
      "Iteration 322, loss = 0.32146274\n",
      "Iteration 323, loss = 0.32086236\n",
      "Iteration 324, loss = 0.32026614\n",
      "Iteration 325, loss = 0.31967446\n",
      "Iteration 326, loss = 0.31908651\n",
      "Iteration 327, loss = 0.31850264\n",
      "Iteration 328, loss = 0.31792289\n",
      "Iteration 329, loss = 0.31734513\n",
      "Iteration 330, loss = 0.31676978\n",
      "Iteration 331, loss = 0.31619766\n",
      "Iteration 332, loss = 0.31562877\n",
      "Iteration 333, loss = 0.31506334\n",
      "Iteration 334, loss = 0.31449783\n",
      "Iteration 335, loss = 0.31393490\n",
      "Iteration 336, loss = 0.31337471\n",
      "Iteration 337, loss = 0.31281785\n",
      "Iteration 338, loss = 0.31226417\n",
      "Iteration 339, loss = 0.31171453\n",
      "Iteration 340, loss = 0.31117112\n",
      "Iteration 341, loss = 0.31063112\n",
      "Iteration 342, loss = 0.31009753\n",
      "Iteration 343, loss = 0.30956978\n",
      "Iteration 344, loss = 0.30904552\n",
      "Iteration 345, loss = 0.30852484\n",
      "Iteration 346, loss = 0.30800745\n",
      "Iteration 347, loss = 0.30749354\n",
      "Iteration 348, loss = 0.30698287\n",
      "Iteration 349, loss = 0.30647565\n",
      "Iteration 350, loss = 0.30597127\n",
      "Iteration 351, loss = 0.30546956\n",
      "Iteration 352, loss = 0.30497165\n",
      "Iteration 353, loss = 0.30447723\n",
      "Iteration 354, loss = 0.30398552\n",
      "Iteration 355, loss = 0.30349688\n",
      "Iteration 356, loss = 0.30301102\n",
      "Iteration 357, loss = 0.30252646\n",
      "Iteration 358, loss = 0.30204400\n",
      "Iteration 359, loss = 0.30156356\n",
      "Iteration 360, loss = 0.30108600\n",
      "Iteration 361, loss = 0.30061253\n",
      "Iteration 362, loss = 0.30014137\n",
      "Iteration 363, loss = 0.29967251\n",
      "Iteration 364, loss = 0.29920733\n",
      "Iteration 365, loss = 0.29874529\n",
      "Iteration 366, loss = 0.29828699\n",
      "Iteration 367, loss = 0.29783409\n",
      "Iteration 368, loss = 0.29738412\n",
      "Iteration 369, loss = 0.29693704\n",
      "Iteration 370, loss = 0.29649396\n",
      "Iteration 371, loss = 0.29605598\n",
      "Iteration 372, loss = 0.29562062\n",
      "Iteration 373, loss = 0.29518890\n",
      "Iteration 374, loss = 0.29476014\n",
      "Iteration 375, loss = 0.29433422\n",
      "Iteration 376, loss = 0.29391121\n",
      "Iteration 377, loss = 0.29349210\n",
      "Iteration 378, loss = 0.29307563\n",
      "Iteration 379, loss = 0.29266197\n",
      "Iteration 380, loss = 0.29225088\n",
      "Iteration 381, loss = 0.29184303\n",
      "Iteration 382, loss = 0.29143767\n",
      "Iteration 383, loss = 0.29103449\n",
      "Iteration 384, loss = 0.29063510\n",
      "Iteration 385, loss = 0.29023820\n",
      "Iteration 386, loss = 0.28984325\n",
      "Iteration 387, loss = 0.28945062\n",
      "Iteration 388, loss = 0.28906059\n",
      "Iteration 389, loss = 0.28867381\n",
      "Iteration 390, loss = 0.28828975\n",
      "Iteration 391, loss = 0.28790811\n",
      "Iteration 392, loss = 0.28753026\n",
      "Iteration 393, loss = 0.28715517\n",
      "Iteration 394, loss = 0.28678256\n",
      "Iteration 395, loss = 0.28641256\n",
      "Iteration 396, loss = 0.28604526\n",
      "Iteration 397, loss = 0.28568053\n",
      "Iteration 398, loss = 0.28531863\n",
      "Iteration 399, loss = 0.28495914\n",
      "Iteration 400, loss = 0.28460239\n",
      "Iteration 401, loss = 0.28424824\n",
      "Iteration 402, loss = 0.28389657\n",
      "Iteration 403, loss = 0.28354726\n",
      "Iteration 404, loss = 0.28320011\n",
      "Iteration 405, loss = 0.28285515\n",
      "Iteration 406, loss = 0.28251267\n",
      "Iteration 407, loss = 0.28217335\n",
      "Iteration 408, loss = 0.28183681\n",
      "Iteration 409, loss = 0.28150181\n",
      "Iteration 410, loss = 0.28116970\n",
      "Iteration 411, loss = 0.28083973\n",
      "Iteration 412, loss = 0.28051205\n",
      "Iteration 413, loss = 0.28018651\n",
      "Iteration 414, loss = 0.27986282\n",
      "Iteration 415, loss = 0.27954173\n",
      "Iteration 416, loss = 0.27922328\n",
      "Iteration 417, loss = 0.27890733\n",
      "Iteration 418, loss = 0.27859283\n",
      "Iteration 419, loss = 0.27828094\n",
      "Iteration 420, loss = 0.27797139\n",
      "Iteration 421, loss = 0.27766379\n",
      "Iteration 422, loss = 0.27735783\n",
      "Iteration 423, loss = 0.27705377\n",
      "Iteration 424, loss = 0.27675178\n",
      "Iteration 425, loss = 0.27645140\n",
      "Iteration 426, loss = 0.27615371\n",
      "Iteration 427, loss = 0.27585786\n",
      "Iteration 428, loss = 0.27556332\n",
      "Iteration 429, loss = 0.27527061\n",
      "Iteration 430, loss = 0.27498075\n",
      "Iteration 431, loss = 0.27469229\n",
      "Iteration 432, loss = 0.27440467\n",
      "Iteration 433, loss = 0.27411859\n",
      "Iteration 434, loss = 0.27383444\n",
      "Iteration 435, loss = 0.27355196\n",
      "Iteration 436, loss = 0.27327026\n",
      "Iteration 437, loss = 0.27298965\n",
      "Iteration 438, loss = 0.27271092\n",
      "Iteration 439, loss = 0.27243372\n",
      "Iteration 440, loss = 0.27215825\n",
      "Iteration 441, loss = 0.27188413\n",
      "Iteration 442, loss = 0.27161138\n",
      "Iteration 443, loss = 0.27134021\n",
      "Iteration 444, loss = 0.27107061\n",
      "Iteration 445, loss = 0.27080283\n",
      "Iteration 446, loss = 0.27053663\n",
      "Iteration 447, loss = 0.27027198\n",
      "Iteration 448, loss = 0.27000869\n",
      "Iteration 449, loss = 0.26974692\n",
      "Iteration 450, loss = 0.26948629\n",
      "Iteration 451, loss = 0.26922767\n",
      "Iteration 452, loss = 0.26897036\n",
      "Iteration 453, loss = 0.26871456\n",
      "Iteration 454, loss = 0.26846013\n",
      "Iteration 455, loss = 0.26820663\n",
      "Iteration 456, loss = 0.26795471\n",
      "Iteration 457, loss = 0.26770467\n",
      "Iteration 458, loss = 0.26745590\n",
      "Iteration 459, loss = 0.26720863\n",
      "Iteration 460, loss = 0.26696371\n",
      "Iteration 461, loss = 0.26672044\n",
      "Iteration 462, loss = 0.26647856\n",
      "Iteration 463, loss = 0.26623826\n",
      "Iteration 464, loss = 0.26599929\n",
      "Iteration 465, loss = 0.26576162\n",
      "Iteration 466, loss = 0.26552563\n",
      "Iteration 467, loss = 0.26529099\n",
      "Iteration 468, loss = 0.26505800\n",
      "Iteration 469, loss = 0.26482599\n",
      "Iteration 470, loss = 0.26459520\n",
      "Iteration 471, loss = 0.26436663\n",
      "Iteration 472, loss = 0.26413980\n",
      "Iteration 473, loss = 0.26391507\n",
      "Iteration 474, loss = 0.26369120\n",
      "Iteration 475, loss = 0.26346779\n",
      "Iteration 476, loss = 0.26324583\n",
      "Iteration 477, loss = 0.26302494\n",
      "Iteration 478, loss = 0.26280573\n",
      "Iteration 479, loss = 0.26258751\n",
      "Iteration 480, loss = 0.26237070\n",
      "Iteration 481, loss = 0.26215459\n",
      "Iteration 482, loss = 0.26194052\n",
      "Iteration 483, loss = 0.26172793\n",
      "Iteration 484, loss = 0.26151619\n",
      "Iteration 485, loss = 0.26130568\n",
      "Iteration 486, loss = 0.26109674\n",
      "Iteration 487, loss = 0.26088864\n",
      "Iteration 488, loss = 0.26068235\n",
      "Iteration 489, loss = 0.26047842\n",
      "Iteration 490, loss = 0.26027522\n",
      "Iteration 491, loss = 0.26007255\n",
      "Iteration 492, loss = 0.25987222\n",
      "Iteration 493, loss = 0.25967299\n",
      "Iteration 494, loss = 0.25947520\n",
      "Iteration 495, loss = 0.25927817\n",
      "Iteration 496, loss = 0.25908209\n",
      "Iteration 497, loss = 0.25888726\n",
      "Iteration 498, loss = 0.25869409\n",
      "Iteration 499, loss = 0.25850250\n",
      "Iteration 500, loss = 0.25831231\n",
      "Iteration 501, loss = 0.25812281\n",
      "Iteration 502, loss = 0.25793486\n",
      "Iteration 503, loss = 0.25774761\n",
      "Iteration 504, loss = 0.25756101\n",
      "Iteration 505, loss = 0.25737554\n",
      "Iteration 506, loss = 0.25719110\n",
      "Iteration 507, loss = 0.25700800\n",
      "Iteration 508, loss = 0.25682556\n",
      "Iteration 509, loss = 0.25664455\n",
      "Iteration 510, loss = 0.25646440\n",
      "Iteration 511, loss = 0.25628572\n",
      "Iteration 512, loss = 0.25610777\n",
      "Iteration 513, loss = 0.25593086\n",
      "Iteration 514, loss = 0.25575453\n",
      "Iteration 515, loss = 0.25557965\n",
      "Iteration 516, loss = 0.25540678\n",
      "Iteration 517, loss = 0.25523443\n",
      "Iteration 518, loss = 0.25506334\n",
      "Iteration 519, loss = 0.25489329\n",
      "Iteration 520, loss = 0.25472437\n",
      "Iteration 521, loss = 0.25455624\n",
      "Iteration 522, loss = 0.25438863\n",
      "Iteration 523, loss = 0.25422240\n",
      "Iteration 524, loss = 0.25405856\n",
      "Iteration 525, loss = 0.25389530\n",
      "Iteration 526, loss = 0.25373268\n",
      "Iteration 527, loss = 0.25357200\n",
      "Iteration 528, loss = 0.25341266\n",
      "Iteration 529, loss = 0.25325393\n",
      "Iteration 530, loss = 0.25309565\n",
      "Iteration 531, loss = 0.25293901\n",
      "Iteration 532, loss = 0.25278318\n",
      "Iteration 533, loss = 0.25262866\n",
      "Iteration 534, loss = 0.25247506\n",
      "Iteration 535, loss = 0.25232114\n",
      "Iteration 536, loss = 0.25216815\n",
      "Iteration 537, loss = 0.25201580\n",
      "Iteration 538, loss = 0.25186427\n",
      "Iteration 539, loss = 0.25171308\n",
      "Iteration 540, loss = 0.25156258\n",
      "Iteration 541, loss = 0.25141284\n",
      "Iteration 542, loss = 0.25126408\n",
      "Iteration 543, loss = 0.25111589\n",
      "Iteration 544, loss = 0.25096836\n",
      "Iteration 545, loss = 0.25082195\n",
      "Iteration 546, loss = 0.25067680\n",
      "Iteration 547, loss = 0.25053217\n",
      "Iteration 548, loss = 0.25038831\n",
      "Iteration 549, loss = 0.25024472\n",
      "Iteration 550, loss = 0.25010181\n",
      "Iteration 551, loss = 0.24995994\n",
      "Iteration 552, loss = 0.24981928\n",
      "Iteration 553, loss = 0.24967890\n",
      "Iteration 554, loss = 0.24953935\n",
      "Iteration 555, loss = 0.24940064\n",
      "Iteration 556, loss = 0.24926250\n",
      "Iteration 557, loss = 0.24912539\n",
      "Iteration 558, loss = 0.24898914\n",
      "Iteration 559, loss = 0.24885476\n",
      "Iteration 560, loss = 0.24872170\n",
      "Iteration 561, loss = 0.24858865\n",
      "Iteration 562, loss = 0.24845674\n",
      "Iteration 563, loss = 0.24832633\n",
      "Iteration 564, loss = 0.24819688\n",
      "Iteration 565, loss = 0.24806801\n",
      "Iteration 566, loss = 0.24793984\n",
      "Iteration 567, loss = 0.24781243\n",
      "Iteration 568, loss = 0.24768524\n",
      "Iteration 569, loss = 0.24755869\n",
      "Iteration 570, loss = 0.24743315\n",
      "Iteration 571, loss = 0.24730833\n",
      "Iteration 572, loss = 0.24718442\n",
      "Iteration 573, loss = 0.24706171\n",
      "Iteration 574, loss = 0.24694005\n",
      "Iteration 575, loss = 0.24681852\n",
      "Iteration 576, loss = 0.24669754\n",
      "Iteration 577, loss = 0.24657701\n",
      "Iteration 578, loss = 0.24645683\n",
      "Iteration 579, loss = 0.24633671\n",
      "Iteration 580, loss = 0.24621752\n",
      "Iteration 581, loss = 0.24609961\n",
      "Iteration 582, loss = 0.24598180\n",
      "Iteration 583, loss = 0.24586474\n",
      "Iteration 584, loss = 0.24574734\n",
      "Iteration 585, loss = 0.24563097\n",
      "Iteration 586, loss = 0.24551527\n",
      "Iteration 587, loss = 0.24540037\n",
      "Iteration 588, loss = 0.24528615\n",
      "Iteration 589, loss = 0.24517213\n",
      "Iteration 590, loss = 0.24505838\n",
      "Iteration 591, loss = 0.24494547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 592, loss = 0.24483299\n",
      "Iteration 593, loss = 0.24472011\n",
      "Iteration 594, loss = 0.24460860\n",
      "Iteration 595, loss = 0.24449704\n",
      "Iteration 596, loss = 0.24438565\n",
      "Iteration 597, loss = 0.24427473\n",
      "Iteration 598, loss = 0.24416513\n",
      "Iteration 599, loss = 0.24405835\n",
      "Iteration 600, loss = 0.24395220\n",
      "Iteration 601, loss = 0.24384682\n",
      "Iteration 602, loss = 0.24374172\n",
      "Iteration 603, loss = 0.24363698\n",
      "Iteration 604, loss = 0.24353399\n",
      "Iteration 605, loss = 0.24343137\n",
      "Iteration 606, loss = 0.24332866\n",
      "Iteration 607, loss = 0.24322668\n",
      "Iteration 608, loss = 0.24312682\n",
      "Iteration 609, loss = 0.24302690\n",
      "Iteration 610, loss = 0.24292676\n",
      "Iteration 611, loss = 0.24282794\n",
      "Iteration 612, loss = 0.24272954\n",
      "Iteration 613, loss = 0.24263151\n",
      "Iteration 614, loss = 0.24253443\n",
      "Iteration 615, loss = 0.24243693\n",
      "Iteration 616, loss = 0.24234093\n",
      "Iteration 617, loss = 0.24224550\n",
      "Iteration 618, loss = 0.24214986\n",
      "Iteration 619, loss = 0.24205481\n",
      "Iteration 620, loss = 0.24196086\n",
      "Iteration 621, loss = 0.24186641\n",
      "Iteration 622, loss = 0.24177306\n",
      "Iteration 623, loss = 0.24168035\n",
      "Iteration 624, loss = 0.24158815\n",
      "Iteration 625, loss = 0.24149668\n",
      "Iteration 626, loss = 0.24140555\n",
      "Iteration 627, loss = 0.24131450\n",
      "Iteration 628, loss = 0.24122331\n",
      "Iteration 629, loss = 0.24113281\n",
      "Iteration 630, loss = 0.24104270\n",
      "Iteration 631, loss = 0.24095320\n",
      "Iteration 632, loss = 0.24086347\n",
      "Iteration 633, loss = 0.24077400\n",
      "Iteration 634, loss = 0.24068482\n",
      "Iteration 635, loss = 0.24059707\n",
      "Iteration 636, loss = 0.24050900\n",
      "Iteration 637, loss = 0.24042021\n",
      "Iteration 638, loss = 0.24033259\n",
      "Iteration 639, loss = 0.24024530\n",
      "Iteration 640, loss = 0.24015900\n",
      "Iteration 641, loss = 0.24007278\n",
      "Iteration 642, loss = 0.23998782\n",
      "Iteration 643, loss = 0.23990325\n",
      "Iteration 644, loss = 0.23981849\n",
      "Iteration 645, loss = 0.23973477\n",
      "Iteration 646, loss = 0.23965114\n",
      "Iteration 647, loss = 0.23956747\n",
      "Iteration 648, loss = 0.23948417\n",
      "Iteration 649, loss = 0.23940102\n",
      "Iteration 650, loss = 0.23931905\n",
      "Iteration 651, loss = 0.23923660\n",
      "Iteration 652, loss = 0.23915414\n",
      "Iteration 653, loss = 0.23907292\n",
      "Iteration 654, loss = 0.23899222\n",
      "Iteration 655, loss = 0.23891149\n",
      "Iteration 656, loss = 0.23883124\n",
      "Iteration 657, loss = 0.23875168\n",
      "Iteration 658, loss = 0.23867256\n",
      "Iteration 659, loss = 0.23859414\n",
      "Iteration 660, loss = 0.23851515\n",
      "Iteration 661, loss = 0.23843643\n",
      "Iteration 662, loss = 0.23835934\n",
      "Iteration 663, loss = 0.23828263\n",
      "Iteration 664, loss = 0.23820549\n",
      "Iteration 665, loss = 0.23812862\n",
      "Iteration 666, loss = 0.23805249\n",
      "Iteration 667, loss = 0.23797681\n",
      "Iteration 668, loss = 0.23790229\n",
      "Iteration 669, loss = 0.23782734\n",
      "Iteration 670, loss = 0.23775310\n",
      "Iteration 671, loss = 0.23767968\n",
      "Iteration 672, loss = 0.23760586\n",
      "Iteration 673, loss = 0.23753218\n",
      "Iteration 674, loss = 0.23745959\n",
      "Iteration 675, loss = 0.23738676\n",
      "Iteration 676, loss = 0.23731434\n",
      "Iteration 677, loss = 0.23724302\n",
      "Iteration 678, loss = 0.23717145\n",
      "Iteration 679, loss = 0.23710006\n",
      "Iteration 680, loss = 0.23702924\n",
      "Iteration 681, loss = 0.23695864\n",
      "Iteration 682, loss = 0.23688860\n",
      "Iteration 683, loss = 0.23681838\n",
      "Iteration 684, loss = 0.23674868\n",
      "Iteration 685, loss = 0.23667974\n",
      "Iteration 686, loss = 0.23661093\n",
      "Iteration 687, loss = 0.23654306\n",
      "Iteration 688, loss = 0.23647535\n",
      "Iteration 689, loss = 0.23640883\n",
      "Iteration 690, loss = 0.23634214\n",
      "Iteration 691, loss = 0.23627479\n",
      "Iteration 692, loss = 0.23620855\n",
      "Iteration 693, loss = 0.23614235\n",
      "Iteration 694, loss = 0.23607623\n",
      "Iteration 695, loss = 0.23601071\n",
      "Iteration 696, loss = 0.23594605\n",
      "Iteration 697, loss = 0.23588067\n",
      "Iteration 698, loss = 0.23581597\n",
      "Iteration 699, loss = 0.23575190\n",
      "Iteration 700, loss = 0.23568807\n",
      "Iteration 701, loss = 0.23562438\n",
      "Iteration 702, loss = 0.23556089\n",
      "Iteration 703, loss = 0.23549730\n",
      "Iteration 704, loss = 0.23543370\n",
      "Iteration 705, loss = 0.23537189\n",
      "Iteration 706, loss = 0.23531033\n",
      "Iteration 707, loss = 0.23524789\n",
      "Iteration 708, loss = 0.23518565\n",
      "Iteration 709, loss = 0.23512448\n",
      "Iteration 710, loss = 0.23506401\n",
      "Iteration 711, loss = 0.23500346\n",
      "Iteration 712, loss = 0.23494305\n",
      "Iteration 713, loss = 0.23488233\n",
      "Iteration 714, loss = 0.23482254\n",
      "Iteration 715, loss = 0.23476344\n",
      "Iteration 716, loss = 0.23470452\n",
      "Iteration 717, loss = 0.23464514\n",
      "Iteration 718, loss = 0.23458592\n",
      "Iteration 719, loss = 0.23452798\n",
      "Iteration 720, loss = 0.23446963\n",
      "Iteration 721, loss = 0.23441075\n",
      "Iteration 722, loss = 0.23435354\n",
      "Iteration 723, loss = 0.23429512\n",
      "Iteration 724, loss = 0.23423778\n",
      "Iteration 725, loss = 0.23418090\n",
      "Iteration 726, loss = 0.23412362\n",
      "Iteration 727, loss = 0.23406756\n",
      "Iteration 728, loss = 0.23401107\n",
      "Iteration 729, loss = 0.23395497\n",
      "Iteration 730, loss = 0.23389891\n",
      "Iteration 731, loss = 0.23384307\n",
      "Iteration 732, loss = 0.23378712\n",
      "Iteration 733, loss = 0.23373157\n",
      "Iteration 734, loss = 0.23367802\n",
      "Iteration 735, loss = 0.23362381\n",
      "Iteration 736, loss = 0.23356868\n",
      "Iteration 737, loss = 0.23351471\n",
      "Iteration 738, loss = 0.23346091\n",
      "Iteration 739, loss = 0.23340693\n",
      "Iteration 740, loss = 0.23335381\n",
      "Iteration 741, loss = 0.23330052\n",
      "Iteration 742, loss = 0.23324730\n",
      "Iteration 743, loss = 0.23319442\n",
      "Iteration 744, loss = 0.23314209\n",
      "Iteration 745, loss = 0.23308799\n",
      "Iteration 746, loss = 0.23303381\n",
      "Iteration 747, loss = 0.23298118\n",
      "Iteration 748, loss = 0.23292828\n",
      "Iteration 749, loss = 0.23287440\n",
      "Iteration 750, loss = 0.23282122\n",
      "Iteration 751, loss = 0.23276868\n",
      "Iteration 752, loss = 0.23271651\n",
      "Iteration 753, loss = 0.23266317\n",
      "Iteration 754, loss = 0.23260950\n",
      "Iteration 755, loss = 0.23255807\n",
      "Iteration 756, loss = 0.23250808\n",
      "Iteration 757, loss = 0.23245810\n",
      "Iteration 758, loss = 0.23240888\n",
      "Iteration 759, loss = 0.23235837\n",
      "Iteration 760, loss = 0.23230804\n",
      "Iteration 761, loss = 0.23225769\n",
      "Iteration 762, loss = 0.23220818\n",
      "Iteration 763, loss = 0.23215859\n",
      "Iteration 764, loss = 0.23210847\n",
      "Iteration 765, loss = 0.23205796\n",
      "Iteration 766, loss = 0.23200893\n",
      "Iteration 767, loss = 0.23196033\n",
      "Iteration 768, loss = 0.23191062\n",
      "Iteration 769, loss = 0.23186191\n",
      "Iteration 770, loss = 0.23181415\n",
      "Iteration 771, loss = 0.23176628\n",
      "Iteration 772, loss = 0.23171844\n",
      "Iteration 773, loss = 0.23167088\n",
      "Iteration 774, loss = 0.23162327\n",
      "Iteration 775, loss = 0.23157725\n",
      "Iteration 776, loss = 0.23153039\n",
      "Iteration 777, loss = 0.23148337\n",
      "Iteration 778, loss = 0.23143727\n",
      "Iteration 779, loss = 0.23139107\n",
      "Iteration 780, loss = 0.23134580\n",
      "Iteration 781, loss = 0.23130038\n",
      "Iteration 782, loss = 0.23125497\n",
      "Iteration 783, loss = 0.23120912\n",
      "Iteration 784, loss = 0.23116523\n",
      "Iteration 785, loss = 0.23112090\n",
      "Iteration 786, loss = 0.23107662\n",
      "Iteration 787, loss = 0.23103212\n",
      "Iteration 788, loss = 0.23098699\n",
      "Iteration 789, loss = 0.23094317\n",
      "Iteration 790, loss = 0.23089968\n",
      "Iteration 791, loss = 0.23085626\n",
      "Iteration 792, loss = 0.23081276\n",
      "Iteration 793, loss = 0.23076992\n",
      "Iteration 794, loss = 0.23072695\n",
      "Iteration 795, loss = 0.23068390\n",
      "Iteration 796, loss = 0.23064105\n",
      "Iteration 797, loss = 0.23059792\n",
      "Iteration 798, loss = 0.23055627\n",
      "Iteration 799, loss = 0.23051372\n",
      "Iteration 800, loss = 0.23047220\n",
      "Iteration 801, loss = 0.23043136\n",
      "Iteration 802, loss = 0.23038964\n",
      "Iteration 803, loss = 0.23034819\n",
      "Iteration 804, loss = 0.23030743\n",
      "Iteration 805, loss = 0.23026694\n",
      "Iteration 806, loss = 0.23022588\n",
      "Iteration 807, loss = 0.23018488\n",
      "Iteration 808, loss = 0.23014426\n",
      "Iteration 809, loss = 0.23010375\n",
      "Iteration 810, loss = 0.23006329\n",
      "Iteration 811, loss = 0.23002170\n",
      "Iteration 812, loss = 0.22998186\n",
      "Iteration 813, loss = 0.22994265\n",
      "Iteration 814, loss = 0.22990332\n",
      "Iteration 815, loss = 0.22986437\n",
      "Iteration 816, loss = 0.22982534\n",
      "Iteration 817, loss = 0.22978530\n",
      "Iteration 818, loss = 0.22974524\n",
      "Iteration 819, loss = 0.22970675\n",
      "Iteration 820, loss = 0.22966733\n",
      "Iteration 821, loss = 0.22962870\n",
      "Iteration 822, loss = 0.22958857\n",
      "Iteration 823, loss = 0.22954967\n",
      "Iteration 824, loss = 0.22951056\n",
      "Iteration 825, loss = 0.22947119\n",
      "Iteration 826, loss = 0.22943273\n",
      "Iteration 827, loss = 0.22939324\n",
      "Iteration 828, loss = 0.22935497\n",
      "Iteration 829, loss = 0.22931672\n",
      "Iteration 830, loss = 0.22927693\n",
      "Iteration 831, loss = 0.22923920\n",
      "Iteration 832, loss = 0.22920145\n",
      "Iteration 833, loss = 0.22916296\n",
      "Iteration 834, loss = 0.22912462\n",
      "Iteration 835, loss = 0.22908767\n",
      "Iteration 836, loss = 0.22905259\n",
      "Iteration 837, loss = 0.22901940\n",
      "Iteration 838, loss = 0.22898676\n",
      "Iteration 839, loss = 0.22895413\n",
      "Iteration 840, loss = 0.22892112\n",
      "Iteration 841, loss = 0.22888742\n",
      "Iteration 842, loss = 0.22885358\n",
      "Iteration 843, loss = 0.22881879\n",
      "Iteration 844, loss = 0.22878610\n",
      "Iteration 845, loss = 0.22875360\n",
      "Iteration 846, loss = 0.22872008\n",
      "Iteration 847, loss = 0.22868596\n",
      "Iteration 848, loss = 0.22865375\n",
      "Iteration 849, loss = 0.22862159\n",
      "Iteration 850, loss = 0.22858819\n",
      "Iteration 851, loss = 0.22855541\n",
      "Iteration 852, loss = 0.22852358\n",
      "Iteration 853, loss = 0.22849027\n",
      "Iteration 854, loss = 0.22845846\n",
      "Iteration 855, loss = 0.22842644\n",
      "Iteration 856, loss = 0.22839422\n",
      "Iteration 857, loss = 0.22836315\n",
      "Iteration 858, loss = 0.22833253\n",
      "Iteration 859, loss = 0.22830331\n",
      "Iteration 860, loss = 0.22827283\n",
      "Iteration 861, loss = 0.22824066\n",
      "Iteration 862, loss = 0.22821028\n",
      "Iteration 863, loss = 0.22817957\n",
      "Iteration 864, loss = 0.22814912\n",
      "Iteration 865, loss = 0.22811868\n",
      "Iteration 866, loss = 0.22808843\n",
      "Iteration 867, loss = 0.22805831\n",
      "Iteration 868, loss = 0.22802729\n",
      "Iteration 869, loss = 0.22799597\n",
      "Iteration 870, loss = 0.22796471\n",
      "Iteration 871, loss = 0.22793406\n",
      "Iteration 872, loss = 0.22790303\n",
      "Iteration 873, loss = 0.22787333\n",
      "Iteration 874, loss = 0.22784231\n",
      "Iteration 875, loss = 0.22781325\n",
      "Iteration 876, loss = 0.22778526\n",
      "Iteration 877, loss = 0.22775595\n",
      "Iteration 878, loss = 0.22772557\n",
      "Iteration 879, loss = 0.22769547\n",
      "Iteration 880, loss = 0.22766755\n",
      "Iteration 881, loss = 0.22763839\n",
      "Iteration 882, loss = 0.22760912\n",
      "Iteration 883, loss = 0.22758021\n",
      "Iteration 884, loss = 0.22755094\n",
      "Iteration 885, loss = 0.22752113\n",
      "Iteration 886, loss = 0.22749157\n",
      "Iteration 887, loss = 0.22746340\n",
      "Iteration 888, loss = 0.22743570\n",
      "Iteration 889, loss = 0.22740779\n",
      "Iteration 890, loss = 0.22737902\n",
      "Iteration 891, loss = 0.22734964\n",
      "Iteration 892, loss = 0.22732192\n",
      "Iteration 893, loss = 0.22729550\n",
      "Iteration 894, loss = 0.22726762\n",
      "Iteration 895, loss = 0.22723891\n",
      "Iteration 896, loss = 0.22721258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 897, loss = 0.22718598\n",
      "Iteration 898, loss = 0.22715854\n",
      "Iteration 899, loss = 0.22713073\n",
      "Iteration 900, loss = 0.22710498\n",
      "Iteration 901, loss = 0.22707928\n",
      "Iteration 902, loss = 0.22705284\n",
      "Iteration 903, loss = 0.22702708\n",
      "Iteration 904, loss = 0.22700141\n",
      "Iteration 905, loss = 0.22697568\n",
      "Iteration 906, loss = 0.22695057\n",
      "Iteration 907, loss = 0.22692559\n",
      "Iteration 908, loss = 0.22690017\n",
      "Iteration 909, loss = 0.22687393\n",
      "Iteration 910, loss = 0.22684895\n",
      "Iteration 911, loss = 0.22682347\n",
      "Iteration 912, loss = 0.22679861\n",
      "Iteration 913, loss = 0.22677354\n",
      "Iteration 914, loss = 0.22674834\n",
      "Iteration 915, loss = 0.22672329\n",
      "Iteration 916, loss = 0.22669920\n",
      "Iteration 917, loss = 0.22667417\n",
      "Iteration 918, loss = 0.22664877\n",
      "Iteration 919, loss = 0.22662385\n",
      "Iteration 920, loss = 0.22659927\n",
      "Iteration 921, loss = 0.22657489\n",
      "Iteration 922, loss = 0.22655065\n",
      "Iteration 923, loss = 0.22652658\n",
      "Iteration 924, loss = 0.22650192\n",
      "Iteration 925, loss = 0.22647715\n",
      "Iteration 926, loss = 0.22645261\n",
      "Iteration 927, loss = 0.22642884\n",
      "Iteration 928, loss = 0.22640448\n",
      "Iteration 929, loss = 0.22638066\n",
      "Iteration 930, loss = 0.22635661\n",
      "Iteration 931, loss = 0.22633261\n",
      "Iteration 932, loss = 0.22630892\n",
      "Iteration 933, loss = 0.22628444\n",
      "Iteration 934, loss = 0.22626116\n",
      "Iteration 935, loss = 0.22623718\n",
      "Iteration 936, loss = 0.22621275\n",
      "Iteration 937, loss = 0.22618903\n",
      "Iteration 938, loss = 0.22616472\n",
      "Iteration 939, loss = 0.22614018\n",
      "Iteration 940, loss = 0.22611550\n",
      "Iteration 941, loss = 0.22609068\n",
      "Iteration 942, loss = 0.22606580\n",
      "Iteration 943, loss = 0.22604151\n",
      "Iteration 944, loss = 0.22601884\n",
      "Iteration 945, loss = 0.22599502\n",
      "Iteration 946, loss = 0.22597337\n",
      "Iteration 947, loss = 0.22594961\n",
      "Iteration 948, loss = 0.22592618\n",
      "Iteration 949, loss = 0.22590391\n",
      "Iteration 950, loss = 0.22588205\n",
      "Iteration 951, loss = 0.22585984\n",
      "Iteration 952, loss = 0.22583698\n",
      "Iteration 953, loss = 0.22581421\n",
      "Iteration 954, loss = 0.22579226\n",
      "Iteration 955, loss = 0.22577073\n",
      "Iteration 956, loss = 0.22574884\n",
      "Iteration 957, loss = 0.22572655\n",
      "Iteration 958, loss = 0.22570458\n",
      "Iteration 959, loss = 0.22568256\n",
      "Iteration 960, loss = 0.22566059\n",
      "Iteration 961, loss = 0.22564117\n",
      "Iteration 962, loss = 0.22561900\n",
      "Iteration 963, loss = 0.22559678\n",
      "Iteration 964, loss = 0.22557527\n",
      "Iteration 965, loss = 0.22555473\n",
      "Iteration 966, loss = 0.22553331\n",
      "Iteration 967, loss = 0.22551213\n",
      "Iteration 968, loss = 0.22549032\n",
      "Iteration 969, loss = 0.22546894\n",
      "Iteration 970, loss = 0.22544747\n",
      "Iteration 971, loss = 0.22542719\n",
      "Iteration 972, loss = 0.22540525\n",
      "Iteration 973, loss = 0.22538369\n",
      "Iteration 974, loss = 0.22536393\n",
      "Iteration 975, loss = 0.22534348\n",
      "Iteration 976, loss = 0.22532223\n",
      "Iteration 977, loss = 0.22530154\n",
      "Iteration 978, loss = 0.22528106\n",
      "Iteration 979, loss = 0.22526134\n",
      "Iteration 980, loss = 0.22524077\n",
      "Iteration 981, loss = 0.22521999\n",
      "Iteration 982, loss = 0.22519923\n",
      "Iteration 983, loss = 0.22517970\n",
      "Iteration 984, loss = 0.22516074\n",
      "Iteration 985, loss = 0.22514117\n",
      "Iteration 986, loss = 0.22512076\n",
      "Iteration 987, loss = 0.22510067\n",
      "Iteration 988, loss = 0.22508160\n",
      "Iteration 989, loss = 0.22506147\n",
      "Iteration 990, loss = 0.22504166\n",
      "Iteration 991, loss = 0.22502246\n",
      "Iteration 992, loss = 0.22500369\n",
      "Iteration 993, loss = 0.22498439\n",
      "Iteration 994, loss = 0.22496426\n",
      "Iteration 995, loss = 0.22494590\n",
      "Iteration 996, loss = 0.22492699\n",
      "Iteration 997, loss = 0.22490829\n",
      "Iteration 998, loss = 0.22488925\n",
      "Iteration 999, loss = 0.22486967\n",
      "Iteration 1000, loss = 0.22485240\n",
      "Iteration 1001, loss = 0.22483376\n",
      "Iteration 1002, loss = 0.22481551\n",
      "Iteration 1003, loss = 0.22479572\n",
      "Iteration 1004, loss = 0.22477600\n",
      "Iteration 1005, loss = 0.22475731\n",
      "Iteration 1006, loss = 0.22473776\n",
      "Iteration 1007, loss = 0.22471788\n",
      "Iteration 1008, loss = 0.22469790\n",
      "Iteration 1009, loss = 0.22467884\n",
      "Iteration 1010, loss = 0.22466000\n",
      "Iteration 1011, loss = 0.22464220\n",
      "Iteration 1012, loss = 0.22462378\n",
      "Iteration 1013, loss = 0.22460549\n",
      "Iteration 1014, loss = 0.22458767\n",
      "Iteration 1015, loss = 0.22456979\n",
      "Iteration 1016, loss = 0.22455120\n",
      "Iteration 1017, loss = 0.22453343\n",
      "Iteration 1018, loss = 0.22451665\n",
      "Iteration 1019, loss = 0.22449921\n",
      "Iteration 1020, loss = 0.22448123\n",
      "Iteration 1021, loss = 0.22446254\n",
      "Iteration 1022, loss = 0.22444606\n",
      "Iteration 1023, loss = 0.22442855\n",
      "Iteration 1024, loss = 0.22441168\n",
      "Iteration 1025, loss = 0.22439389\n",
      "Iteration 1026, loss = 0.22437671\n",
      "Iteration 1027, loss = 0.22435938\n",
      "Iteration 1028, loss = 0.22434248\n",
      "Iteration 1029, loss = 0.22432655\n",
      "Iteration 1030, loss = 0.22430990\n",
      "Iteration 1031, loss = 0.22429245\n",
      "Iteration 1032, loss = 0.22427750\n",
      "Iteration 1033, loss = 0.22426168\n",
      "Iteration 1034, loss = 0.22424569\n",
      "Iteration 1035, loss = 0.22422936\n",
      "Iteration 1036, loss = 0.22421268\n",
      "Iteration 1037, loss = 0.22419608\n",
      "Iteration 1038, loss = 0.22418140\n",
      "Iteration 1039, loss = 0.22416647\n",
      "Iteration 1040, loss = 0.22415027\n",
      "Iteration 1041, loss = 0.22413396\n",
      "Iteration 1042, loss = 0.22411894\n",
      "Iteration 1043, loss = 0.22410309\n",
      "Iteration 1044, loss = 0.22408800\n",
      "Iteration 1045, loss = 0.22407256\n",
      "Iteration 1046, loss = 0.22405789\n",
      "Iteration 1047, loss = 0.22404142\n",
      "Iteration 1048, loss = 0.22402627\n",
      "Iteration 1049, loss = 0.22401271\n",
      "Iteration 1050, loss = 0.22399843\n",
      "Iteration 1051, loss = 0.22398431\n",
      "Iteration 1052, loss = 0.22396903\n",
      "Iteration 1053, loss = 0.22395464\n",
      "Iteration 1054, loss = 0.22393966\n",
      "Iteration 1055, loss = 0.22392425\n",
      "Iteration 1056, loss = 0.22391043\n",
      "Iteration 1057, loss = 0.22389612\n",
      "Iteration 1058, loss = 0.22388255\n",
      "Iteration 1059, loss = 0.22386894\n",
      "Iteration 1060, loss = 0.22385471\n",
      "Iteration 1061, loss = 0.22383942\n",
      "Iteration 1062, loss = 0.22382509\n",
      "Iteration 1063, loss = 0.22381198\n",
      "Iteration 1064, loss = 0.22379803\n",
      "Iteration 1065, loss = 0.22378337\n",
      "Iteration 1066, loss = 0.22376929\n",
      "Iteration 1067, loss = 0.22375532\n",
      "Iteration 1068, loss = 0.22374153\n",
      "Iteration 1069, loss = 0.22372833\n",
      "Iteration 1070, loss = 0.22371449\n",
      "Iteration 1071, loss = 0.22370014\n",
      "Iteration 1072, loss = 0.22368600\n",
      "Iteration 1073, loss = 0.22367238\n",
      "Iteration 1074, loss = 0.22365836\n",
      "Iteration 1075, loss = 0.22364490\n",
      "Iteration 1076, loss = 0.22363129\n",
      "Iteration 1077, loss = 0.22361805\n",
      "Iteration 1078, loss = 0.22360418\n",
      "Iteration 1079, loss = 0.22359134\n",
      "Iteration 1080, loss = 0.22357714\n",
      "Iteration 1081, loss = 0.22356328\n",
      "Iteration 1082, loss = 0.22354966\n",
      "Iteration 1083, loss = 0.22353544\n",
      "Iteration 1084, loss = 0.22352124\n",
      "Iteration 1085, loss = 0.22350651\n",
      "Iteration 1086, loss = 0.22349198\n",
      "Iteration 1087, loss = 0.22347859\n",
      "Iteration 1088, loss = 0.22346441\n",
      "Iteration 1089, loss = 0.22345089\n",
      "Iteration 1090, loss = 0.22343769\n",
      "Iteration 1091, loss = 0.22342494\n",
      "Iteration 1092, loss = 0.22341094\n",
      "Iteration 1093, loss = 0.22339700\n",
      "Iteration 1094, loss = 0.22338360\n",
      "Iteration 1095, loss = 0.22337099\n",
      "Iteration 1096, loss = 0.22335704\n",
      "Iteration 1097, loss = 0.22334481\n",
      "Iteration 1098, loss = 0.22333161\n",
      "Iteration 1099, loss = 0.22331745\n",
      "Iteration 1100, loss = 0.22330272\n",
      "Iteration 1101, loss = 0.22329050\n",
      "Iteration 1102, loss = 0.22327861\n",
      "Iteration 1103, loss = 0.22326547\n",
      "Iteration 1104, loss = 0.22325169\n",
      "Iteration 1105, loss = 0.22323875\n",
      "Iteration 1106, loss = 0.22322543\n",
      "Iteration 1107, loss = 0.22321108\n",
      "Iteration 1108, loss = 0.22319638\n",
      "Iteration 1109, loss = 0.22318300\n",
      "Iteration 1110, loss = 0.22317026\n",
      "Iteration 1111, loss = 0.22315531\n",
      "Iteration 1112, loss = 0.22314074\n",
      "Iteration 1113, loss = 0.22312545\n",
      "Iteration 1114, loss = 0.22311137\n",
      "Iteration 1115, loss = 0.22309624\n",
      "Iteration 1116, loss = 0.22308076\n",
      "Iteration 1117, loss = 0.22306632\n",
      "Iteration 1118, loss = 0.22305297\n",
      "Iteration 1119, loss = 0.22303856\n",
      "Iteration 1120, loss = 0.22302575\n",
      "Iteration 1121, loss = 0.22301258\n",
      "Iteration 1122, loss = 0.22300004\n",
      "Iteration 1123, loss = 0.22298647\n",
      "Iteration 1124, loss = 0.22297353\n",
      "Iteration 1125, loss = 0.22296051\n",
      "Iteration 1126, loss = 0.22294755\n",
      "Iteration 1127, loss = 0.22293413\n",
      "Iteration 1128, loss = 0.22292047\n",
      "Iteration 1129, loss = 0.22290727\n",
      "Iteration 1130, loss = 0.22289402\n",
      "Iteration 1131, loss = 0.22288068\n",
      "Iteration 1132, loss = 0.22286537\n",
      "Iteration 1133, loss = 0.22285145\n",
      "Iteration 1134, loss = 0.22283790\n",
      "Iteration 1135, loss = 0.22282430\n",
      "Iteration 1136, loss = 0.22280953\n",
      "Iteration 1137, loss = 0.22279484\n",
      "Iteration 1138, loss = 0.22278077\n",
      "Iteration 1139, loss = 0.22276623\n",
      "Iteration 1140, loss = 0.22275091\n",
      "Iteration 1141, loss = 0.22273674\n",
      "Iteration 1142, loss = 0.22272179\n",
      "Iteration 1143, loss = 0.22270640\n",
      "Iteration 1144, loss = 0.22269111\n",
      "Iteration 1145, loss = 0.22267650\n",
      "Iteration 1146, loss = 0.22265956\n",
      "Iteration 1147, loss = 0.22264513\n",
      "Iteration 1148, loss = 0.22263067\n",
      "Iteration 1149, loss = 0.22261472\n",
      "Iteration 1150, loss = 0.22259924\n",
      "Iteration 1151, loss = 0.22258550\n",
      "Iteration 1152, loss = 0.22257117\n",
      "Iteration 1153, loss = 0.22255552\n",
      "Iteration 1154, loss = 0.22254123\n",
      "Iteration 1155, loss = 0.22252587\n",
      "Iteration 1156, loss = 0.22251097\n",
      "Iteration 1157, loss = 0.22249596\n",
      "Iteration 1158, loss = 0.22248116\n",
      "Iteration 1159, loss = 0.22246556\n",
      "Iteration 1160, loss = 0.22244894\n",
      "Iteration 1161, loss = 0.22243422\n",
      "Iteration 1162, loss = 0.22242026\n",
      "Iteration 1163, loss = 0.22240485\n",
      "Iteration 1164, loss = 0.22238830\n",
      "Iteration 1165, loss = 0.22237376\n",
      "Iteration 1166, loss = 0.22235895\n",
      "Iteration 1167, loss = 0.22234242\n",
      "Iteration 1168, loss = 0.22232641\n",
      "Iteration 1169, loss = 0.22231206\n",
      "Iteration 1170, loss = 0.22229744\n",
      "Iteration 1171, loss = 0.22228138\n",
      "Iteration 1172, loss = 0.22226573\n",
      "Iteration 1173, loss = 0.22224926\n",
      "Iteration 1174, loss = 0.22223426\n",
      "Iteration 1175, loss = 0.22221921\n",
      "Iteration 1176, loss = 0.22220362\n",
      "Iteration 1177, loss = 0.22218782\n",
      "Iteration 1178, loss = 0.22217299\n",
      "Iteration 1179, loss = 0.22215896\n",
      "Iteration 1180, loss = 0.22214298\n",
      "Iteration 1181, loss = 0.22212796\n",
      "Iteration 1182, loss = 0.22211501\n",
      "Iteration 1183, loss = 0.22210253\n",
      "Iteration 1184, loss = 0.22208820\n",
      "Iteration 1185, loss = 0.22207329\n",
      "Iteration 1186, loss = 0.22205906\n",
      "Iteration 1187, loss = 0.22204443\n",
      "Iteration 1188, loss = 0.22203026\n",
      "Iteration 1189, loss = 0.22201595\n",
      "Iteration 1190, loss = 0.22200240\n",
      "Iteration 1191, loss = 0.22198917\n",
      "Iteration 1192, loss = 0.22197476\n",
      "Iteration 1193, loss = 0.22196079\n",
      "Iteration 1194, loss = 0.22194688\n",
      "Iteration 1195, loss = 0.22193255\n",
      "Iteration 1196, loss = 0.22191997\n",
      "Iteration 1197, loss = 0.22190654\n",
      "Iteration 1198, loss = 0.22189246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1199, loss = 0.22187914\n",
      "Iteration 1200, loss = 0.22186560\n",
      "Iteration 1201, loss = 0.22185109\n",
      "Iteration 1202, loss = 0.22183865\n",
      "Iteration 1203, loss = 0.22182562\n",
      "Iteration 1204, loss = 0.22181251\n",
      "Iteration 1205, loss = 0.22179941\n",
      "Iteration 1206, loss = 0.22178664\n",
      "Iteration 1207, loss = 0.22177437\n",
      "Iteration 1208, loss = 0.22176170\n",
      "Iteration 1209, loss = 0.22174963\n",
      "Iteration 1210, loss = 0.22173659\n",
      "Iteration 1211, loss = 0.22172412\n",
      "Iteration 1212, loss = 0.22171250\n",
      "Iteration 1213, loss = 0.22170013\n",
      "Iteration 1214, loss = 0.22168676\n",
      "Iteration 1215, loss = 0.22167527\n",
      "Iteration 1216, loss = 0.22166336\n",
      "Iteration 1217, loss = 0.22165065\n",
      "Iteration 1218, loss = 0.22163847\n",
      "Iteration 1219, loss = 0.22162596\n",
      "Iteration 1220, loss = 0.22161387\n",
      "Iteration 1221, loss = 0.22160192\n",
      "Iteration 1222, loss = 0.22158942\n",
      "Iteration 1223, loss = 0.22157831\n",
      "Iteration 1224, loss = 0.22156608\n",
      "Iteration 1225, loss = 0.22155337\n",
      "Iteration 1226, loss = 0.22154104\n",
      "Iteration 1227, loss = 0.22152868\n",
      "Iteration 1228, loss = 0.22151707\n",
      "Iteration 1229, loss = 0.22150415\n",
      "Iteration 1230, loss = 0.22149171\n",
      "Iteration 1231, loss = 0.22147925\n",
      "Iteration 1232, loss = 0.22146733\n",
      "Iteration 1233, loss = 0.22145376\n",
      "Iteration 1234, loss = 0.22144098\n",
      "Iteration 1235, loss = 0.22143059\n",
      "Iteration 1236, loss = 0.22141748\n",
      "Iteration 1237, loss = 0.22140438\n",
      "Iteration 1238, loss = 0.22139139\n",
      "Iteration 1239, loss = 0.22137890\n",
      "Iteration 1240, loss = 0.22136648\n",
      "Iteration 1241, loss = 0.22135392\n",
      "Iteration 1242, loss = 0.22134059\n",
      "Iteration 1243, loss = 0.22132907\n",
      "Iteration 1244, loss = 0.22131697\n",
      "Iteration 1245, loss = 0.22130519\n",
      "Iteration 1246, loss = 0.22129269\n",
      "Iteration 1247, loss = 0.22127961\n",
      "Iteration 1248, loss = 0.22126566\n",
      "Iteration 1249, loss = 0.22125219\n",
      "Iteration 1250, loss = 0.22123904\n",
      "Iteration 1251, loss = 0.22122631\n",
      "Iteration 1252, loss = 0.22121471\n",
      "Iteration 1253, loss = 0.22120198\n",
      "Iteration 1254, loss = 0.22119036\n",
      "Iteration 1255, loss = 0.22117807\n",
      "Iteration 1256, loss = 0.22116618\n",
      "Iteration 1257, loss = 0.22115442\n",
      "Iteration 1258, loss = 0.22114243\n",
      "Iteration 1259, loss = 0.22113059\n",
      "Iteration 1260, loss = 0.22111852\n",
      "Iteration 1261, loss = 0.22110589\n",
      "Iteration 1262, loss = 0.22109429\n",
      "Iteration 1263, loss = 0.22108247\n",
      "Iteration 1264, loss = 0.22106952\n",
      "Iteration 1265, loss = 0.22105721\n",
      "Iteration 1266, loss = 0.22104499\n",
      "Iteration 1267, loss = 0.22103200\n",
      "Iteration 1268, loss = 0.22101849\n",
      "Iteration 1269, loss = 0.22100587\n",
      "Iteration 1270, loss = 0.22099357\n",
      "Iteration 1271, loss = 0.22098139\n",
      "Iteration 1272, loss = 0.22096845\n",
      "Iteration 1273, loss = 0.22095582\n",
      "Iteration 1274, loss = 0.22094371\n",
      "Iteration 1275, loss = 0.22093261\n",
      "Iteration 1276, loss = 0.22091996\n",
      "Iteration 1277, loss = 0.22090533\n",
      "Iteration 1278, loss = 0.22089340\n",
      "Iteration 1279, loss = 0.22088148\n",
      "Iteration 1280, loss = 0.22086922\n",
      "Iteration 1281, loss = 0.22085713\n",
      "Iteration 1282, loss = 0.22084516\n",
      "Iteration 1283, loss = 0.22083331\n",
      "Iteration 1284, loss = 0.22082052\n",
      "Iteration 1285, loss = 0.22080715\n",
      "Iteration 1286, loss = 0.22079455\n",
      "Iteration 1287, loss = 0.22078226\n",
      "Iteration 1288, loss = 0.22077045\n",
      "Iteration 1289, loss = 0.22075883\n",
      "Iteration 1290, loss = 0.22074719\n",
      "Iteration 1291, loss = 0.22073415\n",
      "Iteration 1292, loss = 0.22072155\n",
      "Iteration 1293, loss = 0.22070963\n",
      "Iteration 1294, loss = 0.22069727\n",
      "Iteration 1295, loss = 0.22068525\n",
      "Iteration 1296, loss = 0.22067222\n",
      "Iteration 1297, loss = 0.22066098\n",
      "Iteration 1298, loss = 0.22064933\n",
      "Iteration 1299, loss = 0.22063695\n",
      "Iteration 1300, loss = 0.22062473\n",
      "Iteration 1301, loss = 0.22061266\n",
      "Iteration 1302, loss = 0.22060278\n",
      "Iteration 1303, loss = 0.22059240\n",
      "Iteration 1304, loss = 0.22058122\n",
      "Iteration 1305, loss = 0.22056990\n",
      "Iteration 1306, loss = 0.22055823\n",
      "Iteration 1307, loss = 0.22054867\n",
      "Iteration 1308, loss = 0.22053770\n",
      "Iteration 1309, loss = 0.22052668\n",
      "Iteration 1310, loss = 0.22051531\n",
      "Iteration 1311, loss = 0.22050390\n",
      "Iteration 1312, loss = 0.22049397\n",
      "Iteration 1313, loss = 0.22048471\n",
      "Iteration 1314, loss = 0.22047500\n",
      "Iteration 1315, loss = 0.22046492\n",
      "Iteration 1316, loss = 0.22045462\n",
      "Iteration 1317, loss = 0.22044350\n",
      "Iteration 1318, loss = 0.22043231\n",
      "Iteration 1319, loss = 0.22042249\n",
      "Iteration 1320, loss = 0.22041328\n",
      "Iteration 1321, loss = 0.22040290\n",
      "Iteration 1322, loss = 0.22039216\n",
      "Iteration 1323, loss = 0.22037906\n",
      "Iteration 1324, loss = 0.22036536\n",
      "Iteration 1325, loss = 0.22035241\n",
      "Iteration 1326, loss = 0.22033880\n",
      "Iteration 1327, loss = 0.22032502\n",
      "Iteration 1328, loss = 0.22031137\n",
      "Iteration 1329, loss = 0.22029557\n",
      "Iteration 1330, loss = 0.22027901\n",
      "Iteration 1331, loss = 0.22026266\n",
      "Iteration 1332, loss = 0.22024762\n",
      "Iteration 1333, loss = 0.22023245\n",
      "Iteration 1334, loss = 0.22021627\n",
      "Iteration 1335, loss = 0.22019890\n",
      "Iteration 1336, loss = 0.22018149\n",
      "Iteration 1337, loss = 0.22016610\n",
      "Iteration 1338, loss = 0.22014954\n",
      "Iteration 1339, loss = 0.22013197\n",
      "Iteration 1340, loss = 0.22011396\n",
      "Iteration 1341, loss = 0.22009649\n",
      "Iteration 1342, loss = 0.22008023\n",
      "Iteration 1343, loss = 0.22006326\n",
      "Iteration 1344, loss = 0.22004764\n",
      "Iteration 1345, loss = 0.22003057\n",
      "Iteration 1346, loss = 0.22001498\n",
      "Iteration 1347, loss = 0.21999862\n",
      "Iteration 1348, loss = 0.21998287\n",
      "Iteration 1349, loss = 0.21996625\n",
      "Iteration 1350, loss = 0.21994765\n",
      "Iteration 1351, loss = 0.21992543\n",
      "Iteration 1352, loss = 0.21990246\n",
      "Iteration 1353, loss = 0.21987924\n",
      "Iteration 1354, loss = 0.21985312\n",
      "Iteration 1355, loss = 0.21982775\n",
      "Iteration 1356, loss = 0.21980120\n",
      "Iteration 1357, loss = 0.21977608\n",
      "Iteration 1358, loss = 0.21975103\n",
      "Iteration 1359, loss = 0.21972323\n",
      "Iteration 1360, loss = 0.21969518\n",
      "Iteration 1361, loss = 0.21966782\n",
      "Iteration 1362, loss = 0.21964169\n",
      "Iteration 1363, loss = 0.21961538\n",
      "Iteration 1364, loss = 0.21958828\n",
      "Iteration 1365, loss = 0.21956099\n",
      "Iteration 1366, loss = 0.21953370\n",
      "Iteration 1367, loss = 0.21950602\n",
      "Iteration 1368, loss = 0.21948048\n",
      "Iteration 1369, loss = 0.21945799\n",
      "Iteration 1370, loss = 0.21943496\n",
      "Iteration 1371, loss = 0.21941431\n",
      "Iteration 1372, loss = 0.21939426\n",
      "Iteration 1373, loss = 0.21937450\n",
      "Iteration 1374, loss = 0.21935523\n",
      "Iteration 1375, loss = 0.21933607\n",
      "Iteration 1376, loss = 0.21931787\n",
      "Iteration 1377, loss = 0.21929899\n",
      "Iteration 1378, loss = 0.21927977\n",
      "Iteration 1379, loss = 0.21926122\n",
      "Iteration 1380, loss = 0.21924425\n",
      "Iteration 1381, loss = 0.21923273\n",
      "Iteration 1382, loss = 0.21922120\n",
      "Iteration 1383, loss = 0.21920979\n",
      "Iteration 1384, loss = 0.21919617\n",
      "Iteration 1385, loss = 0.21918255\n",
      "Iteration 1386, loss = 0.21917050\n",
      "Iteration 1387, loss = 0.21915972\n",
      "Iteration 1388, loss = 0.21914790\n",
      "Iteration 1389, loss = 0.21913668\n",
      "Iteration 1390, loss = 0.21912363\n",
      "Iteration 1391, loss = 0.21911076\n",
      "Iteration 1392, loss = 0.21909848\n",
      "Iteration 1393, loss = 0.21908621\n",
      "Iteration 1394, loss = 0.21907414\n",
      "Iteration 1395, loss = 0.21906242\n",
      "Iteration 1396, loss = 0.21905108\n",
      "Iteration 1397, loss = 0.21903775\n",
      "Iteration 1398, loss = 0.21902481\n",
      "Iteration 1399, loss = 0.21901369\n",
      "Iteration 1400, loss = 0.21900194\n",
      "Iteration 1401, loss = 0.21898944\n",
      "Iteration 1402, loss = 0.21897780\n",
      "Iteration 1403, loss = 0.21896626\n",
      "Iteration 1404, loss = 0.21895502\n",
      "Iteration 1405, loss = 0.21894298\n",
      "Iteration 1406, loss = 0.21893205\n",
      "Iteration 1407, loss = 0.21892068\n",
      "Iteration 1408, loss = 0.21891047\n",
      "Iteration 1409, loss = 0.21889996\n",
      "Iteration 1410, loss = 0.21888835\n",
      "Iteration 1411, loss = 0.21887876\n",
      "Iteration 1412, loss = 0.21886867\n",
      "Iteration 1413, loss = 0.21885763\n",
      "Iteration 1414, loss = 0.21884593\n",
      "Iteration 1415, loss = 0.21883618\n",
      "Iteration 1416, loss = 0.21882701\n",
      "Iteration 1417, loss = 0.21881645\n",
      "Iteration 1418, loss = 0.21880415\n",
      "Iteration 1419, loss = 0.21879500\n",
      "Iteration 1420, loss = 0.21878573\n",
      "Iteration 1421, loss = 0.21877722\n",
      "Iteration 1422, loss = 0.21876755\n",
      "Iteration 1423, loss = 0.21875623\n",
      "Iteration 1424, loss = 0.21874470\n",
      "Iteration 1425, loss = 0.21873394\n",
      "Iteration 1426, loss = 0.21872486\n",
      "Iteration 1427, loss = 0.21871579\n",
      "Iteration 1428, loss = 0.21870442\n",
      "Iteration 1429, loss = 0.21869428\n",
      "Iteration 1430, loss = 0.21868308\n",
      "Iteration 1431, loss = 0.21867184\n",
      "Iteration 1432, loss = 0.21866237\n",
      "Iteration 1433, loss = 0.21865215\n",
      "Iteration 1434, loss = 0.21864151\n",
      "Iteration 1435, loss = 0.21863175\n",
      "Iteration 1436, loss = 0.21862176\n",
      "Iteration 1437, loss = 0.21861054\n",
      "Iteration 1438, loss = 0.21860110\n",
      "Iteration 1439, loss = 0.21859276\n",
      "Iteration 1440, loss = 0.21858359\n",
      "Iteration 1441, loss = 0.21857335\n",
      "Iteration 1442, loss = 0.21856383\n",
      "Iteration 1443, loss = 0.21855380\n",
      "Iteration 1444, loss = 0.21854394\n",
      "Iteration 1445, loss = 0.21853370\n",
      "Iteration 1446, loss = 0.21852501\n",
      "Iteration 1447, loss = 0.21851632\n",
      "Iteration 1448, loss = 0.21850679\n",
      "Iteration 1449, loss = 0.21849780\n",
      "Iteration 1450, loss = 0.21848874\n",
      "Iteration 1451, loss = 0.21847887\n",
      "Iteration 1452, loss = 0.21846862\n",
      "Iteration 1453, loss = 0.21845948\n",
      "Iteration 1454, loss = 0.21845095\n",
      "Iteration 1455, loss = 0.21844106\n",
      "Iteration 1456, loss = 0.21843175\n",
      "Iteration 1457, loss = 0.21842223\n",
      "Iteration 1458, loss = 0.21841270\n",
      "Iteration 1459, loss = 0.21840307\n",
      "Iteration 1460, loss = 0.21839372\n",
      "Iteration 1461, loss = 0.21838502\n",
      "Iteration 1462, loss = 0.21837614\n",
      "Iteration 1463, loss = 0.21836654\n",
      "Iteration 1464, loss = 0.21835773\n",
      "Iteration 1465, loss = 0.21834891\n",
      "Iteration 1466, loss = 0.21833979\n",
      "Iteration 1467, loss = 0.21833092\n",
      "Iteration 1468, loss = 0.21832215\n",
      "Iteration 1469, loss = 0.21831342\n",
      "Iteration 1470, loss = 0.21830478\n",
      "Iteration 1471, loss = 0.21829527\n",
      "Iteration 1472, loss = 0.21828631\n",
      "Iteration 1473, loss = 0.21827776\n",
      "Iteration 1474, loss = 0.21826996\n",
      "Iteration 1475, loss = 0.21825971\n",
      "Iteration 1476, loss = 0.21825030\n",
      "Iteration 1477, loss = 0.21824249\n",
      "Iteration 1478, loss = 0.21823446\n",
      "Iteration 1479, loss = 0.21822660\n",
      "Iteration 1480, loss = 0.21821819\n",
      "Iteration 1481, loss = 0.21820920\n",
      "Iteration 1482, loss = 0.21820123\n",
      "Iteration 1483, loss = 0.21819260\n",
      "Iteration 1484, loss = 0.21818492\n",
      "Iteration 1485, loss = 0.21817781\n",
      "Iteration 1486, loss = 0.21816897\n",
      "Iteration 1487, loss = 0.21816041\n",
      "Iteration 1488, loss = 0.21815198\n",
      "Iteration 1489, loss = 0.21814327\n",
      "Iteration 1490, loss = 0.21813637\n",
      "Iteration 1491, loss = 0.21812903\n",
      "Iteration 1492, loss = 0.21812076\n",
      "Iteration 1493, loss = 0.21811251\n",
      "Iteration 1494, loss = 0.21810595\n",
      "Iteration 1495, loss = 0.21809902\n",
      "Iteration 1496, loss = 0.21809200\n",
      "Iteration 1497, loss = 0.21808503\n",
      "Iteration 1498, loss = 0.21807785\n",
      "Iteration 1499, loss = 0.21806982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1500, loss = 0.21806415\n",
      "Iteration 1501, loss = 0.21805798\n",
      "Iteration 1502, loss = 0.21805085\n",
      "Iteration 1503, loss = 0.21804415\n",
      "Iteration 1504, loss = 0.21803788\n",
      "Iteration 1505, loss = 0.21803198\n",
      "Iteration 1506, loss = 0.21802514\n",
      "Iteration 1507, loss = 0.21801839\n",
      "Iteration 1508, loss = 0.21801160\n",
      "Iteration 1509, loss = 0.21800533\n",
      "Iteration 1510, loss = 0.21799854\n",
      "Iteration 1511, loss = 0.21799200\n",
      "Iteration 1512, loss = 0.21798700\n",
      "Iteration 1513, loss = 0.21797917\n",
      "Iteration 1514, loss = 0.21797279\n",
      "Iteration 1515, loss = 0.21796692\n",
      "Iteration 1516, loss = 0.21796051\n",
      "Iteration 1517, loss = 0.21795559\n",
      "Iteration 1518, loss = 0.21795055\n",
      "Iteration 1519, loss = 0.21794385\n",
      "Iteration 1520, loss = 0.21793748\n",
      "Iteration 1521, loss = 0.21793110\n",
      "Iteration 1522, loss = 0.21792508\n",
      "Iteration 1523, loss = 0.21791962\n",
      "Iteration 1524, loss = 0.21791493\n",
      "Iteration 1525, loss = 0.21790704\n",
      "Iteration 1526, loss = 0.21790136\n",
      "Iteration 1527, loss = 0.21789359\n",
      "Iteration 1528, loss = 0.21788535\n",
      "Iteration 1529, loss = 0.21787686\n",
      "Iteration 1530, loss = 0.21786735\n",
      "Iteration 1531, loss = 0.21785724\n",
      "Iteration 1532, loss = 0.21784757\n",
      "Iteration 1533, loss = 0.21783570\n",
      "Iteration 1534, loss = 0.21782567\n",
      "Iteration 1535, loss = 0.21781441\n",
      "Iteration 1536, loss = 0.21780313\n",
      "Iteration 1537, loss = 0.21779005\n",
      "Iteration 1538, loss = 0.21777757\n",
      "Iteration 1539, loss = 0.21776681\n",
      "Iteration 1540, loss = 0.21775517\n",
      "Iteration 1541, loss = 0.21774251\n",
      "Iteration 1542, loss = 0.21773035\n",
      "Iteration 1543, loss = 0.21771800\n",
      "Iteration 1544, loss = 0.21770416\n",
      "Iteration 1545, loss = 0.21769206\n",
      "Iteration 1546, loss = 0.21767874\n",
      "Iteration 1547, loss = 0.21766714\n",
      "Iteration 1548, loss = 0.21765504\n",
      "Iteration 1549, loss = 0.21764287\n",
      "Iteration 1550, loss = 0.21763056\n",
      "Iteration 1551, loss = 0.21761769\n",
      "Iteration 1552, loss = 0.21760615\n",
      "Iteration 1553, loss = 0.21759401\n",
      "Iteration 1554, loss = 0.21758171\n",
      "Iteration 1555, loss = 0.21757001\n",
      "Iteration 1556, loss = 0.21755835\n",
      "Iteration 1557, loss = 0.21754436\n",
      "Iteration 1558, loss = 0.21753190\n",
      "Iteration 1559, loss = 0.21751351\n",
      "Iteration 1560, loss = 0.21749419\n",
      "Iteration 1561, loss = 0.21747273\n",
      "Iteration 1562, loss = 0.21745113\n",
      "Iteration 1563, loss = 0.21743108\n",
      "Iteration 1564, loss = 0.21740911\n",
      "Iteration 1565, loss = 0.21738727\n",
      "Iteration 1566, loss = 0.21736575\n",
      "Iteration 1567, loss = 0.21734350\n",
      "Iteration 1568, loss = 0.21732072\n",
      "Iteration 1569, loss = 0.21729928\n",
      "Iteration 1570, loss = 0.21727843\n",
      "Iteration 1571, loss = 0.21725495\n",
      "Iteration 1572, loss = 0.21723241\n",
      "Iteration 1573, loss = 0.21720957\n",
      "Iteration 1574, loss = 0.21718718\n",
      "Iteration 1575, loss = 0.21716479\n",
      "Iteration 1576, loss = 0.21713998\n",
      "Iteration 1577, loss = 0.21711667\n",
      "Iteration 1578, loss = 0.21709528\n",
      "Iteration 1579, loss = 0.21707312\n",
      "Iteration 1580, loss = 0.21705449\n",
      "Iteration 1581, loss = 0.21703562\n",
      "Iteration 1582, loss = 0.21701711\n",
      "Iteration 1583, loss = 0.21700004\n",
      "Iteration 1584, loss = 0.21698349\n",
      "Iteration 1585, loss = 0.21696628\n",
      "Iteration 1586, loss = 0.21694902\n",
      "Iteration 1587, loss = 0.21693297\n",
      "Iteration 1588, loss = 0.21691732\n",
      "Iteration 1589, loss = 0.21690152\n",
      "Iteration 1590, loss = 0.21688985\n",
      "Iteration 1591, loss = 0.21688020\n",
      "Iteration 1592, loss = 0.21687076\n",
      "Iteration 1593, loss = 0.21686274\n",
      "Iteration 1594, loss = 0.21685445\n",
      "Iteration 1595, loss = 0.21684759\n",
      "Iteration 1596, loss = 0.21683888\n",
      "Iteration 1597, loss = 0.21683052\n",
      "Iteration 1598, loss = 0.21682167\n",
      "Iteration 1599, loss = 0.21681301\n",
      "Iteration 1600, loss = 0.21680525\n",
      "Iteration 1601, loss = 0.21679777\n",
      "Iteration 1602, loss = 0.21678922\n",
      "Iteration 1603, loss = 0.21678134\n",
      "Iteration 1604, loss = 0.21677206\n",
      "Iteration 1605, loss = 0.21676437\n",
      "Iteration 1606, loss = 0.21675619\n",
      "Iteration 1607, loss = 0.21674754\n",
      "Iteration 1608, loss = 0.21673957\n",
      "Iteration 1609, loss = 0.21673115\n",
      "Iteration 1610, loss = 0.21672239\n",
      "Iteration 1611, loss = 0.21671358\n",
      "Iteration 1612, loss = 0.21670494\n",
      "Iteration 1613, loss = 0.21669682\n",
      "Iteration 1614, loss = 0.21668731\n",
      "Iteration 1615, loss = 0.21667784\n",
      "Iteration 1616, loss = 0.21666960\n",
      "Iteration 1617, loss = 0.21666107\n",
      "Iteration 1618, loss = 0.21665214\n",
      "Iteration 1619, loss = 0.21664261\n",
      "Iteration 1620, loss = 0.21663387\n",
      "Iteration 1621, loss = 0.21662590\n",
      "Iteration 1622, loss = 0.21661611\n",
      "Iteration 1623, loss = 0.21660724\n",
      "Iteration 1624, loss = 0.21659716\n",
      "Iteration 1625, loss = 0.21658760\n",
      "Iteration 1626, loss = 0.21657970\n",
      "Iteration 1627, loss = 0.21657086\n",
      "Iteration 1628, loss = 0.21656062\n",
      "Iteration 1629, loss = 0.21655236\n",
      "Iteration 1630, loss = 0.21654418\n",
      "Iteration 1631, loss = 0.21653574\n",
      "Iteration 1632, loss = 0.21652589\n",
      "Iteration 1633, loss = 0.21651700\n",
      "Iteration 1634, loss = 0.21650828\n",
      "Iteration 1635, loss = 0.21650046\n",
      "Iteration 1636, loss = 0.21649174\n",
      "Iteration 1637, loss = 0.21648333\n",
      "Iteration 1638, loss = 0.21647411\n",
      "Iteration 1639, loss = 0.21646715\n",
      "Iteration 1640, loss = 0.21645771\n",
      "Iteration 1641, loss = 0.21645021\n",
      "Iteration 1642, loss = 0.21644113\n",
      "Iteration 1643, loss = 0.21643325\n",
      "Iteration 1644, loss = 0.21642351\n",
      "Iteration 1645, loss = 0.21641466\n",
      "Iteration 1646, loss = 0.21640718\n",
      "Iteration 1647, loss = 0.21640001\n",
      "Iteration 1648, loss = 0.21639077\n",
      "Iteration 1649, loss = 0.21638093\n",
      "Iteration 1650, loss = 0.21637254\n",
      "Iteration 1651, loss = 0.21636512\n",
      "Iteration 1652, loss = 0.21635690\n",
      "Iteration 1653, loss = 0.21634878\n",
      "Iteration 1654, loss = 0.21634113\n",
      "Iteration 1655, loss = 0.21633335\n",
      "Iteration 1656, loss = 0.21632536\n",
      "Iteration 1657, loss = 0.21631749\n",
      "Iteration 1658, loss = 0.21630901\n",
      "Iteration 1659, loss = 0.21630092\n",
      "Iteration 1660, loss = 0.21629481\n",
      "Iteration 1661, loss = 0.21628756\n",
      "Iteration 1662, loss = 0.21627862\n",
      "Iteration 1663, loss = 0.21627220\n",
      "Iteration 1664, loss = 0.21626694\n",
      "Iteration 1665, loss = 0.21626052\n",
      "Iteration 1666, loss = 0.21625388\n",
      "Iteration 1667, loss = 0.21624720\n",
      "Iteration 1668, loss = 0.21624103\n",
      "Iteration 1669, loss = 0.21623497\n",
      "Iteration 1670, loss = 0.21622747\n",
      "Iteration 1671, loss = 0.21622019\n",
      "Iteration 1672, loss = 0.21621416\n",
      "Iteration 1673, loss = 0.21620782\n",
      "Iteration 1674, loss = 0.21620002\n",
      "Iteration 1675, loss = 0.21619318\n",
      "Iteration 1676, loss = 0.21618622\n",
      "Iteration 1677, loss = 0.21617842\n",
      "Iteration 1678, loss = 0.21616959\n",
      "Iteration 1679, loss = 0.21616242\n",
      "Iteration 1680, loss = 0.21615490\n",
      "Iteration 1681, loss = 0.21614570\n",
      "Iteration 1682, loss = 0.21613779\n",
      "Iteration 1683, loss = 0.21612830\n",
      "Iteration 1684, loss = 0.21612002\n",
      "Iteration 1685, loss = 0.21611260\n",
      "Iteration 1686, loss = 0.21610334\n",
      "Iteration 1687, loss = 0.21609358\n",
      "Iteration 1688, loss = 0.21608443\n",
      "Iteration 1689, loss = 0.21607804\n",
      "Iteration 1690, loss = 0.21607005\n",
      "Iteration 1691, loss = 0.21605977\n",
      "Iteration 1692, loss = 0.21605032\n",
      "Iteration 1693, loss = 0.21604305\n",
      "Iteration 1694, loss = 0.21603423\n",
      "Iteration 1695, loss = 0.21602543\n",
      "Iteration 1696, loss = 0.21601634\n",
      "Iteration 1697, loss = 0.21601000\n",
      "Iteration 1698, loss = 0.21600108\n",
      "Iteration 1699, loss = 0.21599039\n",
      "Iteration 1700, loss = 0.21598063\n",
      "Iteration 1701, loss = 0.21597342\n",
      "Iteration 1702, loss = 0.21596941\n",
      "Iteration 1703, loss = 0.21596183\n",
      "Iteration 1704, loss = 0.21595631\n",
      "Iteration 1705, loss = 0.21594937\n",
      "Iteration 1706, loss = 0.21594333\n",
      "Iteration 1707, loss = 0.21593706\n",
      "Iteration 1708, loss = 0.21592966\n",
      "Iteration 1709, loss = 0.21592011\n",
      "Iteration 1710, loss = 0.21591148\n",
      "Iteration 1711, loss = 0.21589982\n",
      "Iteration 1712, loss = 0.21588963\n",
      "Iteration 1713, loss = 0.21587889\n",
      "Iteration 1714, loss = 0.21586706\n",
      "Iteration 1715, loss = 0.21585374\n",
      "Iteration 1716, loss = 0.21584087\n",
      "Iteration 1717, loss = 0.21582911\n",
      "Iteration 1718, loss = 0.21581627\n",
      "Iteration 1719, loss = 0.21580383\n",
      "Iteration 1720, loss = 0.21578584\n",
      "Iteration 1721, loss = 0.21576773\n",
      "Iteration 1722, loss = 0.21574640\n",
      "Iteration 1723, loss = 0.21572414\n",
      "Iteration 1724, loss = 0.21570045\n",
      "Iteration 1725, loss = 0.21567589\n",
      "Iteration 1726, loss = 0.21565065\n",
      "Iteration 1727, loss = 0.21562548\n",
      "Iteration 1728, loss = 0.21560602\n",
      "Iteration 1729, loss = 0.21558917\n",
      "Iteration 1730, loss = 0.21557276\n",
      "Iteration 1731, loss = 0.21555644\n",
      "Iteration 1732, loss = 0.21554174\n",
      "Iteration 1733, loss = 0.21552631\n",
      "Iteration 1734, loss = 0.21551187\n",
      "Iteration 1735, loss = 0.21549780\n",
      "Iteration 1736, loss = 0.21548369\n",
      "Iteration 1737, loss = 0.21546839\n",
      "Iteration 1738, loss = 0.21545253\n",
      "Iteration 1739, loss = 0.21543803\n",
      "Iteration 1740, loss = 0.21542670\n",
      "Iteration 1741, loss = 0.21541319\n",
      "Iteration 1742, loss = 0.21540159\n",
      "Iteration 1743, loss = 0.21538975\n",
      "Iteration 1744, loss = 0.21537872\n",
      "Iteration 1745, loss = 0.21536732\n",
      "Iteration 1746, loss = 0.21535502\n",
      "Iteration 1747, loss = 0.21534336\n",
      "Iteration 1748, loss = 0.21533249\n",
      "Iteration 1749, loss = 0.21532111\n",
      "Iteration 1750, loss = 0.21531108\n",
      "Iteration 1751, loss = 0.21529877\n",
      "Iteration 1752, loss = 0.21528665\n",
      "Iteration 1753, loss = 0.21527614\n",
      "Iteration 1754, loss = 0.21526395\n",
      "Iteration 1755, loss = 0.21524847\n",
      "Iteration 1756, loss = 0.21523425\n",
      "Iteration 1757, loss = 0.21522518\n",
      "Iteration 1758, loss = 0.21521842\n",
      "Iteration 1759, loss = 0.21520683\n",
      "Iteration 1760, loss = 0.21519805\n",
      "Iteration 1761, loss = 0.21519067\n",
      "Iteration 1762, loss = 0.21518252\n",
      "Iteration 1763, loss = 0.21517402\n",
      "Iteration 1764, loss = 0.21516716\n",
      "Iteration 1765, loss = 0.21516062\n",
      "Iteration 1766, loss = 0.21515272\n",
      "Iteration 1767, loss = 0.21514447\n",
      "Iteration 1768, loss = 0.21513716\n",
      "Iteration 1769, loss = 0.21512848\n",
      "Iteration 1770, loss = 0.21511966\n",
      "Iteration 1771, loss = 0.21511025\n",
      "Iteration 1772, loss = 0.21510159\n",
      "Iteration 1773, loss = 0.21509449\n",
      "Iteration 1774, loss = 0.21508586\n",
      "Iteration 1775, loss = 0.21507852\n",
      "Iteration 1776, loss = 0.21507444\n",
      "Iteration 1777, loss = 0.21506730\n",
      "Iteration 1778, loss = 0.21506158\n",
      "Iteration 1779, loss = 0.21505452\n",
      "Iteration 1780, loss = 0.21504862\n",
      "Iteration 1781, loss = 0.21504180\n",
      "Iteration 1782, loss = 0.21503324\n",
      "Iteration 1783, loss = 0.21502486\n",
      "Iteration 1784, loss = 0.21501885\n",
      "Iteration 1785, loss = 0.21501360\n",
      "Iteration 1786, loss = 0.21500826\n",
      "Iteration 1787, loss = 0.21500202\n",
      "Iteration 1788, loss = 0.21499455\n",
      "Iteration 1789, loss = 0.21498708\n",
      "Iteration 1790, loss = 0.21498216\n",
      "Iteration 1791, loss = 0.21497527\n",
      "Iteration 1792, loss = 0.21496827\n",
      "Iteration 1793, loss = 0.21496163\n",
      "Iteration 1794, loss = 0.21495506\n",
      "Iteration 1795, loss = 0.21494968\n",
      "Iteration 1796, loss = 0.21494224\n",
      "Iteration 1797, loss = 0.21493653\n",
      "Iteration 1798, loss = 0.21493088\n",
      "Iteration 1799, loss = 0.21492378\n",
      "Iteration 1800, loss = 0.21491714\n",
      "Iteration 1801, loss = 0.21491106\n",
      "Iteration 1802, loss = 0.21490595\n",
      "Iteration 1803, loss = 0.21490025\n",
      "Iteration 1804, loss = 0.21489387\n",
      "Iteration 1805, loss = 0.21488875\n",
      "Iteration 1806, loss = 0.21488470\n",
      "Iteration 1807, loss = 0.21487853\n",
      "Iteration 1808, loss = 0.21487310\n",
      "Iteration 1809, loss = 0.21486829\n",
      "Iteration 1810, loss = 0.21486408\n",
      "Iteration 1811, loss = 0.21485895\n",
      "Iteration 1812, loss = 0.21485311\n",
      "Iteration 1813, loss = 0.21484793\n",
      "Iteration 1814, loss = 0.21484277\n",
      "Iteration 1815, loss = 0.21483694\n",
      "Iteration 1816, loss = 0.21483291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1817, loss = 0.21482742\n",
      "Iteration 1818, loss = 0.21482208\n",
      "Iteration 1819, loss = 0.21481707\n",
      "Iteration 1820, loss = 0.21481248\n",
      "Iteration 1821, loss = 0.21480673\n",
      "Iteration 1822, loss = 0.21480189\n",
      "Iteration 1823, loss = 0.21479677\n",
      "Iteration 1824, loss = 0.21479192\n",
      "Iteration 1825, loss = 0.21478687\n",
      "Iteration 1826, loss = 0.21478329\n",
      "Iteration 1827, loss = 0.21477884\n",
      "Iteration 1828, loss = 0.21477265\n",
      "Iteration 1829, loss = 0.21476827\n",
      "Iteration 1830, loss = 0.21476296\n",
      "Iteration 1831, loss = 0.21475811\n",
      "Iteration 1832, loss = 0.21475268\n",
      "Iteration 1833, loss = 0.21474599\n",
      "Iteration 1834, loss = 0.21474062\n",
      "Iteration 1835, loss = 0.21473582\n",
      "Iteration 1836, loss = 0.21473233\n",
      "Iteration 1837, loss = 0.21472658\n",
      "Iteration 1838, loss = 0.21472243\n",
      "Iteration 1839, loss = 0.21471895\n",
      "Iteration 1840, loss = 0.21471265\n",
      "Iteration 1841, loss = 0.21470905\n",
      "Iteration 1842, loss = 0.21470454\n",
      "Iteration 1843, loss = 0.21469929\n",
      "Iteration 1844, loss = 0.21469556\n",
      "Iteration 1845, loss = 0.21468889\n",
      "Iteration 1846, loss = 0.21468340\n",
      "Iteration 1847, loss = 0.21467914\n",
      "Iteration 1848, loss = 0.21467433\n",
      "Iteration 1849, loss = 0.21466968\n",
      "Iteration 1850, loss = 0.21466484\n",
      "Iteration 1851, loss = 0.21465915\n",
      "Iteration 1852, loss = 0.21465346\n",
      "Iteration 1853, loss = 0.21464902\n",
      "Iteration 1854, loss = 0.21464521\n",
      "Iteration 1855, loss = 0.21463940\n",
      "Iteration 1856, loss = 0.21463502\n",
      "Iteration 1857, loss = 0.21462906\n",
      "Iteration 1858, loss = 0.21462291\n",
      "Iteration 1859, loss = 0.21461749\n",
      "Iteration 1860, loss = 0.21461454\n",
      "Iteration 1861, loss = 0.21460961\n",
      "Iteration 1862, loss = 0.21460377\n",
      "Iteration 1863, loss = 0.21459780\n",
      "Iteration 1864, loss = 0.21459239\n",
      "Iteration 1865, loss = 0.21458651\n",
      "Iteration 1866, loss = 0.21458249\n",
      "Iteration 1867, loss = 0.21457606\n",
      "Iteration 1868, loss = 0.21457193\n",
      "Iteration 1869, loss = 0.21456743\n",
      "Iteration 1870, loss = 0.21456234\n",
      "Iteration 1871, loss = 0.21455648\n",
      "Iteration 1872, loss = 0.21455130\n",
      "Iteration 1873, loss = 0.21454549\n",
      "Iteration 1874, loss = 0.21454285\n",
      "Iteration 1875, loss = 0.21453631\n",
      "Iteration 1876, loss = 0.21453027\n",
      "Iteration 1877, loss = 0.21452635\n",
      "Iteration 1878, loss = 0.21452133\n",
      "Iteration 1879, loss = 0.21451636\n",
      "Iteration 1880, loss = 0.21451128\n",
      "Iteration 1881, loss = 0.21450482\n",
      "Iteration 1882, loss = 0.21450028\n",
      "Iteration 1883, loss = 0.21449482\n",
      "Iteration 1884, loss = 0.21449210\n",
      "Iteration 1885, loss = 0.21448463\n",
      "Iteration 1886, loss = 0.21447959\n",
      "Iteration 1887, loss = 0.21447429\n",
      "Iteration 1888, loss = 0.21446876\n",
      "Iteration 1889, loss = 0.21446425\n",
      "Iteration 1890, loss = 0.21445885\n",
      "Iteration 1891, loss = 0.21445362\n",
      "Iteration 1892, loss = 0.21444972\n",
      "Iteration 1893, loss = 0.21444363\n",
      "Iteration 1894, loss = 0.21443817\n",
      "Iteration 1895, loss = 0.21443270\n",
      "Iteration 1896, loss = 0.21442642\n",
      "Iteration 1897, loss = 0.21442177\n",
      "Iteration 1898, loss = 0.21441723\n",
      "Iteration 1899, loss = 0.21441289\n",
      "Iteration 1900, loss = 0.21440647\n",
      "Iteration 1901, loss = 0.21440121\n",
      "Iteration 1902, loss = 0.21439534\n",
      "Iteration 1903, loss = 0.21439074\n",
      "Iteration 1904, loss = 0.21438634\n",
      "Iteration 1905, loss = 0.21437931\n",
      "Iteration 1906, loss = 0.21437514\n",
      "Iteration 1907, loss = 0.21437242\n",
      "Iteration 1908, loss = 0.21436791\n",
      "Iteration 1909, loss = 0.21436094\n",
      "Iteration 1910, loss = 0.21435575\n",
      "Iteration 1911, loss = 0.21435147\n",
      "Iteration 1912, loss = 0.21434450\n",
      "Iteration 1913, loss = 0.21434022\n",
      "Iteration 1914, loss = 0.21433636\n",
      "Iteration 1915, loss = 0.21432849\n",
      "Iteration 1916, loss = 0.21432324\n",
      "Iteration 1917, loss = 0.21431735\n",
      "Iteration 1918, loss = 0.21431391\n",
      "Iteration 1919, loss = 0.21430960\n",
      "Iteration 1920, loss = 0.21430318\n",
      "Iteration 1921, loss = 0.21429778\n",
      "Iteration 1922, loss = 0.21429431\n",
      "Iteration 1923, loss = 0.21429079\n",
      "Iteration 1924, loss = 0.21428278\n",
      "Iteration 1925, loss = 0.21427709\n",
      "Iteration 1926, loss = 0.21427231\n",
      "Iteration 1927, loss = 0.21426992\n",
      "Iteration 1928, loss = 0.21426480\n",
      "Iteration 1929, loss = 0.21425802\n",
      "Iteration 1930, loss = 0.21425154\n",
      "Iteration 1931, loss = 0.21424600\n",
      "Iteration 1932, loss = 0.21424195\n",
      "Iteration 1933, loss = 0.21423675\n",
      "Iteration 1934, loss = 0.21423301\n",
      "Iteration 1935, loss = 0.21422659\n",
      "Iteration 1936, loss = 0.21422112\n",
      "Iteration 1937, loss = 0.21421642\n",
      "Iteration 1938, loss = 0.21421091\n",
      "Iteration 1939, loss = 0.21420692\n",
      "Iteration 1940, loss = 0.21420140\n",
      "Iteration 1941, loss = 0.21419524\n",
      "Iteration 1942, loss = 0.21419159\n",
      "Iteration 1943, loss = 0.21418583\n",
      "Iteration 1944, loss = 0.21417920\n",
      "Iteration 1945, loss = 0.21417627\n",
      "Iteration 1946, loss = 0.21417221\n",
      "Iteration 1947, loss = 0.21416775\n",
      "Iteration 1948, loss = 0.21416255\n",
      "Iteration 1949, loss = 0.21415640\n",
      "Iteration 1950, loss = 0.21415095\n",
      "Iteration 1951, loss = 0.21414786\n",
      "Iteration 1952, loss = 0.21414321\n",
      "Iteration 1953, loss = 0.21413782\n",
      "Iteration 1954, loss = 0.21413219\n",
      "Iteration 1955, loss = 0.21412810\n",
      "Iteration 1956, loss = 0.21412363\n",
      "Iteration 1957, loss = 0.21411893\n",
      "Iteration 1958, loss = 0.21411497\n",
      "Iteration 1959, loss = 0.21410998\n",
      "Iteration 1960, loss = 0.21410354\n",
      "Iteration 1961, loss = 0.21409946\n",
      "Iteration 1962, loss = 0.21409529\n",
      "Iteration 1963, loss = 0.21409017\n",
      "Iteration 1964, loss = 0.21408538\n",
      "Iteration 1965, loss = 0.21408075\n",
      "Iteration 1966, loss = 0.21407600\n",
      "Iteration 1967, loss = 0.21407175\n",
      "Iteration 1968, loss = 0.21406670\n",
      "Iteration 1969, loss = 0.21406185\n",
      "Iteration 1970, loss = 0.21405687\n",
      "Iteration 1971, loss = 0.21405341\n",
      "Iteration 1972, loss = 0.21404773\n",
      "Iteration 1973, loss = 0.21404323\n",
      "Iteration 1974, loss = 0.21403911\n",
      "Iteration 1975, loss = 0.21403366\n",
      "Iteration 1976, loss = 0.21402981\n",
      "Iteration 1977, loss = 0.21402646\n",
      "Iteration 1978, loss = 0.21402190\n",
      "Iteration 1979, loss = 0.21401536\n",
      "Iteration 1980, loss = 0.21400880\n",
      "Iteration 1981, loss = 0.21400465\n",
      "Iteration 1982, loss = 0.21400124\n",
      "Iteration 1983, loss = 0.21399608\n",
      "Iteration 1984, loss = 0.21399000\n",
      "Iteration 1985, loss = 0.21398516\n",
      "Iteration 1986, loss = 0.21398119\n",
      "Iteration 1987, loss = 0.21397716\n",
      "Iteration 1988, loss = 0.21397150\n",
      "Iteration 1989, loss = 0.21396699\n",
      "Iteration 1990, loss = 0.21396331\n",
      "Iteration 1991, loss = 0.21395817\n",
      "Iteration 1992, loss = 0.21395260\n",
      "Iteration 1993, loss = 0.21394746\n",
      "Iteration 1994, loss = 0.21394117\n",
      "Iteration 1995, loss = 0.21393643\n",
      "Iteration 1996, loss = 0.21393195\n",
      "Iteration 1997, loss = 0.21392765\n",
      "Iteration 1998, loss = 0.21392270\n",
      "Iteration 1999, loss = 0.21391632\n",
      "Iteration 2000, loss = 0.21391276\n",
      "Iteration 2001, loss = 0.21390927\n",
      "Iteration 2002, loss = 0.21390535\n",
      "Iteration 2003, loss = 0.21390043\n",
      "Iteration 2004, loss = 0.21389579\n",
      "Iteration 2005, loss = 0.21389170\n",
      "Iteration 2006, loss = 0.21388796\n",
      "Iteration 2007, loss = 0.21388258\n",
      "Iteration 2008, loss = 0.21387697\n",
      "Iteration 2009, loss = 0.21387285\n",
      "Iteration 2010, loss = 0.21386963\n",
      "Iteration 2011, loss = 0.21386567\n",
      "Iteration 2012, loss = 0.21385953\n",
      "Iteration 2013, loss = 0.21385421\n",
      "Iteration 2014, loss = 0.21384826\n",
      "Iteration 2015, loss = 0.21384299\n",
      "Iteration 2016, loss = 0.21383776\n",
      "Iteration 2017, loss = 0.21383392\n",
      "Iteration 2018, loss = 0.21383048\n",
      "Iteration 2019, loss = 0.21382485\n",
      "Iteration 2020, loss = 0.21381968\n",
      "Iteration 2021, loss = 0.21381595\n",
      "Iteration 2022, loss = 0.21381073\n",
      "Iteration 2023, loss = 0.21380428\n",
      "Iteration 2024, loss = 0.21380069\n",
      "Iteration 2025, loss = 0.21379751\n",
      "Iteration 2026, loss = 0.21379204\n",
      "Iteration 2027, loss = 0.21378637\n",
      "Iteration 2028, loss = 0.21378209\n",
      "Iteration 2029, loss = 0.21377869\n",
      "Iteration 2030, loss = 0.21377273\n",
      "Iteration 2031, loss = 0.21376844\n",
      "Iteration 2032, loss = 0.21376304\n",
      "Iteration 2033, loss = 0.21375916\n",
      "Iteration 2034, loss = 0.21375331\n",
      "Iteration 2035, loss = 0.21375035\n",
      "Iteration 2036, loss = 0.21374560\n",
      "Iteration 2037, loss = 0.21373993\n",
      "Iteration 2038, loss = 0.21373504\n",
      "Iteration 2039, loss = 0.21373072\n",
      "Iteration 2040, loss = 0.21372681\n",
      "Iteration 2041, loss = 0.21372220\n",
      "Iteration 2042, loss = 0.21371783\n",
      "Iteration 2043, loss = 0.21371053\n",
      "Iteration 2044, loss = 0.21370698\n",
      "Iteration 2045, loss = 0.21370197\n",
      "Iteration 2046, loss = 0.21369635\n",
      "Iteration 2047, loss = 0.21369210\n",
      "Iteration 2048, loss = 0.21368795\n",
      "Iteration 2049, loss = 0.21368257\n",
      "Iteration 2050, loss = 0.21367694\n",
      "Iteration 2051, loss = 0.21367269\n",
      "Iteration 2052, loss = 0.21366980\n",
      "Iteration 2053, loss = 0.21366414\n",
      "Iteration 2054, loss = 0.21365985\n",
      "Iteration 2055, loss = 0.21365505\n",
      "Iteration 2056, loss = 0.21364966\n",
      "Iteration 2057, loss = 0.21364639\n",
      "Iteration 2058, loss = 0.21364210\n",
      "Iteration 2059, loss = 0.21363659\n",
      "Iteration 2060, loss = 0.21363225\n",
      "Iteration 2061, loss = 0.21362709\n",
      "Iteration 2062, loss = 0.21362183\n",
      "Iteration 2063, loss = 0.21361642\n",
      "Iteration 2064, loss = 0.21361221\n",
      "Iteration 2065, loss = 0.21360688\n",
      "Iteration 2066, loss = 0.21360171\n",
      "Iteration 2067, loss = 0.21359823\n",
      "Iteration 2068, loss = 0.21359375\n",
      "Iteration 2069, loss = 0.21358853\n",
      "Iteration 2070, loss = 0.21358377\n",
      "Iteration 2071, loss = 0.21357923\n",
      "Iteration 2072, loss = 0.21357391\n",
      "Iteration 2073, loss = 0.21356827\n",
      "Iteration 2074, loss = 0.21356329\n",
      "Iteration 2075, loss = 0.21355933\n",
      "Iteration 2076, loss = 0.21355628\n",
      "Iteration 2077, loss = 0.21354970\n",
      "Iteration 2078, loss = 0.21354324\n",
      "Iteration 2079, loss = 0.21353998\n",
      "Iteration 2080, loss = 0.21353739\n",
      "Iteration 2081, loss = 0.21353346\n",
      "Iteration 2082, loss = 0.21352666\n",
      "Iteration 2083, loss = 0.21352160\n",
      "Iteration 2084, loss = 0.21351743\n",
      "Iteration 2085, loss = 0.21351335\n",
      "Iteration 2086, loss = 0.21350786\n",
      "Iteration 2087, loss = 0.21350413\n",
      "Iteration 2088, loss = 0.21349897\n",
      "Iteration 2089, loss = 0.21349479\n",
      "Iteration 2090, loss = 0.21349114\n",
      "Iteration 2091, loss = 0.21348547\n",
      "Iteration 2092, loss = 0.21348079\n",
      "Iteration 2093, loss = 0.21347522\n",
      "Iteration 2094, loss = 0.21347105\n",
      "Iteration 2095, loss = 0.21346684\n",
      "Iteration 2096, loss = 0.21346154\n",
      "Iteration 2097, loss = 0.21345605\n",
      "Iteration 2098, loss = 0.21345085\n",
      "Iteration 2099, loss = 0.21344603\n",
      "Iteration 2100, loss = 0.21344139\n",
      "Iteration 2101, loss = 0.21343645\n",
      "Iteration 2102, loss = 0.21343055\n",
      "Iteration 2103, loss = 0.21342443\n",
      "Iteration 2104, loss = 0.21342093\n",
      "Iteration 2105, loss = 0.21341615\n",
      "Iteration 2106, loss = 0.21341135\n",
      "Iteration 2107, loss = 0.21340802\n",
      "Iteration 2108, loss = 0.21340283\n",
      "Iteration 2109, loss = 0.21339836\n",
      "Iteration 2110, loss = 0.21339371\n",
      "Iteration 2111, loss = 0.21338906\n",
      "Iteration 2112, loss = 0.21338288\n",
      "Iteration 2113, loss = 0.21337926\n",
      "Iteration 2114, loss = 0.21337404\n",
      "Iteration 2115, loss = 0.21336823\n",
      "Iteration 2116, loss = 0.21336378\n",
      "Iteration 2117, loss = 0.21335858\n",
      "Iteration 2118, loss = 0.21335407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2119, loss = 0.21334886\n",
      "Iteration 2120, loss = 0.21334264\n",
      "Iteration 2121, loss = 0.21333838\n",
      "Iteration 2122, loss = 0.21333378\n",
      "Iteration 2123, loss = 0.21332972\n",
      "Iteration 2124, loss = 0.21332404\n",
      "Iteration 2125, loss = 0.21331843\n",
      "Iteration 2126, loss = 0.21331533\n",
      "Iteration 2127, loss = 0.21330874\n",
      "Iteration 2128, loss = 0.21330353\n",
      "Iteration 2129, loss = 0.21329993\n",
      "Iteration 2130, loss = 0.21329594\n",
      "Iteration 2131, loss = 0.21329181\n",
      "Iteration 2132, loss = 0.21328575\n",
      "Iteration 2133, loss = 0.21328143\n",
      "Iteration 2134, loss = 0.21327637\n",
      "Iteration 2135, loss = 0.21327232\n",
      "Iteration 2136, loss = 0.21326635\n",
      "Iteration 2137, loss = 0.21326214\n",
      "Iteration 2138, loss = 0.21325727\n",
      "Iteration 2139, loss = 0.21325112\n",
      "Iteration 2140, loss = 0.21324563\n",
      "Iteration 2141, loss = 0.21324152\n",
      "Iteration 2142, loss = 0.21323696\n",
      "Iteration 2143, loss = 0.21323077\n",
      "Iteration 2144, loss = 0.21322696\n",
      "Iteration 2145, loss = 0.21322220\n",
      "Iteration 2146, loss = 0.21321686\n",
      "Iteration 2147, loss = 0.21321151\n",
      "Iteration 2148, loss = 0.21320713\n",
      "Iteration 2149, loss = 0.21320194\n",
      "Iteration 2150, loss = 0.21319642\n",
      "Iteration 2151, loss = 0.21319216\n",
      "Iteration 2152, loss = 0.21318713\n",
      "Iteration 2153, loss = 0.21318102\n",
      "Iteration 2154, loss = 0.21317643\n",
      "Iteration 2155, loss = 0.21317202\n",
      "Iteration 2156, loss = 0.21316669\n",
      "Iteration 2157, loss = 0.21316160\n",
      "Iteration 2158, loss = 0.21315809\n",
      "Iteration 2159, loss = 0.21315338\n",
      "Iteration 2160, loss = 0.21314723\n",
      "Iteration 2161, loss = 0.21314193\n",
      "Iteration 2162, loss = 0.21313810\n",
      "Iteration 2163, loss = 0.21313306\n",
      "Iteration 2164, loss = 0.21312804\n",
      "Iteration 2165, loss = 0.21312276\n",
      "Iteration 2166, loss = 0.21311759\n",
      "Iteration 2167, loss = 0.21311325\n",
      "Iteration 2168, loss = 0.21310845\n",
      "Iteration 2169, loss = 0.21310146\n",
      "Iteration 2170, loss = 0.21309767\n",
      "Iteration 2171, loss = 0.21309534\n",
      "Iteration 2172, loss = 0.21308955\n",
      "Iteration 2173, loss = 0.21308324\n",
      "Iteration 2174, loss = 0.21307922\n",
      "Iteration 2175, loss = 0.21307387\n",
      "Iteration 2176, loss = 0.21306891\n",
      "Iteration 2177, loss = 0.21306328\n",
      "Iteration 2178, loss = 0.21305808\n",
      "Iteration 2179, loss = 0.21305398\n",
      "Iteration 2180, loss = 0.21304916\n",
      "Iteration 2181, loss = 0.21304289\n",
      "Iteration 2182, loss = 0.21303939\n",
      "Iteration 2183, loss = 0.21303571\n",
      "Iteration 2184, loss = 0.21302982\n",
      "Iteration 2185, loss = 0.21302443\n",
      "Iteration 2186, loss = 0.21301872\n",
      "Iteration 2187, loss = 0.21301497\n",
      "Iteration 2188, loss = 0.21301102\n",
      "Iteration 2189, loss = 0.21300353\n",
      "Iteration 2190, loss = 0.21299927\n",
      "Iteration 2191, loss = 0.21299445\n",
      "Iteration 2192, loss = 0.21298912\n",
      "Iteration 2193, loss = 0.21298373\n",
      "Iteration 2194, loss = 0.21297939\n",
      "Iteration 2195, loss = 0.21297465\n",
      "Iteration 2196, loss = 0.21297001\n",
      "Iteration 2197, loss = 0.21296502\n",
      "Iteration 2198, loss = 0.21295877\n",
      "Iteration 2199, loss = 0.21295378\n",
      "Iteration 2200, loss = 0.21294935\n",
      "Iteration 2201, loss = 0.21294306\n",
      "Iteration 2202, loss = 0.21293822\n",
      "Iteration 2203, loss = 0.21293293\n",
      "Iteration 2204, loss = 0.21292827\n",
      "Iteration 2205, loss = 0.21292270\n",
      "Iteration 2206, loss = 0.21291817\n",
      "Iteration 2207, loss = 0.21291353\n",
      "Iteration 2208, loss = 0.21290923\n",
      "Iteration 2209, loss = 0.21290410\n",
      "Iteration 2210, loss = 0.21290046\n",
      "Iteration 2211, loss = 0.21289428\n",
      "Iteration 2212, loss = 0.21289003\n",
      "Iteration 2213, loss = 0.21288565\n",
      "Iteration 2214, loss = 0.21288072\n",
      "Iteration 2215, loss = 0.21287568\n",
      "Iteration 2216, loss = 0.21286897\n",
      "Iteration 2217, loss = 0.21286326\n",
      "Iteration 2218, loss = 0.21285844\n",
      "Iteration 2219, loss = 0.21285222\n",
      "Iteration 2220, loss = 0.21284820\n",
      "Iteration 2221, loss = 0.21284280\n",
      "Iteration 2222, loss = 0.21283885\n",
      "Iteration 2223, loss = 0.21283502\n",
      "Iteration 2224, loss = 0.21282829\n",
      "Iteration 2225, loss = 0.21282320\n",
      "Iteration 2226, loss = 0.21281696\n",
      "Iteration 2227, loss = 0.21281223\n",
      "Iteration 2228, loss = 0.21280837\n",
      "Iteration 2229, loss = 0.21280136\n",
      "Iteration 2230, loss = 0.21279564\n",
      "Iteration 2231, loss = 0.21279130\n",
      "Iteration 2232, loss = 0.21278811\n",
      "Iteration 2233, loss = 0.21278136\n",
      "Iteration 2234, loss = 0.21277542\n",
      "Iteration 2235, loss = 0.21277036\n",
      "Iteration 2236, loss = 0.21276456\n",
      "Iteration 2237, loss = 0.21276098\n",
      "Iteration 2238, loss = 0.21275753\n",
      "Iteration 2239, loss = 0.21275183\n",
      "Iteration 2240, loss = 0.21274541\n",
      "Iteration 2241, loss = 0.21273968\n",
      "Iteration 2242, loss = 0.21273457\n",
      "Iteration 2243, loss = 0.21273058\n",
      "Iteration 2244, loss = 0.21272575\n",
      "Iteration 2245, loss = 0.21271897\n",
      "Iteration 2246, loss = 0.21271561\n",
      "Iteration 2247, loss = 0.21271023\n",
      "Iteration 2248, loss = 0.21270739\n",
      "Iteration 2249, loss = 0.21270215\n",
      "Iteration 2250, loss = 0.21269552\n",
      "Iteration 2251, loss = 0.21269015\n",
      "Iteration 2252, loss = 0.21268587\n",
      "Iteration 2253, loss = 0.21268274\n",
      "Iteration 2254, loss = 0.21267678\n",
      "Iteration 2255, loss = 0.21266992\n",
      "Iteration 2256, loss = 0.21266637\n",
      "Iteration 2257, loss = 0.21266065\n",
      "Iteration 2258, loss = 0.21265574\n",
      "Iteration 2259, loss = 0.21265221\n",
      "Iteration 2260, loss = 0.21264574\n",
      "Iteration 2261, loss = 0.21264041\n",
      "Iteration 2262, loss = 0.21263450\n",
      "Iteration 2263, loss = 0.21262987\n",
      "Iteration 2264, loss = 0.21262589\n",
      "Iteration 2265, loss = 0.21262029\n",
      "Iteration 2266, loss = 0.21261423\n",
      "Iteration 2267, loss = 0.21260957\n",
      "Iteration 2268, loss = 0.21260497\n",
      "Iteration 2269, loss = 0.21259893\n",
      "Iteration 2270, loss = 0.21259253\n",
      "Iteration 2271, loss = 0.21258642\n",
      "Iteration 2272, loss = 0.21258429\n",
      "Iteration 2273, loss = 0.21257982\n",
      "Iteration 2274, loss = 0.21257175\n",
      "Iteration 2275, loss = 0.21256673\n",
      "Iteration 2276, loss = 0.21256355\n",
      "Iteration 2277, loss = 0.21255757\n",
      "Iteration 2278, loss = 0.21255330\n",
      "Iteration 2279, loss = 0.21254815\n",
      "Iteration 2280, loss = 0.21254088\n",
      "Iteration 2281, loss = 0.21253644\n",
      "Iteration 2282, loss = 0.21253203\n",
      "Iteration 2283, loss = 0.21252710\n",
      "Iteration 2284, loss = 0.21252239\n",
      "Iteration 2285, loss = 0.21251798\n",
      "Iteration 2286, loss = 0.21251405\n",
      "Iteration 2287, loss = 0.21250912\n",
      "Iteration 2288, loss = 0.21250209\n",
      "Iteration 2289, loss = 0.21249754\n",
      "Iteration 2290, loss = 0.21249157\n",
      "Iteration 2291, loss = 0.21248654\n",
      "Iteration 2292, loss = 0.21248336\n",
      "Iteration 2293, loss = 0.21247940\n",
      "Iteration 2294, loss = 0.21247261\n",
      "Iteration 2295, loss = 0.21246834\n",
      "Iteration 2296, loss = 0.21246382\n",
      "Iteration 2297, loss = 0.21245684\n",
      "Iteration 2298, loss = 0.21245351\n",
      "Iteration 2299, loss = 0.21244878\n",
      "Iteration 2300, loss = 0.21244190\n",
      "Iteration 2301, loss = 0.21243605\n",
      "Iteration 2302, loss = 0.21242861\n",
      "Iteration 2303, loss = 0.21242518\n",
      "Iteration 2304, loss = 0.21242028\n",
      "Iteration 2305, loss = 0.21241428\n",
      "Iteration 2306, loss = 0.21240864\n",
      "Iteration 2307, loss = 0.21240273\n",
      "Iteration 2308, loss = 0.21239746\n",
      "Iteration 2309, loss = 0.21239322\n",
      "Iteration 2310, loss = 0.21238731\n",
      "Iteration 2311, loss = 0.21238215\n",
      "Iteration 2312, loss = 0.21237808\n",
      "Iteration 2313, loss = 0.21237198\n",
      "Iteration 2314, loss = 0.21236544\n",
      "Iteration 2315, loss = 0.21236103\n",
      "Iteration 2316, loss = 0.21235674\n",
      "Iteration 2317, loss = 0.21234944\n",
      "Iteration 2318, loss = 0.21234496\n",
      "Iteration 2319, loss = 0.21233960\n",
      "Iteration 2320, loss = 0.21233464\n",
      "Iteration 2321, loss = 0.21233065\n",
      "Iteration 2322, loss = 0.21232715\n",
      "Iteration 2323, loss = 0.21232029\n",
      "Iteration 2324, loss = 0.21231460\n",
      "Iteration 2325, loss = 0.21231157\n",
      "Iteration 2326, loss = 0.21230576\n",
      "Iteration 2327, loss = 0.21230073\n",
      "Iteration 2328, loss = 0.21229401\n",
      "Iteration 2329, loss = 0.21229058\n",
      "Iteration 2330, loss = 0.21228644\n",
      "Iteration 2331, loss = 0.21228025\n",
      "Iteration 2332, loss = 0.21227436\n",
      "Iteration 2333, loss = 0.21227026\n",
      "Iteration 2334, loss = 0.21226381\n",
      "Iteration 2335, loss = 0.21225875\n",
      "Iteration 2336, loss = 0.21225369\n",
      "Iteration 2337, loss = 0.21224768\n",
      "Iteration 2338, loss = 0.21224316\n",
      "Iteration 2339, loss = 0.21223811\n",
      "Iteration 2340, loss = 0.21223311\n",
      "Iteration 2341, loss = 0.21222788\n",
      "Iteration 2342, loss = 0.21222156\n",
      "Iteration 2343, loss = 0.21221476\n",
      "Iteration 2344, loss = 0.21221206\n",
      "Iteration 2345, loss = 0.21220764\n",
      "Iteration 2346, loss = 0.21220121\n",
      "Iteration 2347, loss = 0.21219442\n",
      "Iteration 2348, loss = 0.21219055\n",
      "Iteration 2349, loss = 0.21218579\n",
      "Iteration 2350, loss = 0.21218016\n",
      "Iteration 2351, loss = 0.21217615\n",
      "Iteration 2352, loss = 0.21217030\n",
      "Iteration 2353, loss = 0.21216343\n",
      "Iteration 2354, loss = 0.21215827\n",
      "Iteration 2355, loss = 0.21215321\n",
      "Iteration 2356, loss = 0.21214876\n",
      "Iteration 2357, loss = 0.21214198\n",
      "Iteration 2358, loss = 0.21213739\n",
      "Iteration 2359, loss = 0.21213224\n",
      "Iteration 2360, loss = 0.21212714\n",
      "Iteration 2361, loss = 0.21212294\n",
      "Iteration 2362, loss = 0.21211701\n",
      "Iteration 2363, loss = 0.21211091\n",
      "Iteration 2364, loss = 0.21210542\n",
      "Iteration 2365, loss = 0.21210057\n",
      "Iteration 2366, loss = 0.21209668\n",
      "Iteration 2367, loss = 0.21209137\n",
      "Iteration 2368, loss = 0.21208498\n",
      "Iteration 2369, loss = 0.21207933\n",
      "Iteration 2370, loss = 0.21207335\n",
      "Iteration 2371, loss = 0.21206948\n",
      "Iteration 2372, loss = 0.21206446\n",
      "Iteration 2373, loss = 0.21205882\n",
      "Iteration 2374, loss = 0.21205387\n",
      "Iteration 2375, loss = 0.21204862\n",
      "Iteration 2376, loss = 0.21204432\n",
      "Iteration 2377, loss = 0.21203882\n",
      "Iteration 2378, loss = 0.21203238\n",
      "Iteration 2379, loss = 0.21202730\n",
      "Iteration 2380, loss = 0.21202264\n",
      "Iteration 2381, loss = 0.21201638\n",
      "Iteration 2382, loss = 0.21201116\n",
      "Iteration 2383, loss = 0.21200675\n",
      "Iteration 2384, loss = 0.21200098\n",
      "Iteration 2385, loss = 0.21199493\n",
      "Iteration 2386, loss = 0.21198962\n",
      "Iteration 2387, loss = 0.21198419\n",
      "Iteration 2388, loss = 0.21197964\n",
      "Iteration 2389, loss = 0.21197551\n",
      "Iteration 2390, loss = 0.21196905\n",
      "Iteration 2391, loss = 0.21196275\n",
      "Iteration 2392, loss = 0.21195660\n",
      "Iteration 2393, loss = 0.21195323\n",
      "Iteration 2394, loss = 0.21194769\n",
      "Iteration 2395, loss = 0.21194051\n",
      "Iteration 2396, loss = 0.21193650\n",
      "Iteration 2397, loss = 0.21193219\n",
      "Iteration 2398, loss = 0.21192687\n",
      "Iteration 2399, loss = 0.21192054\n",
      "Iteration 2400, loss = 0.21191416\n",
      "Iteration 2401, loss = 0.21190902\n",
      "Iteration 2402, loss = 0.21190259\n",
      "Iteration 2403, loss = 0.21189722\n",
      "Iteration 2404, loss = 0.21189102\n",
      "Iteration 2405, loss = 0.21188494\n",
      "Iteration 2406, loss = 0.21187840\n",
      "Iteration 2407, loss = 0.21187425\n",
      "Iteration 2408, loss = 0.21186865\n",
      "Iteration 2409, loss = 0.21186146\n",
      "Iteration 2410, loss = 0.21185475\n",
      "Iteration 2411, loss = 0.21184904\n",
      "Iteration 2412, loss = 0.21184282\n",
      "Iteration 2413, loss = 0.21183678\n",
      "Iteration 2414, loss = 0.21183059\n",
      "Iteration 2415, loss = 0.21182331\n",
      "Iteration 2416, loss = 0.21181780\n",
      "Iteration 2417, loss = 0.21181186\n",
      "Iteration 2418, loss = 0.21180607\n",
      "Iteration 2419, loss = 0.21179983\n",
      "Iteration 2420, loss = 0.21179386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2421, loss = 0.21178744\n",
      "Iteration 2422, loss = 0.21178199\n",
      "Iteration 2423, loss = 0.21177726\n",
      "Iteration 2424, loss = 0.21177008\n",
      "Iteration 2425, loss = 0.21176362\n",
      "Iteration 2426, loss = 0.21175715\n",
      "Iteration 2427, loss = 0.21175168\n",
      "Iteration 2428, loss = 0.21174723\n",
      "Iteration 2429, loss = 0.21174028\n",
      "Iteration 2430, loss = 0.21173402\n",
      "Iteration 2431, loss = 0.21172783\n",
      "Iteration 2432, loss = 0.21172125\n",
      "Iteration 2433, loss = 0.21171489\n",
      "Iteration 2434, loss = 0.21170743\n",
      "Iteration 2435, loss = 0.21170217\n",
      "Iteration 2436, loss = 0.21169617\n",
      "Iteration 2437, loss = 0.21168851\n",
      "Iteration 2438, loss = 0.21168296\n",
      "Iteration 2439, loss = 0.21167776\n",
      "Iteration 2440, loss = 0.21167205\n",
      "Iteration 2441, loss = 0.21166524\n",
      "Iteration 2442, loss = 0.21165712\n",
      "Iteration 2443, loss = 0.21165157\n",
      "Iteration 2444, loss = 0.21164733\n",
      "Iteration 2445, loss = 0.21164003\n",
      "Iteration 2446, loss = 0.21163449\n",
      "Iteration 2447, loss = 0.21162753\n",
      "Iteration 2448, loss = 0.21162029\n",
      "Iteration 2449, loss = 0.21161478\n",
      "Iteration 2450, loss = 0.21160840\n",
      "Iteration 2451, loss = 0.21160306\n",
      "Iteration 2452, loss = 0.21159581\n",
      "Iteration 2453, loss = 0.21158971\n",
      "Iteration 2454, loss = 0.21158382\n",
      "Iteration 2455, loss = 0.21157792\n",
      "Iteration 2456, loss = 0.21157164\n",
      "Iteration 2457, loss = 0.21156464\n",
      "Iteration 2458, loss = 0.21155999\n",
      "Iteration 2459, loss = 0.21155579\n",
      "Iteration 2460, loss = 0.21154909\n",
      "Iteration 2461, loss = 0.21154291\n",
      "Iteration 2462, loss = 0.21153740\n",
      "Iteration 2463, loss = 0.21153078\n",
      "Iteration 2464, loss = 0.21152430\n",
      "Iteration 2465, loss = 0.21151708\n",
      "Iteration 2466, loss = 0.21151006\n",
      "Iteration 2467, loss = 0.21150516\n",
      "Iteration 2468, loss = 0.21149921\n",
      "Iteration 2469, loss = 0.21149415\n",
      "Iteration 2470, loss = 0.21148987\n",
      "Iteration 2471, loss = 0.21148322\n",
      "Iteration 2472, loss = 0.21147697\n",
      "Iteration 2473, loss = 0.21147019\n",
      "Iteration 2474, loss = 0.21146526\n",
      "Iteration 2475, loss = 0.21145954\n",
      "Iteration 2476, loss = 0.21145139\n",
      "Iteration 2477, loss = 0.21144621\n",
      "Iteration 2478, loss = 0.21144126\n",
      "Iteration 2479, loss = 0.21143392\n",
      "Iteration 2480, loss = 0.21142668\n",
      "Iteration 2481, loss = 0.21142257\n",
      "Iteration 2482, loss = 0.21141566\n",
      "Iteration 2483, loss = 0.21140951\n",
      "Iteration 2484, loss = 0.21140434\n",
      "Iteration 2485, loss = 0.21139695\n",
      "Iteration 2486, loss = 0.21139106\n",
      "Iteration 2487, loss = 0.21138596\n",
      "Iteration 2488, loss = 0.21138198\n",
      "Iteration 2489, loss = 0.21137559\n",
      "Iteration 2490, loss = 0.21136800\n",
      "Iteration 2491, loss = 0.21136132\n",
      "Iteration 2492, loss = 0.21135414\n",
      "Iteration 2493, loss = 0.21134667\n",
      "Iteration 2494, loss = 0.21133907\n",
      "Iteration 2495, loss = 0.21133145\n",
      "Iteration 2496, loss = 0.21132317\n",
      "Iteration 2497, loss = 0.21131653\n",
      "Iteration 2498, loss = 0.21130939\n",
      "Iteration 2499, loss = 0.21129997\n",
      "Iteration 2500, loss = 0.21129208\n",
      "Iteration 2501, loss = 0.21128342\n",
      "Iteration 2502, loss = 0.21127591\n",
      "Iteration 2503, loss = 0.21126854\n",
      "Iteration 2504, loss = 0.21126084\n",
      "Iteration 2505, loss = 0.21125095\n",
      "Iteration 2506, loss = 0.21124181\n",
      "Iteration 2507, loss = 0.21123216\n",
      "Iteration 2508, loss = 0.21122551\n",
      "Iteration 2509, loss = 0.21121793\n",
      "Iteration 2510, loss = 0.21121008\n",
      "Iteration 2511, loss = 0.21120308\n",
      "Iteration 2512, loss = 0.21119360\n",
      "Iteration 2513, loss = 0.21118519\n",
      "Iteration 2514, loss = 0.21117851\n",
      "Iteration 2515, loss = 0.21116984\n",
      "Iteration 2516, loss = 0.21116127\n",
      "Iteration 2517, loss = 0.21115442\n",
      "Iteration 2518, loss = 0.21114736\n",
      "Iteration 2519, loss = 0.21113867\n",
      "Iteration 2520, loss = 0.21112897\n",
      "Iteration 2521, loss = 0.21112186\n",
      "Iteration 2522, loss = 0.21111542\n",
      "Iteration 2523, loss = 0.21110800\n",
      "Iteration 2524, loss = 0.21110038\n",
      "Iteration 2525, loss = 0.21109221\n",
      "Iteration 2526, loss = 0.21108534\n",
      "Iteration 2527, loss = 0.21107916\n",
      "Iteration 2528, loss = 0.21107223\n",
      "Iteration 2529, loss = 0.21106598\n",
      "Iteration 2530, loss = 0.21105898\n",
      "Iteration 2531, loss = 0.21105091\n",
      "Iteration 2532, loss = 0.21104212\n",
      "Iteration 2533, loss = 0.21103655\n",
      "Iteration 2534, loss = 0.21102894\n",
      "Iteration 2535, loss = 0.21102151\n",
      "Iteration 2536, loss = 0.21101437\n",
      "Iteration 2537, loss = 0.21100742\n",
      "Iteration 2538, loss = 0.21100063\n",
      "Iteration 2539, loss = 0.21099359\n",
      "Iteration 2540, loss = 0.21098614\n",
      "Iteration 2541, loss = 0.21097998\n",
      "Iteration 2542, loss = 0.21097408\n",
      "Iteration 2543, loss = 0.21096761\n",
      "Iteration 2544, loss = 0.21095909\n",
      "Iteration 2545, loss = 0.21095123\n",
      "Iteration 2546, loss = 0.21094385\n",
      "Iteration 2547, loss = 0.21093757\n",
      "Iteration 2548, loss = 0.21093147\n",
      "Iteration 2549, loss = 0.21092493\n",
      "Iteration 2550, loss = 0.21091823\n",
      "Iteration 2551, loss = 0.21091073\n",
      "Iteration 2552, loss = 0.21090415\n",
      "Iteration 2553, loss = 0.21089759\n",
      "Iteration 2554, loss = 0.21088948\n",
      "Iteration 2555, loss = 0.21088310\n",
      "Iteration 2556, loss = 0.21087656\n",
      "Iteration 2557, loss = 0.21087091\n",
      "Iteration 2558, loss = 0.21086323\n",
      "Iteration 2559, loss = 0.21085402\n",
      "Iteration 2560, loss = 0.21084907\n",
      "Iteration 2561, loss = 0.21084197\n",
      "Iteration 2562, loss = 0.21083692\n",
      "Iteration 2563, loss = 0.21082888\n",
      "Iteration 2564, loss = 0.21082193\n",
      "Iteration 2565, loss = 0.21081568\n",
      "Iteration 2566, loss = 0.21080885\n",
      "Iteration 2567, loss = 0.21080166\n",
      "Iteration 2568, loss = 0.21079501\n",
      "Iteration 2569, loss = 0.21078542\n",
      "Iteration 2570, loss = 0.21077919\n",
      "Iteration 2571, loss = 0.21077143\n",
      "Iteration 2572, loss = 0.21076364\n",
      "Iteration 2573, loss = 0.21075514\n",
      "Iteration 2574, loss = 0.21074621\n",
      "Iteration 2575, loss = 0.21073795\n",
      "Iteration 2576, loss = 0.21073100\n",
      "Iteration 2577, loss = 0.21072410\n",
      "Iteration 2578, loss = 0.21071644\n",
      "Iteration 2579, loss = 0.21070630\n",
      "Iteration 2580, loss = 0.21069911\n",
      "Iteration 2581, loss = 0.21069157\n",
      "Iteration 2582, loss = 0.21068400\n",
      "Iteration 2583, loss = 0.21067493\n",
      "Iteration 2584, loss = 0.21066563\n",
      "Iteration 2585, loss = 0.21065567\n",
      "Iteration 2586, loss = 0.21064958\n",
      "Iteration 2587, loss = 0.21064034\n",
      "Iteration 2588, loss = 0.21063010\n",
      "Iteration 2589, loss = 0.21062199\n",
      "Iteration 2590, loss = 0.21061430\n",
      "Iteration 2591, loss = 0.21060615\n",
      "Iteration 2592, loss = 0.21059925\n",
      "Iteration 2593, loss = 0.21059171\n",
      "Iteration 2594, loss = 0.21058268\n",
      "Iteration 2595, loss = 0.21057279\n",
      "Iteration 2596, loss = 0.21056697\n",
      "Iteration 2597, loss = 0.21055894\n",
      "Iteration 2598, loss = 0.21055342\n",
      "Iteration 2599, loss = 0.21054480\n",
      "Iteration 2600, loss = 0.21053499\n",
      "Iteration 2601, loss = 0.21052771\n",
      "Iteration 2602, loss = 0.21051845\n",
      "Iteration 2603, loss = 0.21051026\n",
      "Iteration 2604, loss = 0.21050156\n",
      "Iteration 2605, loss = 0.21049286\n",
      "Iteration 2606, loss = 0.21048533\n",
      "Iteration 2607, loss = 0.21047655\n",
      "Iteration 2608, loss = 0.21046777\n",
      "Iteration 2609, loss = 0.21046019\n",
      "Iteration 2610, loss = 0.21045243\n",
      "Iteration 2611, loss = 0.21044392\n",
      "Iteration 2612, loss = 0.21043448\n",
      "Iteration 2613, loss = 0.21042536\n",
      "Iteration 2614, loss = 0.21041656\n",
      "Iteration 2615, loss = 0.21040850\n",
      "Iteration 2616, loss = 0.21040167\n",
      "Iteration 2617, loss = 0.21039335\n",
      "Iteration 2618, loss = 0.21038481\n",
      "Iteration 2619, loss = 0.21037709\n",
      "Iteration 2620, loss = 0.21036938\n",
      "Iteration 2621, loss = 0.21036099\n",
      "Iteration 2622, loss = 0.21035212\n",
      "Iteration 2623, loss = 0.21034358\n",
      "Iteration 2624, loss = 0.21033613\n",
      "Iteration 2625, loss = 0.21032826\n",
      "Iteration 2626, loss = 0.21031920\n",
      "Iteration 2627, loss = 0.21031103\n",
      "Iteration 2628, loss = 0.21030539\n",
      "Iteration 2629, loss = 0.21029733\n",
      "Iteration 2630, loss = 0.21028846\n",
      "Iteration 2631, loss = 0.21027973\n",
      "Iteration 2632, loss = 0.21027032\n",
      "Iteration 2633, loss = 0.21026252\n",
      "Iteration 2634, loss = 0.21025625\n",
      "Iteration 2635, loss = 0.21024775\n",
      "Iteration 2636, loss = 0.21023711\n",
      "Iteration 2637, loss = 0.21023302\n",
      "Iteration 2638, loss = 0.21022611\n",
      "Iteration 2639, loss = 0.21021759\n",
      "Iteration 2640, loss = 0.21020943\n",
      "Iteration 2641, loss = 0.21020294\n",
      "Iteration 2642, loss = 0.21019574\n",
      "Iteration 2643, loss = 0.21019111\n",
      "Iteration 2644, loss = 0.21018439\n",
      "Iteration 2645, loss = 0.21018109\n",
      "Iteration 2646, loss = 0.21017365\n",
      "Iteration 2647, loss = 0.21016417\n",
      "Iteration 2648, loss = 0.21016093\n",
      "Iteration 2649, loss = 0.21015582\n",
      "Iteration 2650, loss = 0.21014954\n",
      "Iteration 2651, loss = 0.21014416\n",
      "Iteration 2652, loss = 0.21013708\n",
      "Iteration 2653, loss = 0.21012880\n",
      "Iteration 2654, loss = 0.21012116\n",
      "Iteration 2655, loss = 0.21011755\n",
      "Iteration 2656, loss = 0.21011093\n",
      "Iteration 2657, loss = 0.21010275\n",
      "Iteration 2658, loss = 0.21009472\n",
      "Iteration 2659, loss = 0.21008670\n",
      "Iteration 2660, loss = 0.21008381\n",
      "Iteration 2661, loss = 0.21007791\n",
      "Iteration 2662, loss = 0.21007209\n",
      "Iteration 2663, loss = 0.21006626\n",
      "Iteration 2664, loss = 0.21005698\n",
      "Iteration 2665, loss = 0.21004789\n",
      "Iteration 2666, loss = 0.21004055\n",
      "Iteration 2667, loss = 0.21003522\n",
      "Iteration 2668, loss = 0.21002792\n",
      "Iteration 2669, loss = 0.21001967\n",
      "Iteration 2670, loss = 0.21001216\n",
      "Iteration 2671, loss = 0.21000095\n",
      "Iteration 2672, loss = 0.20999635\n",
      "Iteration 2673, loss = 0.20998934\n",
      "Iteration 2674, loss = 0.20998004\n",
      "Iteration 2675, loss = 0.20997293\n",
      "Iteration 2676, loss = 0.20996596\n",
      "Iteration 2677, loss = 0.20995613\n",
      "Iteration 2678, loss = 0.20994558\n",
      "Iteration 2679, loss = 0.20993867\n",
      "Iteration 2680, loss = 0.20993223\n",
      "Iteration 2681, loss = 0.20992535\n",
      "Iteration 2682, loss = 0.20991629\n",
      "Iteration 2683, loss = 0.20990474\n",
      "Iteration 2684, loss = 0.20989898\n",
      "Iteration 2685, loss = 0.20989034\n",
      "Iteration 2686, loss = 0.20987981\n",
      "Iteration 2687, loss = 0.20987316\n",
      "Iteration 2688, loss = 0.20986457\n",
      "Iteration 2689, loss = 0.20985358\n",
      "Iteration 2690, loss = 0.20984653\n",
      "Iteration 2691, loss = 0.20983878\n",
      "Iteration 2692, loss = 0.20983030\n",
      "Iteration 2693, loss = 0.20982313\n",
      "Iteration 2694, loss = 0.20981698\n",
      "Iteration 2695, loss = 0.20980753\n",
      "Iteration 2696, loss = 0.20980164\n",
      "Iteration 2697, loss = 0.20979574\n",
      "Iteration 2698, loss = 0.20978729\n",
      "Iteration 2699, loss = 0.20977868\n",
      "Iteration 2700, loss = 0.20977066\n",
      "Iteration 2701, loss = 0.20976477\n",
      "Iteration 2702, loss = 0.20975724\n",
      "Iteration 2703, loss = 0.20974907\n",
      "Iteration 2704, loss = 0.20974087\n",
      "Iteration 2705, loss = 0.20973342\n",
      "Iteration 2706, loss = 0.20972388\n",
      "Iteration 2707, loss = 0.20971663\n",
      "Iteration 2708, loss = 0.20971015\n",
      "Iteration 2709, loss = 0.20970209\n",
      "Iteration 2710, loss = 0.20969521\n",
      "Iteration 2711, loss = 0.20968724\n",
      "Iteration 2712, loss = 0.20967787\n",
      "Iteration 2713, loss = 0.20967245\n",
      "Iteration 2714, loss = 0.20966524\n",
      "Iteration 2715, loss = 0.20965722\n",
      "Iteration 2716, loss = 0.20965216\n",
      "Iteration 2717, loss = 0.20964076\n",
      "Iteration 2718, loss = 0.20963330\n",
      "Iteration 2719, loss = 0.20962633\n",
      "Iteration 2720, loss = 0.20961836\n",
      "Iteration 2721, loss = 0.20961043\n",
      "Iteration 2722, loss = 0.20959932\n",
      "Iteration 2723, loss = 0.20958903\n",
      "Iteration 2724, loss = 0.20958005\n",
      "Iteration 2725, loss = 0.20957183\n",
      "Iteration 2726, loss = 0.20956246\n",
      "Iteration 2727, loss = 0.20954590\n",
      "Iteration 2728, loss = 0.20952556\n",
      "Iteration 2729, loss = 0.20950440\n",
      "Iteration 2730, loss = 0.20948773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2731, loss = 0.20947646\n",
      "Iteration 2732, loss = 0.20946490\n",
      "Iteration 2733, loss = 0.20945183\n",
      "Iteration 2734, loss = 0.20944198\n",
      "Iteration 2735, loss = 0.20943029\n",
      "Iteration 2736, loss = 0.20942075\n",
      "Iteration 2737, loss = 0.20941542\n",
      "Iteration 2738, loss = 0.20940922\n",
      "Iteration 2739, loss = 0.20940008\n",
      "Iteration 2740, loss = 0.20939108\n",
      "Iteration 2741, loss = 0.20938439\n",
      "Iteration 2742, loss = 0.20937717\n",
      "Iteration 2743, loss = 0.20936958\n",
      "Iteration 2744, loss = 0.20936148\n",
      "Iteration 2745, loss = 0.20935314\n",
      "Iteration 2746, loss = 0.20934478\n",
      "Iteration 2747, loss = 0.20933793\n",
      "Iteration 2748, loss = 0.20932932\n",
      "Iteration 2749, loss = 0.20932196\n",
      "Iteration 2750, loss = 0.20931384\n",
      "Iteration 2751, loss = 0.20930829\n",
      "Iteration 2752, loss = 0.20929941\n",
      "Iteration 2753, loss = 0.20929125\n",
      "Iteration 2754, loss = 0.20928330\n",
      "Iteration 2755, loss = 0.20927631\n",
      "Iteration 2756, loss = 0.20926859\n",
      "Iteration 2757, loss = 0.20926018\n",
      "Iteration 2758, loss = 0.20925262\n",
      "Iteration 2759, loss = 0.20924606\n",
      "Iteration 2760, loss = 0.20923798\n",
      "Iteration 2761, loss = 0.20922947\n",
      "Iteration 2762, loss = 0.20921972\n",
      "Iteration 2763, loss = 0.20921160\n",
      "Iteration 2764, loss = 0.20920471\n",
      "Iteration 2765, loss = 0.20919754\n",
      "Iteration 2766, loss = 0.20918845\n",
      "Iteration 2767, loss = 0.20918054\n",
      "Iteration 2768, loss = 0.20917309\n",
      "Iteration 2769, loss = 0.20916562\n",
      "Iteration 2770, loss = 0.20915749\n",
      "Iteration 2771, loss = 0.20914993\n",
      "Iteration 2772, loss = 0.20914242\n",
      "Iteration 2773, loss = 0.20913616\n",
      "Iteration 2774, loss = 0.20912726\n",
      "Iteration 2775, loss = 0.20911823\n",
      "Iteration 2776, loss = 0.20911029\n",
      "Iteration 2777, loss = 0.20910395\n",
      "Iteration 2778, loss = 0.20909563\n",
      "Iteration 2779, loss = 0.20908663\n",
      "Iteration 2780, loss = 0.20907843\n",
      "Iteration 2781, loss = 0.20907000\n",
      "Iteration 2782, loss = 0.20906430\n",
      "Iteration 2783, loss = 0.20905845\n",
      "Iteration 2784, loss = 0.20904766\n",
      "Iteration 2785, loss = 0.20904095\n",
      "Iteration 2786, loss = 0.20903262\n",
      "Iteration 2787, loss = 0.20902409\n",
      "Iteration 2788, loss = 0.20901768\n",
      "Iteration 2789, loss = 0.20901119\n",
      "Iteration 2790, loss = 0.20900336\n",
      "Iteration 2791, loss = 0.20899599\n",
      "Iteration 2792, loss = 0.20898821\n",
      "Iteration 2793, loss = 0.20898013\n",
      "Iteration 2794, loss = 0.20897421\n",
      "Iteration 2795, loss = 0.20896719\n",
      "Iteration 2796, loss = 0.20895945\n",
      "Iteration 2797, loss = 0.20895073\n",
      "Iteration 2798, loss = 0.20894411\n",
      "Iteration 2799, loss = 0.20893671\n",
      "Iteration 2800, loss = 0.20893015\n",
      "Iteration 2801, loss = 0.20892182\n",
      "Iteration 2802, loss = 0.20891245\n",
      "Iteration 2803, loss = 0.20890467\n",
      "Iteration 2804, loss = 0.20889786\n",
      "Iteration 2805, loss = 0.20888943\n",
      "Iteration 2806, loss = 0.20888365\n",
      "Iteration 2807, loss = 0.20887760\n",
      "Iteration 2808, loss = 0.20887216\n",
      "Iteration 2809, loss = 0.20886639\n",
      "Iteration 2810, loss = 0.20885991\n",
      "Iteration 2811, loss = 0.20885367\n",
      "Iteration 2812, loss = 0.20884859\n",
      "Iteration 2813, loss = 0.20884342\n",
      "Iteration 2814, loss = 0.20883482\n",
      "Iteration 2815, loss = 0.20882934\n",
      "Iteration 2816, loss = 0.20882429\n",
      "Iteration 2817, loss = 0.20881924\n",
      "Iteration 2818, loss = 0.20881176\n",
      "Iteration 2819, loss = 0.20880530\n",
      "Iteration 2820, loss = 0.20879973\n",
      "Iteration 2821, loss = 0.20879600\n",
      "Iteration 2822, loss = 0.20878852\n",
      "Iteration 2823, loss = 0.20878112\n",
      "Iteration 2824, loss = 0.20877315\n",
      "Iteration 2825, loss = 0.20876569\n",
      "Iteration 2826, loss = 0.20875956\n",
      "Iteration 2827, loss = 0.20875325\n",
      "Iteration 2828, loss = 0.20874766\n",
      "Iteration 2829, loss = 0.20874120\n",
      "Iteration 2830, loss = 0.20873404\n",
      "Iteration 2831, loss = 0.20872800\n",
      "Iteration 2832, loss = 0.20872259\n",
      "Iteration 2833, loss = 0.20871628\n",
      "Iteration 2834, loss = 0.20871183\n",
      "Iteration 2835, loss = 0.20870496\n",
      "Iteration 2836, loss = 0.20869975\n",
      "Iteration 2837, loss = 0.20869370\n",
      "Iteration 2838, loss = 0.20868707\n",
      "Iteration 2839, loss = 0.20868140\n",
      "Iteration 2840, loss = 0.20867434\n",
      "Iteration 2841, loss = 0.20866954\n",
      "Iteration 2842, loss = 0.20866422\n",
      "Iteration 2843, loss = 0.20865853\n",
      "Iteration 2844, loss = 0.20865114\n",
      "Iteration 2845, loss = 0.20864509\n",
      "Iteration 2846, loss = 0.20863908\n",
      "Iteration 2847, loss = 0.20863437\n",
      "Iteration 2848, loss = 0.20862963\n",
      "Iteration 2849, loss = 0.20862513\n",
      "Iteration 2850, loss = 0.20861913\n",
      "Iteration 2851, loss = 0.20861099\n",
      "Iteration 2852, loss = 0.20860294\n",
      "Iteration 2853, loss = 0.20859749\n",
      "Iteration 2854, loss = 0.20858974\n",
      "Iteration 2855, loss = 0.20858457\n",
      "Iteration 2856, loss = 0.20857810\n",
      "Iteration 2857, loss = 0.20857221\n",
      "Iteration 2858, loss = 0.20856817\n",
      "Iteration 2859, loss = 0.20856199\n",
      "Iteration 2860, loss = 0.20855494\n",
      "Iteration 2861, loss = 0.20854957\n",
      "Iteration 2862, loss = 0.20854500\n",
      "Iteration 2863, loss = 0.20853853\n",
      "Iteration 2864, loss = 0.20853099\n",
      "Iteration 2865, loss = 0.20852681\n",
      "Iteration 2866, loss = 0.20852157\n",
      "Iteration 2867, loss = 0.20851483\n",
      "Iteration 2868, loss = 0.20850803\n",
      "Iteration 2869, loss = 0.20850262\n",
      "Iteration 2870, loss = 0.20849620\n",
      "Iteration 2871, loss = 0.20849019\n",
      "Iteration 2872, loss = 0.20848510\n",
      "Iteration 2873, loss = 0.20847952\n",
      "Iteration 2874, loss = 0.20847336\n",
      "Iteration 2875, loss = 0.20846600\n",
      "Iteration 2876, loss = 0.20846118\n",
      "Iteration 2877, loss = 0.20845554\n",
      "Iteration 2878, loss = 0.20844878\n",
      "Iteration 2879, loss = 0.20844146\n",
      "Iteration 2880, loss = 0.20843717\n",
      "Iteration 2881, loss = 0.20843165\n",
      "Iteration 2882, loss = 0.20842776\n",
      "Iteration 2883, loss = 0.20842050\n",
      "Iteration 2884, loss = 0.20841403\n",
      "Iteration 2885, loss = 0.20840814\n",
      "Iteration 2886, loss = 0.20840303\n",
      "Iteration 2887, loss = 0.20839785\n",
      "Iteration 2888, loss = 0.20839028\n",
      "Iteration 2889, loss = 0.20838380\n",
      "Iteration 2890, loss = 0.20837985\n",
      "Iteration 2891, loss = 0.20837480\n",
      "Iteration 2892, loss = 0.20837061\n",
      "Iteration 2893, loss = 0.20836244\n",
      "Iteration 2894, loss = 0.20835510\n",
      "Iteration 2895, loss = 0.20835094\n",
      "Iteration 2896, loss = 0.20834566\n",
      "Iteration 2897, loss = 0.20834004\n",
      "Iteration 2898, loss = 0.20833509\n",
      "Iteration 2899, loss = 0.20832972\n",
      "Iteration 2900, loss = 0.20832117\n",
      "Iteration 2901, loss = 0.20831599\n",
      "Iteration 2902, loss = 0.20831030\n",
      "Iteration 2903, loss = 0.20830539\n",
      "Iteration 2904, loss = 0.20830031\n",
      "Iteration 2905, loss = 0.20829379\n",
      "Iteration 2906, loss = 0.20828723\n",
      "Iteration 2907, loss = 0.20827998\n",
      "Iteration 2908, loss = 0.20827499\n",
      "Iteration 2909, loss = 0.20827083\n",
      "Iteration 2910, loss = 0.20826494\n",
      "Iteration 2911, loss = 0.20825729\n",
      "Iteration 2912, loss = 0.20825189\n",
      "Iteration 2913, loss = 0.20824685\n",
      "Iteration 2914, loss = 0.20824052\n",
      "Iteration 2915, loss = 0.20823291\n",
      "Iteration 2916, loss = 0.20822798\n",
      "Iteration 2917, loss = 0.20822228\n",
      "Iteration 2918, loss = 0.20821533\n",
      "Iteration 2919, loss = 0.20821137\n",
      "Iteration 2920, loss = 0.20820509\n",
      "Iteration 2921, loss = 0.20819849\n",
      "Iteration 2922, loss = 0.20819351\n",
      "Iteration 2923, loss = 0.20818822\n",
      "Iteration 2924, loss = 0.20818352\n",
      "Iteration 2925, loss = 0.20817486\n",
      "Iteration 2926, loss = 0.20816880\n",
      "Iteration 2927, loss = 0.20816671\n",
      "Iteration 2928, loss = 0.20816135\n",
      "Iteration 2929, loss = 0.20815365\n",
      "Iteration 2930, loss = 0.20814735\n",
      "Iteration 2931, loss = 0.20814167\n",
      "Iteration 2932, loss = 0.20813631\n",
      "Iteration 2933, loss = 0.20813011\n",
      "Iteration 2934, loss = 0.20812430\n",
      "Iteration 2935, loss = 0.20811825\n",
      "Iteration 2936, loss = 0.20811197\n",
      "Iteration 2937, loss = 0.20810468\n",
      "Iteration 2938, loss = 0.20810031\n",
      "Iteration 2939, loss = 0.20809338\n",
      "Iteration 2940, loss = 0.20808539\n",
      "Iteration 2941, loss = 0.20808213\n",
      "Iteration 2942, loss = 0.20807490\n",
      "Iteration 2943, loss = 0.20807189\n",
      "Iteration 2944, loss = 0.20806725\n",
      "Iteration 2945, loss = 0.20806002\n",
      "Iteration 2946, loss = 0.20805323\n",
      "Iteration 2947, loss = 0.20804766\n",
      "Iteration 2948, loss = 0.20804110\n",
      "Iteration 2949, loss = 0.20803553\n",
      "Iteration 2950, loss = 0.20802739\n",
      "Iteration 2951, loss = 0.20802312\n",
      "Iteration 2952, loss = 0.20801564\n",
      "Iteration 2953, loss = 0.20800896\n",
      "Iteration 2954, loss = 0.20800249\n",
      "Iteration 2955, loss = 0.20799808\n",
      "Iteration 2956, loss = 0.20799286\n",
      "Iteration 2957, loss = 0.20798658\n",
      "Iteration 2958, loss = 0.20797976\n",
      "Iteration 2959, loss = 0.20797653\n",
      "Iteration 2960, loss = 0.20797088\n",
      "Iteration 2961, loss = 0.20796579\n",
      "Iteration 2962, loss = 0.20796113\n",
      "Iteration 2963, loss = 0.20795403\n",
      "Iteration 2964, loss = 0.20794647\n",
      "Iteration 2965, loss = 0.20794298\n",
      "Iteration 2966, loss = 0.20793760\n",
      "Iteration 2967, loss = 0.20793052\n",
      "Iteration 2968, loss = 0.20792307\n",
      "Iteration 2969, loss = 0.20791703\n",
      "Iteration 2970, loss = 0.20791152\n",
      "Iteration 2971, loss = 0.20790679\n",
      "Iteration 2972, loss = 0.20790282\n",
      "Iteration 2973, loss = 0.20789585\n",
      "Iteration 2974, loss = 0.20788924\n",
      "Iteration 2975, loss = 0.20788184\n",
      "Iteration 2976, loss = 0.20787542\n",
      "Iteration 2977, loss = 0.20787029\n",
      "Iteration 2978, loss = 0.20786386\n",
      "Iteration 2979, loss = 0.20785718\n",
      "Iteration 2980, loss = 0.20785200\n",
      "Iteration 2981, loss = 0.20784482\n",
      "Iteration 2982, loss = 0.20783999\n",
      "Iteration 2983, loss = 0.20783504\n",
      "Iteration 2984, loss = 0.20782866\n",
      "Iteration 2985, loss = 0.20782299\n",
      "Iteration 2986, loss = 0.20781723\n",
      "Iteration 2987, loss = 0.20781033\n",
      "Iteration 2988, loss = 0.20780676\n",
      "Iteration 2989, loss = 0.20779966\n",
      "Iteration 2990, loss = 0.20779318\n",
      "Iteration 2991, loss = 0.20778642\n",
      "Iteration 2992, loss = 0.20778078\n",
      "Iteration 2993, loss = 0.20777536\n",
      "Iteration 2994, loss = 0.20777094\n",
      "Iteration 2995, loss = 0.20776536\n",
      "Iteration 2996, loss = 0.20775811\n",
      "Iteration 2997, loss = 0.20775016\n",
      "Iteration 2998, loss = 0.20774561\n",
      "Iteration 2999, loss = 0.20774121\n",
      "Iteration 3000, loss = 0.20773404\n",
      "Iteration 3001, loss = 0.20772663\n",
      "Iteration 3002, loss = 0.20772214\n",
      "Iteration 3003, loss = 0.20771582\n",
      "Iteration 3004, loss = 0.20771280\n",
      "Iteration 3005, loss = 0.20770589\n",
      "Iteration 3006, loss = 0.20769903\n",
      "Iteration 3007, loss = 0.20769278\n",
      "Iteration 3008, loss = 0.20768743\n",
      "Iteration 3009, loss = 0.20768130\n",
      "Iteration 3010, loss = 0.20767601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3011, loss = 0.20767087\n",
      "Iteration 3012, loss = 0.20766634\n",
      "Iteration 3013, loss = 0.20765883\n",
      "Iteration 3014, loss = 0.20765189\n",
      "Iteration 3015, loss = 0.20764834\n",
      "Iteration 3016, loss = 0.20764275\n",
      "Iteration 3017, loss = 0.20763649\n",
      "Iteration 3018, loss = 0.20763157\n",
      "Iteration 3019, loss = 0.20762672\n",
      "Iteration 3020, loss = 0.20761975\n",
      "Iteration 3021, loss = 0.20761333\n",
      "Iteration 3022, loss = 0.20760886\n",
      "Iteration 3023, loss = 0.20760315\n",
      "Iteration 3024, loss = 0.20759901\n",
      "Iteration 3025, loss = 0.20759100\n",
      "Iteration 3026, loss = 0.20758337\n",
      "Iteration 3027, loss = 0.20757821\n",
      "Iteration 3028, loss = 0.20757200\n",
      "Iteration 3029, loss = 0.20756502\n",
      "Iteration 3030, loss = 0.20756042\n",
      "Iteration 3031, loss = 0.20755688\n",
      "Iteration 3032, loss = 0.20754948\n",
      "Iteration 3033, loss = 0.20754440\n",
      "Iteration 3034, loss = 0.20753739\n",
      "Iteration 3035, loss = 0.20753364\n",
      "Iteration 3036, loss = 0.20752641\n",
      "Iteration 3037, loss = 0.20751862\n",
      "Iteration 3038, loss = 0.20751507\n",
      "Iteration 3039, loss = 0.20750920\n",
      "Iteration 3040, loss = 0.20750169\n",
      "Iteration 3041, loss = 0.20749530\n",
      "Iteration 3042, loss = 0.20748889\n",
      "Iteration 3043, loss = 0.20748379\n",
      "Iteration 3044, loss = 0.20747910\n",
      "Iteration 3045, loss = 0.20747263\n",
      "Iteration 3046, loss = 0.20746753\n",
      "Iteration 3047, loss = 0.20746281\n",
      "Iteration 3048, loss = 0.20745750\n",
      "Iteration 3049, loss = 0.20745238\n",
      "Iteration 3050, loss = 0.20744413\n",
      "Iteration 3051, loss = 0.20743806\n",
      "Iteration 3052, loss = 0.20743356\n",
      "Iteration 3053, loss = 0.20742854\n",
      "Iteration 3054, loss = 0.20742316\n",
      "Iteration 3055, loss = 0.20741505\n",
      "Iteration 3056, loss = 0.20741058\n",
      "Iteration 3057, loss = 0.20740627\n",
      "Iteration 3058, loss = 0.20740344\n",
      "Iteration 3059, loss = 0.20739710\n",
      "Iteration 3060, loss = 0.20738881\n",
      "Iteration 3061, loss = 0.20738120\n",
      "Iteration 3062, loss = 0.20737457\n",
      "Iteration 3063, loss = 0.20736983\n",
      "Iteration 3064, loss = 0.20736683\n",
      "Iteration 3065, loss = 0.20736080\n",
      "Iteration 3066, loss = 0.20735202\n",
      "Iteration 3067, loss = 0.20734726\n",
      "Iteration 3068, loss = 0.20734385\n",
      "Iteration 3069, loss = 0.20733588\n",
      "Iteration 3070, loss = 0.20732953\n",
      "Iteration 3071, loss = 0.20732462\n",
      "Iteration 3072, loss = 0.20731786\n",
      "Iteration 3073, loss = 0.20731337\n",
      "Iteration 3074, loss = 0.20731015\n",
      "Iteration 3075, loss = 0.20730438\n",
      "Iteration 3076, loss = 0.20729767\n",
      "Iteration 3077, loss = 0.20729262\n",
      "Iteration 3078, loss = 0.20728812\n",
      "Iteration 3079, loss = 0.20728148\n",
      "Iteration 3080, loss = 0.20727298\n",
      "Iteration 3081, loss = 0.20726932\n",
      "Iteration 3082, loss = 0.20726528\n",
      "Iteration 3083, loss = 0.20725848\n",
      "Iteration 3084, loss = 0.20725121\n",
      "Iteration 3085, loss = 0.20724588\n",
      "Iteration 3086, loss = 0.20724204\n",
      "Iteration 3087, loss = 0.20723692\n",
      "Iteration 3088, loss = 0.20723053\n",
      "Iteration 3089, loss = 0.20722274\n",
      "Iteration 3090, loss = 0.20721666\n",
      "Iteration 3091, loss = 0.20721183\n",
      "Iteration 3092, loss = 0.20720656\n",
      "Iteration 3093, loss = 0.20720008\n",
      "Iteration 3094, loss = 0.20719562\n",
      "Iteration 3095, loss = 0.20719217\n",
      "Iteration 3096, loss = 0.20718716\n",
      "Iteration 3097, loss = 0.20718005\n",
      "Iteration 3098, loss = 0.20717211\n",
      "Iteration 3099, loss = 0.20716811\n",
      "Iteration 3100, loss = 0.20716403\n",
      "Iteration 3101, loss = 0.20715774\n",
      "Iteration 3102, loss = 0.20714843\n",
      "Iteration 3103, loss = 0.20714640\n",
      "Iteration 3104, loss = 0.20714258\n",
      "Iteration 3105, loss = 0.20713604\n",
      "Iteration 3106, loss = 0.20713036\n",
      "Iteration 3107, loss = 0.20712227\n",
      "Iteration 3108, loss = 0.20711847\n",
      "Iteration 3109, loss = 0.20711370\n",
      "Iteration 3110, loss = 0.20710971\n",
      "Iteration 3111, loss = 0.20710312\n",
      "Iteration 3112, loss = 0.20709388\n",
      "Iteration 3113, loss = 0.20709035\n",
      "Iteration 3114, loss = 0.20708789\n",
      "Iteration 3115, loss = 0.20708279\n",
      "Iteration 3116, loss = 0.20707486\n",
      "Iteration 3117, loss = 0.20706896\n",
      "Iteration 3118, loss = 0.20706256\n",
      "Iteration 3119, loss = 0.20705807\n",
      "Iteration 3120, loss = 0.20705229\n",
      "Iteration 3121, loss = 0.20704641\n",
      "Iteration 3122, loss = 0.20703975\n",
      "Iteration 3123, loss = 0.20703071\n",
      "Iteration 3124, loss = 0.20702965\n",
      "Iteration 3125, loss = 0.20702562\n",
      "Iteration 3126, loss = 0.20701815\n",
      "Iteration 3127, loss = 0.20701441\n",
      "Iteration 3128, loss = 0.20700824\n",
      "Iteration 3129, loss = 0.20700160\n",
      "Iteration 3130, loss = 0.20699444\n",
      "Iteration 3131, loss = 0.20699048\n",
      "Iteration 3132, loss = 0.20698465\n",
      "Iteration 3133, loss = 0.20698023\n",
      "Iteration 3134, loss = 0.20697514\n",
      "Iteration 3135, loss = 0.20696693\n",
      "Iteration 3136, loss = 0.20696048\n",
      "Iteration 3137, loss = 0.20695770\n",
      "Iteration 3138, loss = 0.20695251\n",
      "Iteration 3139, loss = 0.20694819\n",
      "Iteration 3140, loss = 0.20694114\n",
      "Iteration 3141, loss = 0.20693448\n",
      "Iteration 3142, loss = 0.20692747\n",
      "Iteration 3143, loss = 0.20692303\n",
      "Iteration 3144, loss = 0.20691955\n",
      "Iteration 3145, loss = 0.20691228\n",
      "Iteration 3146, loss = 0.20690588\n",
      "Iteration 3147, loss = 0.20690094\n",
      "Iteration 3148, loss = 0.20689613\n",
      "Iteration 3149, loss = 0.20689106\n",
      "Iteration 3150, loss = 0.20688597\n",
      "Iteration 3151, loss = 0.20688050\n",
      "Iteration 3152, loss = 0.20687617\n",
      "Iteration 3153, loss = 0.20686955\n",
      "Iteration 3154, loss = 0.20686325\n",
      "Iteration 3155, loss = 0.20685810\n",
      "Iteration 3156, loss = 0.20685250\n",
      "Iteration 3157, loss = 0.20684601\n",
      "Iteration 3158, loss = 0.20684277\n",
      "Iteration 3159, loss = 0.20683712\n",
      "Iteration 3160, loss = 0.20683028\n",
      "Iteration 3161, loss = 0.20682437\n",
      "Iteration 3162, loss = 0.20682110\n",
      "Iteration 3163, loss = 0.20681552\n",
      "Iteration 3164, loss = 0.20680945\n",
      "Iteration 3165, loss = 0.20680193\n",
      "Iteration 3166, loss = 0.20679706\n",
      "Iteration 3167, loss = 0.20679285\n",
      "Iteration 3168, loss = 0.20678781\n",
      "Iteration 3169, loss = 0.20678170\n",
      "Iteration 3170, loss = 0.20677463\n",
      "Iteration 3171, loss = 0.20676970\n",
      "Iteration 3172, loss = 0.20676651\n",
      "Iteration 3173, loss = 0.20676046\n",
      "Iteration 3174, loss = 0.20675340\n",
      "Iteration 3175, loss = 0.20674816\n",
      "Iteration 3176, loss = 0.20674225\n",
      "Iteration 3177, loss = 0.20673978\n",
      "Iteration 3178, loss = 0.20673321\n",
      "Iteration 3179, loss = 0.20672951\n",
      "Iteration 3180, loss = 0.20672517\n",
      "Iteration 3181, loss = 0.20671731\n",
      "Iteration 3182, loss = 0.20671090\n",
      "Iteration 3183, loss = 0.20670559\n",
      "Iteration 3184, loss = 0.20669924\n",
      "Iteration 3185, loss = 0.20669700\n",
      "Iteration 3186, loss = 0.20669030\n",
      "Iteration 3187, loss = 0.20668266\n",
      "Iteration 3188, loss = 0.20667949\n",
      "Iteration 3189, loss = 0.20667460\n",
      "Iteration 3190, loss = 0.20667030\n",
      "Iteration 3191, loss = 0.20666325\n",
      "Iteration 3192, loss = 0.20665677\n",
      "Iteration 3193, loss = 0.20665255\n",
      "Iteration 3194, loss = 0.20664562\n",
      "Iteration 3195, loss = 0.20663780\n",
      "Iteration 3196, loss = 0.20663559\n",
      "Iteration 3197, loss = 0.20663196\n",
      "Iteration 3198, loss = 0.20662655\n",
      "Iteration 3199, loss = 0.20662084\n",
      "Iteration 3200, loss = 0.20661421\n",
      "Iteration 3201, loss = 0.20660749\n",
      "Iteration 3202, loss = 0.20660319\n",
      "Iteration 3203, loss = 0.20659869\n",
      "Iteration 3204, loss = 0.20659294\n",
      "Iteration 3205, loss = 0.20658632\n",
      "Iteration 3206, loss = 0.20658268\n",
      "Iteration 3207, loss = 0.20657739\n",
      "Iteration 3208, loss = 0.20656982\n",
      "Iteration 3209, loss = 0.20656609\n",
      "Iteration 3210, loss = 0.20656204\n",
      "Iteration 3211, loss = 0.20655515\n",
      "Iteration 3212, loss = 0.20654909\n",
      "Iteration 3213, loss = 0.20654409\n",
      "Iteration 3214, loss = 0.20653912\n",
      "Iteration 3215, loss = 0.20653217\n",
      "Iteration 3216, loss = 0.20652900\n",
      "Iteration 3217, loss = 0.20652488\n",
      "Iteration 3218, loss = 0.20651980\n",
      "Iteration 3219, loss = 0.20651347\n",
      "Iteration 3220, loss = 0.20650811\n",
      "Iteration 3221, loss = 0.20650267\n",
      "Iteration 3222, loss = 0.20649738\n",
      "Iteration 3223, loss = 0.20649024\n",
      "Iteration 3224, loss = 0.20648648\n",
      "Iteration 3225, loss = 0.20648102\n",
      "Iteration 3226, loss = 0.20647452\n",
      "Iteration 3227, loss = 0.20646892\n",
      "Iteration 3228, loss = 0.20646350\n",
      "Iteration 3229, loss = 0.20645997\n",
      "Iteration 3230, loss = 0.20645526\n",
      "Iteration 3231, loss = 0.20644641\n",
      "Iteration 3232, loss = 0.20644040\n",
      "Iteration 3233, loss = 0.20643581\n",
      "Iteration 3234, loss = 0.20643123\n",
      "Iteration 3235, loss = 0.20642424\n",
      "Iteration 3236, loss = 0.20641772\n",
      "Iteration 3237, loss = 0.20641245\n",
      "Iteration 3238, loss = 0.20640984\n",
      "Iteration 3239, loss = 0.20640403\n",
      "Iteration 3240, loss = 0.20639415\n",
      "Iteration 3241, loss = 0.20638803\n",
      "Iteration 3242, loss = 0.20638447\n",
      "Iteration 3243, loss = 0.20637941\n",
      "Iteration 3244, loss = 0.20637207\n",
      "Iteration 3245, loss = 0.20636703\n",
      "Iteration 3246, loss = 0.20636434\n",
      "Iteration 3247, loss = 0.20635766\n",
      "Iteration 3248, loss = 0.20634978\n",
      "Iteration 3249, loss = 0.20634544\n",
      "Iteration 3250, loss = 0.20633911\n",
      "Iteration 3251, loss = 0.20633403\n",
      "Iteration 3252, loss = 0.20632937\n",
      "Iteration 3253, loss = 0.20632326\n",
      "Iteration 3254, loss = 0.20631552\n",
      "Iteration 3255, loss = 0.20630895\n",
      "Iteration 3256, loss = 0.20630724\n",
      "Iteration 3257, loss = 0.20630167\n",
      "Iteration 3258, loss = 0.20629603\n",
      "Iteration 3259, loss = 0.20628911\n",
      "Iteration 3260, loss = 0.20628046\n",
      "Iteration 3261, loss = 0.20627787\n",
      "Iteration 3262, loss = 0.20627226\n",
      "Iteration 3263, loss = 0.20626500\n",
      "Iteration 3264, loss = 0.20625935\n",
      "Iteration 3265, loss = 0.20625453\n",
      "Iteration 3266, loss = 0.20624641\n",
      "Iteration 3267, loss = 0.20624241\n",
      "Iteration 3268, loss = 0.20623764\n",
      "Iteration 3269, loss = 0.20623115\n",
      "Iteration 3270, loss = 0.20622543\n",
      "Iteration 3271, loss = 0.20621814\n",
      "Iteration 3272, loss = 0.20621138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3273, loss = 0.20620601\n",
      "Iteration 3274, loss = 0.20620108\n",
      "Iteration 3275, loss = 0.20619562\n",
      "Iteration 3276, loss = 0.20618899\n",
      "Iteration 3277, loss = 0.20618188\n",
      "Iteration 3278, loss = 0.20617507\n",
      "Iteration 3279, loss = 0.20616685\n",
      "Iteration 3280, loss = 0.20616199\n",
      "Iteration 3281, loss = 0.20615723\n",
      "Iteration 3282, loss = 0.20615105\n",
      "Iteration 3283, loss = 0.20614526\n",
      "Iteration 3284, loss = 0.20613848\n",
      "Iteration 3285, loss = 0.20613475\n",
      "Iteration 3286, loss = 0.20612791\n",
      "Iteration 3287, loss = 0.20612215\n",
      "Iteration 3288, loss = 0.20611492\n",
      "Iteration 3289, loss = 0.20611187\n",
      "Iteration 3290, loss = 0.20610562\n",
      "Iteration 3291, loss = 0.20609873\n",
      "Iteration 3292, loss = 0.20609186\n",
      "Iteration 3293, loss = 0.20608605\n",
      "Iteration 3294, loss = 0.20608138\n",
      "Iteration 3295, loss = 0.20607565\n",
      "Iteration 3296, loss = 0.20606760\n",
      "Iteration 3297, loss = 0.20606325\n",
      "Iteration 3298, loss = 0.20605915\n",
      "Iteration 3299, loss = 0.20605320\n",
      "Iteration 3300, loss = 0.20604662\n",
      "Iteration 3301, loss = 0.20603829\n",
      "Iteration 3302, loss = 0.20603149\n",
      "Iteration 3303, loss = 0.20602754\n",
      "Iteration 3304, loss = 0.20602387\n",
      "Iteration 3305, loss = 0.20601619\n",
      "Iteration 3306, loss = 0.20600771\n",
      "Iteration 3307, loss = 0.20600023\n",
      "Iteration 3308, loss = 0.20599590\n",
      "Iteration 3309, loss = 0.20599033\n",
      "Iteration 3310, loss = 0.20598448\n",
      "Iteration 3311, loss = 0.20597728\n",
      "Iteration 3312, loss = 0.20597343\n",
      "Iteration 3313, loss = 0.20596716\n",
      "Iteration 3314, loss = 0.20596193\n",
      "Iteration 3315, loss = 0.20595409\n",
      "Iteration 3316, loss = 0.20594748\n",
      "Iteration 3317, loss = 0.20594280\n",
      "Iteration 3318, loss = 0.20593794\n",
      "Iteration 3319, loss = 0.20593058\n",
      "Iteration 3320, loss = 0.20592429\n",
      "Iteration 3321, loss = 0.20591678\n",
      "Iteration 3322, loss = 0.20590886\n",
      "Iteration 3323, loss = 0.20590249\n",
      "Iteration 3324, loss = 0.20589563\n",
      "Iteration 3325, loss = 0.20589027\n",
      "Iteration 3326, loss = 0.20588122\n",
      "Iteration 3327, loss = 0.20587331\n",
      "Iteration 3328, loss = 0.20586713\n",
      "Iteration 3329, loss = 0.20585989\n",
      "Iteration 3330, loss = 0.20585289\n",
      "Iteration 3331, loss = 0.20584557\n",
      "Iteration 3332, loss = 0.20583725\n",
      "Iteration 3333, loss = 0.20582924\n",
      "Iteration 3334, loss = 0.20582294\n",
      "Iteration 3335, loss = 0.20581605\n",
      "Iteration 3336, loss = 0.20581015\n",
      "Iteration 3337, loss = 0.20580208\n",
      "Iteration 3338, loss = 0.20579063\n",
      "Iteration 3339, loss = 0.20578688\n",
      "Iteration 3340, loss = 0.20578182\n",
      "Iteration 3341, loss = 0.20577355\n",
      "Iteration 3342, loss = 0.20576455\n",
      "Iteration 3343, loss = 0.20575724\n",
      "Iteration 3344, loss = 0.20575054\n",
      "Iteration 3345, loss = 0.20574369\n",
      "Iteration 3346, loss = 0.20573676\n",
      "Iteration 3347, loss = 0.20572771\n",
      "Iteration 3348, loss = 0.20572281\n",
      "Iteration 3349, loss = 0.20571657\n",
      "Iteration 3350, loss = 0.20570986\n",
      "Iteration 3351, loss = 0.20570308\n",
      "Iteration 3352, loss = 0.20569625\n",
      "Iteration 3353, loss = 0.20568970\n",
      "Iteration 3354, loss = 0.20568368\n",
      "Iteration 3355, loss = 0.20567697\n",
      "Iteration 3356, loss = 0.20567052\n",
      "Iteration 3357, loss = 0.20566468\n",
      "Iteration 3358, loss = 0.20565500\n",
      "Iteration 3359, loss = 0.20564779\n",
      "Iteration 3360, loss = 0.20564279\n",
      "Iteration 3361, loss = 0.20563588\n",
      "Iteration 3362, loss = 0.20562725\n",
      "Iteration 3363, loss = 0.20561747\n",
      "Iteration 3364, loss = 0.20561044\n",
      "Iteration 3365, loss = 0.20560456\n",
      "Iteration 3366, loss = 0.20559641\n",
      "Iteration 3367, loss = 0.20558712\n",
      "Iteration 3368, loss = 0.20557953\n",
      "Iteration 3369, loss = 0.20557570\n",
      "Iteration 3370, loss = 0.20556679\n",
      "Iteration 3371, loss = 0.20555717\n",
      "Iteration 3372, loss = 0.20554871\n",
      "Iteration 3373, loss = 0.20554257\n",
      "Iteration 3374, loss = 0.20553474\n",
      "Iteration 3375, loss = 0.20552682\n",
      "Iteration 3376, loss = 0.20551892\n",
      "Iteration 3377, loss = 0.20551047\n",
      "Iteration 3378, loss = 0.20550232\n",
      "Iteration 3379, loss = 0.20549620\n",
      "Iteration 3380, loss = 0.20548984\n",
      "Iteration 3381, loss = 0.20548195\n",
      "Iteration 3382, loss = 0.20547400\n",
      "Iteration 3383, loss = 0.20546418\n",
      "Iteration 3384, loss = 0.20545728\n",
      "Iteration 3385, loss = 0.20545144\n",
      "Iteration 3386, loss = 0.20544277\n",
      "Iteration 3387, loss = 0.20543583\n",
      "Iteration 3388, loss = 0.20543123\n",
      "Iteration 3389, loss = 0.20542565\n",
      "Iteration 3390, loss = 0.20541480\n",
      "Iteration 3391, loss = 0.20540730\n",
      "Iteration 3392, loss = 0.20540352\n",
      "Iteration 3393, loss = 0.20539788\n",
      "Iteration 3394, loss = 0.20538883\n",
      "Iteration 3395, loss = 0.20537940\n",
      "Iteration 3396, loss = 0.20537209\n",
      "Iteration 3397, loss = 0.20536770\n",
      "Iteration 3398, loss = 0.20536150\n",
      "Iteration 3399, loss = 0.20535402\n",
      "Iteration 3400, loss = 0.20534351\n",
      "Iteration 3401, loss = 0.20533300\n",
      "Iteration 3402, loss = 0.20532737\n",
      "Iteration 3403, loss = 0.20532058\n",
      "Iteration 3404, loss = 0.20531296\n",
      "Iteration 3405, loss = 0.20530758\n",
      "Iteration 3406, loss = 0.20530161\n",
      "Iteration 3407, loss = 0.20529395\n",
      "Iteration 3408, loss = 0.20528321\n",
      "Iteration 3409, loss = 0.20527908\n",
      "Iteration 3410, loss = 0.20527520\n",
      "Iteration 3411, loss = 0.20526737\n",
      "Iteration 3412, loss = 0.20525835\n",
      "Iteration 3413, loss = 0.20525056\n",
      "Iteration 3414, loss = 0.20524503\n",
      "Iteration 3415, loss = 0.20523506\n",
      "Iteration 3416, loss = 0.20523098\n",
      "Iteration 3417, loss = 0.20522410\n",
      "Iteration 3418, loss = 0.20521495\n",
      "Iteration 3419, loss = 0.20520358\n",
      "Iteration 3420, loss = 0.20519609\n",
      "Iteration 3421, loss = 0.20518990\n",
      "Iteration 3422, loss = 0.20518390\n",
      "Iteration 3423, loss = 0.20517634\n",
      "Iteration 3424, loss = 0.20516755\n",
      "Iteration 3425, loss = 0.20515529\n",
      "Iteration 3426, loss = 0.20514502\n",
      "Iteration 3427, loss = 0.20513996\n",
      "Iteration 3428, loss = 0.20513267\n",
      "Iteration 3429, loss = 0.20512233\n",
      "Iteration 3430, loss = 0.20511311\n",
      "Iteration 3431, loss = 0.20510783\n",
      "Iteration 3432, loss = 0.20510119\n",
      "Iteration 3433, loss = 0.20509363\n",
      "Iteration 3434, loss = 0.20508426\n",
      "Iteration 3435, loss = 0.20507425\n",
      "Iteration 3436, loss = 0.20506601\n",
      "Iteration 3437, loss = 0.20505673\n",
      "Iteration 3438, loss = 0.20505149\n",
      "Iteration 3439, loss = 0.20504375\n",
      "Iteration 3440, loss = 0.20503157\n",
      "Iteration 3441, loss = 0.20502376\n",
      "Iteration 3442, loss = 0.20501786\n",
      "Iteration 3443, loss = 0.20500999\n",
      "Iteration 3444, loss = 0.20500110\n",
      "Iteration 3445, loss = 0.20499201\n",
      "Iteration 3446, loss = 0.20498628\n",
      "Iteration 3447, loss = 0.20497889\n",
      "Iteration 3448, loss = 0.20497142\n",
      "Iteration 3449, loss = 0.20496313\n",
      "Iteration 3450, loss = 0.20495532\n",
      "Iteration 3451, loss = 0.20494692\n",
      "Iteration 3452, loss = 0.20493960\n",
      "Iteration 3453, loss = 0.20493191\n",
      "Iteration 3454, loss = 0.20492511\n",
      "Iteration 3455, loss = 0.20491689\n",
      "Iteration 3456, loss = 0.20490879\n",
      "Iteration 3457, loss = 0.20490268\n",
      "Iteration 3458, loss = 0.20489439\n",
      "Iteration 3459, loss = 0.20488610\n",
      "Iteration 3460, loss = 0.20488115\n",
      "Iteration 3461, loss = 0.20487513\n",
      "Iteration 3462, loss = 0.20486849\n",
      "Iteration 3463, loss = 0.20486038\n",
      "Iteration 3464, loss = 0.20485403\n",
      "Iteration 3465, loss = 0.20484858\n",
      "Iteration 3466, loss = 0.20484386\n",
      "Iteration 3467, loss = 0.20483767\n",
      "Iteration 3468, loss = 0.20483218\n",
      "Iteration 3469, loss = 0.20482582\n",
      "Iteration 3470, loss = 0.20481893\n",
      "Iteration 3471, loss = 0.20481236\n",
      "Iteration 3472, loss = 0.20480838\n",
      "Iteration 3473, loss = 0.20480214\n",
      "Iteration 3474, loss = 0.20479380\n",
      "Iteration 3475, loss = 0.20478634\n",
      "Iteration 3476, loss = 0.20477959\n",
      "Iteration 3477, loss = 0.20477398\n",
      "Iteration 3478, loss = 0.20476769\n",
      "Iteration 3479, loss = 0.20476236\n",
      "Iteration 3480, loss = 0.20475306\n",
      "Iteration 3481, loss = 0.20474688\n",
      "Iteration 3482, loss = 0.20474453\n",
      "Iteration 3483, loss = 0.20473897\n",
      "Iteration 3484, loss = 0.20473200\n",
      "Iteration 3485, loss = 0.20472305\n",
      "Iteration 3486, loss = 0.20471754\n",
      "Iteration 3487, loss = 0.20471089\n",
      "Iteration 3488, loss = 0.20470678\n",
      "Iteration 3489, loss = 0.20469986\n",
      "Iteration 3490, loss = 0.20469078\n",
      "Iteration 3491, loss = 0.20468749\n",
      "Iteration 3492, loss = 0.20468263\n",
      "Iteration 3493, loss = 0.20467742\n",
      "Iteration 3494, loss = 0.20467013\n",
      "Iteration 3495, loss = 0.20466116\n",
      "Iteration 3496, loss = 0.20465787\n",
      "Iteration 3497, loss = 0.20465614\n",
      "Iteration 3498, loss = 0.20465066\n",
      "Iteration 3499, loss = 0.20463928\n",
      "Iteration 3500, loss = 0.20463411\n",
      "Iteration 3501, loss = 0.20462995\n",
      "Iteration 3502, loss = 0.20462647\n",
      "Iteration 3503, loss = 0.20461941\n",
      "Iteration 3504, loss = 0.20461111\n",
      "Iteration 3505, loss = 0.20460222\n",
      "Iteration 3506, loss = 0.20459560\n",
      "Iteration 3507, loss = 0.20458892\n",
      "Iteration 3508, loss = 0.20458235\n",
      "Iteration 3509, loss = 0.20457688\n",
      "Iteration 3510, loss = 0.20456894\n",
      "Iteration 3511, loss = 0.20456480\n",
      "Iteration 3512, loss = 0.20456256\n",
      "Iteration 3513, loss = 0.20455733\n",
      "Iteration 3514, loss = 0.20455141\n",
      "Iteration 3515, loss = 0.20454740\n",
      "Iteration 3516, loss = 0.20454233\n",
      "Iteration 3517, loss = 0.20453759\n",
      "Iteration 3518, loss = 0.20453202\n",
      "Iteration 3519, loss = 0.20452707\n",
      "Iteration 3520, loss = 0.20452167\n",
      "Iteration 3521, loss = 0.20451720\n",
      "Iteration 3522, loss = 0.20451250\n",
      "Iteration 3523, loss = 0.20450794\n",
      "Iteration 3524, loss = 0.20450340\n",
      "Iteration 3525, loss = 0.20449717\n",
      "Iteration 3526, loss = 0.20449401\n",
      "Iteration 3527, loss = 0.20448836\n",
      "Iteration 3528, loss = 0.20448174\n",
      "Iteration 3529, loss = 0.20447599\n",
      "Iteration 3530, loss = 0.20447313\n",
      "Iteration 3531, loss = 0.20446939\n",
      "Iteration 3532, loss = 0.20446490\n",
      "Iteration 3533, loss = 0.20445771\n",
      "Iteration 3534, loss = 0.20444916\n",
      "Iteration 3535, loss = 0.20444706\n",
      "Iteration 3536, loss = 0.20444365\n",
      "Iteration 3537, loss = 0.20443730\n",
      "Iteration 3538, loss = 0.20443194\n",
      "Iteration 3539, loss = 0.20442506\n",
      "Iteration 3540, loss = 0.20442025\n",
      "Iteration 3541, loss = 0.20441752\n",
      "Iteration 3542, loss = 0.20441256\n",
      "Iteration 3543, loss = 0.20440659\n",
      "Iteration 3544, loss = 0.20439992\n",
      "Iteration 3545, loss = 0.20439617\n",
      "Iteration 3546, loss = 0.20439176\n",
      "Iteration 3547, loss = 0.20438842\n",
      "Iteration 3548, loss = 0.20438268\n",
      "Iteration 3549, loss = 0.20437560\n",
      "Iteration 3550, loss = 0.20437038\n",
      "Iteration 3551, loss = 0.20436554\n",
      "Iteration 3552, loss = 0.20435970\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3553, loss = 0.20435488\n",
      "Iteration 3554, loss = 0.20435185\n",
      "Iteration 3555, loss = 0.20434511\n",
      "Iteration 3556, loss = 0.20434042\n",
      "Iteration 3557, loss = 0.20433646\n",
      "Iteration 3558, loss = 0.20433111\n",
      "Iteration 3559, loss = 0.20432650\n",
      "Iteration 3560, loss = 0.20432152\n",
      "Iteration 3561, loss = 0.20431661\n",
      "Iteration 3562, loss = 0.20431160\n",
      "Iteration 3563, loss = 0.20430862\n",
      "Iteration 3564, loss = 0.20430375\n",
      "Iteration 3565, loss = 0.20429853\n",
      "Iteration 3566, loss = 0.20429556\n",
      "Iteration 3567, loss = 0.20429150\n",
      "Iteration 3568, loss = 0.20428508\n",
      "Iteration 3569, loss = 0.20427965\n",
      "Iteration 3570, loss = 0.20427621\n",
      "Iteration 3571, loss = 0.20427207\n",
      "Iteration 3572, loss = 0.20426750\n",
      "Iteration 3573, loss = 0.20426141\n",
      "Iteration 3574, loss = 0.20425661\n",
      "Iteration 3575, loss = 0.20425502\n",
      "Iteration 3576, loss = 0.20425079\n",
      "Iteration 3577, loss = 0.20424370\n",
      "Iteration 3578, loss = 0.20423868\n",
      "Iteration 3579, loss = 0.20423643\n",
      "Iteration 3580, loss = 0.20423403\n",
      "Iteration 3581, loss = 0.20422893\n",
      "Iteration 3582, loss = 0.20422531\n",
      "Iteration 3583, loss = 0.20422409\n",
      "Iteration 3584, loss = 0.20422114\n",
      "Iteration 3585, loss = 0.20421578\n",
      "Iteration 3586, loss = 0.20420956\n",
      "Iteration 3587, loss = 0.20420882\n",
      "Iteration 3588, loss = 0.20420559\n",
      "Iteration 3589, loss = 0.20420146\n",
      "Iteration 3590, loss = 0.20419734\n",
      "Iteration 3591, loss = 0.20419229\n",
      "Iteration 3592, loss = 0.20419012\n",
      "Iteration 3593, loss = 0.20418567\n",
      "Iteration 3594, loss = 0.20418218\n",
      "Iteration 3595, loss = 0.20417866\n",
      "Iteration 3596, loss = 0.20417376\n",
      "Iteration 3597, loss = 0.20417035\n",
      "Iteration 3598, loss = 0.20416785\n",
      "Iteration 3599, loss = 0.20416549\n",
      "Iteration 3600, loss = 0.20416102\n",
      "Iteration 3601, loss = 0.20415831\n",
      "Iteration 3602, loss = 0.20415412\n",
      "Iteration 3603, loss = 0.20414991\n",
      "Iteration 3604, loss = 0.20414878\n",
      "Iteration 3605, loss = 0.20414486\n",
      "Iteration 3606, loss = 0.20413899\n",
      "Iteration 3607, loss = 0.20413684\n",
      "Iteration 3608, loss = 0.20413224\n",
      "Iteration 3609, loss = 0.20412925\n",
      "Iteration 3610, loss = 0.20412554\n",
      "Iteration 3611, loss = 0.20412090\n",
      "Iteration 3612, loss = 0.20411746\n",
      "Iteration 3613, loss = 0.20411419\n",
      "Iteration 3614, loss = 0.20411152\n",
      "Iteration 3615, loss = 0.20410906\n",
      "Iteration 3616, loss = 0.20410410\n",
      "Iteration 3617, loss = 0.20410171\n",
      "Iteration 3618, loss = 0.20409737\n",
      "Iteration 3619, loss = 0.20409451\n",
      "Iteration 3620, loss = 0.20408914\n",
      "Iteration 3621, loss = 0.20408644\n",
      "Iteration 3622, loss = 0.20408510\n",
      "Iteration 3623, loss = 0.20408166\n",
      "Iteration 3624, loss = 0.20407753\n",
      "Iteration 3625, loss = 0.20407255\n",
      "Iteration 3626, loss = 0.20407102\n",
      "Iteration 3627, loss = 0.20406580\n",
      "Iteration 3628, loss = 0.20406218\n",
      "Iteration 3629, loss = 0.20405898\n",
      "Iteration 3630, loss = 0.20405790\n",
      "Iteration 3631, loss = 0.20405342\n",
      "Iteration 3632, loss = 0.20404794\n",
      "Iteration 3633, loss = 0.20404608\n",
      "Iteration 3634, loss = 0.20404512\n",
      "Iteration 3635, loss = 0.20404241\n",
      "Iteration 3636, loss = 0.20403874\n",
      "Iteration 3637, loss = 0.20403318\n",
      "Iteration 3638, loss = 0.20402796\n",
      "Iteration 3639, loss = 0.20402541\n",
      "Iteration 3640, loss = 0.20402156\n",
      "Iteration 3641, loss = 0.20401792\n",
      "Iteration 3642, loss = 0.20401648\n",
      "Iteration 3643, loss = 0.20401336\n",
      "Iteration 3644, loss = 0.20400996\n",
      "Iteration 3645, loss = 0.20400578\n",
      "Iteration 3646, loss = 0.20400193\n",
      "Iteration 3647, loss = 0.20399915\n",
      "Iteration 3648, loss = 0.20399630\n",
      "Iteration 3649, loss = 0.20399093\n",
      "Iteration 3650, loss = 0.20398984\n",
      "Iteration 3651, loss = 0.20398660\n",
      "Iteration 3652, loss = 0.20398193\n",
      "Iteration 3653, loss = 0.20397924\n",
      "Iteration 3654, loss = 0.20397822\n",
      "Iteration 3655, loss = 0.20397556\n",
      "Iteration 3656, loss = 0.20397064\n",
      "Iteration 3657, loss = 0.20396810\n",
      "Iteration 3658, loss = 0.20396655\n",
      "Iteration 3659, loss = 0.20396365\n",
      "Iteration 3660, loss = 0.20396104\n",
      "Iteration 3661, loss = 0.20395439\n",
      "Iteration 3662, loss = 0.20394983\n",
      "Iteration 3663, loss = 0.20394878\n",
      "Iteration 3664, loss = 0.20394660\n",
      "Iteration 3665, loss = 0.20394125\n",
      "Iteration 3666, loss = 0.20393501\n",
      "Iteration 3667, loss = 0.20393341\n",
      "Iteration 3668, loss = 0.20393255\n",
      "Iteration 3669, loss = 0.20392760\n",
      "Iteration 3670, loss = 0.20392323\n",
      "Iteration 3671, loss = 0.20391988\n",
      "Iteration 3672, loss = 0.20391808\n",
      "Iteration 3673, loss = 0.20391545\n",
      "Iteration 3674, loss = 0.20391297\n",
      "Iteration 3675, loss = 0.20390817\n",
      "Iteration 3676, loss = 0.20390291\n",
      "Iteration 3677, loss = 0.20389980\n",
      "Iteration 3678, loss = 0.20389732\n",
      "Iteration 3679, loss = 0.20389419\n",
      "Iteration 3680, loss = 0.20389011\n",
      "Iteration 3681, loss = 0.20388660\n",
      "Iteration 3682, loss = 0.20388494\n",
      "Iteration 3683, loss = 0.20388234\n",
      "Iteration 3684, loss = 0.20388055\n",
      "Iteration 3685, loss = 0.20387389\n",
      "Iteration 3686, loss = 0.20387187\n",
      "Iteration 3687, loss = 0.20386998\n",
      "Iteration 3688, loss = 0.20386511\n",
      "Iteration 3689, loss = 0.20386154\n",
      "Iteration 3690, loss = 0.20385999\n",
      "Iteration 3691, loss = 0.20385669\n",
      "Iteration 3692, loss = 0.20385275\n",
      "Iteration 3693, loss = 0.20384836\n",
      "Iteration 3694, loss = 0.20384483\n",
      "Iteration 3695, loss = 0.20384186\n",
      "Iteration 3696, loss = 0.20383910\n",
      "Iteration 3697, loss = 0.20383582\n",
      "Iteration 3698, loss = 0.20383401\n",
      "Iteration 3699, loss = 0.20383157\n",
      "Iteration 3700, loss = 0.20382947\n",
      "Iteration 3701, loss = 0.20382395\n",
      "Iteration 3702, loss = 0.20381834\n",
      "Iteration 3703, loss = 0.20381935\n",
      "Iteration 3704, loss = 0.20381710\n",
      "Iteration 3705, loss = 0.20381462\n",
      "Iteration 3706, loss = 0.20381023\n",
      "Iteration 3707, loss = 0.20380593\n",
      "Iteration 3708, loss = 0.20380351\n",
      "Iteration 3709, loss = 0.20380176\n",
      "Iteration 3710, loss = 0.20379783\n",
      "Iteration 3711, loss = 0.20379438\n",
      "Iteration 3712, loss = 0.20379001\n",
      "Iteration 3713, loss = 0.20378848\n",
      "Iteration 3714, loss = 0.20378681\n",
      "Iteration 3715, loss = 0.20378084\n",
      "Iteration 3716, loss = 0.20377643\n",
      "Iteration 3717, loss = 0.20377554\n",
      "Iteration 3718, loss = 0.20377500\n",
      "Iteration 3719, loss = 0.20377091\n",
      "Iteration 3720, loss = 0.20376674\n",
      "Iteration 3721, loss = 0.20376235\n",
      "Iteration 3722, loss = 0.20375770\n",
      "Iteration 3723, loss = 0.20375330\n",
      "Iteration 3724, loss = 0.20375170\n",
      "Iteration 3725, loss = 0.20375097\n",
      "Iteration 3726, loss = 0.20374925\n",
      "Iteration 3727, loss = 0.20374294\n",
      "Iteration 3728, loss = 0.20373890\n",
      "Iteration 3729, loss = 0.20373567\n",
      "Iteration 3730, loss = 0.20373416\n",
      "Iteration 3731, loss = 0.20373020\n",
      "Iteration 3732, loss = 0.20373049\n",
      "Iteration 3733, loss = 0.20372806\n",
      "Iteration 3734, loss = 0.20372319\n",
      "Iteration 3735, loss = 0.20371650\n",
      "Iteration 3736, loss = 0.20371288\n",
      "Iteration 3737, loss = 0.20371288\n",
      "Iteration 3738, loss = 0.20371023\n",
      "Iteration 3739, loss = 0.20370718\n",
      "Iteration 3740, loss = 0.20370281\n",
      "Iteration 3741, loss = 0.20370025\n",
      "Iteration 3742, loss = 0.20369733\n",
      "Iteration 3743, loss = 0.20369593\n",
      "Iteration 3744, loss = 0.20369268\n",
      "Iteration 3745, loss = 0.20368658\n",
      "Iteration 3746, loss = 0.20368510\n",
      "Iteration 3747, loss = 0.20368279\n",
      "Iteration 3748, loss = 0.20367881\n",
      "Iteration 3749, loss = 0.20367569\n",
      "Iteration 3750, loss = 0.20367185\n",
      "Iteration 3751, loss = 0.20366886\n",
      "Iteration 3752, loss = 0.20366621\n",
      "Iteration 3753, loss = 0.20366192\n",
      "Iteration 3754, loss = 0.20365891\n",
      "Iteration 3755, loss = 0.20365686\n",
      "Iteration 3756, loss = 0.20365350\n",
      "Iteration 3757, loss = 0.20364933\n",
      "Iteration 3758, loss = 0.20364448\n",
      "Iteration 3759, loss = 0.20364423\n",
      "Iteration 3760, loss = 0.20364113\n",
      "Iteration 3761, loss = 0.20363729\n",
      "Iteration 3762, loss = 0.20363350\n",
      "Iteration 3763, loss = 0.20363221\n",
      "Iteration 3764, loss = 0.20362871\n",
      "Iteration 3765, loss = 0.20362294\n",
      "Iteration 3766, loss = 0.20361927\n",
      "Iteration 3767, loss = 0.20361695\n",
      "Iteration 3768, loss = 0.20361510\n",
      "Iteration 3769, loss = 0.20361094\n",
      "Iteration 3770, loss = 0.20360579\n",
      "Iteration 3771, loss = 0.20360374\n",
      "Iteration 3772, loss = 0.20360285\n",
      "Iteration 3773, loss = 0.20359967\n",
      "Iteration 3774, loss = 0.20359462\n",
      "Iteration 3775, loss = 0.20359178\n",
      "Iteration 3776, loss = 0.20358737\n",
      "Iteration 3777, loss = 0.20358468\n",
      "Iteration 3778, loss = 0.20358124\n",
      "Iteration 3779, loss = 0.20358065\n",
      "Iteration 3780, loss = 0.20357743\n",
      "Iteration 3781, loss = 0.20357186\n",
      "Iteration 3782, loss = 0.20356780\n",
      "Iteration 3783, loss = 0.20356866\n",
      "Iteration 3784, loss = 0.20356674\n",
      "Iteration 3785, loss = 0.20355989\n",
      "Iteration 3786, loss = 0.20355535\n",
      "Iteration 3787, loss = 0.20355145\n",
      "Iteration 3788, loss = 0.20354963\n",
      "Iteration 3789, loss = 0.20354438\n",
      "Iteration 3790, loss = 0.20354231\n",
      "Iteration 3791, loss = 0.20354001\n",
      "Iteration 3792, loss = 0.20353579\n",
      "Iteration 3793, loss = 0.20352795\n",
      "Iteration 3794, loss = 0.20352467\n",
      "Iteration 3795, loss = 0.20352371\n",
      "Iteration 3796, loss = 0.20351952\n",
      "Iteration 3797, loss = 0.20351385\n",
      "Iteration 3798, loss = 0.20350932\n",
      "Iteration 3799, loss = 0.20350217\n",
      "Iteration 3800, loss = 0.20349967\n",
      "Iteration 3801, loss = 0.20349711\n",
      "Iteration 3802, loss = 0.20349364\n",
      "Iteration 3803, loss = 0.20348782\n",
      "Iteration 3804, loss = 0.20347957\n",
      "Iteration 3805, loss = 0.20347259\n",
      "Iteration 3806, loss = 0.20347208\n",
      "Iteration 3807, loss = 0.20346825\n",
      "Iteration 3808, loss = 0.20346130\n",
      "Iteration 3809, loss = 0.20345388\n",
      "Iteration 3810, loss = 0.20344753\n",
      "Iteration 3811, loss = 0.20344175\n",
      "Iteration 3812, loss = 0.20343879\n",
      "Iteration 3813, loss = 0.20343430\n",
      "Iteration 3814, loss = 0.20342715\n",
      "Iteration 3815, loss = 0.20341981\n",
      "Iteration 3816, loss = 0.20341857\n",
      "Iteration 3817, loss = 0.20341490\n",
      "Iteration 3818, loss = 0.20340751\n",
      "Iteration 3819, loss = 0.20340123\n",
      "Iteration 3820, loss = 0.20339491\n",
      "Iteration 3821, loss = 0.20339169\n",
      "Iteration 3822, loss = 0.20338513\n",
      "Iteration 3823, loss = 0.20337589\n",
      "Iteration 3824, loss = 0.20336852\n",
      "Iteration 3825, loss = 0.20336454\n",
      "Iteration 3826, loss = 0.20335947\n",
      "Iteration 3827, loss = 0.20335338\n",
      "Iteration 3828, loss = 0.20334713\n",
      "Iteration 3829, loss = 0.20334161\n",
      "Iteration 3830, loss = 0.20333521\n",
      "Iteration 3831, loss = 0.20333169\n",
      "Iteration 3832, loss = 0.20332536\n",
      "Iteration 3833, loss = 0.20331881\n",
      "Iteration 3834, loss = 0.20330919\n",
      "Iteration 3835, loss = 0.20330810\n",
      "Iteration 3836, loss = 0.20330460\n",
      "Iteration 3837, loss = 0.20329815\n",
      "Iteration 3838, loss = 0.20328785\n",
      "Iteration 3839, loss = 0.20327889\n",
      "Iteration 3840, loss = 0.20327527\n",
      "Iteration 3841, loss = 0.20327089\n",
      "Iteration 3842, loss = 0.20326225\n",
      "Iteration 3843, loss = 0.20325091\n",
      "Iteration 3844, loss = 0.20324535\n",
      "Iteration 3845, loss = 0.20323966\n",
      "Iteration 3846, loss = 0.20323276\n",
      "Iteration 3847, loss = 0.20322421\n",
      "Iteration 3848, loss = 0.20321456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3849, loss = 0.20320829\n",
      "Iteration 3850, loss = 0.20320285\n",
      "Iteration 3851, loss = 0.20319411\n",
      "Iteration 3852, loss = 0.20318305\n",
      "Iteration 3853, loss = 0.20317390\n",
      "Iteration 3854, loss = 0.20316615\n",
      "Iteration 3855, loss = 0.20315949\n",
      "Iteration 3856, loss = 0.20314893\n",
      "Iteration 3857, loss = 0.20313768\n",
      "Iteration 3858, loss = 0.20312914\n",
      "Iteration 3859, loss = 0.20312151\n",
      "Iteration 3860, loss = 0.20311298\n",
      "Iteration 3861, loss = 0.20310334\n",
      "Iteration 3862, loss = 0.20309331\n",
      "Iteration 3863, loss = 0.20308517\n",
      "Iteration 3864, loss = 0.20307744\n",
      "Iteration 3865, loss = 0.20306950\n",
      "Iteration 3866, loss = 0.20305993\n",
      "Iteration 3867, loss = 0.20305193\n",
      "Iteration 3868, loss = 0.20304488\n",
      "Iteration 3869, loss = 0.20303608\n",
      "Iteration 3870, loss = 0.20302990\n",
      "Iteration 3871, loss = 0.20302132\n",
      "Iteration 3872, loss = 0.20301102\n",
      "Iteration 3873, loss = 0.20300538\n",
      "Iteration 3874, loss = 0.20299708\n",
      "Iteration 3875, loss = 0.20299047\n",
      "Iteration 3876, loss = 0.20298393\n",
      "Iteration 3877, loss = 0.20297420\n",
      "Iteration 3878, loss = 0.20296809\n",
      "Iteration 3879, loss = 0.20296027\n",
      "Iteration 3880, loss = 0.20295431\n",
      "Iteration 3881, loss = 0.20295086\n",
      "Iteration 3882, loss = 0.20294347\n",
      "Iteration 3883, loss = 0.20293812\n",
      "Iteration 3884, loss = 0.20293244\n",
      "Iteration 3885, loss = 0.20292485\n",
      "Iteration 3886, loss = 0.20291842\n",
      "Iteration 3887, loss = 0.20291230\n",
      "Iteration 3888, loss = 0.20290982\n",
      "Iteration 3889, loss = 0.20290365\n",
      "Iteration 3890, loss = 0.20289759\n",
      "Iteration 3891, loss = 0.20289179\n",
      "Iteration 3892, loss = 0.20288593\n",
      "Iteration 3893, loss = 0.20288208\n",
      "Iteration 3894, loss = 0.20287768\n",
      "Iteration 3895, loss = 0.20287047\n",
      "Iteration 3896, loss = 0.20286451\n",
      "Iteration 3897, loss = 0.20286057\n",
      "Iteration 3898, loss = 0.20286038\n",
      "Iteration 3899, loss = 0.20285554\n",
      "Iteration 3900, loss = 0.20284958\n",
      "Iteration 3901, loss = 0.20285023\n",
      "Iteration 3902, loss = 0.20284792\n",
      "Iteration 3903, loss = 0.20284481\n",
      "Iteration 3904, loss = 0.20284035\n",
      "Iteration 3905, loss = 0.20283658\n",
      "Iteration 3906, loss = 0.20283432\n",
      "Iteration 3907, loss = 0.20283038\n",
      "Iteration 3908, loss = 0.20282758\n",
      "Iteration 3909, loss = 0.20282533\n",
      "Iteration 3910, loss = 0.20282307\n",
      "Iteration 3911, loss = 0.20281722\n",
      "Iteration 3912, loss = 0.20281787\n",
      "Iteration 3913, loss = 0.20281500\n",
      "Iteration 3914, loss = 0.20281123\n",
      "Iteration 3915, loss = 0.20280843\n",
      "Iteration 3916, loss = 0.20280514\n",
      "Iteration 3917, loss = 0.20280311\n",
      "Iteration 3918, loss = 0.20280106\n",
      "Iteration 3919, loss = 0.20279699\n",
      "Iteration 3920, loss = 0.20279310\n",
      "Iteration 3921, loss = 0.20279176\n",
      "Iteration 3922, loss = 0.20278990\n",
      "Iteration 3923, loss = 0.20278883\n",
      "Iteration 3924, loss = 0.20278452\n",
      "Iteration 3925, loss = 0.20277758\n",
      "Iteration 3926, loss = 0.20277775\n",
      "Iteration 3927, loss = 0.20277739\n",
      "Iteration 3928, loss = 0.20277488\n",
      "Iteration 3929, loss = 0.20277053\n",
      "Iteration 3930, loss = 0.20276964\n",
      "Iteration 3931, loss = 0.20276680\n",
      "Iteration 3932, loss = 0.20276280\n",
      "Iteration 3933, loss = 0.20275865\n",
      "Iteration 3934, loss = 0.20275778\n",
      "Iteration 3935, loss = 0.20275382\n",
      "Iteration 3936, loss = 0.20274894\n",
      "Iteration 3937, loss = 0.20274726\n",
      "Iteration 3938, loss = 0.20274289\n",
      "Iteration 3939, loss = 0.20274151\n",
      "Iteration 3940, loss = 0.20273885\n",
      "Iteration 3941, loss = 0.20273500\n",
      "Iteration 3942, loss = 0.20273007\n",
      "Iteration 3943, loss = 0.20272776\n",
      "Iteration 3944, loss = 0.20272675\n",
      "Iteration 3945, loss = 0.20272422\n",
      "Iteration 3946, loss = 0.20271975\n",
      "Iteration 3947, loss = 0.20271708\n",
      "Iteration 3948, loss = 0.20271426\n",
      "Iteration 3949, loss = 0.20271335\n",
      "Iteration 3950, loss = 0.20271092\n",
      "Iteration 3951, loss = 0.20270632\n",
      "Iteration 3952, loss = 0.20270477\n",
      "Iteration 3953, loss = 0.20270246\n",
      "Iteration 3954, loss = 0.20269931\n",
      "Iteration 3955, loss = 0.20269858\n",
      "Iteration 3956, loss = 0.20269457\n",
      "Iteration 3957, loss = 0.20269226\n",
      "Iteration 3958, loss = 0.20268855\n",
      "Iteration 3959, loss = 0.20268705\n",
      "Iteration 3960, loss = 0.20268721\n",
      "Iteration 3961, loss = 0.20268370\n",
      "Iteration 3962, loss = 0.20267866\n",
      "Iteration 3963, loss = 0.20267302\n",
      "Iteration 3964, loss = 0.20267412\n",
      "Iteration 3965, loss = 0.20267517\n",
      "Iteration 3966, loss = 0.20267154\n",
      "Iteration 3967, loss = 0.20266395\n",
      "Iteration 3968, loss = 0.20266043\n",
      "Iteration 3969, loss = 0.20265927\n",
      "Iteration 3970, loss = 0.20265922\n",
      "Iteration 3971, loss = 0.20265569\n",
      "Iteration 3972, loss = 0.20265129\n",
      "Iteration 3973, loss = 0.20264793\n",
      "Iteration 3974, loss = 0.20264702\n",
      "Iteration 3975, loss = 0.20264535\n",
      "Iteration 3976, loss = 0.20264173\n",
      "Iteration 3977, loss = 0.20263740\n",
      "Iteration 3978, loss = 0.20263594\n",
      "Iteration 3979, loss = 0.20263366\n",
      "Iteration 3980, loss = 0.20262897\n",
      "Iteration 3981, loss = 0.20262495\n",
      "Iteration 3982, loss = 0.20262519\n",
      "Iteration 3983, loss = 0.20262326\n",
      "Iteration 3984, loss = 0.20261841\n",
      "Iteration 3985, loss = 0.20261612\n",
      "Iteration 3986, loss = 0.20261372\n",
      "Iteration 3987, loss = 0.20261008\n",
      "Iteration 3988, loss = 0.20260930\n",
      "Iteration 3989, loss = 0.20260659\n",
      "Iteration 3990, loss = 0.20260243\n",
      "Iteration 3991, loss = 0.20260168\n",
      "Iteration 3992, loss = 0.20260077\n",
      "Iteration 3993, loss = 0.20259621\n",
      "Iteration 3994, loss = 0.20259134\n",
      "Iteration 3995, loss = 0.20259187\n",
      "Iteration 3996, loss = 0.20258910\n",
      "Iteration 3997, loss = 0.20258460\n",
      "Iteration 3998, loss = 0.20258282\n",
      "Iteration 3999, loss = 0.20258318\n",
      "Iteration 4000, loss = 0.20258022\n",
      "Iteration 4001, loss = 0.20257751\n",
      "Iteration 4002, loss = 0.20257257\n",
      "Iteration 4003, loss = 0.20256926\n",
      "Iteration 4004, loss = 0.20256892\n",
      "Iteration 4005, loss = 0.20256664\n",
      "Iteration 4006, loss = 0.20256322\n",
      "Iteration 4007, loss = 0.20256016\n",
      "Iteration 4008, loss = 0.20255685\n",
      "Iteration 4009, loss = 0.20255591\n",
      "Iteration 4010, loss = 0.20255290\n",
      "Iteration 4011, loss = 0.20254852\n",
      "Iteration 4012, loss = 0.20254390\n",
      "Iteration 4013, loss = 0.20254298\n",
      "Iteration 4014, loss = 0.20254033\n",
      "Iteration 4015, loss = 0.20253787\n",
      "Iteration 4016, loss = 0.20253644\n",
      "Iteration 4017, loss = 0.20253411\n",
      "Iteration 4018, loss = 0.20252967\n",
      "Iteration 4019, loss = 0.20252620\n",
      "Iteration 4020, loss = 0.20252609\n",
      "Iteration 4021, loss = 0.20252265\n",
      "Iteration 4022, loss = 0.20252119\n",
      "Iteration 4023, loss = 0.20251787\n",
      "Iteration 4024, loss = 0.20251534\n",
      "Iteration 4025, loss = 0.20251421\n",
      "Iteration 4026, loss = 0.20251254\n",
      "Iteration 4027, loss = 0.20250761\n",
      "Iteration 4028, loss = 0.20250457\n",
      "Iteration 4029, loss = 0.20250364\n",
      "Iteration 4030, loss = 0.20250174\n",
      "Iteration 4031, loss = 0.20249713\n",
      "Iteration 4032, loss = 0.20249531\n",
      "Iteration 4033, loss = 0.20249388\n",
      "Iteration 4034, loss = 0.20248920\n",
      "Iteration 4035, loss = 0.20248480\n",
      "Iteration 4036, loss = 0.20248073\n",
      "Iteration 4037, loss = 0.20247927\n",
      "Iteration 4038, loss = 0.20247537\n",
      "Iteration 4039, loss = 0.20247415\n",
      "Iteration 4040, loss = 0.20246725\n",
      "Iteration 4041, loss = 0.20246261\n",
      "Iteration 4042, loss = 0.20245987\n",
      "Iteration 4043, loss = 0.20245701\n",
      "Iteration 4044, loss = 0.20245449\n",
      "Iteration 4045, loss = 0.20244861\n",
      "Iteration 4046, loss = 0.20244402\n",
      "Iteration 4047, loss = 0.20243958\n",
      "Iteration 4048, loss = 0.20243472\n",
      "Iteration 4049, loss = 0.20243030\n",
      "Iteration 4050, loss = 0.20242796\n",
      "Iteration 4051, loss = 0.20242272\n",
      "Iteration 4052, loss = 0.20242068\n",
      "Iteration 4053, loss = 0.20241662\n",
      "Iteration 4054, loss = 0.20240991\n",
      "Iteration 4055, loss = 0.20240520\n",
      "Iteration 4056, loss = 0.20240189\n",
      "Iteration 4057, loss = 0.20239872\n",
      "Iteration 4058, loss = 0.20239456\n",
      "Iteration 4059, loss = 0.20239021\n",
      "Iteration 4060, loss = 0.20238637\n",
      "Iteration 4061, loss = 0.20238278\n",
      "Iteration 4062, loss = 0.20238087\n",
      "Iteration 4063, loss = 0.20237673\n",
      "Iteration 4064, loss = 0.20237218\n",
      "Iteration 4065, loss = 0.20236814\n",
      "Iteration 4066, loss = 0.20236544\n",
      "Iteration 4067, loss = 0.20235982\n",
      "Iteration 4068, loss = 0.20235727\n",
      "Iteration 4069, loss = 0.20235229\n",
      "Iteration 4070, loss = 0.20234731\n",
      "Iteration 4071, loss = 0.20234386\n",
      "Iteration 4072, loss = 0.20234127\n",
      "Iteration 4073, loss = 0.20233590\n",
      "Iteration 4074, loss = 0.20233293\n",
      "Iteration 4075, loss = 0.20232828\n",
      "Iteration 4076, loss = 0.20232415\n",
      "Iteration 4077, loss = 0.20231952\n",
      "Iteration 4078, loss = 0.20231551\n",
      "Iteration 4079, loss = 0.20231227\n",
      "Iteration 4080, loss = 0.20230892\n",
      "Iteration 4081, loss = 0.20230671\n",
      "Iteration 4082, loss = 0.20230179\n",
      "Iteration 4083, loss = 0.20229491\n",
      "Iteration 4084, loss = 0.20229413\n",
      "Iteration 4085, loss = 0.20229043\n",
      "Iteration 4086, loss = 0.20228285\n",
      "Iteration 4087, loss = 0.20227896\n",
      "Iteration 4088, loss = 0.20227519\n",
      "Iteration 4089, loss = 0.20227033\n",
      "Iteration 4090, loss = 0.20226489\n",
      "Iteration 4091, loss = 0.20225998\n",
      "Iteration 4092, loss = 0.20225508\n",
      "Iteration 4093, loss = 0.20225017\n",
      "Iteration 4094, loss = 0.20224430\n",
      "Iteration 4095, loss = 0.20223852\n",
      "Iteration 4096, loss = 0.20223285\n",
      "Iteration 4097, loss = 0.20222736\n",
      "Iteration 4098, loss = 0.20222093\n",
      "Iteration 4099, loss = 0.20221516\n",
      "Iteration 4100, loss = 0.20220664\n",
      "Iteration 4101, loss = 0.20219931\n",
      "Iteration 4102, loss = 0.20219397\n",
      "Iteration 4103, loss = 0.20218574\n",
      "Iteration 4104, loss = 0.20217751\n",
      "Iteration 4105, loss = 0.20217176\n",
      "Iteration 4106, loss = 0.20216279\n",
      "Iteration 4107, loss = 0.20215336\n",
      "Iteration 4108, loss = 0.20214249\n",
      "Iteration 4109, loss = 0.20213610\n",
      "Iteration 4110, loss = 0.20212853\n",
      "Iteration 4111, loss = 0.20211778\n",
      "Iteration 4112, loss = 0.20210556\n",
      "Iteration 4113, loss = 0.20209330\n",
      "Iteration 4114, loss = 0.20208335\n",
      "Iteration 4115, loss = 0.20207301\n",
      "Iteration 4116, loss = 0.20206160\n",
      "Iteration 4117, loss = 0.20205050\n",
      "Iteration 4118, loss = 0.20203871\n",
      "Iteration 4119, loss = 0.20202812\n",
      "Iteration 4120, loss = 0.20201848\n",
      "Iteration 4121, loss = 0.20200733\n",
      "Iteration 4122, loss = 0.20199610\n",
      "Iteration 4123, loss = 0.20198770\n",
      "Iteration 4124, loss = 0.20197687\n",
      "Iteration 4125, loss = 0.20196512\n",
      "Iteration 4126, loss = 0.20195369\n",
      "Iteration 4127, loss = 0.20194500\n",
      "Iteration 4128, loss = 0.20193757\n",
      "Iteration 4129, loss = 0.20192610\n",
      "Iteration 4130, loss = 0.20191356\n",
      "Iteration 4131, loss = 0.20190472\n",
      "Iteration 4132, loss = 0.20189546\n",
      "Iteration 4133, loss = 0.20188581\n",
      "Iteration 4134, loss = 0.20187605\n",
      "Iteration 4135, loss = 0.20186649\n",
      "Iteration 4136, loss = 0.20185546\n",
      "Iteration 4137, loss = 0.20184358\n",
      "Iteration 4138, loss = 0.20183625\n",
      "Iteration 4139, loss = 0.20182990\n",
      "Iteration 4140, loss = 0.20181950\n",
      "Iteration 4141, loss = 0.20181081\n",
      "Iteration 4142, loss = 0.20180239\n",
      "Iteration 4143, loss = 0.20179253\n",
      "Iteration 4144, loss = 0.20178580\n",
      "Iteration 4145, loss = 0.20177587\n",
      "Iteration 4146, loss = 0.20176618\n",
      "Iteration 4147, loss = 0.20175760\n",
      "Iteration 4148, loss = 0.20175037\n",
      "Iteration 4149, loss = 0.20174436\n",
      "Iteration 4150, loss = 0.20173559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4151, loss = 0.20172654\n",
      "Iteration 4152, loss = 0.20171938\n",
      "Iteration 4153, loss = 0.20171266\n",
      "Iteration 4154, loss = 0.20170398\n",
      "Iteration 4155, loss = 0.20169342\n",
      "Iteration 4156, loss = 0.20168850\n",
      "Iteration 4157, loss = 0.20168126\n",
      "Iteration 4158, loss = 0.20167157\n",
      "Iteration 4159, loss = 0.20166596\n",
      "Iteration 4160, loss = 0.20165990\n",
      "Iteration 4161, loss = 0.20165608\n",
      "Iteration 4162, loss = 0.20165030\n",
      "Iteration 4163, loss = 0.20164359\n",
      "Iteration 4164, loss = 0.20163743\n",
      "Iteration 4165, loss = 0.20163254\n",
      "Iteration 4166, loss = 0.20162673\n",
      "Iteration 4167, loss = 0.20162132\n",
      "Iteration 4168, loss = 0.20161516\n",
      "Iteration 4169, loss = 0.20160649\n",
      "Iteration 4170, loss = 0.20159958\n",
      "Iteration 4171, loss = 0.20159581\n",
      "Iteration 4172, loss = 0.20159006\n",
      "Iteration 4173, loss = 0.20158434\n",
      "Iteration 4174, loss = 0.20157957\n",
      "Iteration 4175, loss = 0.20157473\n",
      "Iteration 4176, loss = 0.20156997\n",
      "Iteration 4177, loss = 0.20156804\n",
      "Iteration 4178, loss = 0.20156289\n",
      "Iteration 4179, loss = 0.20155492\n",
      "Iteration 4180, loss = 0.20155125\n",
      "Iteration 4181, loss = 0.20154913\n",
      "Iteration 4182, loss = 0.20154559\n",
      "Iteration 4183, loss = 0.20153920\n",
      "Iteration 4184, loss = 0.20153587\n",
      "Iteration 4185, loss = 0.20153411\n",
      "Iteration 4186, loss = 0.20152925\n",
      "Iteration 4187, loss = 0.20152504\n",
      "Iteration 4188, loss = 0.20152205\n",
      "Iteration 4189, loss = 0.20151759\n",
      "Iteration 4190, loss = 0.20151413\n",
      "Iteration 4191, loss = 0.20151010\n",
      "Iteration 4192, loss = 0.20150461\n",
      "Iteration 4193, loss = 0.20150325\n",
      "Iteration 4194, loss = 0.20149991\n",
      "Iteration 4195, loss = 0.20149564\n",
      "Iteration 4196, loss = 0.20148961\n",
      "Iteration 4197, loss = 0.20148650\n",
      "Iteration 4198, loss = 0.20148692\n",
      "Iteration 4199, loss = 0.20148380\n",
      "Iteration 4200, loss = 0.20147577\n",
      "Iteration 4201, loss = 0.20147124\n",
      "Iteration 4202, loss = 0.20147072\n",
      "Iteration 4203, loss = 0.20146748\n",
      "Iteration 4204, loss = 0.20146247\n",
      "Iteration 4205, loss = 0.20145536\n",
      "Iteration 4206, loss = 0.20145225\n",
      "Iteration 4207, loss = 0.20145148\n",
      "Iteration 4208, loss = 0.20144750\n",
      "Iteration 4209, loss = 0.20144187\n",
      "Iteration 4210, loss = 0.20143850\n",
      "Iteration 4211, loss = 0.20143525\n",
      "Iteration 4212, loss = 0.20143226\n",
      "Iteration 4213, loss = 0.20142959\n",
      "Iteration 4214, loss = 0.20142860\n",
      "Iteration 4215, loss = 0.20142464\n",
      "Iteration 4216, loss = 0.20141831\n",
      "Iteration 4217, loss = 0.20141662\n",
      "Iteration 4218, loss = 0.20141412\n",
      "Iteration 4219, loss = 0.20141287\n",
      "Iteration 4220, loss = 0.20140878\n",
      "Iteration 4221, loss = 0.20140379\n",
      "Iteration 4222, loss = 0.20140307\n",
      "Iteration 4223, loss = 0.20140149\n",
      "Iteration 4224, loss = 0.20139681\n",
      "Iteration 4225, loss = 0.20139130\n",
      "Iteration 4226, loss = 0.20138583\n",
      "Iteration 4227, loss = 0.20138259\n",
      "Iteration 4228, loss = 0.20138181\n",
      "Iteration 4229, loss = 0.20137749\n",
      "Iteration 4230, loss = 0.20137588\n",
      "Iteration 4231, loss = 0.20137531\n",
      "Iteration 4232, loss = 0.20137289\n",
      "Iteration 4233, loss = 0.20136746\n",
      "Iteration 4234, loss = 0.20136677\n",
      "Iteration 4235, loss = 0.20136477\n",
      "Iteration 4236, loss = 0.20136307\n",
      "Iteration 4237, loss = 0.20136007\n",
      "Iteration 4238, loss = 0.20135643\n",
      "Iteration 4239, loss = 0.20135418\n",
      "Iteration 4240, loss = 0.20135037\n",
      "Iteration 4241, loss = 0.20134858\n",
      "Iteration 4242, loss = 0.20134747\n",
      "Iteration 4243, loss = 0.20134478\n",
      "Iteration 4244, loss = 0.20134125\n",
      "Iteration 4245, loss = 0.20133853\n",
      "Iteration 4246, loss = 0.20133728\n",
      "Iteration 4247, loss = 0.20133335\n",
      "Iteration 4248, loss = 0.20133120\n",
      "Iteration 4249, loss = 0.20133003\n",
      "Iteration 4250, loss = 0.20132748\n",
      "Iteration 4251, loss = 0.20132414\n",
      "Iteration 4252, loss = 0.20132206\n",
      "Iteration 4253, loss = 0.20132243\n",
      "Iteration 4254, loss = 0.20131996\n",
      "Iteration 4255, loss = 0.20131638\n",
      "Iteration 4256, loss = 0.20131486\n",
      "Iteration 4257, loss = 0.20131488\n",
      "Iteration 4258, loss = 0.20131073\n",
      "Iteration 4259, loss = 0.20130679\n",
      "Iteration 4260, loss = 0.20130728\n",
      "Iteration 4261, loss = 0.20130528\n",
      "Iteration 4262, loss = 0.20130140\n",
      "Iteration 4263, loss = 0.20129725\n",
      "Iteration 4264, loss = 0.20129616\n",
      "Iteration 4265, loss = 0.20129661\n",
      "Iteration 4266, loss = 0.20129189\n",
      "Iteration 4267, loss = 0.20129119\n",
      "Iteration 4268, loss = 0.20128793\n",
      "Iteration 4269, loss = 0.20128712\n",
      "Iteration 4270, loss = 0.20128537\n",
      "Iteration 4271, loss = 0.20127999\n",
      "Iteration 4272, loss = 0.20127651\n",
      "Iteration 4273, loss = 0.20127701\n",
      "Iteration 4274, loss = 0.20127590\n",
      "Iteration 4275, loss = 0.20127230\n",
      "Iteration 4276, loss = 0.20126805\n",
      "Iteration 4277, loss = 0.20126575\n",
      "Iteration 4278, loss = 0.20126483\n",
      "Iteration 4279, loss = 0.20126016\n",
      "Iteration 4280, loss = 0.20125886\n",
      "Iteration 4281, loss = 0.20125777\n",
      "Iteration 4282, loss = 0.20125367\n",
      "Iteration 4283, loss = 0.20124934\n",
      "Iteration 4284, loss = 0.20125049\n",
      "Iteration 4285, loss = 0.20124887\n",
      "Iteration 4286, loss = 0.20124623\n",
      "Iteration 4287, loss = 0.20124236\n",
      "Iteration 4288, loss = 0.20124088\n",
      "Iteration 4289, loss = 0.20123926\n",
      "Iteration 4290, loss = 0.20123550\n",
      "Iteration 4291, loss = 0.20123362\n",
      "Iteration 4292, loss = 0.20123002\n",
      "Iteration 4293, loss = 0.20122879\n",
      "Iteration 4294, loss = 0.20122470\n",
      "Iteration 4295, loss = 0.20122359\n",
      "Iteration 4296, loss = 0.20122421\n",
      "Iteration 4297, loss = 0.20122191\n",
      "Iteration 4298, loss = 0.20121990\n",
      "Iteration 4299, loss = 0.20121761\n",
      "Iteration 4300, loss = 0.20121369\n",
      "Iteration 4301, loss = 0.20121065\n",
      "Iteration 4302, loss = 0.20120959\n",
      "Iteration 4303, loss = 0.20120654\n",
      "Iteration 4304, loss = 0.20120624\n",
      "Iteration 4305, loss = 0.20120348\n",
      "Iteration 4306, loss = 0.20120111\n",
      "Iteration 4307, loss = 0.20119943\n",
      "Iteration 4308, loss = 0.20119771\n",
      "Iteration 4309, loss = 0.20119416\n",
      "Iteration 4310, loss = 0.20119415\n",
      "Iteration 4311, loss = 0.20119330\n",
      "Iteration 4312, loss = 0.20118993\n",
      "Iteration 4313, loss = 0.20118764\n",
      "Iteration 4314, loss = 0.20118703\n",
      "Iteration 4315, loss = 0.20118535\n",
      "Iteration 4316, loss = 0.20118074\n",
      "Iteration 4317, loss = 0.20117974\n",
      "Iteration 4318, loss = 0.20118150\n",
      "Iteration 4319, loss = 0.20117907\n",
      "Iteration 4320, loss = 0.20117389\n",
      "Iteration 4321, loss = 0.20117331\n",
      "Iteration 4322, loss = 0.20116988\n",
      "Iteration 4323, loss = 0.20116824\n",
      "Iteration 4324, loss = 0.20116612\n",
      "Iteration 4325, loss = 0.20116174\n",
      "Iteration 4326, loss = 0.20116298\n",
      "Iteration 4327, loss = 0.20116106\n",
      "Iteration 4328, loss = 0.20116003\n",
      "Iteration 4329, loss = 0.20115646\n",
      "Iteration 4330, loss = 0.20115108\n",
      "Iteration 4331, loss = 0.20115171\n",
      "Iteration 4332, loss = 0.20115159\n",
      "Iteration 4333, loss = 0.20114846\n",
      "Iteration 4334, loss = 0.20114649\n",
      "Iteration 4335, loss = 0.20114369\n",
      "Iteration 4336, loss = 0.20113882\n",
      "Iteration 4337, loss = 0.20113795\n",
      "Iteration 4338, loss = 0.20113838\n",
      "Iteration 4339, loss = 0.20113540\n",
      "Iteration 4340, loss = 0.20113404\n",
      "Iteration 4341, loss = 0.20113097\n",
      "Iteration 4342, loss = 0.20113036\n",
      "Iteration 4343, loss = 0.20112797\n",
      "Iteration 4344, loss = 0.20112606\n",
      "Iteration 4345, loss = 0.20112478\n",
      "Iteration 4346, loss = 0.20112038\n",
      "Iteration 4347, loss = 0.20111431\n",
      "Iteration 4348, loss = 0.20111358\n",
      "Iteration 4349, loss = 0.20111115\n",
      "Iteration 4350, loss = 0.20111068\n",
      "Iteration 4351, loss = 0.20110807\n",
      "Iteration 4352, loss = 0.20110386\n",
      "Iteration 4353, loss = 0.20110511\n",
      "Iteration 4354, loss = 0.20110467\n",
      "Iteration 4355, loss = 0.20110129\n",
      "Iteration 4356, loss = 0.20109711\n",
      "Iteration 4357, loss = 0.20109600\n",
      "Iteration 4358, loss = 0.20109527\n",
      "Iteration 4359, loss = 0.20109345\n",
      "Iteration 4360, loss = 0.20108747\n",
      "Iteration 4361, loss = 0.20108967\n",
      "Iteration 4362, loss = 0.20108865\n",
      "Iteration 4363, loss = 0.20108606\n",
      "Iteration 4364, loss = 0.20108342\n",
      "Iteration 4365, loss = 0.20108090\n",
      "Iteration 4366, loss = 0.20108082\n",
      "Iteration 4367, loss = 0.20107915\n",
      "Iteration 4368, loss = 0.20107830\n",
      "Iteration 4369, loss = 0.20107482\n",
      "Iteration 4370, loss = 0.20107106\n",
      "Iteration 4371, loss = 0.20107058\n",
      "Iteration 4372, loss = 0.20106923\n",
      "Iteration 4373, loss = 0.20106514\n",
      "Iteration 4374, loss = 0.20106173\n",
      "Iteration 4375, loss = 0.20106186\n",
      "Iteration 4376, loss = 0.20106195\n",
      "Iteration 4377, loss = 0.20105744\n",
      "Iteration 4378, loss = 0.20105369\n",
      "Iteration 4379, loss = 0.20105236\n",
      "Iteration 4380, loss = 0.20105216\n",
      "Iteration 4381, loss = 0.20104855\n",
      "Iteration 4382, loss = 0.20104659\n",
      "Iteration 4383, loss = 0.20104683\n",
      "Iteration 4384, loss = 0.20104414\n",
      "Iteration 4385, loss = 0.20104230\n",
      "Iteration 4386, loss = 0.20103967\n",
      "Iteration 4387, loss = 0.20103597\n",
      "Iteration 4388, loss = 0.20103491\n",
      "Iteration 4389, loss = 0.20103419\n",
      "Iteration 4390, loss = 0.20103004\n",
      "Iteration 4391, loss = 0.20102851\n",
      "Iteration 4392, loss = 0.20102634\n",
      "Iteration 4393, loss = 0.20102617\n",
      "Iteration 4394, loss = 0.20102525\n",
      "Iteration 4395, loss = 0.20102125\n",
      "Iteration 4396, loss = 0.20102165\n",
      "Iteration 4397, loss = 0.20102204\n",
      "Iteration 4398, loss = 0.20101915\n",
      "Iteration 4399, loss = 0.20101475\n",
      "Iteration 4400, loss = 0.20101452\n",
      "Iteration 4401, loss = 0.20101418\n",
      "Iteration 4402, loss = 0.20101166\n",
      "Iteration 4403, loss = 0.20100704\n",
      "Iteration 4404, loss = 0.20100659\n",
      "Iteration 4405, loss = 0.20100698\n",
      "Iteration 4406, loss = 0.20100483\n",
      "Iteration 4407, loss = 0.20100085\n",
      "Iteration 4408, loss = 0.20099812\n",
      "Iteration 4409, loss = 0.20099847\n",
      "Iteration 4410, loss = 0.20099658\n",
      "Iteration 4411, loss = 0.20099489\n",
      "Iteration 4412, loss = 0.20099132\n",
      "Iteration 4413, loss = 0.20098962\n",
      "Iteration 4414, loss = 0.20098880\n",
      "Iteration 4415, loss = 0.20098687\n",
      "Iteration 4416, loss = 0.20098308\n",
      "Iteration 4417, loss = 0.20097993\n",
      "Iteration 4418, loss = 0.20097918\n",
      "Iteration 4419, loss = 0.20097873\n",
      "Iteration 4420, loss = 0.20097550\n",
      "Iteration 4421, loss = 0.20097325\n",
      "Iteration 4422, loss = 0.20097424\n",
      "Iteration 4423, loss = 0.20097192\n",
      "Iteration 4424, loss = 0.20096786\n",
      "Iteration 4425, loss = 0.20096556\n",
      "Iteration 4426, loss = 0.20096805\n",
      "Iteration 4427, loss = 0.20096715\n",
      "Iteration 4428, loss = 0.20096463\n",
      "Iteration 4429, loss = 0.20096215\n",
      "Iteration 4430, loss = 0.20095888\n",
      "Iteration 4431, loss = 0.20095426\n",
      "Iteration 4432, loss = 0.20095604\n",
      "Iteration 4433, loss = 0.20095513\n",
      "Iteration 4434, loss = 0.20095135\n",
      "Iteration 4435, loss = 0.20095131\n",
      "Iteration 4436, loss = 0.20095052\n",
      "Iteration 4437, loss = 0.20094828\n",
      "Iteration 4438, loss = 0.20094584\n",
      "Iteration 4439, loss = 0.20094231\n",
      "Iteration 4440, loss = 0.20094154\n",
      "Iteration 4441, loss = 0.20094004\n",
      "Iteration 4442, loss = 0.20093954\n",
      "Iteration 4443, loss = 0.20093760\n",
      "Iteration 4444, loss = 0.20093245\n",
      "Iteration 4445, loss = 0.20093293\n",
      "Iteration 4446, loss = 0.20093424\n",
      "Iteration 4447, loss = 0.20093488\n",
      "Iteration 4448, loss = 0.20093182\n",
      "Iteration 4449, loss = 0.20092667\n",
      "Iteration 4450, loss = 0.20092253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4451, loss = 0.20092166\n",
      "Iteration 4452, loss = 0.20092146\n",
      "Iteration 4453, loss = 0.20091910\n",
      "Iteration 4454, loss = 0.20091838\n",
      "Iteration 4455, loss = 0.20091545\n",
      "Iteration 4456, loss = 0.20091399\n",
      "Iteration 4457, loss = 0.20091293\n",
      "Iteration 4458, loss = 0.20091223\n",
      "Iteration 4459, loss = 0.20091157\n",
      "Iteration 4460, loss = 0.20090995\n",
      "Iteration 4461, loss = 0.20090429\n",
      "Iteration 4462, loss = 0.20090323\n",
      "Iteration 4463, loss = 0.20090560\n",
      "Iteration 4464, loss = 0.20090579\n",
      "Iteration 4465, loss = 0.20090280\n",
      "Iteration 4466, loss = 0.20089630\n",
      "Iteration 4467, loss = 0.20089141\n",
      "Iteration 4468, loss = 0.20089308\n",
      "Iteration 4469, loss = 0.20089160\n",
      "Iteration 4470, loss = 0.20088725\n",
      "Iteration 4471, loss = 0.20088702\n",
      "Iteration 4472, loss = 0.20088697\n",
      "Iteration 4473, loss = 0.20088445\n",
      "Iteration 4474, loss = 0.20088191\n",
      "Iteration 4475, loss = 0.20087948\n",
      "Iteration 4476, loss = 0.20087854\n",
      "Iteration 4477, loss = 0.20087828\n",
      "Iteration 4478, loss = 0.20087544\n",
      "Iteration 4479, loss = 0.20087110\n",
      "Iteration 4480, loss = 0.20086736\n",
      "Iteration 4481, loss = 0.20087007\n",
      "Iteration 4482, loss = 0.20086965\n",
      "Iteration 4483, loss = 0.20086622\n",
      "Iteration 4484, loss = 0.20086281\n",
      "Iteration 4485, loss = 0.20085768\n",
      "Iteration 4486, loss = 0.20086216\n",
      "Iteration 4487, loss = 0.20086332\n",
      "Iteration 4488, loss = 0.20086065\n",
      "Iteration 4489, loss = 0.20085863\n",
      "Iteration 4490, loss = 0.20085602\n",
      "Iteration 4491, loss = 0.20085105\n",
      "Iteration 4492, loss = 0.20084992\n",
      "Iteration 4493, loss = 0.20085013\n",
      "Iteration 4494, loss = 0.20084780\n",
      "Iteration 4495, loss = 0.20084606\n",
      "Iteration 4496, loss = 0.20084212\n",
      "Iteration 4497, loss = 0.20084086\n",
      "Iteration 4498, loss = 0.20083853\n",
      "Iteration 4499, loss = 0.20083640\n",
      "Iteration 4500, loss = 0.20083685\n",
      "Iteration 4501, loss = 0.20083595\n",
      "Iteration 4502, loss = 0.20083332\n",
      "Iteration 4503, loss = 0.20082827\n",
      "Iteration 4504, loss = 0.20082779\n",
      "Iteration 4505, loss = 0.20082808\n",
      "Iteration 4506, loss = 0.20082461\n",
      "Iteration 4507, loss = 0.20082177\n",
      "Iteration 4508, loss = 0.20082111\n",
      "Iteration 4509, loss = 0.20082064\n",
      "Iteration 4510, loss = 0.20082072\n",
      "Iteration 4511, loss = 0.20081605\n",
      "Iteration 4512, loss = 0.20081445\n",
      "Iteration 4513, loss = 0.20081531\n",
      "Iteration 4514, loss = 0.20081353\n",
      "Iteration 4515, loss = 0.20081196\n",
      "Iteration 4516, loss = 0.20081037\n",
      "Iteration 4517, loss = 0.20080808\n",
      "Iteration 4518, loss = 0.20080518\n",
      "Iteration 4519, loss = 0.20080281\n",
      "Iteration 4520, loss = 0.20080167\n",
      "Iteration 4521, loss = 0.20080038\n",
      "Iteration 4522, loss = 0.20079964\n",
      "Iteration 4523, loss = 0.20080269\n",
      "Iteration 4524, loss = 0.20079913\n",
      "Iteration 4525, loss = 0.20079422\n",
      "Iteration 4526, loss = 0.20079421\n",
      "Iteration 4527, loss = 0.20079429\n",
      "Iteration 4528, loss = 0.20079177\n",
      "Iteration 4529, loss = 0.20078991\n",
      "Iteration 4530, loss = 0.20078639\n",
      "Iteration 4531, loss = 0.20078329\n",
      "Iteration 4532, loss = 0.20078038\n",
      "Iteration 4533, loss = 0.20078266\n",
      "Iteration 4534, loss = 0.20078030\n",
      "Iteration 4535, loss = 0.20077828\n",
      "Iteration 4536, loss = 0.20077528\n",
      "Iteration 4537, loss = 0.20077516\n",
      "Iteration 4538, loss = 0.20077608\n",
      "Iteration 4539, loss = 0.20077377\n",
      "Iteration 4540, loss = 0.20077016\n",
      "Iteration 4541, loss = 0.20076961\n",
      "Iteration 4542, loss = 0.20076795\n",
      "Iteration 4543, loss = 0.20076598\n",
      "Iteration 4544, loss = 0.20076272\n",
      "Iteration 4545, loss = 0.20076095\n",
      "Iteration 4546, loss = 0.20076155\n",
      "Iteration 4547, loss = 0.20076266\n",
      "Iteration 4548, loss = 0.20075936\n",
      "Iteration 4549, loss = 0.20075740\n",
      "Iteration 4550, loss = 0.20075481\n",
      "Iteration 4551, loss = 0.20075531\n",
      "Iteration 4552, loss = 0.20075384\n",
      "Iteration 4553, loss = 0.20074940\n",
      "Iteration 4554, loss = 0.20074852\n",
      "Iteration 4555, loss = 0.20074966\n",
      "Iteration 4556, loss = 0.20074779\n",
      "Iteration 4557, loss = 0.20074240\n",
      "Iteration 4558, loss = 0.20074180\n",
      "Iteration 4559, loss = 0.20074047\n",
      "Iteration 4560, loss = 0.20073968\n",
      "Iteration 4561, loss = 0.20073840\n",
      "Iteration 4562, loss = 0.20073439\n",
      "Iteration 4563, loss = 0.20073288\n",
      "Iteration 4564, loss = 0.20073127\n",
      "Iteration 4565, loss = 0.20073134\n",
      "Iteration 4566, loss = 0.20072885\n",
      "Iteration 4567, loss = 0.20072739\n",
      "Iteration 4568, loss = 0.20072763\n",
      "Iteration 4569, loss = 0.20072624\n",
      "Iteration 4570, loss = 0.20072378\n",
      "Iteration 4571, loss = 0.20072267\n",
      "Iteration 4572, loss = 0.20072199\n",
      "Iteration 4573, loss = 0.20072239\n",
      "Iteration 4574, loss = 0.20071940\n",
      "Iteration 4575, loss = 0.20071469\n",
      "Iteration 4576, loss = 0.20071410\n",
      "Iteration 4577, loss = 0.20071433\n",
      "Iteration 4578, loss = 0.20071300\n",
      "Iteration 4579, loss = 0.20071021\n",
      "Iteration 4580, loss = 0.20071035\n",
      "Iteration 4581, loss = 0.20070837\n",
      "Iteration 4582, loss = 0.20070789\n",
      "Iteration 4583, loss = 0.20070637\n",
      "Iteration 4584, loss = 0.20070209\n",
      "Iteration 4585, loss = 0.20070144\n",
      "Iteration 4586, loss = 0.20070191\n",
      "Iteration 4587, loss = 0.20069824\n",
      "Iteration 4588, loss = 0.20069332\n",
      "Iteration 4589, loss = 0.20069559\n",
      "Iteration 4590, loss = 0.20069557\n",
      "Iteration 4591, loss = 0.20069294\n",
      "Iteration 4592, loss = 0.20069068\n",
      "Iteration 4593, loss = 0.20068820\n",
      "Iteration 4594, loss = 0.20068945\n",
      "Iteration 4595, loss = 0.20068863\n",
      "Iteration 4596, loss = 0.20068675\n",
      "Iteration 4597, loss = 0.20068303\n",
      "Iteration 4598, loss = 0.20067986\n",
      "Iteration 4599, loss = 0.20067919\n",
      "Iteration 4600, loss = 0.20067904\n",
      "Iteration 4601, loss = 0.20067483\n",
      "Iteration 4602, loss = 0.20067411\n",
      "Iteration 4603, loss = 0.20067288\n",
      "Iteration 4604, loss = 0.20067094\n",
      "Iteration 4605, loss = 0.20066884\n",
      "Iteration 4606, loss = 0.20066782\n",
      "Iteration 4607, loss = 0.20066770\n",
      "Iteration 4608, loss = 0.20066630\n",
      "Iteration 4609, loss = 0.20066347\n",
      "Iteration 4610, loss = 0.20066131\n",
      "Iteration 4611, loss = 0.20066000\n",
      "Iteration 4612, loss = 0.20065889\n",
      "Iteration 4613, loss = 0.20065870\n",
      "Iteration 4614, loss = 0.20065827\n",
      "Iteration 4615, loss = 0.20065567\n",
      "Iteration 4616, loss = 0.20065331\n",
      "Iteration 4617, loss = 0.20065326\n",
      "Iteration 4618, loss = 0.20065338\n",
      "Iteration 4619, loss = 0.20064936\n",
      "Iteration 4620, loss = 0.20064664\n",
      "Iteration 4621, loss = 0.20064607\n",
      "Iteration 4622, loss = 0.20064610\n",
      "Iteration 4623, loss = 0.20064631\n",
      "Iteration 4624, loss = 0.20064314\n",
      "Iteration 4625, loss = 0.20064170\n",
      "Iteration 4626, loss = 0.20064061\n",
      "Iteration 4627, loss = 0.20063902\n",
      "Iteration 4628, loss = 0.20063678\n",
      "Iteration 4629, loss = 0.20063316\n",
      "Iteration 4630, loss = 0.20063305\n",
      "Iteration 4631, loss = 0.20063252\n",
      "Iteration 4632, loss = 0.20062821\n",
      "Iteration 4633, loss = 0.20063021\n",
      "Iteration 4634, loss = 0.20063064\n",
      "Iteration 4635, loss = 0.20062810\n",
      "Iteration 4636, loss = 0.20062489\n",
      "Iteration 4637, loss = 0.20062033\n",
      "Iteration 4638, loss = 0.20062140\n",
      "Iteration 4639, loss = 0.20062034\n",
      "Iteration 4640, loss = 0.20062064\n",
      "Iteration 4641, loss = 0.20061904\n",
      "Iteration 4642, loss = 0.20061661\n",
      "Iteration 4643, loss = 0.20061570\n",
      "Iteration 4644, loss = 0.20061705\n",
      "Iteration 4645, loss = 0.20061628\n",
      "Iteration 4646, loss = 0.20061389\n",
      "Iteration 4647, loss = 0.20060795\n",
      "Iteration 4648, loss = 0.20060756\n",
      "Iteration 4649, loss = 0.20060958\n",
      "Iteration 4650, loss = 0.20060710\n",
      "Iteration 4651, loss = 0.20060422\n",
      "Iteration 4652, loss = 0.20060096\n",
      "Iteration 4653, loss = 0.20060182\n",
      "Iteration 4654, loss = 0.20060043\n",
      "Iteration 4655, loss = 0.20059862\n",
      "Iteration 4656, loss = 0.20059706\n",
      "Iteration 4657, loss = 0.20059526\n",
      "Iteration 4658, loss = 0.20059529\n",
      "Iteration 4659, loss = 0.20059429\n",
      "Iteration 4660, loss = 0.20058953\n",
      "Iteration 4661, loss = 0.20058783\n",
      "Iteration 4662, loss = 0.20058723\n",
      "Iteration 4663, loss = 0.20058586\n",
      "Iteration 4664, loss = 0.20058615\n",
      "Iteration 4665, loss = 0.20058568\n",
      "Iteration 4666, loss = 0.20058177\n",
      "Iteration 4667, loss = 0.20057960\n",
      "Iteration 4668, loss = 0.20058222\n",
      "Iteration 4669, loss = 0.20057958\n",
      "Iteration 4670, loss = 0.20057718\n",
      "Iteration 4671, loss = 0.20057525\n",
      "Iteration 4672, loss = 0.20057793\n",
      "Iteration 4673, loss = 0.20057736\n",
      "Iteration 4674, loss = 0.20057469\n",
      "Iteration 4675, loss = 0.20057292\n",
      "Iteration 4676, loss = 0.20056918\n",
      "Iteration 4677, loss = 0.20056572\n",
      "Iteration 4678, loss = 0.20056766\n",
      "Iteration 4679, loss = 0.20056588\n",
      "Iteration 4680, loss = 0.20056224\n",
      "Iteration 4681, loss = 0.20056217\n",
      "Iteration 4682, loss = 0.20056249\n",
      "Iteration 4683, loss = 0.20056184\n",
      "Iteration 4684, loss = 0.20056217\n",
      "Iteration 4685, loss = 0.20055775\n",
      "Iteration 4686, loss = 0.20055564\n",
      "Iteration 4687, loss = 0.20055732\n",
      "Iteration 4688, loss = 0.20055556\n",
      "Iteration 4689, loss = 0.20055319\n",
      "Iteration 4690, loss = 0.20055210\n",
      "Iteration 4691, loss = 0.20054937\n",
      "Iteration 4692, loss = 0.20054960\n",
      "Iteration 4693, loss = 0.20054929\n",
      "Iteration 4694, loss = 0.20054571\n",
      "Iteration 4695, loss = 0.20054283\n",
      "Iteration 4696, loss = 0.20054129\n",
      "Iteration 4697, loss = 0.20054449\n",
      "Iteration 4698, loss = 0.20054221\n",
      "Iteration 4699, loss = 0.20053866\n",
      "Iteration 4700, loss = 0.20053597\n",
      "Iteration 4701, loss = 0.20053593\n",
      "Iteration 4702, loss = 0.20053580\n",
      "Iteration 4703, loss = 0.20053378\n",
      "Iteration 4704, loss = 0.20053081\n",
      "Iteration 4705, loss = 0.20052879\n",
      "Iteration 4706, loss = 0.20052713\n",
      "Iteration 4707, loss = 0.20052763\n",
      "Iteration 4708, loss = 0.20052807\n",
      "Iteration 4709, loss = 0.20052303\n",
      "Iteration 4710, loss = 0.20052383\n",
      "Iteration 4711, loss = 0.20052454\n",
      "Iteration 4712, loss = 0.20052286\n",
      "Iteration 4713, loss = 0.20051945\n",
      "Iteration 4714, loss = 0.20051640\n",
      "Iteration 4715, loss = 0.20051705\n",
      "Iteration 4716, loss = 0.20051416\n",
      "Iteration 4717, loss = 0.20051186\n",
      "Iteration 4718, loss = 0.20051224\n",
      "Iteration 4719, loss = 0.20051091\n",
      "Iteration 4720, loss = 0.20050978\n",
      "Iteration 4721, loss = 0.20051151\n",
      "Iteration 4722, loss = 0.20050881\n",
      "Iteration 4723, loss = 0.20050630\n",
      "Iteration 4724, loss = 0.20050499\n",
      "Iteration 4725, loss = 0.20050523\n",
      "Iteration 4726, loss = 0.20050353\n",
      "Iteration 4727, loss = 0.20050217\n",
      "Iteration 4728, loss = 0.20049999\n",
      "Iteration 4729, loss = 0.20049929\n",
      "Iteration 4730, loss = 0.20049769\n",
      "Iteration 4731, loss = 0.20049681\n",
      "Iteration 4732, loss = 0.20049388\n",
      "Iteration 4733, loss = 0.20049134\n",
      "Iteration 4734, loss = 0.20049184\n",
      "Iteration 4735, loss = 0.20049045\n",
      "Iteration 4736, loss = 0.20048997\n",
      "Iteration 4737, loss = 0.20048914\n",
      "Iteration 4738, loss = 0.20048714\n",
      "Iteration 4739, loss = 0.20048768\n",
      "Iteration 4740, loss = 0.20048595\n",
      "Iteration 4741, loss = 0.20048620\n",
      "Iteration 4742, loss = 0.20048427\n",
      "Iteration 4743, loss = 0.20048369\n",
      "Iteration 4744, loss = 0.20048283\n",
      "Iteration 4745, loss = 0.20047944\n",
      "Iteration 4746, loss = 0.20047902\n",
      "Iteration 4747, loss = 0.20047838\n",
      "Iteration 4748, loss = 0.20047764\n",
      "Iteration 4749, loss = 0.20047595\n",
      "Iteration 4750, loss = 0.20047512\n",
      "Iteration 4751, loss = 0.20047226\n",
      "Iteration 4752, loss = 0.20046908\n",
      "Iteration 4753, loss = 0.20046809\n",
      "Iteration 4754, loss = 0.20046839\n",
      "Iteration 4755, loss = 0.20046653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4756, loss = 0.20046364\n",
      "Iteration 4757, loss = 0.20046203\n",
      "Iteration 4758, loss = 0.20046217\n",
      "Iteration 4759, loss = 0.20045903\n",
      "Iteration 4760, loss = 0.20045866\n",
      "Iteration 4761, loss = 0.20045873\n",
      "Iteration 4762, loss = 0.20045579\n",
      "Iteration 4763, loss = 0.20045618\n",
      "Iteration 4764, loss = 0.20045553\n",
      "Iteration 4765, loss = 0.20045407\n",
      "Iteration 4766, loss = 0.20045270\n",
      "Iteration 4767, loss = 0.20045148\n",
      "Iteration 4768, loss = 0.20045377\n",
      "Iteration 4769, loss = 0.20045310\n",
      "Iteration 4770, loss = 0.20045053\n",
      "Iteration 4771, loss = 0.20045065\n",
      "Iteration 4772, loss = 0.20044782\n",
      "Iteration 4773, loss = 0.20044940\n",
      "Iteration 4774, loss = 0.20044949\n",
      "Iteration 4775, loss = 0.20044676\n",
      "Iteration 4776, loss = 0.20044303\n",
      "Iteration 4777, loss = 0.20044171\n",
      "Iteration 4778, loss = 0.20043813\n",
      "Iteration 4779, loss = 0.20043383\n",
      "Iteration 4780, loss = 0.20043429\n",
      "Iteration 4781, loss = 0.20043316\n",
      "Iteration 4782, loss = 0.20043008\n",
      "Iteration 4783, loss = 0.20042906\n",
      "Iteration 4784, loss = 0.20042979\n",
      "Iteration 4785, loss = 0.20042817\n",
      "Iteration 4786, loss = 0.20042432\n",
      "Iteration 4787, loss = 0.20042275\n",
      "Iteration 4788, loss = 0.20042360\n",
      "Iteration 4789, loss = 0.20042148\n",
      "Iteration 4790, loss = 0.20041980\n",
      "Iteration 4791, loss = 0.20042056\n",
      "Iteration 4792, loss = 0.20042167\n",
      "Iteration 4793, loss = 0.20041718\n",
      "Iteration 4794, loss = 0.20041565\n",
      "Iteration 4795, loss = 0.20041639\n",
      "Iteration 4796, loss = 0.20041531\n",
      "Iteration 4797, loss = 0.20041127\n",
      "Iteration 4798, loss = 0.20041355\n",
      "Iteration 4799, loss = 0.20041395\n",
      "Iteration 4800, loss = 0.20041183\n",
      "Iteration 4801, loss = 0.20040811\n",
      "Iteration 4802, loss = 0.20040858\n",
      "Iteration 4803, loss = 0.20040622\n",
      "Iteration 4804, loss = 0.20040762\n",
      "Iteration 4805, loss = 0.20040594\n",
      "Iteration 4806, loss = 0.20040287\n",
      "Iteration 4807, loss = 0.20039951\n",
      "Iteration 4808, loss = 0.20040210\n",
      "Iteration 4809, loss = 0.20040425\n",
      "Iteration 4810, loss = 0.20040323\n",
      "Iteration 4811, loss = 0.20039880\n",
      "Iteration 4812, loss = 0.20039738\n",
      "Iteration 4813, loss = 0.20039658\n",
      "Iteration 4814, loss = 0.20039581\n",
      "Iteration 4815, loss = 0.20039308\n",
      "Iteration 4816, loss = 0.20039154\n",
      "Iteration 4817, loss = 0.20039166\n",
      "Iteration 4818, loss = 0.20038792\n",
      "Iteration 4819, loss = 0.20038688\n",
      "Iteration 4820, loss = 0.20038496\n",
      "Iteration 4821, loss = 0.20038470\n",
      "Iteration 4822, loss = 0.20038190\n",
      "Iteration 4823, loss = 0.20038205\n",
      "Iteration 4824, loss = 0.20038085\n",
      "Iteration 4825, loss = 0.20037947\n",
      "Iteration 4826, loss = 0.20037560\n",
      "Iteration 4827, loss = 0.20037490\n",
      "Iteration 4828, loss = 0.20037347\n",
      "Iteration 4829, loss = 0.20037347\n",
      "Iteration 4830, loss = 0.20037291\n",
      "Iteration 4831, loss = 0.20037078\n",
      "Iteration 4832, loss = 0.20036971\n",
      "Iteration 4833, loss = 0.20036962\n",
      "Iteration 4834, loss = 0.20036760\n",
      "Iteration 4835, loss = 0.20036876\n",
      "Iteration 4836, loss = 0.20036727\n",
      "Iteration 4837, loss = 0.20036546\n",
      "Iteration 4838, loss = 0.20036320\n",
      "Iteration 4839, loss = 0.20036184\n",
      "Iteration 4840, loss = 0.20036037\n",
      "Iteration 4841, loss = 0.20035937\n",
      "Iteration 4842, loss = 0.20035829\n",
      "Iteration 4843, loss = 0.20035648\n",
      "Iteration 4844, loss = 0.20035836\n",
      "Iteration 4845, loss = 0.20035800\n",
      "Iteration 4846, loss = 0.20035496\n",
      "Iteration 4847, loss = 0.20035338\n",
      "Iteration 4848, loss = 0.20035302\n",
      "Iteration 4849, loss = 0.20035044\n",
      "Iteration 4850, loss = 0.20034938\n",
      "Iteration 4851, loss = 0.20034851\n",
      "Iteration 4852, loss = 0.20034707\n",
      "Iteration 4853, loss = 0.20034514\n",
      "Iteration 4854, loss = 0.20034401\n",
      "Iteration 4855, loss = 0.20034251\n",
      "Iteration 4856, loss = 0.20034147\n",
      "Iteration 4857, loss = 0.20033997\n",
      "Iteration 4858, loss = 0.20033846\n",
      "Iteration 4859, loss = 0.20033823\n",
      "Iteration 4860, loss = 0.20033742\n",
      "Iteration 4861, loss = 0.20033640\n",
      "Iteration 4862, loss = 0.20033430\n",
      "Iteration 4863, loss = 0.20033358\n",
      "Iteration 4864, loss = 0.20033402\n",
      "Iteration 4865, loss = 0.20033127\n",
      "Iteration 4866, loss = 0.20033093\n",
      "Iteration 4867, loss = 0.20032923\n",
      "Iteration 4868, loss = 0.20032755\n",
      "Iteration 4869, loss = 0.20032599\n",
      "Iteration 4870, loss = 0.20032540\n",
      "Iteration 4871, loss = 0.20032505\n",
      "Iteration 4872, loss = 0.20032346\n",
      "Iteration 4873, loss = 0.20032263\n",
      "Iteration 4874, loss = 0.20032068\n",
      "Iteration 4875, loss = 0.20031845\n",
      "Iteration 4876, loss = 0.20031683\n",
      "Iteration 4877, loss = 0.20031663\n",
      "Iteration 4878, loss = 0.20031491\n",
      "Iteration 4879, loss = 0.20031499\n",
      "Iteration 4880, loss = 0.20031457\n",
      "Iteration 4881, loss = 0.20031305\n",
      "Iteration 4882, loss = 0.20030952\n",
      "Iteration 4883, loss = 0.20030993\n",
      "Iteration 4884, loss = 0.20031020\n",
      "Iteration 4885, loss = 0.20030838\n",
      "Iteration 4886, loss = 0.20030517\n",
      "Iteration 4887, loss = 0.20030540\n",
      "Iteration 4888, loss = 0.20030411\n",
      "Iteration 4889, loss = 0.20030095\n",
      "Iteration 4890, loss = 0.20030139\n",
      "Iteration 4891, loss = 0.20030219\n",
      "Iteration 4892, loss = 0.20029890\n",
      "Iteration 4893, loss = 0.20029817\n",
      "Iteration 4894, loss = 0.20029796\n",
      "Iteration 4895, loss = 0.20029698\n",
      "Iteration 4896, loss = 0.20029460\n",
      "Iteration 4897, loss = 0.20029306\n",
      "Iteration 4898, loss = 0.20029290\n",
      "Iteration 4899, loss = 0.20028989\n",
      "Iteration 4900, loss = 0.20028932\n",
      "Iteration 4901, loss = 0.20028791\n",
      "Iteration 4902, loss = 0.20028578\n",
      "Iteration 4903, loss = 0.20028423\n",
      "Iteration 4904, loss = 0.20028271\n",
      "Iteration 4905, loss = 0.20028311\n",
      "Iteration 4906, loss = 0.20028050\n",
      "Iteration 4907, loss = 0.20027785\n",
      "Iteration 4908, loss = 0.20027704\n",
      "Iteration 4909, loss = 0.20027652\n",
      "Iteration 4910, loss = 0.20027480\n",
      "Iteration 4911, loss = 0.20027581\n",
      "Iteration 4912, loss = 0.20027452\n",
      "Iteration 4913, loss = 0.20027191\n",
      "Iteration 4914, loss = 0.20026959\n",
      "Iteration 4915, loss = 0.20026596\n",
      "Iteration 4916, loss = 0.20026504\n",
      "Iteration 4917, loss = 0.20026439\n",
      "Iteration 4918, loss = 0.20026574\n",
      "Iteration 4919, loss = 0.20026483\n",
      "Iteration 4920, loss = 0.20026185\n",
      "Iteration 4921, loss = 0.20026156\n",
      "Iteration 4922, loss = 0.20026298\n",
      "Iteration 4923, loss = 0.20026396\n",
      "Iteration 4924, loss = 0.20025877\n",
      "Iteration 4925, loss = 0.20025429\n",
      "Iteration 4926, loss = 0.20025540\n",
      "Iteration 4927, loss = 0.20025361\n",
      "Iteration 4928, loss = 0.20025192\n",
      "Iteration 4929, loss = 0.20024952\n",
      "Iteration 4930, loss = 0.20024729\n",
      "Iteration 4931, loss = 0.20024695\n",
      "Iteration 4932, loss = 0.20024577\n",
      "Iteration 4933, loss = 0.20024374\n",
      "Iteration 4934, loss = 0.20024506\n",
      "Iteration 4935, loss = 0.20024361\n",
      "Iteration 4936, loss = 0.20024140\n",
      "Iteration 4937, loss = 0.20024092\n",
      "Iteration 4938, loss = 0.20023712\n",
      "Iteration 4939, loss = 0.20023414\n",
      "Iteration 4940, loss = 0.20023270\n",
      "Iteration 4941, loss = 0.20023356\n",
      "Iteration 4942, loss = 0.20023104\n",
      "Iteration 4943, loss = 0.20022991\n",
      "Iteration 4944, loss = 0.20022988\n",
      "Iteration 4945, loss = 0.20022904\n",
      "Iteration 4946, loss = 0.20022773\n",
      "Iteration 4947, loss = 0.20022558\n",
      "Iteration 4948, loss = 0.20022441\n",
      "Iteration 4949, loss = 0.20022279\n",
      "Iteration 4950, loss = 0.20022170\n",
      "Iteration 4951, loss = 0.20021939\n",
      "Iteration 4952, loss = 0.20021852\n",
      "Iteration 4953, loss = 0.20021684\n",
      "Iteration 4954, loss = 0.20021587\n",
      "Iteration 4955, loss = 0.20021514\n",
      "Iteration 4956, loss = 0.20021251\n",
      "Iteration 4957, loss = 0.20021022\n",
      "Iteration 4958, loss = 0.20020956\n",
      "Iteration 4959, loss = 0.20020921\n",
      "Iteration 4960, loss = 0.20020692\n",
      "Iteration 4961, loss = 0.20020799\n",
      "Iteration 4962, loss = 0.20020929\n",
      "Iteration 4963, loss = 0.20020537\n",
      "Iteration 4964, loss = 0.20020182\n",
      "Iteration 4965, loss = 0.20020032\n",
      "Iteration 4966, loss = 0.20020012\n",
      "Iteration 4967, loss = 0.20019850\n",
      "Iteration 4968, loss = 0.20019566\n",
      "Iteration 4969, loss = 0.20019341\n",
      "Iteration 4970, loss = 0.20019461\n",
      "Iteration 4971, loss = 0.20019546\n",
      "Iteration 4972, loss = 0.20019280\n",
      "Iteration 4973, loss = 0.20019022\n",
      "Iteration 4974, loss = 0.20018876\n",
      "Iteration 4975, loss = 0.20018849\n",
      "Iteration 4976, loss = 0.20018778\n",
      "Iteration 4977, loss = 0.20018473\n",
      "Iteration 4978, loss = 0.20018119\n",
      "Iteration 4979, loss = 0.20018218\n",
      "Iteration 4980, loss = 0.20018317\n",
      "Iteration 4981, loss = 0.20018083\n",
      "Iteration 4982, loss = 0.20017693\n",
      "Iteration 4983, loss = 0.20017638\n",
      "Iteration 4984, loss = 0.20017462\n",
      "Iteration 4985, loss = 0.20017310\n",
      "Iteration 4986, loss = 0.20017270\n",
      "Iteration 4987, loss = 0.20016941\n",
      "Iteration 4988, loss = 0.20016873\n",
      "Iteration 4989, loss = 0.20016955\n",
      "Iteration 4990, loss = 0.20016833\n",
      "Iteration 4991, loss = 0.20016657\n",
      "Iteration 4992, loss = 0.20016374\n",
      "Iteration 4993, loss = 0.20016191\n",
      "Iteration 4994, loss = 0.20016179\n",
      "Iteration 4995, loss = 0.20015985\n",
      "Iteration 4996, loss = 0.20015620\n",
      "Iteration 4997, loss = 0.20015799\n",
      "Iteration 4998, loss = 0.20015832\n",
      "Iteration 4999, loss = 0.20015646\n",
      "Iteration 5000, loss = 0.20015512\n",
      "Iteration 5001, loss = 0.20015222\n",
      "Iteration 5002, loss = 0.20014929\n",
      "Iteration 5003, loss = 0.20015178\n",
      "Iteration 5004, loss = 0.20015096\n",
      "Iteration 5005, loss = 0.20014627\n",
      "Iteration 5006, loss = 0.20014286\n",
      "Iteration 5007, loss = 0.20014293\n",
      "Iteration 5008, loss = 0.20014301\n",
      "Iteration 5009, loss = 0.20014236\n",
      "Iteration 5010, loss = 0.20014099\n",
      "Iteration 5011, loss = 0.20013958\n",
      "Iteration 5012, loss = 0.20013727\n",
      "Iteration 5013, loss = 0.20013501\n",
      "Iteration 5014, loss = 0.20013248\n",
      "Iteration 5015, loss = 0.20013092\n",
      "Iteration 5016, loss = 0.20013140\n",
      "Iteration 5017, loss = 0.20012894\n",
      "Iteration 5018, loss = 0.20012800\n",
      "Iteration 5019, loss = 0.20012718\n",
      "Iteration 5020, loss = 0.20012658\n",
      "Iteration 5021, loss = 0.20012327\n",
      "Iteration 5022, loss = 0.20012454\n",
      "Iteration 5023, loss = 0.20012290\n",
      "Iteration 5024, loss = 0.20012111\n",
      "Iteration 5025, loss = 0.20011814\n",
      "Iteration 5026, loss = 0.20011996\n",
      "Iteration 5027, loss = 0.20011921\n",
      "Iteration 5028, loss = 0.20011799\n",
      "Iteration 5029, loss = 0.20011658\n",
      "Iteration 5030, loss = 0.20011334\n",
      "Iteration 5031, loss = 0.20011150\n",
      "Iteration 5032, loss = 0.20011116\n",
      "Iteration 5033, loss = 0.20011188\n",
      "Iteration 5034, loss = 0.20011147\n",
      "Iteration 5035, loss = 0.20010790\n",
      "Iteration 5036, loss = 0.20010514\n",
      "Iteration 5037, loss = 0.20010635\n",
      "Iteration 5038, loss = 0.20010608\n",
      "Iteration 5039, loss = 0.20010440\n",
      "Iteration 5040, loss = 0.20010110\n",
      "Iteration 5041, loss = 0.20009852\n",
      "Iteration 5042, loss = 0.20009546\n",
      "Iteration 5043, loss = 0.20009451\n",
      "Iteration 5044, loss = 0.20009461\n",
      "Iteration 5045, loss = 0.20009382\n",
      "Iteration 5046, loss = 0.20009051\n",
      "Iteration 5047, loss = 0.20009071\n",
      "Iteration 5048, loss = 0.20009011\n",
      "Iteration 5049, loss = 0.20008956\n",
      "Iteration 5050, loss = 0.20008716\n",
      "Iteration 5051, loss = 0.20008401\n",
      "Iteration 5052, loss = 0.20008527\n",
      "Iteration 5053, loss = 0.20008502\n",
      "Iteration 5054, loss = 0.20008390\n",
      "Iteration 5055, loss = 0.20008085\n",
      "Iteration 5056, loss = 0.20007756\n",
      "Iteration 5057, loss = 0.20007714\n",
      "Iteration 5058, loss = 0.20007835\n",
      "Iteration 5059, loss = 0.20007626\n",
      "Iteration 5060, loss = 0.20007113\n",
      "Iteration 5061, loss = 0.20007025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5062, loss = 0.20007072\n",
      "Iteration 5063, loss = 0.20006951\n",
      "Iteration 5064, loss = 0.20006706\n",
      "Iteration 5065, loss = 0.20006492\n",
      "Iteration 5066, loss = 0.20006478\n",
      "Iteration 5067, loss = 0.20006320\n",
      "Iteration 5068, loss = 0.20006223\n",
      "Iteration 5069, loss = 0.20006042\n",
      "Iteration 5070, loss = 0.20005745\n",
      "Iteration 5071, loss = 0.20005564\n",
      "Iteration 5072, loss = 0.20005613\n",
      "Iteration 5073, loss = 0.20005573\n",
      "Iteration 5074, loss = 0.20005484\n",
      "Iteration 5075, loss = 0.20005193\n",
      "Iteration 5076, loss = 0.20004940\n",
      "Iteration 5077, loss = 0.20004839\n",
      "Iteration 5078, loss = 0.20004761\n",
      "Iteration 5079, loss = 0.20004627\n",
      "Iteration 5080, loss = 0.20004357\n",
      "Iteration 5081, loss = 0.20004236\n",
      "Iteration 5082, loss = 0.20004121\n",
      "Iteration 5083, loss = 0.20004156\n",
      "Iteration 5084, loss = 0.20004036\n",
      "Iteration 5085, loss = 0.20003798\n",
      "Iteration 5086, loss = 0.20003880\n",
      "Iteration 5087, loss = 0.20003626\n",
      "Iteration 5088, loss = 0.20003683\n",
      "Iteration 5089, loss = 0.20003560\n",
      "Iteration 5090, loss = 0.20003028\n",
      "Iteration 5091, loss = 0.20003065\n",
      "Iteration 5092, loss = 0.20003225\n",
      "Iteration 5093, loss = 0.20003101\n",
      "Iteration 5094, loss = 0.20002625\n",
      "Iteration 5095, loss = 0.20002303\n",
      "Iteration 5096, loss = 0.20002384\n",
      "Iteration 5097, loss = 0.20002339\n",
      "Iteration 5098, loss = 0.20001993\n",
      "Iteration 5099, loss = 0.20001977\n",
      "Iteration 5100, loss = 0.20001936\n",
      "Iteration 5101, loss = 0.20001670\n",
      "Iteration 5102, loss = 0.20001522\n",
      "Iteration 5103, loss = 0.20001454\n",
      "Iteration 5104, loss = 0.20001244\n",
      "Iteration 5105, loss = 0.20001080\n",
      "Iteration 5106, loss = 0.20000789\n",
      "Iteration 5107, loss = 0.20000699\n",
      "Iteration 5108, loss = 0.20000465\n",
      "Iteration 5109, loss = 0.20000446\n",
      "Iteration 5110, loss = 0.20000283\n",
      "Iteration 5111, loss = 0.20000106\n",
      "Iteration 5112, loss = 0.19999871\n",
      "Iteration 5113, loss = 0.19999687\n",
      "Iteration 5114, loss = 0.19999650\n",
      "Iteration 5115, loss = 0.19999512\n",
      "Iteration 5116, loss = 0.19999319\n",
      "Iteration 5117, loss = 0.19999230\n",
      "Iteration 5118, loss = 0.19999154\n",
      "Iteration 5119, loss = 0.19998915\n",
      "Iteration 5120, loss = 0.19998669\n",
      "Iteration 5121, loss = 0.19998603\n",
      "Iteration 5122, loss = 0.19998660\n",
      "Iteration 5123, loss = 0.19998598\n",
      "Iteration 5124, loss = 0.19998254\n",
      "Iteration 5125, loss = 0.19998244\n",
      "Iteration 5126, loss = 0.19998187\n",
      "Iteration 5127, loss = 0.19998017\n",
      "Iteration 5128, loss = 0.19997747\n",
      "Iteration 5129, loss = 0.19997425\n",
      "Iteration 5130, loss = 0.19997485\n",
      "Iteration 5131, loss = 0.19997430\n",
      "Iteration 5132, loss = 0.19997179\n",
      "Iteration 5133, loss = 0.19996868\n",
      "Iteration 5134, loss = 0.19996669\n",
      "Iteration 5135, loss = 0.19996720\n",
      "Iteration 5136, loss = 0.19996545\n",
      "Iteration 5137, loss = 0.19996359\n",
      "Iteration 5138, loss = 0.19996082\n",
      "Iteration 5139, loss = 0.19996168\n",
      "Iteration 5140, loss = 0.19996181\n",
      "Iteration 5141, loss = 0.19996070\n",
      "Iteration 5142, loss = 0.19995686\n",
      "Iteration 5143, loss = 0.19995542\n",
      "Iteration 5144, loss = 0.19995551\n",
      "Iteration 5145, loss = 0.19995397\n",
      "Iteration 5146, loss = 0.19995280\n",
      "Iteration 5147, loss = 0.19994948\n",
      "Iteration 5148, loss = 0.19994837\n",
      "Iteration 5149, loss = 0.19994899\n",
      "Iteration 5150, loss = 0.19994651\n",
      "Iteration 5151, loss = 0.19994244\n",
      "Iteration 5152, loss = 0.19994225\n",
      "Iteration 5153, loss = 0.19994059\n",
      "Iteration 5154, loss = 0.19994165\n",
      "Iteration 5155, loss = 0.19993945\n",
      "Iteration 5156, loss = 0.19993462\n",
      "Iteration 5157, loss = 0.19993238\n",
      "Iteration 5158, loss = 0.19993346\n",
      "Iteration 5159, loss = 0.19993185\n",
      "Iteration 5160, loss = 0.19992818\n",
      "Iteration 5161, loss = 0.19992358\n",
      "Iteration 5162, loss = 0.19992200\n",
      "Iteration 5163, loss = 0.19992032\n",
      "Iteration 5164, loss = 0.19992027\n",
      "Iteration 5165, loss = 0.19991766\n",
      "Iteration 5166, loss = 0.19991434\n",
      "Iteration 5167, loss = 0.19991431\n",
      "Iteration 5168, loss = 0.19991304\n",
      "Iteration 5169, loss = 0.19990943\n",
      "Iteration 5170, loss = 0.19990713\n",
      "Iteration 5171, loss = 0.19990496\n",
      "Iteration 5172, loss = 0.19990411\n",
      "Iteration 5173, loss = 0.19990188\n",
      "Iteration 5174, loss = 0.19989940\n",
      "Iteration 5175, loss = 0.19989885\n",
      "Iteration 5176, loss = 0.19989710\n",
      "Iteration 5177, loss = 0.19989487\n",
      "Iteration 5178, loss = 0.19989267\n",
      "Iteration 5179, loss = 0.19988938\n",
      "Iteration 5180, loss = 0.19988738\n",
      "Iteration 5181, loss = 0.19988555\n",
      "Iteration 5182, loss = 0.19988266\n",
      "Iteration 5183, loss = 0.19988132\n",
      "Iteration 5184, loss = 0.19988016\n",
      "Iteration 5185, loss = 0.19987738\n",
      "Iteration 5186, loss = 0.19987361\n",
      "Iteration 5187, loss = 0.19986914\n",
      "Iteration 5188, loss = 0.19986797\n",
      "Iteration 5189, loss = 0.19986707\n",
      "Iteration 5190, loss = 0.19986411\n",
      "Iteration 5191, loss = 0.19986165\n",
      "Iteration 5192, loss = 0.19985902\n",
      "Iteration 5193, loss = 0.19985631\n",
      "Iteration 5194, loss = 0.19985629\n",
      "Iteration 5195, loss = 0.19985405\n",
      "Iteration 5196, loss = 0.19985126\n",
      "Iteration 5197, loss = 0.19984871\n",
      "Iteration 5198, loss = 0.19984618\n",
      "Iteration 5199, loss = 0.19984627\n",
      "Iteration 5200, loss = 0.19984395\n",
      "Iteration 5201, loss = 0.19984143\n",
      "Iteration 5202, loss = 0.19983627\n",
      "Iteration 5203, loss = 0.19983270\n",
      "Iteration 5204, loss = 0.19983132\n",
      "Iteration 5205, loss = 0.19982828\n",
      "Iteration 5206, loss = 0.19982586\n",
      "Iteration 5207, loss = 0.19982125\n",
      "Iteration 5208, loss = 0.19982007\n",
      "Iteration 5209, loss = 0.19981696\n",
      "Iteration 5210, loss = 0.19981642\n",
      "Iteration 5211, loss = 0.19981275\n",
      "Iteration 5212, loss = 0.19981187\n",
      "Iteration 5213, loss = 0.19980893\n",
      "Iteration 5214, loss = 0.19980856\n",
      "Iteration 5215, loss = 0.19980729\n",
      "Iteration 5216, loss = 0.19980455\n",
      "Iteration 5217, loss = 0.19980154\n",
      "Iteration 5218, loss = 0.19979868\n",
      "Iteration 5219, loss = 0.19979361\n",
      "Iteration 5220, loss = 0.19979254\n",
      "Iteration 5221, loss = 0.19979471\n",
      "Iteration 5222, loss = 0.19979386\n",
      "Iteration 5223, loss = 0.19978899\n",
      "Iteration 5224, loss = 0.19978430\n",
      "Iteration 5225, loss = 0.19978312\n",
      "Iteration 5226, loss = 0.19978054\n",
      "Iteration 5227, loss = 0.19977739\n",
      "Iteration 5228, loss = 0.19977499\n",
      "Iteration 5229, loss = 0.19977292\n",
      "Iteration 5230, loss = 0.19976722\n",
      "Iteration 5231, loss = 0.19976539\n",
      "Iteration 5232, loss = 0.19976731\n",
      "Iteration 5233, loss = 0.19976376\n",
      "Iteration 5234, loss = 0.19976060\n",
      "Iteration 5235, loss = 0.19975829\n",
      "Iteration 5236, loss = 0.19975722\n",
      "Iteration 5237, loss = 0.19975463\n",
      "Iteration 5238, loss = 0.19975104\n",
      "Iteration 5239, loss = 0.19974938\n",
      "Iteration 5240, loss = 0.19974640\n",
      "Iteration 5241, loss = 0.19974365\n",
      "Iteration 5242, loss = 0.19974283\n",
      "Iteration 5243, loss = 0.19974072\n",
      "Iteration 5244, loss = 0.19973584\n",
      "Iteration 5245, loss = 0.19973430\n",
      "Iteration 5246, loss = 0.19973372\n",
      "Iteration 5247, loss = 0.19973031\n",
      "Iteration 5248, loss = 0.19972748\n",
      "Iteration 5249, loss = 0.19972332\n",
      "Iteration 5250, loss = 0.19972114\n",
      "Iteration 5251, loss = 0.19971844\n",
      "Iteration 5252, loss = 0.19971595\n",
      "Iteration 5253, loss = 0.19971586\n",
      "Iteration 5254, loss = 0.19971213\n",
      "Iteration 5255, loss = 0.19970821\n",
      "Iteration 5256, loss = 0.19970654\n",
      "Iteration 5257, loss = 0.19970359\n",
      "Iteration 5258, loss = 0.19970258\n",
      "Iteration 5259, loss = 0.19969980\n",
      "Iteration 5260, loss = 0.19969592\n",
      "Iteration 5261, loss = 0.19969413\n",
      "Iteration 5262, loss = 0.19969156\n",
      "Iteration 5263, loss = 0.19968922\n",
      "Iteration 5264, loss = 0.19968916\n",
      "Iteration 5265, loss = 0.19968663\n",
      "Iteration 5266, loss = 0.19968239\n",
      "Iteration 5267, loss = 0.19967945\n",
      "Iteration 5268, loss = 0.19967841\n",
      "Iteration 5269, loss = 0.19967616\n",
      "Iteration 5270, loss = 0.19967419\n",
      "Iteration 5271, loss = 0.19966831\n",
      "Iteration 5272, loss = 0.19966755\n",
      "Iteration 5273, loss = 0.19966646\n",
      "Iteration 5274, loss = 0.19966252\n",
      "Iteration 5275, loss = 0.19966052\n",
      "Iteration 5276, loss = 0.19965837\n",
      "Iteration 5277, loss = 0.19965446\n",
      "Iteration 5278, loss = 0.19965077\n",
      "Iteration 5279, loss = 0.19964844\n",
      "Iteration 5280, loss = 0.19964667\n",
      "Iteration 5281, loss = 0.19964341\n",
      "Iteration 5282, loss = 0.19964104\n",
      "Iteration 5283, loss = 0.19963789\n",
      "Iteration 5284, loss = 0.19963497\n",
      "Iteration 5285, loss = 0.19963128\n",
      "Iteration 5286, loss = 0.19962921\n",
      "Iteration 5287, loss = 0.19962650\n",
      "Iteration 5288, loss = 0.19962378\n",
      "Iteration 5289, loss = 0.19961960\n",
      "Iteration 5290, loss = 0.19961465\n",
      "Iteration 5291, loss = 0.19961131\n",
      "Iteration 5292, loss = 0.19960711\n",
      "Iteration 5293, loss = 0.19960436\n",
      "Iteration 5294, loss = 0.19960181\n",
      "Iteration 5295, loss = 0.19959698\n",
      "Iteration 5296, loss = 0.19959348\n",
      "Iteration 5297, loss = 0.19959201\n",
      "Iteration 5298, loss = 0.19958919\n",
      "Iteration 5299, loss = 0.19958409\n",
      "Iteration 5300, loss = 0.19958166\n",
      "Iteration 5301, loss = 0.19957933\n",
      "Iteration 5302, loss = 0.19957333\n",
      "Iteration 5303, loss = 0.19956944\n",
      "Iteration 5304, loss = 0.19956786\n",
      "Iteration 5305, loss = 0.19956457\n",
      "Iteration 5306, loss = 0.19955879\n",
      "Iteration 5307, loss = 0.19955551\n",
      "Iteration 5308, loss = 0.19955325\n",
      "Iteration 5309, loss = 0.19955104\n",
      "Iteration 5310, loss = 0.19954736\n",
      "Iteration 5311, loss = 0.19954319\n",
      "Iteration 5312, loss = 0.19953700\n",
      "Iteration 5313, loss = 0.19953565\n",
      "Iteration 5314, loss = 0.19953414\n",
      "Iteration 5315, loss = 0.19953087\n",
      "Iteration 5316, loss = 0.19952449\n",
      "Iteration 5317, loss = 0.19951986\n",
      "Iteration 5318, loss = 0.19951605\n",
      "Iteration 5319, loss = 0.19951214\n",
      "Iteration 5320, loss = 0.19950776\n",
      "Iteration 5321, loss = 0.19950388\n",
      "Iteration 5322, loss = 0.19950065\n",
      "Iteration 5323, loss = 0.19949821\n",
      "Iteration 5324, loss = 0.19949184\n",
      "Iteration 5325, loss = 0.19948460\n",
      "Iteration 5326, loss = 0.19948338\n",
      "Iteration 5327, loss = 0.19947972\n",
      "Iteration 5328, loss = 0.19947211\n",
      "Iteration 5329, loss = 0.19946795\n",
      "Iteration 5330, loss = 0.19946503\n",
      "Iteration 5331, loss = 0.19946007\n",
      "Iteration 5332, loss = 0.19945736\n",
      "Iteration 5333, loss = 0.19945116\n",
      "Iteration 5334, loss = 0.19944778\n",
      "Iteration 5335, loss = 0.19944330\n",
      "Iteration 5336, loss = 0.19943881\n",
      "Iteration 5337, loss = 0.19943466\n",
      "Iteration 5338, loss = 0.19942999\n",
      "Iteration 5339, loss = 0.19942573\n",
      "Iteration 5340, loss = 0.19941994\n",
      "Iteration 5341, loss = 0.19941242\n",
      "Iteration 5342, loss = 0.19940777\n",
      "Iteration 5343, loss = 0.19940499\n",
      "Iteration 5344, loss = 0.19940153\n",
      "Iteration 5345, loss = 0.19939486\n",
      "Iteration 5346, loss = 0.19938951\n",
      "Iteration 5347, loss = 0.19938640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5348, loss = 0.19938472\n",
      "Iteration 5349, loss = 0.19937771\n",
      "Iteration 5350, loss = 0.19937230\n",
      "Iteration 5351, loss = 0.19936571\n",
      "Iteration 5352, loss = 0.19936113\n",
      "Iteration 5353, loss = 0.19935762\n",
      "Iteration 5354, loss = 0.19935339\n",
      "Iteration 5355, loss = 0.19934867\n",
      "Iteration 5356, loss = 0.19934399\n",
      "Iteration 5357, loss = 0.19933841\n",
      "Iteration 5358, loss = 0.19933378\n",
      "Iteration 5359, loss = 0.19932602\n",
      "Iteration 5360, loss = 0.19932241\n",
      "Iteration 5361, loss = 0.19932088\n",
      "Iteration 5362, loss = 0.19931346\n",
      "Iteration 5363, loss = 0.19930627\n",
      "Iteration 5364, loss = 0.19930259\n",
      "Iteration 5365, loss = 0.19929885\n",
      "Iteration 5366, loss = 0.19929159\n",
      "Iteration 5367, loss = 0.19928664\n",
      "Iteration 5368, loss = 0.19928385\n",
      "Iteration 5369, loss = 0.19927805\n",
      "Iteration 5370, loss = 0.19927205\n",
      "Iteration 5371, loss = 0.19926744\n",
      "Iteration 5372, loss = 0.19926330\n",
      "Iteration 5373, loss = 0.19925930\n",
      "Iteration 5374, loss = 0.19925455\n",
      "Iteration 5375, loss = 0.19925127\n",
      "Iteration 5376, loss = 0.19924742\n",
      "Iteration 5377, loss = 0.19924209\n",
      "Iteration 5378, loss = 0.19923784\n",
      "Iteration 5379, loss = 0.19923494\n",
      "Iteration 5380, loss = 0.19922956\n",
      "Iteration 5381, loss = 0.19922461\n",
      "Iteration 5382, loss = 0.19921866\n",
      "Iteration 5383, loss = 0.19921496\n",
      "Iteration 5384, loss = 0.19921012\n",
      "Iteration 5385, loss = 0.19920776\n",
      "Iteration 5386, loss = 0.19920392\n",
      "Iteration 5387, loss = 0.19919796\n",
      "Iteration 5388, loss = 0.19919134\n",
      "Iteration 5389, loss = 0.19918347\n",
      "Iteration 5390, loss = 0.19918339\n",
      "Iteration 5391, loss = 0.19917836\n",
      "Iteration 5392, loss = 0.19917139\n",
      "Iteration 5393, loss = 0.19916858\n",
      "Iteration 5394, loss = 0.19916332\n",
      "Iteration 5395, loss = 0.19916193\n",
      "Iteration 5396, loss = 0.19915827\n",
      "Iteration 5397, loss = 0.19914900\n",
      "Iteration 5398, loss = 0.19914560\n",
      "Iteration 5399, loss = 0.19914505\n",
      "Iteration 5400, loss = 0.19914137\n",
      "Iteration 5401, loss = 0.19913598\n",
      "Iteration 5402, loss = 0.19912763\n",
      "Iteration 5403, loss = 0.19912067\n",
      "Iteration 5404, loss = 0.19911868\n",
      "Iteration 5405, loss = 0.19911235\n",
      "Iteration 5406, loss = 0.19910626\n",
      "Iteration 5407, loss = 0.19910258\n",
      "Iteration 5408, loss = 0.19909787\n",
      "Iteration 5409, loss = 0.19909238\n",
      "Iteration 5410, loss = 0.19908527\n",
      "Iteration 5411, loss = 0.19908014\n",
      "Iteration 5412, loss = 0.19907793\n",
      "Iteration 5413, loss = 0.19907096\n",
      "Iteration 5414, loss = 0.19906289\n",
      "Iteration 5415, loss = 0.19905786\n",
      "Iteration 5416, loss = 0.19905470\n",
      "Iteration 5417, loss = 0.19904911\n",
      "Iteration 5418, loss = 0.19904412\n",
      "Iteration 5419, loss = 0.19904236\n",
      "Iteration 5420, loss = 0.19903429\n",
      "Iteration 5421, loss = 0.19902848\n",
      "Iteration 5422, loss = 0.19902536\n",
      "Iteration 5423, loss = 0.19902092\n",
      "Iteration 5424, loss = 0.19901602\n",
      "Iteration 5425, loss = 0.19900976\n",
      "Iteration 5426, loss = 0.19900658\n",
      "Iteration 5427, loss = 0.19900067\n",
      "Iteration 5428, loss = 0.19899273\n",
      "Iteration 5429, loss = 0.19898854\n",
      "Iteration 5430, loss = 0.19898392\n",
      "Iteration 5431, loss = 0.19897715\n",
      "Iteration 5432, loss = 0.19897167\n",
      "Iteration 5433, loss = 0.19896908\n",
      "Iteration 5434, loss = 0.19896471\n",
      "Iteration 5435, loss = 0.19896000\n",
      "Iteration 5436, loss = 0.19895144\n",
      "Iteration 5437, loss = 0.19894595\n",
      "Iteration 5438, loss = 0.19894070\n",
      "Iteration 5439, loss = 0.19893446\n",
      "Iteration 5440, loss = 0.19892936\n",
      "Iteration 5441, loss = 0.19892353\n",
      "Iteration 5442, loss = 0.19891829\n",
      "Iteration 5443, loss = 0.19891168\n",
      "Iteration 5444, loss = 0.19890676\n",
      "Iteration 5445, loss = 0.19890205\n",
      "Iteration 5446, loss = 0.19889481\n",
      "Iteration 5447, loss = 0.19888946\n",
      "Iteration 5448, loss = 0.19888423\n",
      "Iteration 5449, loss = 0.19887869\n",
      "Iteration 5450, loss = 0.19887292\n",
      "Iteration 5451, loss = 0.19886489\n",
      "Iteration 5452, loss = 0.19886379\n",
      "Iteration 5453, loss = 0.19885893\n",
      "Iteration 5454, loss = 0.19885156\n",
      "Iteration 5455, loss = 0.19884348\n",
      "Iteration 5456, loss = 0.19883818\n",
      "Iteration 5457, loss = 0.19883539\n",
      "Iteration 5458, loss = 0.19882863\n",
      "Iteration 5459, loss = 0.19882311\n",
      "Iteration 5460, loss = 0.19881716\n",
      "Iteration 5461, loss = 0.19880988\n",
      "Iteration 5462, loss = 0.19880555\n",
      "Iteration 5463, loss = 0.19879995\n",
      "Iteration 5464, loss = 0.19879347\n",
      "Iteration 5465, loss = 0.19878550\n",
      "Iteration 5466, loss = 0.19877942\n",
      "Iteration 5467, loss = 0.19877534\n",
      "Iteration 5468, loss = 0.19877166\n",
      "Iteration 5469, loss = 0.19876439\n",
      "Iteration 5470, loss = 0.19875713\n",
      "Iteration 5471, loss = 0.19875105\n",
      "Iteration 5472, loss = 0.19874792\n",
      "Iteration 5473, loss = 0.19874563\n",
      "Iteration 5474, loss = 0.19873807\n",
      "Iteration 5475, loss = 0.19872825\n",
      "Iteration 5476, loss = 0.19872219\n",
      "Iteration 5477, loss = 0.19871908\n",
      "Iteration 5478, loss = 0.19871277\n",
      "Iteration 5479, loss = 0.19870386\n",
      "Iteration 5480, loss = 0.19869520\n",
      "Iteration 5481, loss = 0.19868936\n",
      "Iteration 5482, loss = 0.19868343\n",
      "Iteration 5483, loss = 0.19867843\n",
      "Iteration 5484, loss = 0.19867402\n",
      "Iteration 5485, loss = 0.19866730\n",
      "Iteration 5486, loss = 0.19866008\n",
      "Iteration 5487, loss = 0.19865549\n",
      "Iteration 5488, loss = 0.19864728\n",
      "Iteration 5489, loss = 0.19864059\n",
      "Iteration 5490, loss = 0.19863430\n",
      "Iteration 5491, loss = 0.19862926\n",
      "Iteration 5492, loss = 0.19862171\n",
      "Iteration 5493, loss = 0.19861479\n",
      "Iteration 5494, loss = 0.19860905\n",
      "Iteration 5495, loss = 0.19860430\n",
      "Iteration 5496, loss = 0.19859678\n",
      "Iteration 5497, loss = 0.19858854\n",
      "Iteration 5498, loss = 0.19858427\n",
      "Iteration 5499, loss = 0.19857848\n",
      "Iteration 5500, loss = 0.19857327\n",
      "Iteration 5501, loss = 0.19856623\n",
      "Iteration 5502, loss = 0.19855868\n",
      "Iteration 5503, loss = 0.19855103\n",
      "Iteration 5504, loss = 0.19854331\n",
      "Iteration 5505, loss = 0.19853725\n",
      "Iteration 5506, loss = 0.19853313\n",
      "Iteration 5507, loss = 0.19852872\n",
      "Iteration 5508, loss = 0.19852140\n",
      "Iteration 5509, loss = 0.19851433\n",
      "Iteration 5510, loss = 0.19850736\n",
      "Iteration 5511, loss = 0.19849968\n",
      "Iteration 5512, loss = 0.19849445\n",
      "Iteration 5513, loss = 0.19848768\n",
      "Iteration 5514, loss = 0.19847849\n",
      "Iteration 5515, loss = 0.19847202\n",
      "Iteration 5516, loss = 0.19846598\n",
      "Iteration 5517, loss = 0.19846152\n",
      "Iteration 5518, loss = 0.19845461\n",
      "Iteration 5519, loss = 0.19844592\n",
      "Iteration 5520, loss = 0.19843811\n",
      "Iteration 5521, loss = 0.19843025\n",
      "Iteration 5522, loss = 0.19842653\n",
      "Iteration 5523, loss = 0.19841851\n",
      "Iteration 5524, loss = 0.19840930\n",
      "Iteration 5525, loss = 0.19840180\n",
      "Iteration 5526, loss = 0.19839479\n",
      "Iteration 5527, loss = 0.19839031\n",
      "Iteration 5528, loss = 0.19838174\n",
      "Iteration 5529, loss = 0.19837398\n",
      "Iteration 5530, loss = 0.19836875\n",
      "Iteration 5531, loss = 0.19836100\n",
      "Iteration 5532, loss = 0.19835351\n",
      "Iteration 5533, loss = 0.19834650\n",
      "Iteration 5534, loss = 0.19833979\n",
      "Iteration 5535, loss = 0.19833446\n",
      "Iteration 5536, loss = 0.19832726\n",
      "Iteration 5537, loss = 0.19832078\n",
      "Iteration 5538, loss = 0.19831444\n",
      "Iteration 5539, loss = 0.19830783\n",
      "Iteration 5540, loss = 0.19829835\n",
      "Iteration 5541, loss = 0.19829049\n",
      "Iteration 5542, loss = 0.19828331\n",
      "Iteration 5543, loss = 0.19827609\n",
      "Iteration 5544, loss = 0.19826946\n",
      "Iteration 5545, loss = 0.19826450\n",
      "Iteration 5546, loss = 0.19825729\n",
      "Iteration 5547, loss = 0.19824742\n",
      "Iteration 5548, loss = 0.19823919\n",
      "Iteration 5549, loss = 0.19823322\n",
      "Iteration 5550, loss = 0.19822538\n",
      "Iteration 5551, loss = 0.19821865\n",
      "Iteration 5552, loss = 0.19820914\n",
      "Iteration 5553, loss = 0.19820399\n",
      "Iteration 5554, loss = 0.19819511\n",
      "Iteration 5555, loss = 0.19818949\n",
      "Iteration 5556, loss = 0.19818568\n",
      "Iteration 5557, loss = 0.19817474\n",
      "Iteration 5558, loss = 0.19816912\n",
      "Iteration 5559, loss = 0.19816397\n",
      "Iteration 5560, loss = 0.19815512\n",
      "Iteration 5561, loss = 0.19814343\n",
      "Iteration 5562, loss = 0.19813540\n",
      "Iteration 5563, loss = 0.19812830\n",
      "Iteration 5564, loss = 0.19812091\n",
      "Iteration 5565, loss = 0.19811316\n",
      "Iteration 5566, loss = 0.19810555\n",
      "Iteration 5567, loss = 0.19810077\n",
      "Iteration 5568, loss = 0.19809461\n",
      "Iteration 5569, loss = 0.19808485\n",
      "Iteration 5570, loss = 0.19807474\n",
      "Iteration 5571, loss = 0.19807132\n",
      "Iteration 5572, loss = 0.19806626\n",
      "Iteration 5573, loss = 0.19805557\n",
      "Iteration 5574, loss = 0.19804575\n",
      "Iteration 5575, loss = 0.19804144\n",
      "Iteration 5576, loss = 0.19803623\n",
      "Iteration 5577, loss = 0.19802944\n",
      "Iteration 5578, loss = 0.19802039\n",
      "Iteration 5579, loss = 0.19801075\n",
      "Iteration 5580, loss = 0.19800187\n",
      "Iteration 5581, loss = 0.19799228\n",
      "Iteration 5582, loss = 0.19798650\n",
      "Iteration 5583, loss = 0.19798182\n",
      "Iteration 5584, loss = 0.19797509\n",
      "Iteration 5585, loss = 0.19796469\n",
      "Iteration 5586, loss = 0.19795097\n",
      "Iteration 5587, loss = 0.19794525\n",
      "Iteration 5588, loss = 0.19793998\n",
      "Iteration 5589, loss = 0.19793354\n",
      "Iteration 5590, loss = 0.19792426\n",
      "Iteration 5591, loss = 0.19791714\n",
      "Iteration 5592, loss = 0.19790798\n",
      "Iteration 5593, loss = 0.19789782\n",
      "Iteration 5594, loss = 0.19788804\n",
      "Iteration 5595, loss = 0.19788159\n",
      "Iteration 5596, loss = 0.19787503\n",
      "Iteration 5597, loss = 0.19786745\n",
      "Iteration 5598, loss = 0.19785851\n",
      "Iteration 5599, loss = 0.19784690\n",
      "Iteration 5600, loss = 0.19784022\n",
      "Iteration 5601, loss = 0.19783313\n",
      "Iteration 5602, loss = 0.19782609\n",
      "Iteration 5603, loss = 0.19781901\n",
      "Iteration 5604, loss = 0.19781026\n",
      "Iteration 5605, loss = 0.19780090\n",
      "Iteration 5606, loss = 0.19779193\n",
      "Iteration 5607, loss = 0.19778265\n",
      "Iteration 5608, loss = 0.19777521\n",
      "Iteration 5609, loss = 0.19776685\n",
      "Iteration 5610, loss = 0.19775760\n",
      "Iteration 5611, loss = 0.19774724\n",
      "Iteration 5612, loss = 0.19774064\n",
      "Iteration 5613, loss = 0.19773429\n",
      "Iteration 5614, loss = 0.19772451\n",
      "Iteration 5615, loss = 0.19771302\n",
      "Iteration 5616, loss = 0.19770560\n",
      "Iteration 5617, loss = 0.19769848\n",
      "Iteration 5618, loss = 0.19768981\n",
      "Iteration 5619, loss = 0.19768135\n",
      "Iteration 5620, loss = 0.19767649\n",
      "Iteration 5621, loss = 0.19766879\n",
      "Iteration 5622, loss = 0.19765695\n",
      "Iteration 5623, loss = 0.19764906\n",
      "Iteration 5624, loss = 0.19764222\n",
      "Iteration 5625, loss = 0.19763194\n",
      "Iteration 5626, loss = 0.19762461\n",
      "Iteration 5627, loss = 0.19761439\n",
      "Iteration 5628, loss = 0.19760681\n",
      "Iteration 5629, loss = 0.19759747\n",
      "Iteration 5630, loss = 0.19758884\n",
      "Iteration 5631, loss = 0.19757987\n",
      "Iteration 5632, loss = 0.19757280\n",
      "Iteration 5633, loss = 0.19756474\n",
      "Iteration 5634, loss = 0.19755387\n",
      "Iteration 5635, loss = 0.19754516\n",
      "Iteration 5636, loss = 0.19753885\n",
      "Iteration 5637, loss = 0.19753004\n",
      "Iteration 5638, loss = 0.19752146\n",
      "Iteration 5639, loss = 0.19750959\n",
      "Iteration 5640, loss = 0.19749961\n",
      "Iteration 5641, loss = 0.19749213\n",
      "Iteration 5642, loss = 0.19748322\n",
      "Iteration 5643, loss = 0.19747561\n",
      "Iteration 5644, loss = 0.19746697\n",
      "Iteration 5645, loss = 0.19745614\n",
      "Iteration 5646, loss = 0.19744823\n",
      "Iteration 5647, loss = 0.19744022\n",
      "Iteration 5648, loss = 0.19743198\n",
      "Iteration 5649, loss = 0.19742074\n",
      "Iteration 5650, loss = 0.19740926\n",
      "Iteration 5651, loss = 0.19740453\n",
      "Iteration 5652, loss = 0.19739786\n",
      "Iteration 5653, loss = 0.19738571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5654, loss = 0.19737683\n",
      "Iteration 5655, loss = 0.19736934\n",
      "Iteration 5656, loss = 0.19736046\n",
      "Iteration 5657, loss = 0.19735176\n",
      "Iteration 5658, loss = 0.19734381\n",
      "Iteration 5659, loss = 0.19733456\n",
      "Iteration 5660, loss = 0.19732124\n",
      "Iteration 5661, loss = 0.19731184\n",
      "Iteration 5662, loss = 0.19730352\n",
      "Iteration 5663, loss = 0.19729453\n",
      "Iteration 5664, loss = 0.19728505\n",
      "Iteration 5665, loss = 0.19727664\n",
      "Iteration 5666, loss = 0.19726792\n",
      "Iteration 5667, loss = 0.19726185\n",
      "Iteration 5668, loss = 0.19725102\n",
      "Iteration 5669, loss = 0.19724193\n",
      "Iteration 5670, loss = 0.19723479\n",
      "Iteration 5671, loss = 0.19722474\n",
      "Iteration 5672, loss = 0.19721284\n",
      "Iteration 5673, loss = 0.19720386\n",
      "Iteration 5674, loss = 0.19719688\n",
      "Iteration 5675, loss = 0.19718811\n",
      "Iteration 5676, loss = 0.19717738\n",
      "Iteration 5677, loss = 0.19716512\n",
      "Iteration 5678, loss = 0.19715929\n",
      "Iteration 5679, loss = 0.19715185\n",
      "Iteration 5680, loss = 0.19714156\n",
      "Iteration 5681, loss = 0.19713097\n",
      "Iteration 5682, loss = 0.19712474\n",
      "Iteration 5683, loss = 0.19711668\n",
      "Iteration 5684, loss = 0.19710369\n",
      "Iteration 5685, loss = 0.19709323\n",
      "Iteration 5686, loss = 0.19708504\n",
      "Iteration 5687, loss = 0.19707831\n",
      "Iteration 5688, loss = 0.19706722\n",
      "Iteration 5689, loss = 0.19705418\n",
      "Iteration 5690, loss = 0.19704569\n",
      "Iteration 5691, loss = 0.19703973\n",
      "Iteration 5692, loss = 0.19702916\n",
      "Iteration 5693, loss = 0.19701967\n",
      "Iteration 5694, loss = 0.19701196\n",
      "Iteration 5695, loss = 0.19700423\n",
      "Iteration 5696, loss = 0.19699179\n",
      "Iteration 5697, loss = 0.19698273\n",
      "Iteration 5698, loss = 0.19697361\n",
      "Iteration 5699, loss = 0.19696375\n",
      "Iteration 5700, loss = 0.19695343\n",
      "Iteration 5701, loss = 0.19694570\n",
      "Iteration 5702, loss = 0.19693817\n",
      "Iteration 5703, loss = 0.19692703\n",
      "Iteration 5704, loss = 0.19691372\n",
      "Iteration 5705, loss = 0.19690831\n",
      "Iteration 5706, loss = 0.19689984\n",
      "Iteration 5707, loss = 0.19689244\n",
      "Iteration 5708, loss = 0.19688386\n",
      "Iteration 5709, loss = 0.19687341\n",
      "Iteration 5710, loss = 0.19686136\n",
      "Iteration 5711, loss = 0.19685222\n",
      "Iteration 5712, loss = 0.19684229\n",
      "Iteration 5713, loss = 0.19683183\n",
      "Iteration 5714, loss = 0.19682235\n",
      "Iteration 5715, loss = 0.19681130\n",
      "Iteration 5716, loss = 0.19680074\n",
      "Iteration 5717, loss = 0.19679284\n",
      "Iteration 5718, loss = 0.19678418\n",
      "Iteration 5719, loss = 0.19677750\n",
      "Iteration 5720, loss = 0.19676888\n",
      "Iteration 5721, loss = 0.19675576\n",
      "Iteration 5722, loss = 0.19674433\n",
      "Iteration 5723, loss = 0.19673548\n",
      "Iteration 5724, loss = 0.19672954\n",
      "Iteration 5725, loss = 0.19671794\n",
      "Iteration 5726, loss = 0.19670474\n",
      "Iteration 5727, loss = 0.19669703\n",
      "Iteration 5728, loss = 0.19668735\n",
      "Iteration 5729, loss = 0.19667699\n",
      "Iteration 5730, loss = 0.19666774\n",
      "Iteration 5731, loss = 0.19665684\n",
      "Iteration 5732, loss = 0.19664386\n",
      "Iteration 5733, loss = 0.19663721\n",
      "Iteration 5734, loss = 0.19662783\n",
      "Iteration 5735, loss = 0.19661650\n",
      "Iteration 5736, loss = 0.19660554\n",
      "Iteration 5737, loss = 0.19659842\n",
      "Iteration 5738, loss = 0.19658821\n",
      "Iteration 5739, loss = 0.19657618\n",
      "Iteration 5740, loss = 0.19656567\n",
      "Iteration 5741, loss = 0.19655903\n",
      "Iteration 5742, loss = 0.19655064\n",
      "Iteration 5743, loss = 0.19653842\n",
      "Iteration 5744, loss = 0.19652838\n",
      "Iteration 5745, loss = 0.19652117\n",
      "Iteration 5746, loss = 0.19651007\n",
      "Iteration 5747, loss = 0.19649758\n",
      "Iteration 5748, loss = 0.19648901\n",
      "Iteration 5749, loss = 0.19647617\n",
      "Iteration 5750, loss = 0.19646633\n",
      "Iteration 5751, loss = 0.19645919\n",
      "Iteration 5752, loss = 0.19644936\n",
      "Iteration 5753, loss = 0.19643877\n",
      "Iteration 5754, loss = 0.19642612\n",
      "Iteration 5755, loss = 0.19641575\n",
      "Iteration 5756, loss = 0.19640822\n",
      "Iteration 5757, loss = 0.19639607\n",
      "Iteration 5758, loss = 0.19638475\n",
      "Iteration 5759, loss = 0.19637587\n",
      "Iteration 5760, loss = 0.19636579\n",
      "Iteration 5761, loss = 0.19635489\n",
      "Iteration 5762, loss = 0.19634395\n",
      "Iteration 5763, loss = 0.19633511\n",
      "Iteration 5764, loss = 0.19632649\n",
      "Iteration 5765, loss = 0.19631665\n",
      "Iteration 5766, loss = 0.19630539\n",
      "Iteration 5767, loss = 0.19629518\n",
      "Iteration 5768, loss = 0.19628244\n",
      "Iteration 5769, loss = 0.19627566\n",
      "Iteration 5770, loss = 0.19626781\n",
      "Iteration 5771, loss = 0.19625532\n",
      "Iteration 5772, loss = 0.19624308\n",
      "Iteration 5773, loss = 0.19623169\n",
      "Iteration 5774, loss = 0.19622450\n",
      "Iteration 5775, loss = 0.19621429\n",
      "Iteration 5776, loss = 0.19620307\n",
      "Iteration 5777, loss = 0.19619078\n",
      "Iteration 5778, loss = 0.19618402\n",
      "Iteration 5779, loss = 0.19617730\n",
      "Iteration 5780, loss = 0.19616499\n",
      "Iteration 5781, loss = 0.19615449\n",
      "Iteration 5782, loss = 0.19614470\n",
      "Iteration 5783, loss = 0.19613436\n",
      "Iteration 5784, loss = 0.19612686\n",
      "Iteration 5785, loss = 0.19611524\n",
      "Iteration 5786, loss = 0.19610597\n",
      "Iteration 5787, loss = 0.19609351\n",
      "Iteration 5788, loss = 0.19608266\n",
      "Iteration 5789, loss = 0.19607444\n",
      "Iteration 5790, loss = 0.19606108\n",
      "Iteration 5791, loss = 0.19605282\n",
      "Iteration 5792, loss = 0.19604456\n",
      "Iteration 5793, loss = 0.19603328\n",
      "Iteration 5794, loss = 0.19602226\n",
      "Iteration 5795, loss = 0.19601289\n",
      "Iteration 5796, loss = 0.19600517\n",
      "Iteration 5797, loss = 0.19599314\n",
      "Iteration 5798, loss = 0.19598174\n",
      "Iteration 5799, loss = 0.19596802\n",
      "Iteration 5800, loss = 0.19595971\n",
      "Iteration 5801, loss = 0.19595039\n",
      "Iteration 5802, loss = 0.19594139\n",
      "Iteration 5803, loss = 0.19593088\n",
      "Iteration 5804, loss = 0.19592021\n",
      "Iteration 5805, loss = 0.19590871\n",
      "Iteration 5806, loss = 0.19590048\n",
      "Iteration 5807, loss = 0.19589272\n",
      "Iteration 5808, loss = 0.19588044\n",
      "Iteration 5809, loss = 0.19587027\n",
      "Iteration 5810, loss = 0.19585782\n",
      "Iteration 5811, loss = 0.19584883\n",
      "Iteration 5812, loss = 0.19583909\n",
      "Iteration 5813, loss = 0.19582583\n",
      "Iteration 5814, loss = 0.19581680\n",
      "Iteration 5815, loss = 0.19580796\n",
      "Iteration 5816, loss = 0.19580040\n",
      "Iteration 5817, loss = 0.19578781\n",
      "Iteration 5818, loss = 0.19577352\n",
      "Iteration 5819, loss = 0.19576501\n",
      "Iteration 5820, loss = 0.19575235\n",
      "Iteration 5821, loss = 0.19574157\n",
      "Iteration 5822, loss = 0.19572993\n",
      "Iteration 5823, loss = 0.19572157\n",
      "Iteration 5824, loss = 0.19571036\n",
      "Iteration 5825, loss = 0.19569906\n",
      "Iteration 5826, loss = 0.19569066\n",
      "Iteration 5827, loss = 0.19567955\n",
      "Iteration 5828, loss = 0.19567072\n",
      "Iteration 5829, loss = 0.19566094\n",
      "Iteration 5830, loss = 0.19565165\n",
      "Iteration 5831, loss = 0.19564158\n",
      "Iteration 5832, loss = 0.19563087\n",
      "Iteration 5833, loss = 0.19562058\n",
      "Iteration 5834, loss = 0.19561089\n",
      "Iteration 5835, loss = 0.19559732\n",
      "Iteration 5836, loss = 0.19558916\n",
      "Iteration 5837, loss = 0.19557973\n",
      "Iteration 5838, loss = 0.19556828\n",
      "Iteration 5839, loss = 0.19555934\n",
      "Iteration 5840, loss = 0.19554821\n",
      "Iteration 5841, loss = 0.19553733\n",
      "Iteration 5842, loss = 0.19552730\n",
      "Iteration 5843, loss = 0.19551500\n",
      "Iteration 5844, loss = 0.19550749\n",
      "Iteration 5845, loss = 0.19549575\n",
      "Iteration 5846, loss = 0.19548857\n",
      "Iteration 5847, loss = 0.19547685\n",
      "Iteration 5848, loss = 0.19546863\n",
      "Iteration 5849, loss = 0.19545904\n",
      "Iteration 5850, loss = 0.19544767\n",
      "Iteration 5851, loss = 0.19543584\n",
      "Iteration 5852, loss = 0.19542338\n",
      "Iteration 5853, loss = 0.19541378\n",
      "Iteration 5854, loss = 0.19540517\n",
      "Iteration 5855, loss = 0.19539269\n",
      "Iteration 5856, loss = 0.19538424\n",
      "Iteration 5857, loss = 0.19537350\n",
      "Iteration 5858, loss = 0.19535969\n",
      "Iteration 5859, loss = 0.19535211\n",
      "Iteration 5860, loss = 0.19533994\n",
      "Iteration 5861, loss = 0.19532825\n",
      "Iteration 5862, loss = 0.19531970\n",
      "Iteration 5863, loss = 0.19531013\n",
      "Iteration 5864, loss = 0.19530075\n",
      "Iteration 5865, loss = 0.19529023\n",
      "Iteration 5866, loss = 0.19527858\n",
      "Iteration 5867, loss = 0.19526733\n",
      "Iteration 5868, loss = 0.19525556\n",
      "Iteration 5869, loss = 0.19524876\n",
      "Iteration 5870, loss = 0.19523599\n",
      "Iteration 5871, loss = 0.19522483\n",
      "Iteration 5872, loss = 0.19521442\n",
      "Iteration 5873, loss = 0.19520396\n",
      "Iteration 5874, loss = 0.19519091\n",
      "Iteration 5875, loss = 0.19518189\n",
      "Iteration 5876, loss = 0.19517260\n",
      "Iteration 5877, loss = 0.19516438\n",
      "Iteration 5878, loss = 0.19515375\n",
      "Iteration 5879, loss = 0.19514144\n",
      "Iteration 5880, loss = 0.19513034\n",
      "Iteration 5881, loss = 0.19511974\n",
      "Iteration 5882, loss = 0.19511500\n",
      "Iteration 5883, loss = 0.19510542\n",
      "Iteration 5884, loss = 0.19508976\n",
      "Iteration 5885, loss = 0.19508131\n",
      "Iteration 5886, loss = 0.19506971\n",
      "Iteration 5887, loss = 0.19505794\n",
      "Iteration 5888, loss = 0.19505215\n",
      "Iteration 5889, loss = 0.19504004\n",
      "Iteration 5890, loss = 0.19502632\n",
      "Iteration 5891, loss = 0.19501811\n",
      "Iteration 5892, loss = 0.19500897\n",
      "Iteration 5893, loss = 0.19499926\n",
      "Iteration 5894, loss = 0.19498927\n",
      "Iteration 5895, loss = 0.19497547\n",
      "Iteration 5896, loss = 0.19496515\n",
      "Iteration 5897, loss = 0.19495496\n",
      "Iteration 5898, loss = 0.19494545\n",
      "Iteration 5899, loss = 0.19493642\n",
      "Iteration 5900, loss = 0.19492744\n",
      "Iteration 5901, loss = 0.19491537\n",
      "Iteration 5902, loss = 0.19490532\n",
      "Iteration 5903, loss = 0.19489552\n",
      "Iteration 5904, loss = 0.19488463\n",
      "Iteration 5905, loss = 0.19487546\n",
      "Iteration 5906, loss = 0.19486582\n",
      "Iteration 5907, loss = 0.19485479\n",
      "Iteration 5908, loss = 0.19484049\n",
      "Iteration 5909, loss = 0.19482962\n",
      "Iteration 5910, loss = 0.19481990\n",
      "Iteration 5911, loss = 0.19480991\n",
      "Iteration 5912, loss = 0.19480102\n",
      "Iteration 5913, loss = 0.19478795\n",
      "Iteration 5914, loss = 0.19477588\n",
      "Iteration 5915, loss = 0.19476723\n",
      "Iteration 5916, loss = 0.19475572\n",
      "Iteration 5917, loss = 0.19474438\n",
      "Iteration 5918, loss = 0.19473636\n",
      "Iteration 5919, loss = 0.19472790\n",
      "Iteration 5920, loss = 0.19471353\n",
      "Iteration 5921, loss = 0.19469824\n",
      "Iteration 5922, loss = 0.19469429\n",
      "Iteration 5923, loss = 0.19468702\n",
      "Iteration 5924, loss = 0.19467500\n",
      "Iteration 5925, loss = 0.19466179\n",
      "Iteration 5926, loss = 0.19465103\n",
      "Iteration 5927, loss = 0.19464158\n",
      "Iteration 5928, loss = 0.19463026\n",
      "Iteration 5929, loss = 0.19461782\n",
      "Iteration 5930, loss = 0.19460990\n",
      "Iteration 5931, loss = 0.19459788\n",
      "Iteration 5932, loss = 0.19458646\n",
      "Iteration 5933, loss = 0.19457558\n",
      "Iteration 5934, loss = 0.19456482\n",
      "Iteration 5935, loss = 0.19455372\n",
      "Iteration 5936, loss = 0.19454345\n",
      "Iteration 5937, loss = 0.19453110\n",
      "Iteration 5938, loss = 0.19452335\n",
      "Iteration 5939, loss = 0.19451432\n",
      "Iteration 5940, loss = 0.19450333\n",
      "Iteration 5941, loss = 0.19448711\n",
      "Iteration 5942, loss = 0.19447877\n",
      "Iteration 5943, loss = 0.19447189\n",
      "Iteration 5944, loss = 0.19446109\n",
      "Iteration 5945, loss = 0.19444787\n",
      "Iteration 5946, loss = 0.19443665\n",
      "Iteration 5947, loss = 0.19442829\n",
      "Iteration 5948, loss = 0.19441659\n",
      "Iteration 5949, loss = 0.19440473\n",
      "Iteration 5950, loss = 0.19439385\n",
      "Iteration 5951, loss = 0.19438646\n",
      "Iteration 5952, loss = 0.19437476\n",
      "Iteration 5953, loss = 0.19436032\n",
      "Iteration 5954, loss = 0.19434752\n",
      "Iteration 5955, loss = 0.19433709\n",
      "Iteration 5956, loss = 0.19432811\n",
      "Iteration 5957, loss = 0.19431455\n",
      "Iteration 5958, loss = 0.19430320\n",
      "Iteration 5959, loss = 0.19429228\n",
      "Iteration 5960, loss = 0.19428423\n",
      "Iteration 5961, loss = 0.19427284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5962, loss = 0.19426286\n",
      "Iteration 5963, loss = 0.19425135\n",
      "Iteration 5964, loss = 0.19424192\n",
      "Iteration 5965, loss = 0.19422821\n",
      "Iteration 5966, loss = 0.19421910\n",
      "Iteration 5967, loss = 0.19420945\n",
      "Iteration 5968, loss = 0.19419830\n",
      "Iteration 5969, loss = 0.19418665\n",
      "Iteration 5970, loss = 0.19417923\n",
      "Iteration 5971, loss = 0.19416900\n",
      "Iteration 5972, loss = 0.19415362\n",
      "Iteration 5973, loss = 0.19414119\n",
      "Iteration 5974, loss = 0.19413022\n",
      "Iteration 5975, loss = 0.19412134\n",
      "Iteration 5976, loss = 0.19410915\n",
      "Iteration 5977, loss = 0.19409626\n",
      "Iteration 5978, loss = 0.19408820\n",
      "Iteration 5979, loss = 0.19407998\n",
      "Iteration 5980, loss = 0.19406986\n",
      "Iteration 5981, loss = 0.19405490\n",
      "Iteration 5982, loss = 0.19404502\n",
      "Iteration 5983, loss = 0.19403393\n",
      "Iteration 5984, loss = 0.19402139\n",
      "Iteration 5985, loss = 0.19401172\n",
      "Iteration 5986, loss = 0.19400089\n",
      "Iteration 5987, loss = 0.19399151\n",
      "Iteration 5988, loss = 0.19398173\n",
      "Iteration 5989, loss = 0.19397083\n",
      "Iteration 5990, loss = 0.19395713\n",
      "Iteration 5991, loss = 0.19394553\n",
      "Iteration 5992, loss = 0.19393736\n",
      "Iteration 5993, loss = 0.19392688\n",
      "Iteration 5994, loss = 0.19391400\n",
      "Iteration 5995, loss = 0.19390443\n",
      "Iteration 5996, loss = 0.19389384\n",
      "Iteration 5997, loss = 0.19388302\n",
      "Iteration 5998, loss = 0.19387292\n",
      "Iteration 5999, loss = 0.19386309\n",
      "Iteration 6000, loss = 0.19384980\n",
      "Iteration 6001, loss = 0.19383537\n",
      "Iteration 6002, loss = 0.19382994\n",
      "Iteration 6003, loss = 0.19382395\n",
      "Iteration 6004, loss = 0.19381091\n",
      "Iteration 6005, loss = 0.19379886\n",
      "Iteration 6006, loss = 0.19378659\n",
      "Iteration 6007, loss = 0.19377370\n",
      "Iteration 6008, loss = 0.19376593\n",
      "Iteration 6009, loss = 0.19375285\n",
      "Iteration 6010, loss = 0.19374418\n",
      "Iteration 6011, loss = 0.19372917\n",
      "Iteration 6012, loss = 0.19372066\n",
      "Iteration 6013, loss = 0.19370979\n",
      "Iteration 6014, loss = 0.19369645\n",
      "Iteration 6015, loss = 0.19368515\n",
      "Iteration 6016, loss = 0.19367509\n",
      "Iteration 6017, loss = 0.19366294\n",
      "Iteration 6018, loss = 0.19365449\n",
      "Iteration 6019, loss = 0.19364660\n",
      "Iteration 6020, loss = 0.19363506\n",
      "Iteration 6021, loss = 0.19362454\n",
      "Iteration 6022, loss = 0.19361231\n",
      "Iteration 6023, loss = 0.19360179\n",
      "Iteration 6024, loss = 0.19359356\n",
      "Iteration 6025, loss = 0.19358309\n",
      "Iteration 6026, loss = 0.19356879\n",
      "Iteration 6027, loss = 0.19355380\n",
      "Iteration 6028, loss = 0.19354152\n",
      "Iteration 6029, loss = 0.19353308\n",
      "Iteration 6030, loss = 0.19352831\n",
      "Iteration 6031, loss = 0.19351178\n",
      "Iteration 6032, loss = 0.19349796\n",
      "Iteration 6033, loss = 0.19349356\n",
      "Iteration 6034, loss = 0.19348577\n",
      "Iteration 6035, loss = 0.19347268\n",
      "Iteration 6036, loss = 0.19346341\n",
      "Iteration 6037, loss = 0.19344911\n",
      "Iteration 6038, loss = 0.19343749\n",
      "Iteration 6039, loss = 0.19342806\n",
      "Iteration 6040, loss = 0.19341361\n",
      "Iteration 6041, loss = 0.19340429\n",
      "Iteration 6042, loss = 0.19339367\n",
      "Iteration 6043, loss = 0.19338090\n",
      "Iteration 6044, loss = 0.19336843\n",
      "Iteration 6045, loss = 0.19335615\n",
      "Iteration 6046, loss = 0.19334398\n",
      "Iteration 6047, loss = 0.19333576\n",
      "Iteration 6048, loss = 0.19332498\n",
      "Iteration 6049, loss = 0.19331380\n",
      "Iteration 6050, loss = 0.19330370\n",
      "Iteration 6051, loss = 0.19329412\n",
      "Iteration 6052, loss = 0.19328305\n",
      "Iteration 6053, loss = 0.19327445\n",
      "Iteration 6054, loss = 0.19326145\n",
      "Iteration 6055, loss = 0.19324889\n",
      "Iteration 6056, loss = 0.19324040\n",
      "Iteration 6057, loss = 0.19322987\n",
      "Iteration 6058, loss = 0.19321774\n",
      "Iteration 6059, loss = 0.19320746\n",
      "Iteration 6060, loss = 0.19319760\n",
      "Iteration 6061, loss = 0.19318526\n",
      "Iteration 6062, loss = 0.19317285\n",
      "Iteration 6063, loss = 0.19316193\n",
      "Iteration 6064, loss = 0.19315367\n",
      "Iteration 6065, loss = 0.19314121\n",
      "Iteration 6066, loss = 0.19312858\n",
      "Iteration 6067, loss = 0.19312010\n",
      "Iteration 6068, loss = 0.19311228\n",
      "Iteration 6069, loss = 0.19310081\n",
      "Iteration 6070, loss = 0.19308886\n",
      "Iteration 6071, loss = 0.19307684\n",
      "Iteration 6072, loss = 0.19306280\n",
      "Iteration 6073, loss = 0.19305205\n",
      "Iteration 6074, loss = 0.19304138\n",
      "Iteration 6075, loss = 0.19302955\n",
      "Iteration 6076, loss = 0.19302293\n",
      "Iteration 6077, loss = 0.19300999\n",
      "Iteration 6078, loss = 0.19299625\n",
      "Iteration 6079, loss = 0.19298601\n",
      "Iteration 6080, loss = 0.19297662\n",
      "Iteration 6081, loss = 0.19296685\n",
      "Iteration 6082, loss = 0.19295548\n",
      "Iteration 6083, loss = 0.19294416\n",
      "Iteration 6084, loss = 0.19293391\n",
      "Iteration 6085, loss = 0.19292141\n",
      "Iteration 6086, loss = 0.19290792\n",
      "Iteration 6087, loss = 0.19289733\n",
      "Iteration 6088, loss = 0.19288591\n",
      "Iteration 6089, loss = 0.19287257\n",
      "Iteration 6090, loss = 0.19286215\n",
      "Iteration 6091, loss = 0.19284935\n",
      "Iteration 6092, loss = 0.19284067\n",
      "Iteration 6093, loss = 0.19283121\n",
      "Iteration 6094, loss = 0.19282068\n",
      "Iteration 6095, loss = 0.19280586\n",
      "Iteration 6096, loss = 0.19279514\n",
      "Iteration 6097, loss = 0.19278811\n",
      "Iteration 6098, loss = 0.19277610\n",
      "Iteration 6099, loss = 0.19276476\n",
      "Iteration 6100, loss = 0.19275262\n",
      "Iteration 6101, loss = 0.19274348\n",
      "Iteration 6102, loss = 0.19273183\n",
      "Iteration 6103, loss = 0.19271714\n",
      "Iteration 6104, loss = 0.19270781\n",
      "Iteration 6105, loss = 0.19269969\n",
      "Iteration 6106, loss = 0.19268959\n",
      "Iteration 6107, loss = 0.19267492\n",
      "Iteration 6108, loss = 0.19266345\n",
      "Iteration 6109, loss = 0.19265325\n",
      "Iteration 6110, loss = 0.19263975\n",
      "Iteration 6111, loss = 0.19263214\n",
      "Iteration 6112, loss = 0.19261976\n",
      "Iteration 6113, loss = 0.19260781\n",
      "Iteration 6114, loss = 0.19260170\n",
      "Iteration 6115, loss = 0.19258767\n",
      "Iteration 6116, loss = 0.19257523\n",
      "Iteration 6117, loss = 0.19256664\n",
      "Iteration 6118, loss = 0.19255348\n",
      "Iteration 6119, loss = 0.19254203\n",
      "Iteration 6120, loss = 0.19253006\n",
      "Iteration 6121, loss = 0.19252096\n",
      "Iteration 6122, loss = 0.19250791\n",
      "Iteration 6123, loss = 0.19249332\n",
      "Iteration 6124, loss = 0.19248388\n",
      "Iteration 6125, loss = 0.19247247\n",
      "Iteration 6126, loss = 0.19246302\n",
      "Iteration 6127, loss = 0.19245066\n",
      "Iteration 6128, loss = 0.19244059\n",
      "Iteration 6129, loss = 0.19243062\n",
      "Iteration 6130, loss = 0.19242035\n",
      "Iteration 6131, loss = 0.19240743\n",
      "Iteration 6132, loss = 0.19239501\n",
      "Iteration 6133, loss = 0.19238260\n",
      "Iteration 6134, loss = 0.19237219\n",
      "Iteration 6135, loss = 0.19236064\n",
      "Iteration 6136, loss = 0.19234973\n",
      "Iteration 6137, loss = 0.19234244\n",
      "Iteration 6138, loss = 0.19232973\n",
      "Iteration 6139, loss = 0.19231391\n",
      "Iteration 6140, loss = 0.19230413\n",
      "Iteration 6141, loss = 0.19229293\n",
      "Iteration 6142, loss = 0.19228085\n",
      "Iteration 6143, loss = 0.19227028\n",
      "Iteration 6144, loss = 0.19225989\n",
      "Iteration 6145, loss = 0.19225017\n",
      "Iteration 6146, loss = 0.19223791\n",
      "Iteration 6147, loss = 0.19222914\n",
      "Iteration 6148, loss = 0.19221809\n",
      "Iteration 6149, loss = 0.19220913\n",
      "Iteration 6150, loss = 0.19219699\n",
      "Iteration 6151, loss = 0.19218417\n",
      "Iteration 6152, loss = 0.19217466\n",
      "Iteration 6153, loss = 0.19216349\n",
      "Iteration 6154, loss = 0.19215027\n",
      "Iteration 6155, loss = 0.19213806\n",
      "Iteration 6156, loss = 0.19212496\n",
      "Iteration 6157, loss = 0.19211969\n",
      "Iteration 6158, loss = 0.19211093\n",
      "Iteration 6159, loss = 0.19209763\n",
      "Iteration 6160, loss = 0.19208251\n",
      "Iteration 6161, loss = 0.19206914\n",
      "Iteration 6162, loss = 0.19205984\n",
      "Iteration 6163, loss = 0.19204958\n",
      "Iteration 6164, loss = 0.19203962\n",
      "Iteration 6165, loss = 0.19202667\n",
      "Iteration 6166, loss = 0.19201454\n",
      "Iteration 6167, loss = 0.19200285\n",
      "Iteration 6168, loss = 0.19199105\n",
      "Iteration 6169, loss = 0.19198254\n",
      "Iteration 6170, loss = 0.19197051\n",
      "Iteration 6171, loss = 0.19196189\n",
      "Iteration 6172, loss = 0.19194932\n",
      "Iteration 6173, loss = 0.19193623\n",
      "Iteration 6174, loss = 0.19192618\n",
      "Iteration 6175, loss = 0.19191735\n",
      "Iteration 6176, loss = 0.19190766\n",
      "Iteration 6177, loss = 0.19189460\n",
      "Iteration 6178, loss = 0.19187906\n",
      "Iteration 6179, loss = 0.19186911\n",
      "Iteration 6180, loss = 0.19185777\n",
      "Iteration 6181, loss = 0.19185069\n",
      "Iteration 6182, loss = 0.19183684\n",
      "Iteration 6183, loss = 0.19182030\n",
      "Iteration 6184, loss = 0.19181204\n",
      "Iteration 6185, loss = 0.19180548\n",
      "Iteration 6186, loss = 0.19179387\n",
      "Iteration 6187, loss = 0.19178306\n",
      "Iteration 6188, loss = 0.19176934\n",
      "Iteration 6189, loss = 0.19175689\n",
      "Iteration 6190, loss = 0.19174592\n",
      "Iteration 6191, loss = 0.19173504\n",
      "Iteration 6192, loss = 0.19172320\n",
      "Iteration 6193, loss = 0.19171370\n",
      "Iteration 6194, loss = 0.19170481\n",
      "Iteration 6195, loss = 0.19169014\n",
      "Iteration 6196, loss = 0.19167534\n",
      "Iteration 6197, loss = 0.19166679\n",
      "Iteration 6198, loss = 0.19165686\n",
      "Iteration 6199, loss = 0.19164357\n",
      "Iteration 6200, loss = 0.19162927\n",
      "Iteration 6201, loss = 0.19161810\n",
      "Iteration 6202, loss = 0.19160890\n",
      "Iteration 6203, loss = 0.19159536\n",
      "Iteration 6204, loss = 0.19158396\n",
      "Iteration 6205, loss = 0.19156997\n",
      "Iteration 6206, loss = 0.19156047\n",
      "Iteration 6207, loss = 0.19154981\n",
      "Iteration 6208, loss = 0.19153760\n",
      "Iteration 6209, loss = 0.19152952\n",
      "Iteration 6210, loss = 0.19151740\n",
      "Iteration 6211, loss = 0.19150667\n",
      "Iteration 6212, loss = 0.19149651\n",
      "Iteration 6213, loss = 0.19148306\n",
      "Iteration 6214, loss = 0.19147116\n",
      "Iteration 6215, loss = 0.19146459\n",
      "Iteration 6216, loss = 0.19144964\n",
      "Iteration 6217, loss = 0.19143412\n",
      "Iteration 6218, loss = 0.19142849\n",
      "Iteration 6219, loss = 0.19141729\n",
      "Iteration 6220, loss = 0.19140360\n",
      "Iteration 6221, loss = 0.19139527\n",
      "Iteration 6222, loss = 0.19138596\n",
      "Iteration 6223, loss = 0.19137155\n",
      "Iteration 6224, loss = 0.19136054\n",
      "Iteration 6225, loss = 0.19134726\n",
      "Iteration 6226, loss = 0.19133493\n",
      "Iteration 6227, loss = 0.19132369\n",
      "Iteration 6228, loss = 0.19130848\n",
      "Iteration 6229, loss = 0.19129784\n",
      "Iteration 6230, loss = 0.19129018\n",
      "Iteration 6231, loss = 0.19127998\n",
      "Iteration 6232, loss = 0.19126766\n",
      "Iteration 6233, loss = 0.19125890\n",
      "Iteration 6234, loss = 0.19124771\n",
      "Iteration 6235, loss = 0.19123741\n",
      "Iteration 6236, loss = 0.19122357\n",
      "Iteration 6237, loss = 0.19121052\n",
      "Iteration 6238, loss = 0.19120282\n",
      "Iteration 6239, loss = 0.19119167\n",
      "Iteration 6240, loss = 0.19117832\n",
      "Iteration 6241, loss = 0.19116499\n",
      "Iteration 6242, loss = 0.19114965\n",
      "Iteration 6243, loss = 0.19114084\n",
      "Iteration 6244, loss = 0.19113753\n",
      "Iteration 6245, loss = 0.19112719\n",
      "Iteration 6246, loss = 0.19110907\n",
      "Iteration 6247, loss = 0.19109623\n",
      "Iteration 6248, loss = 0.19108408\n",
      "Iteration 6249, loss = 0.19107212\n",
      "Iteration 6250, loss = 0.19106002\n",
      "Iteration 6251, loss = 0.19104689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6252, loss = 0.19103224\n",
      "Iteration 6253, loss = 0.19101709\n",
      "Iteration 6254, loss = 0.19100743\n",
      "Iteration 6255, loss = 0.19099757\n",
      "Iteration 6256, loss = 0.19098464\n",
      "Iteration 6257, loss = 0.19096941\n",
      "Iteration 6258, loss = 0.19095735\n",
      "Iteration 6259, loss = 0.19094524\n",
      "Iteration 6260, loss = 0.19093306\n",
      "Iteration 6261, loss = 0.19091993\n",
      "Iteration 6262, loss = 0.19090495\n",
      "Iteration 6263, loss = 0.19089024\n",
      "Iteration 6264, loss = 0.19087883\n",
      "Iteration 6265, loss = 0.19086639\n",
      "Iteration 6266, loss = 0.19085482\n",
      "Iteration 6267, loss = 0.19084611\n",
      "Iteration 6268, loss = 0.19083600\n",
      "Iteration 6269, loss = 0.19082366\n",
      "Iteration 6270, loss = 0.19081180\n",
      "Iteration 6271, loss = 0.19080058\n",
      "Iteration 6272, loss = 0.19079236\n",
      "Iteration 6273, loss = 0.19077788\n",
      "Iteration 6274, loss = 0.19076593\n",
      "Iteration 6275, loss = 0.19075434\n",
      "Iteration 6276, loss = 0.19074180\n",
      "Iteration 6277, loss = 0.19072960\n",
      "Iteration 6278, loss = 0.19071667\n",
      "Iteration 6279, loss = 0.19070520\n",
      "Iteration 6280, loss = 0.19069915\n",
      "Iteration 6281, loss = 0.19068540\n",
      "Iteration 6282, loss = 0.19067246\n",
      "Iteration 6283, loss = 0.19066135\n",
      "Iteration 6284, loss = 0.19064830\n",
      "Iteration 6285, loss = 0.19063951\n",
      "Iteration 6286, loss = 0.19062817\n",
      "Iteration 6287, loss = 0.19061854\n",
      "Iteration 6288, loss = 0.19060200\n",
      "Iteration 6289, loss = 0.19059015\n",
      "Iteration 6290, loss = 0.19057927\n",
      "Iteration 6291, loss = 0.19056625\n",
      "Iteration 6292, loss = 0.19055396\n",
      "Iteration 6293, loss = 0.19054494\n",
      "Iteration 6294, loss = 0.19053558\n",
      "Iteration 6295, loss = 0.19052302\n",
      "Iteration 6296, loss = 0.19051284\n",
      "Iteration 6297, loss = 0.19049768\n",
      "Iteration 6298, loss = 0.19048368\n",
      "Iteration 6299, loss = 0.19047543\n",
      "Iteration 6300, loss = 0.19046611\n",
      "Iteration 6301, loss = 0.19045218\n",
      "Iteration 6302, loss = 0.19044023\n",
      "Iteration 6303, loss = 0.19042996\n",
      "Iteration 6304, loss = 0.19041951\n",
      "Iteration 6305, loss = 0.19040540\n",
      "Iteration 6306, loss = 0.19039528\n",
      "Iteration 6307, loss = 0.19038456\n",
      "Iteration 6308, loss = 0.19036847\n",
      "Iteration 6309, loss = 0.19035782\n",
      "Iteration 6310, loss = 0.19034616\n",
      "Iteration 6311, loss = 0.19033550\n",
      "Iteration 6312, loss = 0.19032091\n",
      "Iteration 6313, loss = 0.19031292\n",
      "Iteration 6314, loss = 0.19030157\n",
      "Iteration 6315, loss = 0.19029109\n",
      "Iteration 6316, loss = 0.19027820\n",
      "Iteration 6317, loss = 0.19026392\n",
      "Iteration 6318, loss = 0.19025691\n",
      "Iteration 6319, loss = 0.19024581\n",
      "Iteration 6320, loss = 0.19023182\n",
      "Iteration 6321, loss = 0.19021689\n",
      "Iteration 6322, loss = 0.19020595\n",
      "Iteration 6323, loss = 0.19019378\n",
      "Iteration 6324, loss = 0.19018095\n",
      "Iteration 6325, loss = 0.19016848\n",
      "Iteration 6326, loss = 0.19015715\n",
      "Iteration 6327, loss = 0.19014523\n",
      "Iteration 6328, loss = 0.19013538\n",
      "Iteration 6329, loss = 0.19012356\n",
      "Iteration 6330, loss = 0.19010610\n",
      "Iteration 6331, loss = 0.19009496\n",
      "Iteration 6332, loss = 0.19008591\n",
      "Iteration 6333, loss = 0.19007597\n",
      "Iteration 6334, loss = 0.19006140\n",
      "Iteration 6335, loss = 0.19004778\n",
      "Iteration 6336, loss = 0.19003579\n",
      "Iteration 6337, loss = 0.19002334\n",
      "Iteration 6338, loss = 0.19001354\n",
      "Iteration 6339, loss = 0.19000523\n",
      "Iteration 6340, loss = 0.18999039\n",
      "Iteration 6341, loss = 0.18998043\n",
      "Iteration 6342, loss = 0.18996945\n",
      "Iteration 6343, loss = 0.18995839\n",
      "Iteration 6344, loss = 0.18994774\n",
      "Iteration 6345, loss = 0.18993182\n",
      "Iteration 6346, loss = 0.18992455\n",
      "Iteration 6347, loss = 0.18991182\n",
      "Iteration 6348, loss = 0.18989961\n",
      "Iteration 6349, loss = 0.18988658\n",
      "Iteration 6350, loss = 0.18987157\n",
      "Iteration 6351, loss = 0.18986454\n",
      "Iteration 6352, loss = 0.18985466\n",
      "Iteration 6353, loss = 0.18984422\n",
      "Iteration 6354, loss = 0.18982964\n",
      "Iteration 6355, loss = 0.18981538\n",
      "Iteration 6356, loss = 0.18980590\n",
      "Iteration 6357, loss = 0.18979841\n",
      "Iteration 6358, loss = 0.18978480\n",
      "Iteration 6359, loss = 0.18976976\n",
      "Iteration 6360, loss = 0.18976021\n",
      "Iteration 6361, loss = 0.18975039\n",
      "Iteration 6362, loss = 0.18973520\n",
      "Iteration 6363, loss = 0.18972407\n",
      "Iteration 6364, loss = 0.18971231\n",
      "Iteration 6365, loss = 0.18969929\n",
      "Iteration 6366, loss = 0.18968370\n",
      "Iteration 6367, loss = 0.18967413\n",
      "Iteration 6368, loss = 0.18966257\n",
      "Iteration 6369, loss = 0.18965317\n",
      "Iteration 6370, loss = 0.18964083\n",
      "Iteration 6371, loss = 0.18962885\n",
      "Iteration 6372, loss = 0.18961701\n",
      "Iteration 6373, loss = 0.18960354\n",
      "Iteration 6374, loss = 0.18959315\n",
      "Iteration 6375, loss = 0.18958053\n",
      "Iteration 6376, loss = 0.18956937\n",
      "Iteration 6377, loss = 0.18955933\n",
      "Iteration 6378, loss = 0.18954779\n",
      "Iteration 6379, loss = 0.18953750\n",
      "Iteration 6380, loss = 0.18952725\n",
      "Iteration 6381, loss = 0.18951110\n",
      "Iteration 6382, loss = 0.18949626\n",
      "Iteration 6383, loss = 0.18948641\n",
      "Iteration 6384, loss = 0.18947801\n",
      "Iteration 6385, loss = 0.18946403\n",
      "Iteration 6386, loss = 0.18944819\n",
      "Iteration 6387, loss = 0.18943950\n",
      "Iteration 6388, loss = 0.18942663\n",
      "Iteration 6389, loss = 0.18941092\n",
      "Iteration 6390, loss = 0.18939999\n",
      "Iteration 6391, loss = 0.18938515\n",
      "Iteration 6392, loss = 0.18937959\n",
      "Iteration 6393, loss = 0.18936681\n",
      "Iteration 6394, loss = 0.18935209\n",
      "Iteration 6395, loss = 0.18934498\n",
      "Iteration 6396, loss = 0.18933595\n",
      "Iteration 6397, loss = 0.18932044\n",
      "Iteration 6398, loss = 0.18930668\n",
      "Iteration 6399, loss = 0.18929279\n",
      "Iteration 6400, loss = 0.18928075\n",
      "Iteration 6401, loss = 0.18926878\n",
      "Iteration 6402, loss = 0.18925826\n",
      "Iteration 6403, loss = 0.18924750\n",
      "Iteration 6404, loss = 0.18923307\n",
      "Iteration 6405, loss = 0.18922360\n",
      "Iteration 6406, loss = 0.18921003\n",
      "Iteration 6407, loss = 0.18919783\n",
      "Iteration 6408, loss = 0.18918935\n",
      "Iteration 6409, loss = 0.18917949\n",
      "Iteration 6410, loss = 0.18916879\n",
      "Iteration 6411, loss = 0.18915555\n",
      "Iteration 6412, loss = 0.18913834\n",
      "Iteration 6413, loss = 0.18912737\n",
      "Iteration 6414, loss = 0.18911622\n",
      "Iteration 6415, loss = 0.18910390\n",
      "Iteration 6416, loss = 0.18909425\n",
      "Iteration 6417, loss = 0.18908252\n",
      "Iteration 6418, loss = 0.18907039\n",
      "Iteration 6419, loss = 0.18905861\n",
      "Iteration 6420, loss = 0.18904649\n",
      "Iteration 6421, loss = 0.18903195\n",
      "Iteration 6422, loss = 0.18902074\n",
      "Iteration 6423, loss = 0.18900653\n",
      "Iteration 6424, loss = 0.18899693\n",
      "Iteration 6425, loss = 0.18898207\n",
      "Iteration 6426, loss = 0.18897154\n",
      "Iteration 6427, loss = 0.18895677\n",
      "Iteration 6428, loss = 0.18894794\n",
      "Iteration 6429, loss = 0.18893761\n",
      "Iteration 6430, loss = 0.18892331\n",
      "Iteration 6431, loss = 0.18890908\n",
      "Iteration 6432, loss = 0.18889993\n",
      "Iteration 6433, loss = 0.18889089\n",
      "Iteration 6434, loss = 0.18887921\n",
      "Iteration 6435, loss = 0.18886514\n",
      "Iteration 6436, loss = 0.18885400\n",
      "Iteration 6437, loss = 0.18884319\n",
      "Iteration 6438, loss = 0.18883069\n",
      "Iteration 6439, loss = 0.18881767\n",
      "Iteration 6440, loss = 0.18880588\n",
      "Iteration 6441, loss = 0.18879178\n",
      "Iteration 6442, loss = 0.18878335\n",
      "Iteration 6443, loss = 0.18877116\n",
      "Iteration 6444, loss = 0.18875628\n",
      "Iteration 6445, loss = 0.18874306\n",
      "Iteration 6446, loss = 0.18873535\n",
      "Iteration 6447, loss = 0.18872502\n",
      "Iteration 6448, loss = 0.18871046\n",
      "Iteration 6449, loss = 0.18869861\n",
      "Iteration 6450, loss = 0.18868723\n",
      "Iteration 6451, loss = 0.18867057\n",
      "Iteration 6452, loss = 0.18865737\n",
      "Iteration 6453, loss = 0.18864477\n",
      "Iteration 6454, loss = 0.18863362\n",
      "Iteration 6455, loss = 0.18862177\n",
      "Iteration 6456, loss = 0.18860870\n",
      "Iteration 6457, loss = 0.18859733\n",
      "Iteration 6458, loss = 0.18858522\n",
      "Iteration 6459, loss = 0.18857593\n",
      "Iteration 6460, loss = 0.18855981\n",
      "Iteration 6461, loss = 0.18855237\n",
      "Iteration 6462, loss = 0.18853856\n",
      "Iteration 6463, loss = 0.18852731\n",
      "Iteration 6464, loss = 0.18851912\n",
      "Iteration 6465, loss = 0.18850478\n",
      "Iteration 6466, loss = 0.18849038\n",
      "Iteration 6467, loss = 0.18848423\n",
      "Iteration 6468, loss = 0.18847086\n",
      "Iteration 6469, loss = 0.18845252\n",
      "Iteration 6470, loss = 0.18844577\n",
      "Iteration 6471, loss = 0.18843539\n",
      "Iteration 6472, loss = 0.18841691\n",
      "Iteration 6473, loss = 0.18840503\n",
      "Iteration 6474, loss = 0.18839583\n",
      "Iteration 6475, loss = 0.18838379\n",
      "Iteration 6476, loss = 0.18837236\n",
      "Iteration 6477, loss = 0.18835903\n",
      "Iteration 6478, loss = 0.18834654\n",
      "Iteration 6479, loss = 0.18833697\n",
      "Iteration 6480, loss = 0.18832456\n",
      "Iteration 6481, loss = 0.18830967\n",
      "Iteration 6482, loss = 0.18829839\n",
      "Iteration 6483, loss = 0.18828941\n",
      "Iteration 6484, loss = 0.18827293\n",
      "Iteration 6485, loss = 0.18826178\n",
      "Iteration 6486, loss = 0.18825182\n",
      "Iteration 6487, loss = 0.18823894\n",
      "Iteration 6488, loss = 0.18822763\n",
      "Iteration 6489, loss = 0.18821448\n",
      "Iteration 6490, loss = 0.18820354\n",
      "Iteration 6491, loss = 0.18819187\n",
      "Iteration 6492, loss = 0.18817717\n",
      "Iteration 6493, loss = 0.18817037\n",
      "Iteration 6494, loss = 0.18815838\n",
      "Iteration 6495, loss = 0.18814355\n",
      "Iteration 6496, loss = 0.18812959\n",
      "Iteration 6497, loss = 0.18812012\n",
      "Iteration 6498, loss = 0.18811239\n",
      "Iteration 6499, loss = 0.18810182\n",
      "Iteration 6500, loss = 0.18808562\n",
      "Iteration 6501, loss = 0.18807285\n",
      "Iteration 6502, loss = 0.18806103\n",
      "Iteration 6503, loss = 0.18805014\n",
      "Iteration 6504, loss = 0.18804280\n",
      "Iteration 6505, loss = 0.18803085\n",
      "Iteration 6506, loss = 0.18801333\n",
      "Iteration 6507, loss = 0.18799749\n",
      "Iteration 6508, loss = 0.18798611\n",
      "Iteration 6509, loss = 0.18797441\n",
      "Iteration 6510, loss = 0.18796238\n",
      "Iteration 6511, loss = 0.18795130\n",
      "Iteration 6512, loss = 0.18793652\n",
      "Iteration 6513, loss = 0.18792664\n",
      "Iteration 6514, loss = 0.18791437\n",
      "Iteration 6515, loss = 0.18790108\n",
      "Iteration 6516, loss = 0.18789264\n",
      "Iteration 6517, loss = 0.18787467\n",
      "Iteration 6518, loss = 0.18785923\n",
      "Iteration 6519, loss = 0.18785227\n",
      "Iteration 6520, loss = 0.18784072\n",
      "Iteration 6521, loss = 0.18783193\n",
      "Iteration 6522, loss = 0.18781848\n",
      "Iteration 6523, loss = 0.18780279\n",
      "Iteration 6524, loss = 0.18779447\n",
      "Iteration 6525, loss = 0.18778447\n",
      "Iteration 6526, loss = 0.18777133\n",
      "Iteration 6527, loss = 0.18775634\n",
      "Iteration 6528, loss = 0.18774486\n",
      "Iteration 6529, loss = 0.18773431\n",
      "Iteration 6530, loss = 0.18771941\n",
      "Iteration 6531, loss = 0.18770543\n",
      "Iteration 6532, loss = 0.18769792\n",
      "Iteration 6533, loss = 0.18768344\n",
      "Iteration 6534, loss = 0.18766904\n",
      "Iteration 6535, loss = 0.18765692\n",
      "Iteration 6536, loss = 0.18764612\n",
      "Iteration 6537, loss = 0.18763459\n",
      "Iteration 6538, loss = 0.18762283\n",
      "Iteration 6539, loss = 0.18760930\n",
      "Iteration 6540, loss = 0.18759676\n",
      "Iteration 6541, loss = 0.18758262\n",
      "Iteration 6542, loss = 0.18757501\n",
      "Iteration 6543, loss = 0.18755918\n",
      "Iteration 6544, loss = 0.18754904\n",
      "Iteration 6545, loss = 0.18754084\n",
      "Iteration 6546, loss = 0.18752546\n",
      "Iteration 6547, loss = 0.18751252\n",
      "Iteration 6548, loss = 0.18750135\n",
      "Iteration 6549, loss = 0.18749376\n",
      "Iteration 6550, loss = 0.18748430\n",
      "Iteration 6551, loss = 0.18746812\n",
      "Iteration 6552, loss = 0.18745167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6553, loss = 0.18744579\n",
      "Iteration 6554, loss = 0.18743775\n",
      "Iteration 6555, loss = 0.18742355\n",
      "Iteration 6556, loss = 0.18740926\n",
      "Iteration 6557, loss = 0.18739606\n",
      "Iteration 6558, loss = 0.18738251\n",
      "Iteration 6559, loss = 0.18737004\n",
      "Iteration 6560, loss = 0.18735523\n",
      "Iteration 6561, loss = 0.18733871\n",
      "Iteration 6562, loss = 0.18732813\n",
      "Iteration 6563, loss = 0.18732100\n",
      "Iteration 6564, loss = 0.18730550\n",
      "Iteration 6565, loss = 0.18729255\n",
      "Iteration 6566, loss = 0.18728341\n",
      "Iteration 6567, loss = 0.18727566\n",
      "Iteration 6568, loss = 0.18725886\n",
      "Iteration 6569, loss = 0.18724409\n",
      "Iteration 6570, loss = 0.18723358\n",
      "Iteration 6571, loss = 0.18722237\n",
      "Iteration 6572, loss = 0.18721125\n",
      "Iteration 6573, loss = 0.18719683\n",
      "Iteration 6574, loss = 0.18718236\n",
      "Iteration 6575, loss = 0.18716901\n",
      "Iteration 6576, loss = 0.18715562\n",
      "Iteration 6577, loss = 0.18714225\n",
      "Iteration 6578, loss = 0.18713355\n",
      "Iteration 6579, loss = 0.18712063\n",
      "Iteration 6580, loss = 0.18711053\n",
      "Iteration 6581, loss = 0.18709566\n",
      "Iteration 6582, loss = 0.18708438\n",
      "Iteration 6583, loss = 0.18706858\n",
      "Iteration 6584, loss = 0.18705918\n",
      "Iteration 6585, loss = 0.18704952\n",
      "Iteration 6586, loss = 0.18703815\n",
      "Iteration 6587, loss = 0.18702337\n",
      "Iteration 6588, loss = 0.18701027\n",
      "Iteration 6589, loss = 0.18699830\n",
      "Iteration 6590, loss = 0.18698887\n",
      "Iteration 6591, loss = 0.18697771\n",
      "Iteration 6592, loss = 0.18696209\n",
      "Iteration 6593, loss = 0.18694886\n",
      "Iteration 6594, loss = 0.18693478\n",
      "Iteration 6595, loss = 0.18692416\n",
      "Iteration 6596, loss = 0.18691259\n",
      "Iteration 6597, loss = 0.18690028\n",
      "Iteration 6598, loss = 0.18688564\n",
      "Iteration 6599, loss = 0.18687379\n",
      "Iteration 6600, loss = 0.18685924\n",
      "Iteration 6601, loss = 0.18684788\n",
      "Iteration 6602, loss = 0.18684042\n",
      "Iteration 6603, loss = 0.18682599\n",
      "Iteration 6604, loss = 0.18681408\n",
      "Iteration 6605, loss = 0.18680022\n",
      "Iteration 6606, loss = 0.18679253\n",
      "Iteration 6607, loss = 0.18677976\n",
      "Iteration 6608, loss = 0.18676989\n",
      "Iteration 6609, loss = 0.18675559\n",
      "Iteration 6610, loss = 0.18674052\n",
      "Iteration 6611, loss = 0.18673130\n",
      "Iteration 6612, loss = 0.18671968\n",
      "Iteration 6613, loss = 0.18670597\n",
      "Iteration 6614, loss = 0.18669159\n",
      "Iteration 6615, loss = 0.18668146\n",
      "Iteration 6616, loss = 0.18667236\n",
      "Iteration 6617, loss = 0.18666055\n",
      "Iteration 6618, loss = 0.18664668\n",
      "Iteration 6619, loss = 0.18663489\n",
      "Iteration 6620, loss = 0.18662118\n",
      "Iteration 6621, loss = 0.18661056\n",
      "Iteration 6622, loss = 0.18659443\n",
      "Iteration 6623, loss = 0.18657836\n",
      "Iteration 6624, loss = 0.18656523\n",
      "Iteration 6625, loss = 0.18656095\n",
      "Iteration 6626, loss = 0.18654930\n",
      "Iteration 6627, loss = 0.18652751\n",
      "Iteration 6628, loss = 0.18651474\n",
      "Iteration 6629, loss = 0.18650759\n",
      "Iteration 6630, loss = 0.18649862\n",
      "Iteration 6631, loss = 0.18648273\n",
      "Iteration 6632, loss = 0.18647251\n",
      "Iteration 6633, loss = 0.18646166\n",
      "Iteration 6634, loss = 0.18644980\n",
      "Iteration 6635, loss = 0.18643482\n",
      "Iteration 6636, loss = 0.18642222\n",
      "Iteration 6637, loss = 0.18641136\n",
      "Iteration 6638, loss = 0.18639943\n",
      "Iteration 6639, loss = 0.18638285\n",
      "Iteration 6640, loss = 0.18637461\n",
      "Iteration 6641, loss = 0.18636762\n",
      "Iteration 6642, loss = 0.18634905\n",
      "Iteration 6643, loss = 0.18633765\n",
      "Iteration 6644, loss = 0.18632658\n",
      "Iteration 6645, loss = 0.18631411\n",
      "Iteration 6646, loss = 0.18630506\n",
      "Iteration 6647, loss = 0.18629244\n",
      "Iteration 6648, loss = 0.18627701\n",
      "Iteration 6649, loss = 0.18626412\n",
      "Iteration 6650, loss = 0.18625150\n",
      "Iteration 6651, loss = 0.18624049\n",
      "Iteration 6652, loss = 0.18622707\n",
      "Iteration 6653, loss = 0.18621043\n",
      "Iteration 6654, loss = 0.18619578\n",
      "Iteration 6655, loss = 0.18618587\n",
      "Iteration 6656, loss = 0.18617157\n",
      "Iteration 6657, loss = 0.18616048\n",
      "Iteration 6658, loss = 0.18614821\n",
      "Iteration 6659, loss = 0.18613405\n",
      "Iteration 6660, loss = 0.18612186\n",
      "Iteration 6661, loss = 0.18611032\n",
      "Iteration 6662, loss = 0.18610003\n",
      "Iteration 6663, loss = 0.18608355\n",
      "Iteration 6664, loss = 0.18606997\n",
      "Iteration 6665, loss = 0.18605676\n",
      "Iteration 6666, loss = 0.18604732\n",
      "Iteration 6667, loss = 0.18603733\n",
      "Iteration 6668, loss = 0.18601928\n",
      "Iteration 6669, loss = 0.18600408\n",
      "Iteration 6670, loss = 0.18599037\n",
      "Iteration 6671, loss = 0.18598087\n",
      "Iteration 6672, loss = 0.18597690\n",
      "Iteration 6673, loss = 0.18596313\n",
      "Iteration 6674, loss = 0.18594137\n",
      "Iteration 6675, loss = 0.18592542\n",
      "Iteration 6676, loss = 0.18591063\n",
      "Iteration 6677, loss = 0.18589632\n",
      "Iteration 6678, loss = 0.18588423\n",
      "Iteration 6679, loss = 0.18587354\n",
      "Iteration 6680, loss = 0.18585839\n",
      "Iteration 6681, loss = 0.18584122\n",
      "Iteration 6682, loss = 0.18582869\n",
      "Iteration 6683, loss = 0.18581537\n",
      "Iteration 6684, loss = 0.18580167\n",
      "Iteration 6685, loss = 0.18579054\n",
      "Iteration 6686, loss = 0.18577431\n",
      "Iteration 6687, loss = 0.18576055\n",
      "Iteration 6688, loss = 0.18574509\n",
      "Iteration 6689, loss = 0.18573089\n",
      "Iteration 6690, loss = 0.18571515\n",
      "Iteration 6691, loss = 0.18569667\n",
      "Iteration 6692, loss = 0.18568608\n",
      "Iteration 6693, loss = 0.18567764\n",
      "Iteration 6694, loss = 0.18566244\n",
      "Iteration 6695, loss = 0.18564319\n",
      "Iteration 6696, loss = 0.18562981\n",
      "Iteration 6697, loss = 0.18561876\n",
      "Iteration 6698, loss = 0.18560747\n",
      "Iteration 6699, loss = 0.18559124\n",
      "Iteration 6700, loss = 0.18557647\n",
      "Iteration 6701, loss = 0.18555913\n",
      "Iteration 6702, loss = 0.18554452\n",
      "Iteration 6703, loss = 0.18552752\n",
      "Iteration 6704, loss = 0.18551403\n",
      "Iteration 6705, loss = 0.18550260\n",
      "Iteration 6706, loss = 0.18548504\n",
      "Iteration 6707, loss = 0.18546956\n",
      "Iteration 6708, loss = 0.18545600\n",
      "Iteration 6709, loss = 0.18544422\n",
      "Iteration 6710, loss = 0.18542562\n",
      "Iteration 6711, loss = 0.18540976\n",
      "Iteration 6712, loss = 0.18539796\n",
      "Iteration 6713, loss = 0.18538707\n",
      "Iteration 6714, loss = 0.18536983\n",
      "Iteration 6715, loss = 0.18535500\n",
      "Iteration 6716, loss = 0.18533775\n",
      "Iteration 6717, loss = 0.18532226\n",
      "Iteration 6718, loss = 0.18531004\n",
      "Iteration 6719, loss = 0.18529534\n",
      "Iteration 6720, loss = 0.18527597\n",
      "Iteration 6721, loss = 0.18526442\n",
      "Iteration 6722, loss = 0.18525196\n",
      "Iteration 6723, loss = 0.18523584\n",
      "Iteration 6724, loss = 0.18522074\n",
      "Iteration 6725, loss = 0.18520345\n",
      "Iteration 6726, loss = 0.18519299\n",
      "Iteration 6727, loss = 0.18517451\n",
      "Iteration 6728, loss = 0.18516390\n",
      "Iteration 6729, loss = 0.18515014\n",
      "Iteration 6730, loss = 0.18513391\n",
      "Iteration 6731, loss = 0.18512044\n",
      "Iteration 6732, loss = 0.18510662\n",
      "Iteration 6733, loss = 0.18509248\n",
      "Iteration 6734, loss = 0.18507582\n",
      "Iteration 6735, loss = 0.18505850\n",
      "Iteration 6736, loss = 0.18504716\n",
      "Iteration 6737, loss = 0.18503349\n",
      "Iteration 6738, loss = 0.18501573\n",
      "Iteration 6739, loss = 0.18500410\n",
      "Iteration 6740, loss = 0.18499034\n",
      "Iteration 6741, loss = 0.18497677\n",
      "Iteration 6742, loss = 0.18496442\n",
      "Iteration 6743, loss = 0.18494831\n",
      "Iteration 6744, loss = 0.18493641\n",
      "Iteration 6745, loss = 0.18492096\n",
      "Iteration 6746, loss = 0.18490851\n",
      "Iteration 6747, loss = 0.18489199\n",
      "Iteration 6748, loss = 0.18488064\n",
      "Iteration 6749, loss = 0.18486640\n",
      "Iteration 6750, loss = 0.18485295\n",
      "Iteration 6751, loss = 0.18483833\n",
      "Iteration 6752, loss = 0.18482509\n",
      "Iteration 6753, loss = 0.18480922\n",
      "Iteration 6754, loss = 0.18479794\n",
      "Iteration 6755, loss = 0.18478374\n",
      "Iteration 6756, loss = 0.18476450\n",
      "Iteration 6757, loss = 0.18475626\n",
      "Iteration 6758, loss = 0.18474168\n",
      "Iteration 6759, loss = 0.18472872\n",
      "Iteration 6760, loss = 0.18471381\n",
      "Iteration 6761, loss = 0.18469723\n",
      "Iteration 6762, loss = 0.18468460\n",
      "Iteration 6763, loss = 0.18467209\n",
      "Iteration 6764, loss = 0.18466146\n",
      "Iteration 6765, loss = 0.18464271\n",
      "Iteration 6766, loss = 0.18462778\n",
      "Iteration 6767, loss = 0.18461484\n",
      "Iteration 6768, loss = 0.18460505\n",
      "Iteration 6769, loss = 0.18459070\n",
      "Iteration 6770, loss = 0.18457344\n",
      "Iteration 6771, loss = 0.18455631\n",
      "Iteration 6772, loss = 0.18455117\n",
      "Iteration 6773, loss = 0.18453702\n",
      "Iteration 6774, loss = 0.18452612\n",
      "Iteration 6775, loss = 0.18450759\n",
      "Iteration 6776, loss = 0.18449187\n",
      "Iteration 6777, loss = 0.18447628\n",
      "Iteration 6778, loss = 0.18446755\n",
      "Iteration 6779, loss = 0.18445269\n",
      "Iteration 6780, loss = 0.18443687\n",
      "Iteration 6781, loss = 0.18442528\n",
      "Iteration 6782, loss = 0.18441194\n",
      "Iteration 6783, loss = 0.18439711\n",
      "Iteration 6784, loss = 0.18438364\n",
      "Iteration 6785, loss = 0.18436721\n",
      "Iteration 6786, loss = 0.18435524\n",
      "Iteration 6787, loss = 0.18433991\n",
      "Iteration 6788, loss = 0.18432842\n",
      "Iteration 6789, loss = 0.18431477\n",
      "Iteration 6790, loss = 0.18430042\n",
      "Iteration 6791, loss = 0.18428562\n",
      "Iteration 6792, loss = 0.18427410\n",
      "Iteration 6793, loss = 0.18426296\n",
      "Iteration 6794, loss = 0.18424438\n",
      "Iteration 6795, loss = 0.18423273\n",
      "Iteration 6796, loss = 0.18422264\n",
      "Iteration 6797, loss = 0.18420729\n",
      "Iteration 6798, loss = 0.18419403\n",
      "Iteration 6799, loss = 0.18418321\n",
      "Iteration 6800, loss = 0.18416936\n",
      "Iteration 6801, loss = 0.18415826\n",
      "Iteration 6802, loss = 0.18414496\n",
      "Iteration 6803, loss = 0.18412924\n",
      "Iteration 6804, loss = 0.18411313\n",
      "Iteration 6805, loss = 0.18410235\n",
      "Iteration 6806, loss = 0.18408893\n",
      "Iteration 6807, loss = 0.18407415\n",
      "Iteration 6808, loss = 0.18406222\n",
      "Iteration 6809, loss = 0.18405227\n",
      "Iteration 6810, loss = 0.18403438\n",
      "Iteration 6811, loss = 0.18402266\n",
      "Iteration 6812, loss = 0.18400953\n",
      "Iteration 6813, loss = 0.18399554\n",
      "Iteration 6814, loss = 0.18397922\n",
      "Iteration 6815, loss = 0.18397418\n",
      "Iteration 6816, loss = 0.18396242\n",
      "Iteration 6817, loss = 0.18394312\n",
      "Iteration 6818, loss = 0.18392904\n",
      "Iteration 6819, loss = 0.18391727\n",
      "Iteration 6820, loss = 0.18390777\n",
      "Iteration 6821, loss = 0.18389562\n",
      "Iteration 6822, loss = 0.18388262\n",
      "Iteration 6823, loss = 0.18386676\n",
      "Iteration 6824, loss = 0.18385393\n",
      "Iteration 6825, loss = 0.18383930\n",
      "Iteration 6826, loss = 0.18382425\n",
      "Iteration 6827, loss = 0.18381066\n",
      "Iteration 6828, loss = 0.18379953\n",
      "Iteration 6829, loss = 0.18378188\n",
      "Iteration 6830, loss = 0.18376551\n",
      "Iteration 6831, loss = 0.18375365\n",
      "Iteration 6832, loss = 0.18374013\n",
      "Iteration 6833, loss = 0.18372544\n",
      "Iteration 6834, loss = 0.18371584\n",
      "Iteration 6835, loss = 0.18369939\n",
      "Iteration 6836, loss = 0.18368468\n",
      "Iteration 6837, loss = 0.18367134\n",
      "Iteration 6838, loss = 0.18366195\n",
      "Iteration 6839, loss = 0.18364902\n",
      "Iteration 6840, loss = 0.18363667\n",
      "Iteration 6841, loss = 0.18362216\n",
      "Iteration 6842, loss = 0.18360836\n",
      "Iteration 6843, loss = 0.18359933\n",
      "Iteration 6844, loss = 0.18358931\n",
      "Iteration 6845, loss = 0.18357719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6846, loss = 0.18355650\n",
      "Iteration 6847, loss = 0.18354440\n",
      "Iteration 6848, loss = 0.18353517\n",
      "Iteration 6849, loss = 0.18352279\n",
      "Iteration 6850, loss = 0.18350762\n",
      "Iteration 6851, loss = 0.18348956\n",
      "Iteration 6852, loss = 0.18348146\n",
      "Iteration 6853, loss = 0.18346936\n",
      "Iteration 6854, loss = 0.18345239\n",
      "Iteration 6855, loss = 0.18343729\n",
      "Iteration 6856, loss = 0.18342436\n",
      "Iteration 6857, loss = 0.18341242\n",
      "Iteration 6858, loss = 0.18339336\n",
      "Iteration 6859, loss = 0.18338243\n",
      "Iteration 6860, loss = 0.18337176\n",
      "Iteration 6861, loss = 0.18335980\n",
      "Iteration 6862, loss = 0.18334552\n",
      "Iteration 6863, loss = 0.18333436\n",
      "Iteration 6864, loss = 0.18331929\n",
      "Iteration 6865, loss = 0.18330412\n",
      "Iteration 6866, loss = 0.18329259\n",
      "Iteration 6867, loss = 0.18328014\n",
      "Iteration 6868, loss = 0.18326406\n",
      "Iteration 6869, loss = 0.18325066\n",
      "Iteration 6870, loss = 0.18323633\n",
      "Iteration 6871, loss = 0.18322272\n",
      "Iteration 6872, loss = 0.18320916\n",
      "Iteration 6873, loss = 0.18319620\n",
      "Iteration 6874, loss = 0.18318475\n",
      "Iteration 6875, loss = 0.18317423\n",
      "Iteration 6876, loss = 0.18315861\n",
      "Iteration 6877, loss = 0.18313956\n",
      "Iteration 6878, loss = 0.18312824\n",
      "Iteration 6879, loss = 0.18311730\n",
      "Iteration 6880, loss = 0.18310372\n",
      "Iteration 6881, loss = 0.18308912\n",
      "Iteration 6882, loss = 0.18307420\n",
      "Iteration 6883, loss = 0.18306400\n",
      "Iteration 6884, loss = 0.18304963\n",
      "Iteration 6885, loss = 0.18303454\n",
      "Iteration 6886, loss = 0.18302521\n",
      "Iteration 6887, loss = 0.18301124\n",
      "Iteration 6888, loss = 0.18299612\n",
      "Iteration 6889, loss = 0.18298363\n",
      "Iteration 6890, loss = 0.18297025\n",
      "Iteration 6891, loss = 0.18295546\n",
      "Iteration 6892, loss = 0.18294010\n",
      "Iteration 6893, loss = 0.18292973\n",
      "Iteration 6894, loss = 0.18291545\n",
      "Iteration 6895, loss = 0.18290100\n",
      "Iteration 6896, loss = 0.18288942\n",
      "Iteration 6897, loss = 0.18287562\n",
      "Iteration 6898, loss = 0.18286094\n",
      "Iteration 6899, loss = 0.18284929\n",
      "Iteration 6900, loss = 0.18283628\n",
      "Iteration 6901, loss = 0.18282498\n",
      "Iteration 6902, loss = 0.18281135\n",
      "Iteration 6903, loss = 0.18279724\n",
      "Iteration 6904, loss = 0.18278033\n",
      "Iteration 6905, loss = 0.18276931\n",
      "Iteration 6906, loss = 0.18275808\n",
      "Iteration 6907, loss = 0.18274079\n",
      "Iteration 6908, loss = 0.18272771\n",
      "Iteration 6909, loss = 0.18271650\n",
      "Iteration 6910, loss = 0.18270866\n",
      "Iteration 6911, loss = 0.18269435\n",
      "Iteration 6912, loss = 0.18268042\n",
      "Iteration 6913, loss = 0.18266604\n",
      "Iteration 6914, loss = 0.18265122\n",
      "Iteration 6915, loss = 0.18263716\n",
      "Iteration 6916, loss = 0.18262712\n",
      "Iteration 6917, loss = 0.18261557\n",
      "Iteration 6918, loss = 0.18259814\n",
      "Iteration 6919, loss = 0.18258295\n",
      "Iteration 6920, loss = 0.18257441\n",
      "Iteration 6921, loss = 0.18256118\n",
      "Iteration 6922, loss = 0.18254441\n",
      "Iteration 6923, loss = 0.18253120\n",
      "Iteration 6924, loss = 0.18252376\n",
      "Iteration 6925, loss = 0.18250945\n",
      "Iteration 6926, loss = 0.18249546\n",
      "Iteration 6927, loss = 0.18248091\n",
      "Iteration 6928, loss = 0.18246722\n",
      "Iteration 6929, loss = 0.18245262\n",
      "Iteration 6930, loss = 0.18244014\n",
      "Iteration 6931, loss = 0.18242456\n",
      "Iteration 6932, loss = 0.18241016\n",
      "Iteration 6933, loss = 0.18239744\n",
      "Iteration 6934, loss = 0.18238463\n",
      "Iteration 6935, loss = 0.18237670\n",
      "Iteration 6936, loss = 0.18236013\n",
      "Iteration 6937, loss = 0.18234856\n",
      "Iteration 6938, loss = 0.18233789\n",
      "Iteration 6939, loss = 0.18232374\n",
      "Iteration 6940, loss = 0.18230903\n",
      "Iteration 6941, loss = 0.18229304\n",
      "Iteration 6942, loss = 0.18228071\n",
      "Iteration 6943, loss = 0.18227103\n",
      "Iteration 6944, loss = 0.18225938\n",
      "Iteration 6945, loss = 0.18224251\n",
      "Iteration 6946, loss = 0.18222971\n",
      "Iteration 6947, loss = 0.18221547\n",
      "Iteration 6948, loss = 0.18220512\n",
      "Iteration 6949, loss = 0.18219367\n",
      "Iteration 6950, loss = 0.18217718\n",
      "Iteration 6951, loss = 0.18216361\n",
      "Iteration 6952, loss = 0.18215299\n",
      "Iteration 6953, loss = 0.18213872\n",
      "Iteration 6954, loss = 0.18212430\n",
      "Iteration 6955, loss = 0.18210868\n",
      "Iteration 6956, loss = 0.18209832\n",
      "Iteration 6957, loss = 0.18208171\n",
      "Iteration 6958, loss = 0.18206736\n",
      "Iteration 6959, loss = 0.18205298\n",
      "Iteration 6960, loss = 0.18203841\n",
      "Iteration 6961, loss = 0.18202851\n",
      "Iteration 6962, loss = 0.18202037\n",
      "Iteration 6963, loss = 0.18200509\n",
      "Iteration 6964, loss = 0.18198867\n",
      "Iteration 6965, loss = 0.18197455\n",
      "Iteration 6966, loss = 0.18196090\n",
      "Iteration 6967, loss = 0.18194729\n",
      "Iteration 6968, loss = 0.18193800\n",
      "Iteration 6969, loss = 0.18192567\n",
      "Iteration 6970, loss = 0.18190984\n",
      "Iteration 6971, loss = 0.18189513\n",
      "Iteration 6972, loss = 0.18188350\n",
      "Iteration 6973, loss = 0.18187361\n",
      "Iteration 6974, loss = 0.18186105\n",
      "Iteration 6975, loss = 0.18184545\n",
      "Iteration 6976, loss = 0.18183382\n",
      "Iteration 6977, loss = 0.18181907\n",
      "Iteration 6978, loss = 0.18180961\n",
      "Iteration 6979, loss = 0.18179624\n",
      "Iteration 6980, loss = 0.18177766\n",
      "Iteration 6981, loss = 0.18176616\n",
      "Iteration 6982, loss = 0.18175270\n",
      "Iteration 6983, loss = 0.18173680\n",
      "Iteration 6984, loss = 0.18172390\n",
      "Iteration 6985, loss = 0.18171144\n",
      "Iteration 6986, loss = 0.18169697\n",
      "Iteration 6987, loss = 0.18168363\n",
      "Iteration 6988, loss = 0.18167268\n",
      "Iteration 6989, loss = 0.18165813\n",
      "Iteration 6990, loss = 0.18164776\n",
      "Iteration 6991, loss = 0.18163522\n",
      "Iteration 6992, loss = 0.18162301\n",
      "Iteration 6993, loss = 0.18160774\n",
      "Iteration 6994, loss = 0.18159251\n",
      "Iteration 6995, loss = 0.18157963\n",
      "Iteration 6996, loss = 0.18156535\n",
      "Iteration 6997, loss = 0.18155560\n",
      "Iteration 6998, loss = 0.18154254\n",
      "Iteration 6999, loss = 0.18152544\n",
      "Iteration 7000, loss = 0.18151637\n",
      "Iteration 7001, loss = 0.18150535\n",
      "Iteration 7002, loss = 0.18149150\n",
      "Iteration 7003, loss = 0.18147516\n",
      "Iteration 7004, loss = 0.18146229\n",
      "Iteration 7005, loss = 0.18144999\n",
      "Iteration 7006, loss = 0.18143684\n",
      "Iteration 7007, loss = 0.18142613\n",
      "Iteration 7008, loss = 0.18141293\n",
      "Iteration 7009, loss = 0.18139677\n",
      "Iteration 7010, loss = 0.18138442\n",
      "Iteration 7011, loss = 0.18137083\n",
      "Iteration 7012, loss = 0.18135714\n",
      "Iteration 7013, loss = 0.18134346\n",
      "Iteration 7014, loss = 0.18132839\n",
      "Iteration 7015, loss = 0.18131469\n",
      "Iteration 7016, loss = 0.18130362\n",
      "Iteration 7017, loss = 0.18129026\n",
      "Iteration 7018, loss = 0.18127771\n",
      "Iteration 7019, loss = 0.18126411\n",
      "Iteration 7020, loss = 0.18125235\n",
      "Iteration 7021, loss = 0.18123769\n",
      "Iteration 7022, loss = 0.18122837\n",
      "Iteration 7023, loss = 0.18121340\n",
      "Iteration 7024, loss = 0.18119926\n",
      "Iteration 7025, loss = 0.18118594\n",
      "Iteration 7026, loss = 0.18117237\n",
      "Iteration 7027, loss = 0.18115861\n",
      "Iteration 7028, loss = 0.18114758\n",
      "Iteration 7029, loss = 0.18113596\n",
      "Iteration 7030, loss = 0.18112245\n",
      "Iteration 7031, loss = 0.18110511\n",
      "Iteration 7032, loss = 0.18109812\n",
      "Iteration 7033, loss = 0.18108654\n",
      "Iteration 7034, loss = 0.18107502\n",
      "Iteration 7035, loss = 0.18106272\n",
      "Iteration 7036, loss = 0.18104471\n",
      "Iteration 7037, loss = 0.18102663\n",
      "Iteration 7038, loss = 0.18101871\n",
      "Iteration 7039, loss = 0.18100922\n",
      "Iteration 7040, loss = 0.18099869\n",
      "Iteration 7041, loss = 0.18098067\n",
      "Iteration 7042, loss = 0.18096588\n",
      "Iteration 7043, loss = 0.18095584\n",
      "Iteration 7044, loss = 0.18094555\n",
      "Iteration 7045, loss = 0.18093115\n",
      "Iteration 7046, loss = 0.18091824\n",
      "Iteration 7047, loss = 0.18090464\n",
      "Iteration 7048, loss = 0.18089000\n",
      "Iteration 7049, loss = 0.18087797\n",
      "Iteration 7050, loss = 0.18086782\n",
      "Iteration 7051, loss = 0.18085228\n",
      "Iteration 7052, loss = 0.18083489\n",
      "Iteration 7053, loss = 0.18082226\n",
      "Iteration 7054, loss = 0.18081140\n",
      "Iteration 7055, loss = 0.18079877\n",
      "Iteration 7056, loss = 0.18078428\n",
      "Iteration 7057, loss = 0.18077082\n",
      "Iteration 7058, loss = 0.18075727\n",
      "Iteration 7059, loss = 0.18074385\n",
      "Iteration 7060, loss = 0.18073239\n",
      "Iteration 7061, loss = 0.18072137\n",
      "Iteration 7062, loss = 0.18070324\n",
      "Iteration 7063, loss = 0.18068978\n",
      "Iteration 7064, loss = 0.18067953\n",
      "Iteration 7065, loss = 0.18066493\n",
      "Iteration 7066, loss = 0.18065464\n",
      "Iteration 7067, loss = 0.18064162\n",
      "Iteration 7068, loss = 0.18062726\n",
      "Iteration 7069, loss = 0.18061366\n",
      "Iteration 7070, loss = 0.18060106\n",
      "Iteration 7071, loss = 0.18058916\n",
      "Iteration 7072, loss = 0.18057291\n",
      "Iteration 7073, loss = 0.18055778\n",
      "Iteration 7074, loss = 0.18054570\n",
      "Iteration 7075, loss = 0.18053214\n",
      "Iteration 7076, loss = 0.18052164\n",
      "Iteration 7077, loss = 0.18050996\n",
      "Iteration 7078, loss = 0.18049338\n",
      "Iteration 7079, loss = 0.18048160\n",
      "Iteration 7080, loss = 0.18046747\n",
      "Iteration 7081, loss = 0.18045772\n",
      "Iteration 7082, loss = 0.18044318\n",
      "Iteration 7083, loss = 0.18043030\n",
      "Iteration 7084, loss = 0.18041934\n",
      "Iteration 7085, loss = 0.18040528\n",
      "Iteration 7086, loss = 0.18039146\n",
      "Iteration 7087, loss = 0.18037837\n",
      "Iteration 7088, loss = 0.18036765\n",
      "Iteration 7089, loss = 0.18035324\n",
      "Iteration 7090, loss = 0.18034196\n",
      "Iteration 7091, loss = 0.18032635\n",
      "Iteration 7092, loss = 0.18031640\n",
      "Iteration 7093, loss = 0.18030517\n",
      "Iteration 7094, loss = 0.18029053\n",
      "Iteration 7095, loss = 0.18027965\n",
      "Iteration 7096, loss = 0.18026710\n",
      "Iteration 7097, loss = 0.18025123\n",
      "Iteration 7098, loss = 0.18024132\n",
      "Iteration 7099, loss = 0.18023105\n",
      "Iteration 7100, loss = 0.18021764\n",
      "Iteration 7101, loss = 0.18019772\n",
      "Iteration 7102, loss = 0.18018222\n",
      "Iteration 7103, loss = 0.18017764\n",
      "Iteration 7104, loss = 0.18016708\n",
      "Iteration 7105, loss = 0.18014895\n",
      "Iteration 7106, loss = 0.18013800\n",
      "Iteration 7107, loss = 0.18012623\n",
      "Iteration 7108, loss = 0.18011051\n",
      "Iteration 7109, loss = 0.18009933\n",
      "Iteration 7110, loss = 0.18009258\n",
      "Iteration 7111, loss = 0.18007820\n",
      "Iteration 7112, loss = 0.18006017\n",
      "Iteration 7113, loss = 0.18004523\n",
      "Iteration 7114, loss = 0.18003460\n",
      "Iteration 7115, loss = 0.18002322\n",
      "Iteration 7116, loss = 0.18000937\n",
      "Iteration 7117, loss = 0.18000137\n",
      "Iteration 7118, loss = 0.17998545\n",
      "Iteration 7119, loss = 0.17996691\n",
      "Iteration 7120, loss = 0.17996040\n",
      "Iteration 7121, loss = 0.17994932\n",
      "Iteration 7122, loss = 0.17993265\n",
      "Iteration 7123, loss = 0.17992280\n",
      "Iteration 7124, loss = 0.17990762\n",
      "Iteration 7125, loss = 0.17989556\n",
      "Iteration 7126, loss = 0.17988072\n",
      "Iteration 7127, loss = 0.17986625\n",
      "Iteration 7128, loss = 0.17985299\n",
      "Iteration 7129, loss = 0.17984046\n",
      "Iteration 7130, loss = 0.17982639\n",
      "Iteration 7131, loss = 0.17981684\n",
      "Iteration 7132, loss = 0.17980630\n",
      "Iteration 7133, loss = 0.17979148\n",
      "Iteration 7134, loss = 0.17978201\n",
      "Iteration 7135, loss = 0.17976688\n",
      "Iteration 7136, loss = 0.17975243\n",
      "Iteration 7137, loss = 0.17973946\n",
      "Iteration 7138, loss = 0.17972924\n",
      "Iteration 7139, loss = 0.17971242\n",
      "Iteration 7140, loss = 0.17969890\n",
      "Iteration 7141, loss = 0.17968295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7142, loss = 0.17967549\n",
      "Iteration 7143, loss = 0.17966876\n",
      "Iteration 7144, loss = 0.17964936\n",
      "Iteration 7145, loss = 0.17963726\n",
      "Iteration 7146, loss = 0.17962431\n",
      "Iteration 7147, loss = 0.17961217\n",
      "Iteration 7148, loss = 0.17960040\n",
      "Iteration 7149, loss = 0.17959201\n",
      "Iteration 7150, loss = 0.17957812\n",
      "Iteration 7151, loss = 0.17956181\n",
      "Iteration 7152, loss = 0.17954604\n",
      "Iteration 7153, loss = 0.17953562\n",
      "Iteration 7154, loss = 0.17952591\n",
      "Iteration 7155, loss = 0.17951193\n",
      "Iteration 7156, loss = 0.17949969\n",
      "Iteration 7157, loss = 0.17948593\n",
      "Iteration 7158, loss = 0.17947489\n",
      "Iteration 7159, loss = 0.17946470\n",
      "Iteration 7160, loss = 0.17945270\n",
      "Iteration 7161, loss = 0.17943611\n",
      "Iteration 7162, loss = 0.17942514\n",
      "Iteration 7163, loss = 0.17941137\n",
      "Iteration 7164, loss = 0.17939483\n",
      "Iteration 7165, loss = 0.17938306\n",
      "Iteration 7166, loss = 0.17937156\n",
      "Iteration 7167, loss = 0.17935693\n",
      "Iteration 7168, loss = 0.17934060\n",
      "Iteration 7169, loss = 0.17933020\n",
      "Iteration 7170, loss = 0.17931764\n",
      "Iteration 7171, loss = 0.17930465\n",
      "Iteration 7172, loss = 0.17929182\n",
      "Iteration 7173, loss = 0.17927961\n",
      "Iteration 7174, loss = 0.17926688\n",
      "Iteration 7175, loss = 0.17925191\n",
      "Iteration 7176, loss = 0.17924131\n",
      "Iteration 7177, loss = 0.17923074\n",
      "Iteration 7178, loss = 0.17921494\n",
      "Iteration 7179, loss = 0.17920443\n",
      "Iteration 7180, loss = 0.17919336\n",
      "Iteration 7181, loss = 0.17918037\n",
      "Iteration 7182, loss = 0.17916520\n",
      "Iteration 7183, loss = 0.17915164\n",
      "Iteration 7184, loss = 0.17914080\n",
      "Iteration 7185, loss = 0.17912771\n",
      "Iteration 7186, loss = 0.17911043\n",
      "Iteration 7187, loss = 0.17909608\n",
      "Iteration 7188, loss = 0.17908668\n",
      "Iteration 7189, loss = 0.17907732\n",
      "Iteration 7190, loss = 0.17906395\n",
      "Iteration 7191, loss = 0.17905029\n",
      "Iteration 7192, loss = 0.17903881\n",
      "Iteration 7193, loss = 0.17902434\n",
      "Iteration 7194, loss = 0.17901400\n",
      "Iteration 7195, loss = 0.17899965\n",
      "Iteration 7196, loss = 0.17898721\n",
      "Iteration 7197, loss = 0.17897827\n",
      "Iteration 7198, loss = 0.17896522\n",
      "Iteration 7199, loss = 0.17895157\n",
      "Iteration 7200, loss = 0.17893482\n",
      "Iteration 7201, loss = 0.17892347\n",
      "Iteration 7202, loss = 0.17891373\n",
      "Iteration 7203, loss = 0.17890027\n",
      "Iteration 7204, loss = 0.17888754\n",
      "Iteration 7205, loss = 0.17887785\n",
      "Iteration 7206, loss = 0.17886432\n",
      "Iteration 7207, loss = 0.17885293\n",
      "Iteration 7208, loss = 0.17884170\n",
      "Iteration 7209, loss = 0.17882867\n",
      "Iteration 7210, loss = 0.17881424\n",
      "Iteration 7211, loss = 0.17880143\n",
      "Iteration 7212, loss = 0.17878703\n",
      "Iteration 7213, loss = 0.17877280\n",
      "Iteration 7214, loss = 0.17875744\n",
      "Iteration 7215, loss = 0.17874747\n",
      "Iteration 7216, loss = 0.17873013\n",
      "Iteration 7217, loss = 0.17871770\n",
      "Iteration 7218, loss = 0.17871157\n",
      "Iteration 7219, loss = 0.17870144\n",
      "Iteration 7220, loss = 0.17868907\n",
      "Iteration 7221, loss = 0.17867552\n",
      "Iteration 7222, loss = 0.17866162\n",
      "Iteration 7223, loss = 0.17865094\n",
      "Iteration 7224, loss = 0.17863825\n",
      "Iteration 7225, loss = 0.17862668\n",
      "Iteration 7226, loss = 0.17861281\n",
      "Iteration 7227, loss = 0.17859493\n",
      "Iteration 7228, loss = 0.17858460\n",
      "Iteration 7229, loss = 0.17857307\n",
      "Iteration 7230, loss = 0.17856359\n",
      "Iteration 7231, loss = 0.17854610\n",
      "Iteration 7232, loss = 0.17852914\n",
      "Iteration 7233, loss = 0.17852227\n",
      "Iteration 7234, loss = 0.17851191\n",
      "Iteration 7235, loss = 0.17850287\n",
      "Iteration 7236, loss = 0.17848959\n",
      "Iteration 7237, loss = 0.17847613\n",
      "Iteration 7238, loss = 0.17846084\n",
      "Iteration 7239, loss = 0.17844785\n",
      "Iteration 7240, loss = 0.17843532\n",
      "Iteration 7241, loss = 0.17841938\n",
      "Iteration 7242, loss = 0.17840721\n",
      "Iteration 7243, loss = 0.17839639\n",
      "Iteration 7244, loss = 0.17838278\n",
      "Iteration 7245, loss = 0.17836999\n",
      "Iteration 7246, loss = 0.17836058\n",
      "Iteration 7247, loss = 0.17834793\n",
      "Iteration 7248, loss = 0.17833598\n",
      "Iteration 7249, loss = 0.17832354\n",
      "Iteration 7250, loss = 0.17830951\n",
      "Iteration 7251, loss = 0.17829423\n",
      "Iteration 7252, loss = 0.17828361\n",
      "Iteration 7253, loss = 0.17827050\n",
      "Iteration 7254, loss = 0.17825650\n",
      "Iteration 7255, loss = 0.17824068\n",
      "Iteration 7256, loss = 0.17823295\n",
      "Iteration 7257, loss = 0.17821804\n",
      "Iteration 7258, loss = 0.17820835\n",
      "Iteration 7259, loss = 0.17819363\n",
      "Iteration 7260, loss = 0.17818431\n",
      "Iteration 7261, loss = 0.17817367\n",
      "Iteration 7262, loss = 0.17815735\n",
      "Iteration 7263, loss = 0.17814692\n",
      "Iteration 7264, loss = 0.17813259\n",
      "Iteration 7265, loss = 0.17811923\n",
      "Iteration 7266, loss = 0.17810836\n",
      "Iteration 7267, loss = 0.17809868\n",
      "Iteration 7268, loss = 0.17808611\n",
      "Iteration 7269, loss = 0.17807461\n",
      "Iteration 7270, loss = 0.17806303\n",
      "Iteration 7271, loss = 0.17804948\n",
      "Iteration 7272, loss = 0.17803408\n",
      "Iteration 7273, loss = 0.17802385\n",
      "Iteration 7274, loss = 0.17801202\n",
      "Iteration 7275, loss = 0.17800141\n",
      "Iteration 7276, loss = 0.17798894\n",
      "Iteration 7277, loss = 0.17797303\n",
      "Iteration 7278, loss = 0.17796718\n",
      "Iteration 7279, loss = 0.17795579\n",
      "Iteration 7280, loss = 0.17794081\n",
      "Iteration 7281, loss = 0.17792346\n",
      "Iteration 7282, loss = 0.17791031\n",
      "Iteration 7283, loss = 0.17790015\n",
      "Iteration 7284, loss = 0.17788586\n",
      "Iteration 7285, loss = 0.17787599\n",
      "Iteration 7286, loss = 0.17786265\n",
      "Iteration 7287, loss = 0.17784844\n",
      "Iteration 7288, loss = 0.17783693\n",
      "Iteration 7289, loss = 0.17782587\n",
      "Iteration 7290, loss = 0.17781347\n",
      "Iteration 7291, loss = 0.17779723\n",
      "Iteration 7292, loss = 0.17778467\n",
      "Iteration 7293, loss = 0.17777614\n",
      "Iteration 7294, loss = 0.17776155\n",
      "Iteration 7295, loss = 0.17774995\n",
      "Iteration 7296, loss = 0.17773685\n",
      "Iteration 7297, loss = 0.17772538\n",
      "Iteration 7298, loss = 0.17771165\n",
      "Iteration 7299, loss = 0.17769787\n",
      "Iteration 7300, loss = 0.17769026\n",
      "Iteration 7301, loss = 0.17767576\n",
      "Iteration 7302, loss = 0.17766149\n",
      "Iteration 7303, loss = 0.17765129\n",
      "Iteration 7304, loss = 0.17763791\n",
      "Iteration 7305, loss = 0.17762727\n",
      "Iteration 7306, loss = 0.17761525\n",
      "Iteration 7307, loss = 0.17760281\n",
      "Iteration 7308, loss = 0.17759137\n",
      "Iteration 7309, loss = 0.17758321\n",
      "Iteration 7310, loss = 0.17756621\n",
      "Iteration 7311, loss = 0.17755142\n",
      "Iteration 7312, loss = 0.17754382\n",
      "Iteration 7313, loss = 0.17753259\n",
      "Iteration 7314, loss = 0.17752022\n",
      "Iteration 7315, loss = 0.17750586\n",
      "Iteration 7316, loss = 0.17749443\n",
      "Iteration 7317, loss = 0.17747991\n",
      "Iteration 7318, loss = 0.17746690\n",
      "Iteration 7319, loss = 0.17745725\n",
      "Iteration 7320, loss = 0.17744403\n",
      "Iteration 7321, loss = 0.17743093\n",
      "Iteration 7322, loss = 0.17741924\n",
      "Iteration 7323, loss = 0.17740606\n",
      "Iteration 7324, loss = 0.17739653\n",
      "Iteration 7325, loss = 0.17738370\n",
      "Iteration 7326, loss = 0.17737238\n",
      "Iteration 7327, loss = 0.17735877\n",
      "Iteration 7328, loss = 0.17734545\n",
      "Iteration 7329, loss = 0.17733646\n",
      "Iteration 7330, loss = 0.17732494\n",
      "Iteration 7331, loss = 0.17731283\n",
      "Iteration 7332, loss = 0.17730055\n",
      "Iteration 7333, loss = 0.17728739\n",
      "Iteration 7334, loss = 0.17727407\n",
      "Iteration 7335, loss = 0.17725759\n",
      "Iteration 7336, loss = 0.17724872\n",
      "Iteration 7337, loss = 0.17723489\n",
      "Iteration 7338, loss = 0.17722279\n",
      "Iteration 7339, loss = 0.17721238\n",
      "Iteration 7340, loss = 0.17719867\n",
      "Iteration 7341, loss = 0.17718469\n",
      "Iteration 7342, loss = 0.17717149\n",
      "Iteration 7343, loss = 0.17715984\n",
      "Iteration 7344, loss = 0.17714943\n",
      "Iteration 7345, loss = 0.17713559\n",
      "Iteration 7346, loss = 0.17712495\n",
      "Iteration 7347, loss = 0.17710920\n",
      "Iteration 7348, loss = 0.17709794\n",
      "Iteration 7349, loss = 0.17708659\n",
      "Iteration 7350, loss = 0.17707650\n",
      "Iteration 7351, loss = 0.17706212\n",
      "Iteration 7352, loss = 0.17704636\n",
      "Iteration 7353, loss = 0.17703483\n",
      "Iteration 7354, loss = 0.17702670\n",
      "Iteration 7355, loss = 0.17700932\n",
      "Iteration 7356, loss = 0.17699947\n",
      "Iteration 7357, loss = 0.17699062\n",
      "Iteration 7358, loss = 0.17697592\n",
      "Iteration 7359, loss = 0.17696149\n",
      "Iteration 7360, loss = 0.17694893\n",
      "Iteration 7361, loss = 0.17693965\n",
      "Iteration 7362, loss = 0.17692908\n",
      "Iteration 7363, loss = 0.17691524\n",
      "Iteration 7364, loss = 0.17690306\n",
      "Iteration 7365, loss = 0.17689018\n",
      "Iteration 7366, loss = 0.17687705\n",
      "Iteration 7367, loss = 0.17686587\n",
      "Iteration 7368, loss = 0.17685522\n",
      "Iteration 7369, loss = 0.17683997\n",
      "Iteration 7370, loss = 0.17682478\n",
      "Iteration 7371, loss = 0.17681776\n",
      "Iteration 7372, loss = 0.17680601\n",
      "Iteration 7373, loss = 0.17679312\n",
      "Iteration 7374, loss = 0.17677667\n",
      "Iteration 7375, loss = 0.17676373\n",
      "Iteration 7376, loss = 0.17675468\n",
      "Iteration 7377, loss = 0.17674911\n",
      "Iteration 7378, loss = 0.17673483\n",
      "Iteration 7379, loss = 0.17671688\n",
      "Iteration 7380, loss = 0.17670138\n",
      "Iteration 7381, loss = 0.17669804\n",
      "Iteration 7382, loss = 0.17669051\n",
      "Iteration 7383, loss = 0.17668178\n",
      "Iteration 7384, loss = 0.17665828\n",
      "Iteration 7385, loss = 0.17664836\n",
      "Iteration 7386, loss = 0.17664042\n",
      "Iteration 7387, loss = 0.17662966\n",
      "Iteration 7388, loss = 0.17661400\n",
      "Iteration 7389, loss = 0.17659702\n",
      "Iteration 7390, loss = 0.17658306\n",
      "Iteration 7391, loss = 0.17657455\n",
      "Iteration 7392, loss = 0.17656491\n",
      "Iteration 7393, loss = 0.17655336\n",
      "Iteration 7394, loss = 0.17653840\n",
      "Iteration 7395, loss = 0.17652241\n",
      "Iteration 7396, loss = 0.17651365\n",
      "Iteration 7397, loss = 0.17650177\n",
      "Iteration 7398, loss = 0.17649127\n",
      "Iteration 7399, loss = 0.17647821\n",
      "Iteration 7400, loss = 0.17646377\n",
      "Iteration 7401, loss = 0.17644959\n",
      "Iteration 7402, loss = 0.17643125\n",
      "Iteration 7403, loss = 0.17642804\n",
      "Iteration 7404, loss = 0.17642001\n",
      "Iteration 7405, loss = 0.17640679\n",
      "Iteration 7406, loss = 0.17639249\n",
      "Iteration 7407, loss = 0.17637596\n",
      "Iteration 7408, loss = 0.17636034\n",
      "Iteration 7409, loss = 0.17635417\n",
      "Iteration 7410, loss = 0.17634202\n",
      "Iteration 7411, loss = 0.17632735\n",
      "Iteration 7412, loss = 0.17631778\n",
      "Iteration 7413, loss = 0.17630418\n",
      "Iteration 7414, loss = 0.17629225\n",
      "Iteration 7415, loss = 0.17627993\n",
      "Iteration 7416, loss = 0.17626828\n",
      "Iteration 7417, loss = 0.17625725\n",
      "Iteration 7418, loss = 0.17624535\n",
      "Iteration 7419, loss = 0.17623361\n",
      "Iteration 7420, loss = 0.17621738\n",
      "Iteration 7421, loss = 0.17620816\n",
      "Iteration 7422, loss = 0.17619490\n",
      "Iteration 7423, loss = 0.17618281\n",
      "Iteration 7424, loss = 0.17617381\n",
      "Iteration 7425, loss = 0.17615970\n",
      "Iteration 7426, loss = 0.17614738\n",
      "Iteration 7427, loss = 0.17613468\n",
      "Iteration 7428, loss = 0.17612469\n",
      "Iteration 7429, loss = 0.17611378\n",
      "Iteration 7430, loss = 0.17610013\n",
      "Iteration 7431, loss = 0.17608517\n",
      "Iteration 7432, loss = 0.17607418\n",
      "Iteration 7433, loss = 0.17606292\n",
      "Iteration 7434, loss = 0.17604914\n",
      "Iteration 7435, loss = 0.17603925\n",
      "Iteration 7436, loss = 0.17602681\n",
      "Iteration 7437, loss = 0.17601455\n",
      "Iteration 7438, loss = 0.17600037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7439, loss = 0.17598845\n",
      "Iteration 7440, loss = 0.17597625\n",
      "Iteration 7441, loss = 0.17596227\n",
      "Iteration 7442, loss = 0.17594885\n",
      "Iteration 7443, loss = 0.17593667\n",
      "Iteration 7444, loss = 0.17592603\n",
      "Iteration 7445, loss = 0.17591265\n",
      "Iteration 7446, loss = 0.17590051\n",
      "Iteration 7447, loss = 0.17589245\n",
      "Iteration 7448, loss = 0.17588039\n",
      "Iteration 7449, loss = 0.17586580\n",
      "Iteration 7450, loss = 0.17585258\n",
      "Iteration 7451, loss = 0.17584488\n",
      "Iteration 7452, loss = 0.17583284\n",
      "Iteration 7453, loss = 0.17581964\n",
      "Iteration 7454, loss = 0.17580684\n",
      "Iteration 7455, loss = 0.17579274\n",
      "Iteration 7456, loss = 0.17578462\n",
      "Iteration 7457, loss = 0.17577621\n",
      "Iteration 7458, loss = 0.17576558\n",
      "Iteration 7459, loss = 0.17574837\n",
      "Iteration 7460, loss = 0.17573223\n",
      "Iteration 7461, loss = 0.17572496\n",
      "Iteration 7462, loss = 0.17571470\n",
      "Iteration 7463, loss = 0.17570086\n",
      "Iteration 7464, loss = 0.17569098\n",
      "Iteration 7465, loss = 0.17567511\n",
      "Iteration 7466, loss = 0.17566330\n",
      "Iteration 7467, loss = 0.17565169\n",
      "Iteration 7468, loss = 0.17563872\n",
      "Iteration 7469, loss = 0.17562446\n",
      "Iteration 7470, loss = 0.17561262\n",
      "Iteration 7471, loss = 0.17560206\n",
      "Iteration 7472, loss = 0.17558671\n",
      "Iteration 7473, loss = 0.17557180\n",
      "Iteration 7474, loss = 0.17556149\n",
      "Iteration 7475, loss = 0.17554699\n",
      "Iteration 7476, loss = 0.17553315\n",
      "Iteration 7477, loss = 0.17552813\n",
      "Iteration 7478, loss = 0.17551165\n",
      "Iteration 7479, loss = 0.17549742\n",
      "Iteration 7480, loss = 0.17548728\n",
      "Iteration 7481, loss = 0.17547436\n",
      "Iteration 7482, loss = 0.17546236\n",
      "Iteration 7483, loss = 0.17544992\n",
      "Iteration 7484, loss = 0.17544027\n",
      "Iteration 7485, loss = 0.17542464\n",
      "Iteration 7486, loss = 0.17541324\n",
      "Iteration 7487, loss = 0.17540006\n",
      "Iteration 7488, loss = 0.17538946\n",
      "Iteration 7489, loss = 0.17537675\n",
      "Iteration 7490, loss = 0.17535983\n",
      "Iteration 7491, loss = 0.17534703\n",
      "Iteration 7492, loss = 0.17533819\n",
      "Iteration 7493, loss = 0.17532897\n",
      "Iteration 7494, loss = 0.17531970\n",
      "Iteration 7495, loss = 0.17530184\n",
      "Iteration 7496, loss = 0.17528768\n",
      "Iteration 7497, loss = 0.17527740\n",
      "Iteration 7498, loss = 0.17526608\n",
      "Iteration 7499, loss = 0.17525154\n",
      "Iteration 7500, loss = 0.17523884\n",
      "Iteration 7501, loss = 0.17522771\n",
      "Iteration 7502, loss = 0.17521721\n",
      "Iteration 7503, loss = 0.17520401\n",
      "Iteration 7504, loss = 0.17519073\n",
      "Iteration 7505, loss = 0.17518084\n",
      "Iteration 7506, loss = 0.17516660\n",
      "Iteration 7507, loss = 0.17515037\n",
      "Iteration 7508, loss = 0.17514213\n",
      "Iteration 7509, loss = 0.17513026\n",
      "Iteration 7510, loss = 0.17511847\n",
      "Iteration 7511, loss = 0.17510423\n",
      "Iteration 7512, loss = 0.17509028\n",
      "Iteration 7513, loss = 0.17508553\n",
      "Iteration 7514, loss = 0.17507149\n",
      "Iteration 7515, loss = 0.17505771\n",
      "Iteration 7516, loss = 0.17504361\n",
      "Iteration 7517, loss = 0.17502888\n",
      "Iteration 7518, loss = 0.17501618\n",
      "Iteration 7519, loss = 0.17500575\n",
      "Iteration 7520, loss = 0.17499529\n",
      "Iteration 7521, loss = 0.17498080\n",
      "Iteration 7522, loss = 0.17496762\n",
      "Iteration 7523, loss = 0.17495796\n",
      "Iteration 7524, loss = 0.17494657\n",
      "Iteration 7525, loss = 0.17493240\n",
      "Iteration 7526, loss = 0.17491995\n",
      "Iteration 7527, loss = 0.17490621\n",
      "Iteration 7528, loss = 0.17489296\n",
      "Iteration 7529, loss = 0.17488110\n",
      "Iteration 7530, loss = 0.17486711\n",
      "Iteration 7531, loss = 0.17485475\n",
      "Iteration 7532, loss = 0.17484184\n",
      "Iteration 7533, loss = 0.17483106\n",
      "Iteration 7534, loss = 0.17482046\n",
      "Iteration 7535, loss = 0.17480795\n",
      "Iteration 7536, loss = 0.17479740\n",
      "Iteration 7537, loss = 0.17478110\n",
      "Iteration 7538, loss = 0.17477047\n",
      "Iteration 7539, loss = 0.17475705\n",
      "Iteration 7540, loss = 0.17474545\n",
      "Iteration 7541, loss = 0.17473656\n",
      "Iteration 7542, loss = 0.17471883\n",
      "Iteration 7543, loss = 0.17470642\n",
      "Iteration 7544, loss = 0.17469647\n",
      "Iteration 7545, loss = 0.17468656\n",
      "Iteration 7546, loss = 0.17467490\n",
      "Iteration 7547, loss = 0.17466526\n",
      "Iteration 7548, loss = 0.17465044\n",
      "Iteration 7549, loss = 0.17463926\n",
      "Iteration 7550, loss = 0.17462540\n",
      "Iteration 7551, loss = 0.17461166\n",
      "Iteration 7552, loss = 0.17459877\n",
      "Iteration 7553, loss = 0.17458961\n",
      "Iteration 7554, loss = 0.17457565\n",
      "Iteration 7555, loss = 0.17456377\n",
      "Iteration 7556, loss = 0.17455656\n",
      "Iteration 7557, loss = 0.17454041\n",
      "Iteration 7558, loss = 0.17452705\n",
      "Iteration 7559, loss = 0.17451762\n",
      "Iteration 7560, loss = 0.17450454\n",
      "Iteration 7561, loss = 0.17449395\n",
      "Iteration 7562, loss = 0.17447894\n",
      "Iteration 7563, loss = 0.17446620\n",
      "Iteration 7564, loss = 0.17445262\n",
      "Iteration 7565, loss = 0.17443730\n",
      "Iteration 7566, loss = 0.17442669\n",
      "Iteration 7567, loss = 0.17441808\n",
      "Iteration 7568, loss = 0.17440668\n",
      "Iteration 7569, loss = 0.17439265\n",
      "Iteration 7570, loss = 0.17438409\n",
      "Iteration 7571, loss = 0.17437151\n",
      "Iteration 7572, loss = 0.17435766\n",
      "Iteration 7573, loss = 0.17434453\n",
      "Iteration 7574, loss = 0.17433212\n",
      "Iteration 7575, loss = 0.17431891\n",
      "Iteration 7576, loss = 0.17430414\n",
      "Iteration 7577, loss = 0.17429329\n",
      "Iteration 7578, loss = 0.17427777\n",
      "Iteration 7579, loss = 0.17426593\n",
      "Iteration 7580, loss = 0.17425472\n",
      "Iteration 7581, loss = 0.17423873\n",
      "Iteration 7582, loss = 0.17422625\n",
      "Iteration 7583, loss = 0.17421520\n",
      "Iteration 7584, loss = 0.17420255\n",
      "Iteration 7585, loss = 0.17418911\n",
      "Iteration 7586, loss = 0.17417434\n",
      "Iteration 7587, loss = 0.17416226\n",
      "Iteration 7588, loss = 0.17414732\n",
      "Iteration 7589, loss = 0.17413347\n",
      "Iteration 7590, loss = 0.17412520\n",
      "Iteration 7591, loss = 0.17411147\n",
      "Iteration 7592, loss = 0.17409441\n",
      "Iteration 7593, loss = 0.17408069\n",
      "Iteration 7594, loss = 0.17407012\n",
      "Iteration 7595, loss = 0.17405626\n",
      "Iteration 7596, loss = 0.17404225\n",
      "Iteration 7597, loss = 0.17402937\n",
      "Iteration 7598, loss = 0.17401725\n",
      "Iteration 7599, loss = 0.17400340\n",
      "Iteration 7600, loss = 0.17398961\n",
      "Iteration 7601, loss = 0.17397302\n",
      "Iteration 7602, loss = 0.17396010\n",
      "Iteration 7603, loss = 0.17394614\n",
      "Iteration 7604, loss = 0.17393253\n",
      "Iteration 7605, loss = 0.17392016\n",
      "Iteration 7606, loss = 0.17390549\n",
      "Iteration 7607, loss = 0.17389148\n",
      "Iteration 7608, loss = 0.17387842\n",
      "Iteration 7609, loss = 0.17386525\n",
      "Iteration 7610, loss = 0.17385370\n",
      "Iteration 7611, loss = 0.17383584\n",
      "Iteration 7612, loss = 0.17382658\n",
      "Iteration 7613, loss = 0.17381064\n",
      "Iteration 7614, loss = 0.17379657\n",
      "Iteration 7615, loss = 0.17377750\n",
      "Iteration 7616, loss = 0.17376162\n",
      "Iteration 7617, loss = 0.17374563\n",
      "Iteration 7618, loss = 0.17373352\n",
      "Iteration 7619, loss = 0.17372124\n",
      "Iteration 7620, loss = 0.17370581\n",
      "Iteration 7621, loss = 0.17368753\n",
      "Iteration 7622, loss = 0.17367168\n",
      "Iteration 7623, loss = 0.17366400\n",
      "Iteration 7624, loss = 0.17364968\n",
      "Iteration 7625, loss = 0.17363283\n",
      "Iteration 7626, loss = 0.17361970\n",
      "Iteration 7627, loss = 0.17360593\n",
      "Iteration 7628, loss = 0.17359087\n",
      "Iteration 7629, loss = 0.17357492\n",
      "Iteration 7630, loss = 0.17355865\n",
      "Iteration 7631, loss = 0.17354453\n",
      "Iteration 7632, loss = 0.17352851\n",
      "Iteration 7633, loss = 0.17351431\n",
      "Iteration 7634, loss = 0.17350059\n",
      "Iteration 7635, loss = 0.17348329\n",
      "Iteration 7636, loss = 0.17347233\n",
      "Iteration 7637, loss = 0.17345153\n",
      "Iteration 7638, loss = 0.17343762\n",
      "Iteration 7639, loss = 0.17342619\n",
      "Iteration 7640, loss = 0.17341000\n",
      "Iteration 7641, loss = 0.17339555\n",
      "Iteration 7642, loss = 0.17337786\n",
      "Iteration 7643, loss = 0.17336002\n",
      "Iteration 7644, loss = 0.17334271\n",
      "Iteration 7645, loss = 0.17332930\n",
      "Iteration 7646, loss = 0.17331338\n",
      "Iteration 7647, loss = 0.17329669\n",
      "Iteration 7648, loss = 0.17327770\n",
      "Iteration 7649, loss = 0.17326292\n",
      "Iteration 7650, loss = 0.17324476\n",
      "Iteration 7651, loss = 0.17323444\n",
      "Iteration 7652, loss = 0.17321873\n",
      "Iteration 7653, loss = 0.17319658\n",
      "Iteration 7654, loss = 0.17318258\n",
      "Iteration 7655, loss = 0.17316638\n",
      "Iteration 7656, loss = 0.17314982\n",
      "Iteration 7657, loss = 0.17313446\n",
      "Iteration 7658, loss = 0.17311814\n",
      "Iteration 7659, loss = 0.17310097\n",
      "Iteration 7660, loss = 0.17308407\n",
      "Iteration 7661, loss = 0.17306724\n",
      "Iteration 7662, loss = 0.17305139\n",
      "Iteration 7663, loss = 0.17303405\n",
      "Iteration 7664, loss = 0.17302020\n",
      "Iteration 7665, loss = 0.17300541\n",
      "Iteration 7666, loss = 0.17299487\n",
      "Iteration 7667, loss = 0.17299126\n",
      "Iteration 7668, loss = 0.17296620\n",
      "Iteration 7669, loss = 0.17294607\n",
      "Iteration 7670, loss = 0.17293304\n",
      "Iteration 7671, loss = 0.17292270\n",
      "Iteration 7672, loss = 0.17290750\n",
      "Iteration 7673, loss = 0.17289112\n",
      "Iteration 7674, loss = 0.17288131\n",
      "Iteration 7675, loss = 0.17286967\n",
      "Iteration 7676, loss = 0.17285483\n",
      "Iteration 7677, loss = 0.17283993\n",
      "Iteration 7678, loss = 0.17282940\n",
      "Iteration 7679, loss = 0.17281339\n",
      "Iteration 7680, loss = 0.17279730\n",
      "Iteration 7681, loss = 0.17278151\n",
      "Iteration 7682, loss = 0.17277113\n",
      "Iteration 7683, loss = 0.17275614\n",
      "Iteration 7684, loss = 0.17274275\n",
      "Iteration 7685, loss = 0.17272755\n",
      "Iteration 7686, loss = 0.17270891\n",
      "Iteration 7687, loss = 0.17269458\n",
      "Iteration 7688, loss = 0.17269156\n",
      "Iteration 7689, loss = 0.17266884\n",
      "Iteration 7690, loss = 0.17265145\n",
      "Iteration 7691, loss = 0.17264450\n",
      "Iteration 7692, loss = 0.17263590\n",
      "Iteration 7693, loss = 0.17262232\n",
      "Iteration 7694, loss = 0.17260705\n",
      "Iteration 7695, loss = 0.17259230\n",
      "Iteration 7696, loss = 0.17258041\n",
      "Iteration 7697, loss = 0.17256852\n",
      "Iteration 7698, loss = 0.17255359\n",
      "Iteration 7699, loss = 0.17254090\n",
      "Iteration 7700, loss = 0.17252787\n",
      "Iteration 7701, loss = 0.17251586\n",
      "Iteration 7702, loss = 0.17250153\n",
      "Iteration 7703, loss = 0.17248583\n",
      "Iteration 7704, loss = 0.17247016\n",
      "Iteration 7705, loss = 0.17246216\n",
      "Iteration 7706, loss = 0.17244496\n",
      "Iteration 7707, loss = 0.17243255\n",
      "Iteration 7708, loss = 0.17242546\n",
      "Iteration 7709, loss = 0.17241358\n",
      "Iteration 7710, loss = 0.17239985\n",
      "Iteration 7711, loss = 0.17238776\n",
      "Iteration 7712, loss = 0.17238023\n",
      "Iteration 7713, loss = 0.17236519\n",
      "Iteration 7714, loss = 0.17234684\n",
      "Iteration 7715, loss = 0.17233344\n",
      "Iteration 7716, loss = 0.17232698\n",
      "Iteration 7717, loss = 0.17231107\n",
      "Iteration 7718, loss = 0.17229504\n",
      "Iteration 7719, loss = 0.17227662\n",
      "Iteration 7720, loss = 0.17226178\n",
      "Iteration 7721, loss = 0.17225246\n",
      "Iteration 7722, loss = 0.17223843\n",
      "Iteration 7723, loss = 0.17222863\n",
      "Iteration 7724, loss = 0.17221457\n",
      "Iteration 7725, loss = 0.17220119\n",
      "Iteration 7726, loss = 0.17219382\n",
      "Iteration 7727, loss = 0.17218181\n",
      "Iteration 7728, loss = 0.17216786\n",
      "Iteration 7729, loss = 0.17215770\n",
      "Iteration 7730, loss = 0.17214043\n",
      "Iteration 7731, loss = 0.17212879\n",
      "Iteration 7732, loss = 0.17211649\n",
      "Iteration 7733, loss = 0.17210877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7734, loss = 0.17209162\n",
      "Iteration 7735, loss = 0.17207695\n",
      "Iteration 7736, loss = 0.17206935\n",
      "Iteration 7737, loss = 0.17205717\n",
      "Iteration 7738, loss = 0.17204430\n",
      "Iteration 7739, loss = 0.17202736\n",
      "Iteration 7740, loss = 0.17201723\n",
      "Iteration 7741, loss = 0.17200589\n",
      "Iteration 7742, loss = 0.17199366\n",
      "Iteration 7743, loss = 0.17197931\n",
      "Iteration 7744, loss = 0.17196571\n",
      "Iteration 7745, loss = 0.17195182\n",
      "Iteration 7746, loss = 0.17194558\n",
      "Iteration 7747, loss = 0.17193019\n",
      "Iteration 7748, loss = 0.17191852\n",
      "Iteration 7749, loss = 0.17190570\n",
      "Iteration 7750, loss = 0.17189421\n",
      "Iteration 7751, loss = 0.17188854\n",
      "Iteration 7752, loss = 0.17187391\n",
      "Iteration 7753, loss = 0.17185745\n",
      "Iteration 7754, loss = 0.17184615\n",
      "Iteration 7755, loss = 0.17183515\n",
      "Iteration 7756, loss = 0.17182200\n",
      "Iteration 7757, loss = 0.17180383\n",
      "Iteration 7758, loss = 0.17179498\n",
      "Iteration 7759, loss = 0.17178307\n",
      "Iteration 7760, loss = 0.17177599\n",
      "Iteration 7761, loss = 0.17176088\n",
      "Iteration 7762, loss = 0.17175022\n",
      "Iteration 7763, loss = 0.17173288\n",
      "Iteration 7764, loss = 0.17172315\n",
      "Iteration 7765, loss = 0.17172272\n",
      "Iteration 7766, loss = 0.17170489\n",
      "Iteration 7767, loss = 0.17169026\n",
      "Iteration 7768, loss = 0.17167768\n",
      "Iteration 7769, loss = 0.17166782\n",
      "Iteration 7770, loss = 0.17166060\n",
      "Iteration 7771, loss = 0.17164871\n",
      "Iteration 7772, loss = 0.17163606\n",
      "Iteration 7773, loss = 0.17162153\n",
      "Iteration 7774, loss = 0.17160669\n",
      "Iteration 7775, loss = 0.17159342\n",
      "Iteration 7776, loss = 0.17158449\n",
      "Iteration 7777, loss = 0.17157585\n",
      "Iteration 7778, loss = 0.17155784\n",
      "Iteration 7779, loss = 0.17155157\n",
      "Iteration 7780, loss = 0.17153790\n",
      "Iteration 7781, loss = 0.17151936\n",
      "Iteration 7782, loss = 0.17151120\n",
      "Iteration 7783, loss = 0.17150158\n",
      "Iteration 7784, loss = 0.17148820\n",
      "Iteration 7785, loss = 0.17147806\n",
      "Iteration 7786, loss = 0.17146308\n",
      "Iteration 7787, loss = 0.17145133\n",
      "Iteration 7788, loss = 0.17143841\n",
      "Iteration 7789, loss = 0.17142123\n",
      "Iteration 7790, loss = 0.17141682\n",
      "Iteration 7791, loss = 0.17140478\n",
      "Iteration 7792, loss = 0.17139105\n",
      "Iteration 7793, loss = 0.17137872\n",
      "Iteration 7794, loss = 0.17136992\n",
      "Iteration 7795, loss = 0.17136114\n",
      "Iteration 7796, loss = 0.17134970\n",
      "Iteration 7797, loss = 0.17133762\n",
      "Iteration 7798, loss = 0.17132311\n",
      "Iteration 7799, loss = 0.17130571\n",
      "Iteration 7800, loss = 0.17129487\n",
      "Iteration 7801, loss = 0.17128014\n",
      "Iteration 7802, loss = 0.17127036\n",
      "Iteration 7803, loss = 0.17126000\n",
      "Iteration 7804, loss = 0.17125096\n",
      "Iteration 7805, loss = 0.17124419\n",
      "Iteration 7806, loss = 0.17123157\n",
      "Iteration 7807, loss = 0.17121775\n",
      "Iteration 7808, loss = 0.17120627\n",
      "Iteration 7809, loss = 0.17119459\n",
      "Iteration 7810, loss = 0.17118145\n",
      "Iteration 7811, loss = 0.17116804\n",
      "Iteration 7812, loss = 0.17115237\n",
      "Iteration 7813, loss = 0.17113723\n",
      "Iteration 7814, loss = 0.17113077\n",
      "Iteration 7815, loss = 0.17111519\n",
      "Iteration 7816, loss = 0.17109700\n",
      "Iteration 7817, loss = 0.17108542\n",
      "Iteration 7818, loss = 0.17107305\n",
      "Iteration 7819, loss = 0.17106154\n",
      "Iteration 7820, loss = 0.17104672\n",
      "Iteration 7821, loss = 0.17103300\n",
      "Iteration 7822, loss = 0.17101872\n",
      "Iteration 7823, loss = 0.17100383\n",
      "Iteration 7824, loss = 0.17099592\n",
      "Iteration 7825, loss = 0.17098330\n",
      "Iteration 7826, loss = 0.17097195\n",
      "Iteration 7827, loss = 0.17095638\n",
      "Iteration 7828, loss = 0.17094703\n",
      "Iteration 7829, loss = 0.17093255\n",
      "Iteration 7830, loss = 0.17092069\n",
      "Iteration 7831, loss = 0.17091229\n",
      "Iteration 7832, loss = 0.17089967\n",
      "Iteration 7833, loss = 0.17089051\n",
      "Iteration 7834, loss = 0.17087737\n",
      "Iteration 7835, loss = 0.17086584\n",
      "Iteration 7836, loss = 0.17085997\n",
      "Iteration 7837, loss = 0.17084966\n",
      "Iteration 7838, loss = 0.17083463\n",
      "Iteration 7839, loss = 0.17082025\n",
      "Iteration 7840, loss = 0.17080992\n",
      "Iteration 7841, loss = 0.17080085\n",
      "Iteration 7842, loss = 0.17079089\n",
      "Iteration 7843, loss = 0.17077603\n",
      "Iteration 7844, loss = 0.17075697\n",
      "Iteration 7845, loss = 0.17074375\n",
      "Iteration 7846, loss = 0.17073863\n",
      "Iteration 7847, loss = 0.17072437\n",
      "Iteration 7848, loss = 0.17071197\n",
      "Iteration 7849, loss = 0.17070142\n",
      "Iteration 7850, loss = 0.17068414\n",
      "Iteration 7851, loss = 0.17067655\n",
      "Iteration 7852, loss = 0.17065980\n",
      "Iteration 7853, loss = 0.17064889\n",
      "Iteration 7854, loss = 0.17063543\n",
      "Iteration 7855, loss = 0.17062361\n",
      "Iteration 7856, loss = 0.17061106\n",
      "Iteration 7857, loss = 0.17060011\n",
      "Iteration 7858, loss = 0.17058446\n",
      "Iteration 7859, loss = 0.17056742\n",
      "Iteration 7860, loss = 0.17055685\n",
      "Iteration 7861, loss = 0.17054131\n",
      "Iteration 7862, loss = 0.17052549\n",
      "Iteration 7863, loss = 0.17051521\n",
      "Iteration 7864, loss = 0.17050703\n",
      "Iteration 7865, loss = 0.17049469\n",
      "Iteration 7866, loss = 0.17048252\n",
      "Iteration 7867, loss = 0.17046903\n",
      "Iteration 7868, loss = 0.17046073\n",
      "Iteration 7869, loss = 0.17044648\n",
      "Iteration 7870, loss = 0.17043043\n",
      "Iteration 7871, loss = 0.17041874\n",
      "Iteration 7872, loss = 0.17041145\n",
      "Iteration 7873, loss = 0.17039956\n",
      "Iteration 7874, loss = 0.17038650\n",
      "Iteration 7875, loss = 0.17037416\n",
      "Iteration 7876, loss = 0.17036373\n",
      "Iteration 7877, loss = 0.17035064\n",
      "Iteration 7878, loss = 0.17033833\n",
      "Iteration 7879, loss = 0.17032912\n",
      "Iteration 7880, loss = 0.17031629\n",
      "Iteration 7881, loss = 0.17030681\n",
      "Iteration 7882, loss = 0.17029917\n",
      "Iteration 7883, loss = 0.17028555\n",
      "Iteration 7884, loss = 0.17026858\n",
      "Iteration 7885, loss = 0.17025872\n",
      "Iteration 7886, loss = 0.17024651\n",
      "Iteration 7887, loss = 0.17022993\n",
      "Iteration 7888, loss = 0.17021608\n",
      "Iteration 7889, loss = 0.17021633\n",
      "Iteration 7890, loss = 0.17020463\n",
      "Iteration 7891, loss = 0.17018474\n",
      "Iteration 7892, loss = 0.17017579\n",
      "Iteration 7893, loss = 0.17016480\n",
      "Iteration 7894, loss = 0.17015589\n",
      "Iteration 7895, loss = 0.17014366\n",
      "Iteration 7896, loss = 0.17013448\n",
      "Iteration 7897, loss = 0.17012510\n",
      "Iteration 7898, loss = 0.17011188\n",
      "Iteration 7899, loss = 0.17009758\n",
      "Iteration 7900, loss = 0.17008571\n",
      "Iteration 7901, loss = 0.17007815\n",
      "Iteration 7902, loss = 0.17006469\n",
      "Iteration 7903, loss = 0.17004634\n",
      "Iteration 7904, loss = 0.17003193\n",
      "Iteration 7905, loss = 0.17002360\n",
      "Iteration 7906, loss = 0.17001264\n",
      "Iteration 7907, loss = 0.17000098\n",
      "Iteration 7908, loss = 0.16998875\n",
      "Iteration 7909, loss = 0.16997360\n",
      "Iteration 7910, loss = 0.16996262\n",
      "Iteration 7911, loss = 0.16995038\n",
      "Iteration 7912, loss = 0.16993799\n",
      "Iteration 7913, loss = 0.16992811\n",
      "Iteration 7914, loss = 0.16991337\n",
      "Iteration 7915, loss = 0.16990033\n",
      "Iteration 7916, loss = 0.16988935\n",
      "Iteration 7917, loss = 0.16988187\n",
      "Iteration 7918, loss = 0.16986618\n",
      "Iteration 7919, loss = 0.16985999\n",
      "Iteration 7920, loss = 0.16984619\n",
      "Iteration 7921, loss = 0.16983299\n",
      "Iteration 7922, loss = 0.16981807\n",
      "Iteration 7923, loss = 0.16980605\n",
      "Iteration 7924, loss = 0.16979314\n",
      "Iteration 7925, loss = 0.16978385\n",
      "Iteration 7926, loss = 0.16977335\n",
      "Iteration 7927, loss = 0.16975987\n",
      "Iteration 7928, loss = 0.16974721\n",
      "Iteration 7929, loss = 0.16973775\n",
      "Iteration 7930, loss = 0.16972562\n",
      "Iteration 7931, loss = 0.16971480\n",
      "Iteration 7932, loss = 0.16970081\n",
      "Iteration 7933, loss = 0.16969223\n",
      "Iteration 7934, loss = 0.16967920\n",
      "Iteration 7935, loss = 0.16966177\n",
      "Iteration 7936, loss = 0.16965471\n",
      "Iteration 7937, loss = 0.16964659\n",
      "Iteration 7938, loss = 0.16963724\n",
      "Iteration 7939, loss = 0.16962271\n",
      "Iteration 7940, loss = 0.16960738\n",
      "Iteration 7941, loss = 0.16959911\n",
      "Iteration 7942, loss = 0.16958849\n",
      "Iteration 7943, loss = 0.16957541\n",
      "Iteration 7944, loss = 0.16956653\n",
      "Iteration 7945, loss = 0.16955422\n",
      "Iteration 7946, loss = 0.16954020\n",
      "Iteration 7947, loss = 0.16952709\n",
      "Iteration 7948, loss = 0.16951702\n",
      "Iteration 7949, loss = 0.16950919\n",
      "Iteration 7950, loss = 0.16949601\n",
      "Iteration 7951, loss = 0.16948399\n",
      "Iteration 7952, loss = 0.16947018\n",
      "Iteration 7953, loss = 0.16946566\n",
      "Iteration 7954, loss = 0.16945443\n",
      "Iteration 7955, loss = 0.16944360\n",
      "Iteration 7956, loss = 0.16943521\n",
      "Iteration 7957, loss = 0.16942136\n",
      "Iteration 7958, loss = 0.16940572\n",
      "Iteration 7959, loss = 0.16938823\n",
      "Iteration 7960, loss = 0.16937986\n",
      "Iteration 7961, loss = 0.16936808\n",
      "Iteration 7962, loss = 0.16935789\n",
      "Iteration 7963, loss = 0.16934744\n",
      "Iteration 7964, loss = 0.16933596\n",
      "Iteration 7965, loss = 0.16932459\n",
      "Iteration 7966, loss = 0.16931388\n",
      "Iteration 7967, loss = 0.16930115\n",
      "Iteration 7968, loss = 0.16928759\n",
      "Iteration 7969, loss = 0.16927719\n",
      "Iteration 7970, loss = 0.16926703\n",
      "Iteration 7971, loss = 0.16925390\n",
      "Iteration 7972, loss = 0.16924596\n",
      "Iteration 7973, loss = 0.16923237\n",
      "Iteration 7974, loss = 0.16922546\n",
      "Iteration 7975, loss = 0.16921609\n",
      "Iteration 7976, loss = 0.16920088\n",
      "Iteration 7977, loss = 0.16919052\n",
      "Iteration 7978, loss = 0.16918121\n",
      "Iteration 7979, loss = 0.16916814\n",
      "Iteration 7980, loss = 0.16915578\n",
      "Iteration 7981, loss = 0.16914249\n",
      "Iteration 7982, loss = 0.16913143\n",
      "Iteration 7983, loss = 0.16912137\n",
      "Iteration 7984, loss = 0.16911534\n",
      "Iteration 7985, loss = 0.16910309\n",
      "Iteration 7986, loss = 0.16908621\n",
      "Iteration 7987, loss = 0.16907314\n",
      "Iteration 7988, loss = 0.16906162\n",
      "Iteration 7989, loss = 0.16905188\n",
      "Iteration 7990, loss = 0.16903882\n",
      "Iteration 7991, loss = 0.16903089\n",
      "Iteration 7992, loss = 0.16901976\n",
      "Iteration 7993, loss = 0.16900717\n",
      "Iteration 7994, loss = 0.16899717\n",
      "Iteration 7995, loss = 0.16898732\n",
      "Iteration 7996, loss = 0.16897569\n",
      "Iteration 7997, loss = 0.16896246\n",
      "Iteration 7998, loss = 0.16894696\n",
      "Iteration 7999, loss = 0.16894731\n",
      "Iteration 8000, loss = 0.16893515\n",
      "Iteration 8001, loss = 0.16891746\n",
      "Iteration 8002, loss = 0.16890686\n",
      "Iteration 8003, loss = 0.16889412\n",
      "Iteration 8004, loss = 0.16888411\n",
      "Iteration 8005, loss = 0.16887426\n",
      "Iteration 8006, loss = 0.16885870\n",
      "Iteration 8007, loss = 0.16885080\n",
      "Iteration 8008, loss = 0.16884142\n",
      "Iteration 8009, loss = 0.16883495\n",
      "Iteration 8010, loss = 0.16882410\n",
      "Iteration 8011, loss = 0.16881366\n",
      "Iteration 8012, loss = 0.16880010\n",
      "Iteration 8013, loss = 0.16878550\n",
      "Iteration 8014, loss = 0.16877459\n",
      "Iteration 8015, loss = 0.16876211\n",
      "Iteration 8016, loss = 0.16874967\n",
      "Iteration 8017, loss = 0.16873769\n",
      "Iteration 8018, loss = 0.16873009\n",
      "Iteration 8019, loss = 0.16871949\n",
      "Iteration 8020, loss = 0.16870612\n",
      "Iteration 8021, loss = 0.16869838\n",
      "Iteration 8022, loss = 0.16868688\n",
      "Iteration 8023, loss = 0.16867267\n",
      "Iteration 8024, loss = 0.16866272\n",
      "Iteration 8025, loss = 0.16865475\n",
      "Iteration 8026, loss = 0.16864393\n",
      "Iteration 8027, loss = 0.16863267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8028, loss = 0.16862149\n",
      "Iteration 8029, loss = 0.16861010\n",
      "Iteration 8030, loss = 0.16860215\n",
      "Iteration 8031, loss = 0.16859024\n",
      "Iteration 8032, loss = 0.16857848\n",
      "Iteration 8033, loss = 0.16856528\n",
      "Iteration 8034, loss = 0.16855590\n",
      "Iteration 8035, loss = 0.16854362\n",
      "Iteration 8036, loss = 0.16853650\n",
      "Iteration 8037, loss = 0.16852828\n",
      "Iteration 8038, loss = 0.16851897\n",
      "Iteration 8039, loss = 0.16850502\n",
      "Iteration 8040, loss = 0.16849361\n",
      "Iteration 8041, loss = 0.16848093\n",
      "Iteration 8042, loss = 0.16846897\n",
      "Iteration 8043, loss = 0.16845731\n",
      "Iteration 8044, loss = 0.16844838\n",
      "Iteration 8045, loss = 0.16843792\n",
      "Iteration 8046, loss = 0.16842723\n",
      "Iteration 8047, loss = 0.16841455\n",
      "Iteration 8048, loss = 0.16840324\n",
      "Iteration 8049, loss = 0.16839626\n",
      "Iteration 8050, loss = 0.16838467\n",
      "Iteration 8051, loss = 0.16837502\n",
      "Iteration 8052, loss = 0.16836321\n",
      "Iteration 8053, loss = 0.16834927\n",
      "Iteration 8054, loss = 0.16833788\n",
      "Iteration 8055, loss = 0.16833072\n",
      "Iteration 8056, loss = 0.16831950\n",
      "Iteration 8057, loss = 0.16830592\n",
      "Iteration 8058, loss = 0.16829825\n",
      "Iteration 8059, loss = 0.16828726\n",
      "Iteration 8060, loss = 0.16827188\n",
      "Iteration 8061, loss = 0.16826317\n",
      "Iteration 8062, loss = 0.16826097\n",
      "Iteration 8063, loss = 0.16824978\n",
      "Iteration 8064, loss = 0.16823108\n",
      "Iteration 8065, loss = 0.16822028\n",
      "Iteration 8066, loss = 0.16821528\n",
      "Iteration 8067, loss = 0.16820433\n",
      "Iteration 8068, loss = 0.16819780\n",
      "Iteration 8069, loss = 0.16818538\n",
      "Iteration 8070, loss = 0.16817158\n",
      "Iteration 8071, loss = 0.16815570\n",
      "Iteration 8072, loss = 0.16814420\n",
      "Iteration 8073, loss = 0.16813107\n",
      "Iteration 8074, loss = 0.16811890\n",
      "Iteration 8075, loss = 0.16810603\n",
      "Iteration 8076, loss = 0.16809613\n",
      "Iteration 8077, loss = 0.16808659\n",
      "Iteration 8078, loss = 0.16807315\n",
      "Iteration 8079, loss = 0.16806519\n",
      "Iteration 8080, loss = 0.16805550\n",
      "Iteration 8081, loss = 0.16804304\n",
      "Iteration 8082, loss = 0.16803425\n",
      "Iteration 8083, loss = 0.16802789\n",
      "Iteration 8084, loss = 0.16801719\n",
      "Iteration 8085, loss = 0.16800193\n",
      "Iteration 8086, loss = 0.16799090\n",
      "Iteration 8087, loss = 0.16798623\n",
      "Iteration 8088, loss = 0.16797317\n",
      "Iteration 8089, loss = 0.16796112\n",
      "Iteration 8090, loss = 0.16794924\n",
      "Iteration 8091, loss = 0.16793846\n",
      "Iteration 8092, loss = 0.16792747\n",
      "Iteration 8093, loss = 0.16791935\n",
      "Iteration 8094, loss = 0.16790756\n",
      "Iteration 8095, loss = 0.16789638\n",
      "Iteration 8096, loss = 0.16788532\n",
      "Iteration 8097, loss = 0.16787411\n",
      "Iteration 8098, loss = 0.16786513\n",
      "Iteration 8099, loss = 0.16784896\n",
      "Iteration 8100, loss = 0.16784012\n",
      "Iteration 8101, loss = 0.16783216\n",
      "Iteration 8102, loss = 0.16782115\n",
      "Iteration 8103, loss = 0.16781342\n",
      "Iteration 8104, loss = 0.16780127\n",
      "Iteration 8105, loss = 0.16778531\n",
      "Iteration 8106, loss = 0.16777258\n",
      "Iteration 8107, loss = 0.16776871\n",
      "Iteration 8108, loss = 0.16775633\n",
      "Iteration 8109, loss = 0.16773934\n",
      "Iteration 8110, loss = 0.16772826\n",
      "Iteration 8111, loss = 0.16772127\n",
      "Iteration 8112, loss = 0.16770622\n",
      "Iteration 8113, loss = 0.16769946\n",
      "Iteration 8114, loss = 0.16769119\n",
      "Iteration 8115, loss = 0.16767145\n",
      "Iteration 8116, loss = 0.16766467\n",
      "Iteration 8117, loss = 0.16765467\n",
      "Iteration 8118, loss = 0.16764430\n",
      "Iteration 8119, loss = 0.16763298\n",
      "Iteration 8120, loss = 0.16761970\n",
      "Iteration 8121, loss = 0.16760971\n",
      "Iteration 8122, loss = 0.16760163\n",
      "Iteration 8123, loss = 0.16758978\n",
      "Iteration 8124, loss = 0.16757770\n",
      "Iteration 8125, loss = 0.16756672\n",
      "Iteration 8126, loss = 0.16755984\n",
      "Iteration 8127, loss = 0.16755103\n",
      "Iteration 8128, loss = 0.16753712\n",
      "Iteration 8129, loss = 0.16752645\n",
      "Iteration 8130, loss = 0.16751519\n",
      "Iteration 8131, loss = 0.16750290\n",
      "Iteration 8132, loss = 0.16749258\n",
      "Iteration 8133, loss = 0.16747947\n",
      "Iteration 8134, loss = 0.16746764\n",
      "Iteration 8135, loss = 0.16746183\n",
      "Iteration 8136, loss = 0.16745437\n",
      "Iteration 8137, loss = 0.16744470\n",
      "Iteration 8138, loss = 0.16743336\n",
      "Iteration 8139, loss = 0.16742146\n",
      "Iteration 8140, loss = 0.16741181\n",
      "Iteration 8141, loss = 0.16740152\n",
      "Iteration 8142, loss = 0.16738902\n",
      "Iteration 8143, loss = 0.16737394\n",
      "Iteration 8144, loss = 0.16736347\n",
      "Iteration 8145, loss = 0.16735191\n",
      "Iteration 8146, loss = 0.16734470\n",
      "Iteration 8147, loss = 0.16733668\n",
      "Iteration 8148, loss = 0.16732872\n",
      "Iteration 8149, loss = 0.16731351\n",
      "Iteration 8150, loss = 0.16730013\n",
      "Iteration 8151, loss = 0.16729675\n",
      "Iteration 8152, loss = 0.16728349\n",
      "Iteration 8153, loss = 0.16726784\n",
      "Iteration 8154, loss = 0.16725961\n",
      "Iteration 8155, loss = 0.16725105\n",
      "Iteration 8156, loss = 0.16723895\n",
      "Iteration 8157, loss = 0.16722681\n",
      "Iteration 8158, loss = 0.16721689\n",
      "Iteration 8159, loss = 0.16720514\n",
      "Iteration 8160, loss = 0.16719514\n",
      "Iteration 8161, loss = 0.16718978\n",
      "Iteration 8162, loss = 0.16717617\n",
      "Iteration 8163, loss = 0.16716560\n",
      "Iteration 8164, loss = 0.16715468\n",
      "Iteration 8165, loss = 0.16714397\n",
      "Iteration 8166, loss = 0.16713250\n",
      "Iteration 8167, loss = 0.16712323\n",
      "Iteration 8168, loss = 0.16711226\n",
      "Iteration 8169, loss = 0.16709604\n",
      "Iteration 8170, loss = 0.16709000\n",
      "Iteration 8171, loss = 0.16707870\n",
      "Iteration 8172, loss = 0.16707173\n",
      "Iteration 8173, loss = 0.16706095\n",
      "Iteration 8174, loss = 0.16704570\n",
      "Iteration 8175, loss = 0.16703693\n",
      "Iteration 8176, loss = 0.16702872\n",
      "Iteration 8177, loss = 0.16701867\n",
      "Iteration 8178, loss = 0.16700704\n",
      "Iteration 8179, loss = 0.16699718\n",
      "Iteration 8180, loss = 0.16698592\n",
      "Iteration 8181, loss = 0.16697166\n",
      "Iteration 8182, loss = 0.16696110\n",
      "Iteration 8183, loss = 0.16694884\n",
      "Iteration 8184, loss = 0.16693826\n",
      "Iteration 8185, loss = 0.16693005\n",
      "Iteration 8186, loss = 0.16692038\n",
      "Iteration 8187, loss = 0.16690803\n",
      "Iteration 8188, loss = 0.16689889\n",
      "Iteration 8189, loss = 0.16688996\n",
      "Iteration 8190, loss = 0.16687636\n",
      "Iteration 8191, loss = 0.16686620\n",
      "Iteration 8192, loss = 0.16685527\n",
      "Iteration 8193, loss = 0.16684537\n",
      "Iteration 8194, loss = 0.16684153\n",
      "Iteration 8195, loss = 0.16682979\n",
      "Iteration 8196, loss = 0.16681665\n",
      "Iteration 8197, loss = 0.16680440\n",
      "Iteration 8198, loss = 0.16679158\n",
      "Iteration 8199, loss = 0.16678435\n",
      "Iteration 8200, loss = 0.16677379\n",
      "Iteration 8201, loss = 0.16676623\n",
      "Iteration 8202, loss = 0.16675448\n",
      "Iteration 8203, loss = 0.16674372\n",
      "Iteration 8204, loss = 0.16673220\n",
      "Iteration 8205, loss = 0.16672273\n",
      "Iteration 8206, loss = 0.16671056\n",
      "Iteration 8207, loss = 0.16670081\n",
      "Iteration 8208, loss = 0.16668939\n",
      "Iteration 8209, loss = 0.16668244\n",
      "Iteration 8210, loss = 0.16666980\n",
      "Iteration 8211, loss = 0.16665894\n",
      "Iteration 8212, loss = 0.16664907\n",
      "Iteration 8213, loss = 0.16663746\n",
      "Iteration 8214, loss = 0.16662867\n",
      "Iteration 8215, loss = 0.16661885\n",
      "Iteration 8216, loss = 0.16661034\n",
      "Iteration 8217, loss = 0.16659733\n",
      "Iteration 8218, loss = 0.16658668\n",
      "Iteration 8219, loss = 0.16657555\n",
      "Iteration 8220, loss = 0.16656560\n",
      "Iteration 8221, loss = 0.16655780\n",
      "Iteration 8222, loss = 0.16654379\n",
      "Iteration 8223, loss = 0.16653566\n",
      "Iteration 8224, loss = 0.16652588\n",
      "Iteration 8225, loss = 0.16651644\n",
      "Iteration 8226, loss = 0.16650438\n",
      "Iteration 8227, loss = 0.16649459\n",
      "Iteration 8228, loss = 0.16648371\n",
      "Iteration 8229, loss = 0.16647606\n",
      "Iteration 8230, loss = 0.16646608\n",
      "Iteration 8231, loss = 0.16645263\n",
      "Iteration 8232, loss = 0.16644425\n",
      "Iteration 8233, loss = 0.16643519\n",
      "Iteration 8234, loss = 0.16642620\n",
      "Iteration 8235, loss = 0.16641383\n",
      "Iteration 8236, loss = 0.16639907\n",
      "Iteration 8237, loss = 0.16638701\n",
      "Iteration 8238, loss = 0.16637971\n",
      "Iteration 8239, loss = 0.16637301\n",
      "Iteration 8240, loss = 0.16636098\n",
      "Iteration 8241, loss = 0.16635050\n",
      "Iteration 8242, loss = 0.16634352\n",
      "Iteration 8243, loss = 0.16633178\n",
      "Iteration 8244, loss = 0.16632202\n",
      "Iteration 8245, loss = 0.16630730\n",
      "Iteration 8246, loss = 0.16629621\n",
      "Iteration 8247, loss = 0.16628709\n",
      "Iteration 8248, loss = 0.16627670\n",
      "Iteration 8249, loss = 0.16626832\n",
      "Iteration 8250, loss = 0.16625931\n",
      "Iteration 8251, loss = 0.16625060\n",
      "Iteration 8252, loss = 0.16624119\n",
      "Iteration 8253, loss = 0.16623146\n",
      "Iteration 8254, loss = 0.16621870\n",
      "Iteration 8255, loss = 0.16620708\n",
      "Iteration 8256, loss = 0.16619747\n",
      "Iteration 8257, loss = 0.16618696\n",
      "Iteration 8258, loss = 0.16617817\n",
      "Iteration 8259, loss = 0.16616621\n",
      "Iteration 8260, loss = 0.16615613\n",
      "Iteration 8261, loss = 0.16615110\n",
      "Iteration 8262, loss = 0.16614174\n",
      "Iteration 8263, loss = 0.16613336\n",
      "Iteration 8264, loss = 0.16612224\n",
      "Iteration 8265, loss = 0.16610933\n",
      "Iteration 8266, loss = 0.16609599\n",
      "Iteration 8267, loss = 0.16608444\n",
      "Iteration 8268, loss = 0.16607950\n",
      "Iteration 8269, loss = 0.16606736\n",
      "Iteration 8270, loss = 0.16605245\n",
      "Iteration 8271, loss = 0.16604564\n",
      "Iteration 8272, loss = 0.16603623\n",
      "Iteration 8273, loss = 0.16602511\n",
      "Iteration 8274, loss = 0.16601496\n",
      "Iteration 8275, loss = 0.16600188\n",
      "Iteration 8276, loss = 0.16599482\n",
      "Iteration 8277, loss = 0.16598773\n",
      "Iteration 8278, loss = 0.16597610\n",
      "Iteration 8279, loss = 0.16596392\n",
      "Iteration 8280, loss = 0.16595536\n",
      "Iteration 8281, loss = 0.16594333\n",
      "Iteration 8282, loss = 0.16593151\n",
      "Iteration 8283, loss = 0.16592747\n",
      "Iteration 8284, loss = 0.16592013\n",
      "Iteration 8285, loss = 0.16590300\n",
      "Iteration 8286, loss = 0.16589129\n",
      "Iteration 8287, loss = 0.16588629\n",
      "Iteration 8288, loss = 0.16587744\n",
      "Iteration 8289, loss = 0.16586505\n",
      "Iteration 8290, loss = 0.16585908\n",
      "Iteration 8291, loss = 0.16584664\n",
      "Iteration 8292, loss = 0.16583180\n",
      "Iteration 8293, loss = 0.16582094\n",
      "Iteration 8294, loss = 0.16581494\n",
      "Iteration 8295, loss = 0.16580329\n",
      "Iteration 8296, loss = 0.16579191\n",
      "Iteration 8297, loss = 0.16578442\n",
      "Iteration 8298, loss = 0.16577140\n",
      "Iteration 8299, loss = 0.16576472\n",
      "Iteration 8300, loss = 0.16575692\n",
      "Iteration 8301, loss = 0.16574652\n",
      "Iteration 8302, loss = 0.16573891\n",
      "Iteration 8303, loss = 0.16572588\n",
      "Iteration 8304, loss = 0.16571297\n",
      "Iteration 8305, loss = 0.16570282\n",
      "Iteration 8306, loss = 0.16569259\n",
      "Iteration 8307, loss = 0.16568418\n",
      "Iteration 8308, loss = 0.16567708\n",
      "Iteration 8309, loss = 0.16566625\n",
      "Iteration 8310, loss = 0.16565350\n",
      "Iteration 8311, loss = 0.16564584\n",
      "Iteration 8312, loss = 0.16563446\n",
      "Iteration 8313, loss = 0.16562367\n",
      "Iteration 8314, loss = 0.16561212\n",
      "Iteration 8315, loss = 0.16560247\n",
      "Iteration 8316, loss = 0.16559712\n",
      "Iteration 8317, loss = 0.16558684\n",
      "Iteration 8318, loss = 0.16557291\n",
      "Iteration 8319, loss = 0.16556393\n",
      "Iteration 8320, loss = 0.16555384\n",
      "Iteration 8321, loss = 0.16554683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8322, loss = 0.16553903\n",
      "Iteration 8323, loss = 0.16552710\n",
      "Iteration 8324, loss = 0.16551551\n",
      "Iteration 8325, loss = 0.16550286\n",
      "Iteration 8326, loss = 0.16548868\n",
      "Iteration 8327, loss = 0.16548255\n",
      "Iteration 8328, loss = 0.16547292\n",
      "Iteration 8329, loss = 0.16546419\n",
      "Iteration 8330, loss = 0.16545016\n",
      "Iteration 8331, loss = 0.16544030\n",
      "Iteration 8332, loss = 0.16543106\n",
      "Iteration 8333, loss = 0.16541957\n",
      "Iteration 8334, loss = 0.16540786\n",
      "Iteration 8335, loss = 0.16539839\n",
      "Iteration 8336, loss = 0.16539214\n",
      "Iteration 8337, loss = 0.16538325\n",
      "Iteration 8338, loss = 0.16536893\n",
      "Iteration 8339, loss = 0.16536013\n",
      "Iteration 8340, loss = 0.16535198\n",
      "Iteration 8341, loss = 0.16534215\n",
      "Iteration 8342, loss = 0.16533455\n",
      "Iteration 8343, loss = 0.16532339\n",
      "Iteration 8344, loss = 0.16531533\n",
      "Iteration 8345, loss = 0.16530364\n",
      "Iteration 8346, loss = 0.16529067\n",
      "Iteration 8347, loss = 0.16528635\n",
      "Iteration 8348, loss = 0.16527901\n",
      "Iteration 8349, loss = 0.16526625\n",
      "Iteration 8350, loss = 0.16525090\n",
      "Iteration 8351, loss = 0.16524421\n",
      "Iteration 8352, loss = 0.16523357\n",
      "Iteration 8353, loss = 0.16522614\n",
      "Iteration 8354, loss = 0.16521715\n",
      "Iteration 8355, loss = 0.16520604\n",
      "Iteration 8356, loss = 0.16519379\n",
      "Iteration 8357, loss = 0.16518574\n",
      "Iteration 8358, loss = 0.16517642\n",
      "Iteration 8359, loss = 0.16516399\n",
      "Iteration 8360, loss = 0.16515605\n",
      "Iteration 8361, loss = 0.16514229\n",
      "Iteration 8362, loss = 0.16513222\n",
      "Iteration 8363, loss = 0.16512345\n",
      "Iteration 8364, loss = 0.16511696\n",
      "Iteration 8365, loss = 0.16510213\n",
      "Iteration 8366, loss = 0.16509469\n",
      "Iteration 8367, loss = 0.16508672\n",
      "Iteration 8368, loss = 0.16507881\n",
      "Iteration 8369, loss = 0.16506815\n",
      "Iteration 8370, loss = 0.16505786\n",
      "Iteration 8371, loss = 0.16504921\n",
      "Iteration 8372, loss = 0.16504044\n",
      "Iteration 8373, loss = 0.16502785\n",
      "Iteration 8374, loss = 0.16501557\n",
      "Iteration 8375, loss = 0.16500699\n",
      "Iteration 8376, loss = 0.16499456\n",
      "Iteration 8377, loss = 0.16498335\n",
      "Iteration 8378, loss = 0.16497442\n",
      "Iteration 8379, loss = 0.16496873\n",
      "Iteration 8380, loss = 0.16495601\n",
      "Iteration 8381, loss = 0.16494853\n",
      "Iteration 8382, loss = 0.16494081\n",
      "Iteration 8383, loss = 0.16492674\n",
      "Iteration 8384, loss = 0.16491678\n",
      "Iteration 8385, loss = 0.16490539\n",
      "Iteration 8386, loss = 0.16489866\n",
      "Iteration 8387, loss = 0.16489054\n",
      "Iteration 8388, loss = 0.16487924\n",
      "Iteration 8389, loss = 0.16486491\n",
      "Iteration 8390, loss = 0.16486282\n",
      "Iteration 8391, loss = 0.16485405\n",
      "Iteration 8392, loss = 0.16484112\n",
      "Iteration 8393, loss = 0.16482981\n",
      "Iteration 8394, loss = 0.16482101\n",
      "Iteration 8395, loss = 0.16481118\n",
      "Iteration 8396, loss = 0.16480169\n",
      "Iteration 8397, loss = 0.16479045\n",
      "Iteration 8398, loss = 0.16477996\n",
      "Iteration 8399, loss = 0.16476903\n",
      "Iteration 8400, loss = 0.16476236\n",
      "Iteration 8401, loss = 0.16475069\n",
      "Iteration 8402, loss = 0.16474171\n",
      "Iteration 8403, loss = 0.16473103\n",
      "Iteration 8404, loss = 0.16472136\n",
      "Iteration 8405, loss = 0.16471308\n",
      "Iteration 8406, loss = 0.16470287\n",
      "Iteration 8407, loss = 0.16469192\n",
      "Iteration 8408, loss = 0.16468040\n",
      "Iteration 8409, loss = 0.16466934\n",
      "Iteration 8410, loss = 0.16466684\n",
      "Iteration 8411, loss = 0.16465825\n",
      "Iteration 8412, loss = 0.16464347\n",
      "Iteration 8413, loss = 0.16463298\n",
      "Iteration 8414, loss = 0.16462795\n",
      "Iteration 8415, loss = 0.16462090\n",
      "Iteration 8416, loss = 0.16461053\n",
      "Iteration 8417, loss = 0.16459879\n",
      "Iteration 8418, loss = 0.16459280\n",
      "Iteration 8419, loss = 0.16458017\n",
      "Iteration 8420, loss = 0.16456648\n",
      "Iteration 8421, loss = 0.16455612\n",
      "Iteration 8422, loss = 0.16455135\n",
      "Iteration 8423, loss = 0.16453744\n",
      "Iteration 8424, loss = 0.16452558\n",
      "Iteration 8425, loss = 0.16451453\n",
      "Iteration 8426, loss = 0.16450586\n",
      "Iteration 8427, loss = 0.16449865\n",
      "Iteration 8428, loss = 0.16448871\n",
      "Iteration 8429, loss = 0.16448580\n",
      "Iteration 8430, loss = 0.16447213\n",
      "Iteration 8431, loss = 0.16446144\n",
      "Iteration 8432, loss = 0.16445279\n",
      "Iteration 8433, loss = 0.16444553\n",
      "Iteration 8434, loss = 0.16443024\n",
      "Iteration 8435, loss = 0.16442418\n",
      "Iteration 8436, loss = 0.16441571\n",
      "Iteration 8437, loss = 0.16440025\n",
      "Iteration 8438, loss = 0.16439072\n",
      "Iteration 8439, loss = 0.16438645\n",
      "Iteration 8440, loss = 0.16437713\n",
      "Iteration 8441, loss = 0.16436868\n",
      "Iteration 8442, loss = 0.16436055\n",
      "Iteration 8443, loss = 0.16434412\n",
      "Iteration 8444, loss = 0.16433566\n",
      "Iteration 8445, loss = 0.16432746\n",
      "Iteration 8446, loss = 0.16431820\n",
      "Iteration 8447, loss = 0.16430992\n",
      "Iteration 8448, loss = 0.16429829\n",
      "Iteration 8449, loss = 0.16428733\n",
      "Iteration 8450, loss = 0.16427839\n",
      "Iteration 8451, loss = 0.16426771\n",
      "Iteration 8452, loss = 0.16425756\n",
      "Iteration 8453, loss = 0.16424989\n",
      "Iteration 8454, loss = 0.16424015\n",
      "Iteration 8455, loss = 0.16423044\n",
      "Iteration 8456, loss = 0.16421981\n",
      "Iteration 8457, loss = 0.16421221\n",
      "Iteration 8458, loss = 0.16420168\n",
      "Iteration 8459, loss = 0.16419452\n",
      "Iteration 8460, loss = 0.16418528\n",
      "Iteration 8461, loss = 0.16417974\n",
      "Iteration 8462, loss = 0.16417127\n",
      "Iteration 8463, loss = 0.16415764\n",
      "Iteration 8464, loss = 0.16414878\n",
      "Iteration 8465, loss = 0.16413933\n",
      "Iteration 8466, loss = 0.16412797\n",
      "Iteration 8467, loss = 0.16412311\n",
      "Iteration 8468, loss = 0.16411567\n",
      "Iteration 8469, loss = 0.16410035\n",
      "Iteration 8470, loss = 0.16408989\n",
      "Iteration 8471, loss = 0.16408226\n",
      "Iteration 8472, loss = 0.16407030\n",
      "Iteration 8473, loss = 0.16405941\n",
      "Iteration 8474, loss = 0.16404981\n",
      "Iteration 8475, loss = 0.16404185\n",
      "Iteration 8476, loss = 0.16403445\n",
      "Iteration 8477, loss = 0.16402526\n",
      "Iteration 8478, loss = 0.16401817\n",
      "Iteration 8479, loss = 0.16400663\n",
      "Iteration 8480, loss = 0.16399530\n",
      "Iteration 8481, loss = 0.16398788\n",
      "Iteration 8482, loss = 0.16397784\n",
      "Iteration 8483, loss = 0.16396841\n",
      "Iteration 8484, loss = 0.16396041\n",
      "Iteration 8485, loss = 0.16395237\n",
      "Iteration 8486, loss = 0.16394482\n",
      "Iteration 8487, loss = 0.16393502\n",
      "Iteration 8488, loss = 0.16392112\n",
      "Iteration 8489, loss = 0.16390633\n",
      "Iteration 8490, loss = 0.16390693\n",
      "Iteration 8491, loss = 0.16389831\n",
      "Iteration 8492, loss = 0.16388715\n",
      "Iteration 8493, loss = 0.16386962\n",
      "Iteration 8494, loss = 0.16386013\n",
      "Iteration 8495, loss = 0.16385174\n",
      "Iteration 8496, loss = 0.16384125\n",
      "Iteration 8497, loss = 0.16383274\n",
      "Iteration 8498, loss = 0.16382028\n",
      "Iteration 8499, loss = 0.16381669\n",
      "Iteration 8500, loss = 0.16380799\n",
      "Iteration 8501, loss = 0.16379386\n",
      "Iteration 8502, loss = 0.16378253\n",
      "Iteration 8503, loss = 0.16377656\n",
      "Iteration 8504, loss = 0.16376849\n",
      "Iteration 8505, loss = 0.16375554\n",
      "Iteration 8506, loss = 0.16374690\n",
      "Iteration 8507, loss = 0.16374042\n",
      "Iteration 8508, loss = 0.16373118\n",
      "Iteration 8509, loss = 0.16372102\n",
      "Iteration 8510, loss = 0.16370780\n",
      "Iteration 8511, loss = 0.16369972\n",
      "Iteration 8512, loss = 0.16369176\n",
      "Iteration 8513, loss = 0.16368318\n",
      "Iteration 8514, loss = 0.16367505\n",
      "Iteration 8515, loss = 0.16366286\n",
      "Iteration 8516, loss = 0.16365172\n",
      "Iteration 8517, loss = 0.16364507\n",
      "Iteration 8518, loss = 0.16363490\n",
      "Iteration 8519, loss = 0.16362258\n",
      "Iteration 8520, loss = 0.16360938\n",
      "Iteration 8521, loss = 0.16360131\n",
      "Iteration 8522, loss = 0.16359424\n",
      "Iteration 8523, loss = 0.16358546\n",
      "Iteration 8524, loss = 0.16357422\n",
      "Iteration 8525, loss = 0.16356640\n",
      "Iteration 8526, loss = 0.16355559\n",
      "Iteration 8527, loss = 0.16354784\n",
      "Iteration 8528, loss = 0.16354198\n",
      "Iteration 8529, loss = 0.16353264\n",
      "Iteration 8530, loss = 0.16351858\n",
      "Iteration 8531, loss = 0.16350633\n",
      "Iteration 8532, loss = 0.16349916\n",
      "Iteration 8533, loss = 0.16348759\n",
      "Iteration 8534, loss = 0.16347818\n",
      "Iteration 8535, loss = 0.16347247\n",
      "Iteration 8536, loss = 0.16346270\n",
      "Iteration 8537, loss = 0.16345186\n",
      "Iteration 8538, loss = 0.16344211\n",
      "Iteration 8539, loss = 0.16343085\n",
      "Iteration 8540, loss = 0.16341973\n",
      "Iteration 8541, loss = 0.16341045\n",
      "Iteration 8542, loss = 0.16339625\n",
      "Iteration 8543, loss = 0.16338707\n",
      "Iteration 8544, loss = 0.16337702\n",
      "Iteration 8545, loss = 0.16337031\n",
      "Iteration 8546, loss = 0.16335732\n",
      "Iteration 8547, loss = 0.16335434\n",
      "Iteration 8548, loss = 0.16334324\n",
      "Iteration 8549, loss = 0.16332776\n",
      "Iteration 8550, loss = 0.16332192\n",
      "Iteration 8551, loss = 0.16331808\n",
      "Iteration 8552, loss = 0.16330894\n",
      "Iteration 8553, loss = 0.16329806\n",
      "Iteration 8554, loss = 0.16328566\n",
      "Iteration 8555, loss = 0.16327453\n",
      "Iteration 8556, loss = 0.16326662\n",
      "Iteration 8557, loss = 0.16325280\n",
      "Iteration 8558, loss = 0.16324455\n",
      "Iteration 8559, loss = 0.16323135\n",
      "Iteration 8560, loss = 0.16321906\n",
      "Iteration 8561, loss = 0.16321627\n",
      "Iteration 8562, loss = 0.16320444\n",
      "Iteration 8563, loss = 0.16319373\n",
      "Iteration 8564, loss = 0.16318508\n",
      "Iteration 8565, loss = 0.16317237\n",
      "Iteration 8566, loss = 0.16315850\n",
      "Iteration 8567, loss = 0.16314076\n",
      "Iteration 8568, loss = 0.16312828\n",
      "Iteration 8569, loss = 0.16311854\n",
      "Iteration 8570, loss = 0.16310481\n",
      "Iteration 8571, loss = 0.16309189\n",
      "Iteration 8572, loss = 0.16307791\n",
      "Iteration 8573, loss = 0.16306358\n",
      "Iteration 8574, loss = 0.16304952\n",
      "Iteration 8575, loss = 0.16303975\n",
      "Iteration 8576, loss = 0.16302612\n",
      "Iteration 8577, loss = 0.16301197\n",
      "Iteration 8578, loss = 0.16299588\n",
      "Iteration 8579, loss = 0.16298448\n",
      "Iteration 8580, loss = 0.16296744\n",
      "Iteration 8581, loss = 0.16295004\n",
      "Iteration 8582, loss = 0.16294221\n",
      "Iteration 8583, loss = 0.16293852\n",
      "Iteration 8584, loss = 0.16292699\n",
      "Iteration 8585, loss = 0.16291302\n",
      "Iteration 8586, loss = 0.16289852\n",
      "Iteration 8587, loss = 0.16289090\n",
      "Iteration 8588, loss = 0.16288424\n",
      "Iteration 8589, loss = 0.16287711\n",
      "Iteration 8590, loss = 0.16286999\n",
      "Iteration 8591, loss = 0.16285645\n",
      "Iteration 8592, loss = 0.16285049\n",
      "Iteration 8593, loss = 0.16283890\n",
      "Iteration 8594, loss = 0.16282906\n",
      "Iteration 8595, loss = 0.16281969\n",
      "Iteration 8596, loss = 0.16280685\n",
      "Iteration 8597, loss = 0.16279325\n",
      "Iteration 8598, loss = 0.16278106\n",
      "Iteration 8599, loss = 0.16277469\n",
      "Iteration 8600, loss = 0.16276768\n",
      "Iteration 8601, loss = 0.16275689\n",
      "Iteration 8602, loss = 0.16274140\n",
      "Iteration 8603, loss = 0.16273166\n",
      "Iteration 8604, loss = 0.16272358\n",
      "Iteration 8605, loss = 0.16271555\n",
      "Iteration 8606, loss = 0.16271157\n",
      "Iteration 8607, loss = 0.16269856\n",
      "Iteration 8608, loss = 0.16268932\n",
      "Iteration 8609, loss = 0.16267855\n",
      "Iteration 8610, loss = 0.16266676\n",
      "Iteration 8611, loss = 0.16265592\n",
      "Iteration 8612, loss = 0.16264380\n",
      "Iteration 8613, loss = 0.16264167\n",
      "Iteration 8614, loss = 0.16262957\n",
      "Iteration 8615, loss = 0.16261628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8616, loss = 0.16261037\n",
      "Iteration 8617, loss = 0.16260065\n",
      "Iteration 8618, loss = 0.16259153\n",
      "Iteration 8619, loss = 0.16258491\n",
      "Iteration 8620, loss = 0.16257716\n",
      "Iteration 8621, loss = 0.16256272\n",
      "Iteration 8622, loss = 0.16255053\n",
      "Iteration 8623, loss = 0.16254134\n",
      "Iteration 8624, loss = 0.16253656\n",
      "Iteration 8625, loss = 0.16253184\n",
      "Iteration 8626, loss = 0.16251943\n",
      "Iteration 8627, loss = 0.16250568\n",
      "Iteration 8628, loss = 0.16250050\n",
      "Iteration 8629, loss = 0.16249531\n",
      "Iteration 8630, loss = 0.16248610\n",
      "Iteration 8631, loss = 0.16247505\n",
      "Iteration 8632, loss = 0.16246197\n",
      "Iteration 8633, loss = 0.16244833\n",
      "Iteration 8634, loss = 0.16243757\n",
      "Iteration 8635, loss = 0.16242830\n",
      "Iteration 8636, loss = 0.16241982\n",
      "Iteration 8637, loss = 0.16241844\n",
      "Iteration 8638, loss = 0.16241028\n",
      "Iteration 8639, loss = 0.16239669\n",
      "Iteration 8640, loss = 0.16238496\n",
      "Iteration 8641, loss = 0.16238047\n",
      "Iteration 8642, loss = 0.16237207\n",
      "Iteration 8643, loss = 0.16235945\n",
      "Iteration 8644, loss = 0.16235561\n",
      "Iteration 8645, loss = 0.16234459\n",
      "Iteration 8646, loss = 0.16233354\n",
      "Iteration 8647, loss = 0.16232196\n",
      "Iteration 8648, loss = 0.16231194\n",
      "Iteration 8649, loss = 0.16230523\n",
      "Iteration 8650, loss = 0.16229813\n",
      "Iteration 8651, loss = 0.16228789\n",
      "Iteration 8652, loss = 0.16226851\n",
      "Iteration 8653, loss = 0.16226225\n",
      "Iteration 8654, loss = 0.16225572\n",
      "Iteration 8655, loss = 0.16224586\n",
      "Iteration 8656, loss = 0.16223508\n",
      "Iteration 8657, loss = 0.16223031\n",
      "Iteration 8658, loss = 0.16222127\n",
      "Iteration 8659, loss = 0.16220836\n",
      "Iteration 8660, loss = 0.16219561\n",
      "Iteration 8661, loss = 0.16218608\n",
      "Iteration 8662, loss = 0.16217585\n",
      "Iteration 8663, loss = 0.16217139\n",
      "Iteration 8664, loss = 0.16216289\n",
      "Iteration 8665, loss = 0.16214864\n",
      "Iteration 8666, loss = 0.16213448\n",
      "Iteration 8667, loss = 0.16212571\n",
      "Iteration 8668, loss = 0.16211837\n",
      "Iteration 8669, loss = 0.16211408\n",
      "Iteration 8670, loss = 0.16210521\n",
      "Iteration 8671, loss = 0.16209198\n",
      "Iteration 8672, loss = 0.16208195\n",
      "Iteration 8673, loss = 0.16207558\n",
      "Iteration 8674, loss = 0.16206501\n",
      "Iteration 8675, loss = 0.16205375\n",
      "Iteration 8676, loss = 0.16204643\n",
      "Iteration 8677, loss = 0.16204076\n",
      "Iteration 8678, loss = 0.16203127\n",
      "Iteration 8679, loss = 0.16202016\n",
      "Iteration 8680, loss = 0.16200886\n",
      "Iteration 8681, loss = 0.16200274\n",
      "Iteration 8682, loss = 0.16199459\n",
      "Iteration 8683, loss = 0.16198174\n",
      "Iteration 8684, loss = 0.16197382\n",
      "Iteration 8685, loss = 0.16196230\n",
      "Iteration 8686, loss = 0.16195192\n",
      "Iteration 8687, loss = 0.16194198\n",
      "Iteration 8688, loss = 0.16193562\n",
      "Iteration 8689, loss = 0.16192480\n",
      "Iteration 8690, loss = 0.16191369\n",
      "Iteration 8691, loss = 0.16190488\n",
      "Iteration 8692, loss = 0.16189986\n",
      "Iteration 8693, loss = 0.16189180\n",
      "Iteration 8694, loss = 0.16188387\n",
      "Iteration 8695, loss = 0.16187689\n",
      "Iteration 8696, loss = 0.16186611\n",
      "Iteration 8697, loss = 0.16185464\n",
      "Iteration 8698, loss = 0.16184796\n",
      "Iteration 8699, loss = 0.16183725\n",
      "Iteration 8700, loss = 0.16182447\n",
      "Iteration 8701, loss = 0.16181418\n",
      "Iteration 8702, loss = 0.16180303\n",
      "Iteration 8703, loss = 0.16179494\n",
      "Iteration 8704, loss = 0.16178717\n",
      "Iteration 8705, loss = 0.16177592\n",
      "Iteration 8706, loss = 0.16176997\n",
      "Iteration 8707, loss = 0.16176372\n",
      "Iteration 8708, loss = 0.16175122\n",
      "Iteration 8709, loss = 0.16174042\n",
      "Iteration 8710, loss = 0.16173296\n",
      "Iteration 8711, loss = 0.16172640\n",
      "Iteration 8712, loss = 0.16172077\n",
      "Iteration 8713, loss = 0.16170814\n",
      "Iteration 8714, loss = 0.16169671\n",
      "Iteration 8715, loss = 0.16169037\n",
      "Iteration 8716, loss = 0.16168525\n",
      "Iteration 8717, loss = 0.16167622\n",
      "Iteration 8718, loss = 0.16167009\n",
      "Iteration 8719, loss = 0.16165571\n",
      "Iteration 8720, loss = 0.16164574\n",
      "Iteration 8721, loss = 0.16163969\n",
      "Iteration 8722, loss = 0.16163150\n",
      "Iteration 8723, loss = 0.16162134\n",
      "Iteration 8724, loss = 0.16161389\n",
      "Iteration 8725, loss = 0.16160422\n",
      "Iteration 8726, loss = 0.16159461\n",
      "Iteration 8727, loss = 0.16158230\n",
      "Iteration 8728, loss = 0.16157306\n",
      "Iteration 8729, loss = 0.16156833\n",
      "Iteration 8730, loss = 0.16156145\n",
      "Iteration 8731, loss = 0.16154639\n",
      "Iteration 8732, loss = 0.16153884\n",
      "Iteration 8733, loss = 0.16153588\n",
      "Iteration 8734, loss = 0.16152593\n",
      "Iteration 8735, loss = 0.16151376\n",
      "Iteration 8736, loss = 0.16150164\n",
      "Iteration 8737, loss = 0.16149465\n",
      "Iteration 8738, loss = 0.16148656\n",
      "Iteration 8739, loss = 0.16147226\n",
      "Iteration 8740, loss = 0.16147146\n",
      "Iteration 8741, loss = 0.16146554\n",
      "Iteration 8742, loss = 0.16145948\n",
      "Iteration 8743, loss = 0.16144842\n",
      "Iteration 8744, loss = 0.16143811\n",
      "Iteration 8745, loss = 0.16142921\n",
      "Iteration 8746, loss = 0.16141718\n",
      "Iteration 8747, loss = 0.16140734\n",
      "Iteration 8748, loss = 0.16139494\n",
      "Iteration 8749, loss = 0.16139148\n",
      "Iteration 8750, loss = 0.16138544\n",
      "Iteration 8751, loss = 0.16137823\n",
      "Iteration 8752, loss = 0.16136946\n",
      "Iteration 8753, loss = 0.16135929\n",
      "Iteration 8754, loss = 0.16134203\n",
      "Iteration 8755, loss = 0.16134245\n",
      "Iteration 8756, loss = 0.16133523\n",
      "Iteration 8757, loss = 0.16132367\n",
      "Iteration 8758, loss = 0.16130598\n",
      "Iteration 8759, loss = 0.16129883\n",
      "Iteration 8760, loss = 0.16129726\n",
      "Iteration 8761, loss = 0.16128761\n",
      "Iteration 8762, loss = 0.16127414\n",
      "Iteration 8763, loss = 0.16126849\n",
      "Iteration 8764, loss = 0.16126288\n",
      "Iteration 8765, loss = 0.16125139\n",
      "Iteration 8766, loss = 0.16124406\n",
      "Iteration 8767, loss = 0.16123592\n",
      "Iteration 8768, loss = 0.16122728\n",
      "Iteration 8769, loss = 0.16121557\n",
      "Iteration 8770, loss = 0.16120818\n",
      "Iteration 8771, loss = 0.16119978\n",
      "Iteration 8772, loss = 0.16118965\n",
      "Iteration 8773, loss = 0.16117728\n",
      "Iteration 8774, loss = 0.16117123\n",
      "Iteration 8775, loss = 0.16116567\n",
      "Iteration 8776, loss = 0.16115677\n",
      "Iteration 8777, loss = 0.16114558\n",
      "Iteration 8778, loss = 0.16113582\n",
      "Iteration 8779, loss = 0.16112444\n",
      "Iteration 8780, loss = 0.16111898\n",
      "Iteration 8781, loss = 0.16111370\n",
      "Iteration 8782, loss = 0.16110084\n",
      "Iteration 8783, loss = 0.16109248\n",
      "Iteration 8784, loss = 0.16108481\n",
      "Iteration 8785, loss = 0.16107588\n",
      "Iteration 8786, loss = 0.16107139\n",
      "Iteration 8787, loss = 0.16106101\n",
      "Iteration 8788, loss = 0.16104699\n",
      "Iteration 8789, loss = 0.16104142\n",
      "Iteration 8790, loss = 0.16103454\n",
      "Iteration 8791, loss = 0.16102244\n",
      "Iteration 8792, loss = 0.16101383\n",
      "Iteration 8793, loss = 0.16100802\n",
      "Iteration 8794, loss = 0.16099399\n",
      "Iteration 8795, loss = 0.16098440\n",
      "Iteration 8796, loss = 0.16098288\n",
      "Iteration 8797, loss = 0.16097062\n",
      "Iteration 8798, loss = 0.16096223\n",
      "Iteration 8799, loss = 0.16095941\n",
      "Iteration 8800, loss = 0.16094880\n",
      "Iteration 8801, loss = 0.16093647\n",
      "Iteration 8802, loss = 0.16092965\n",
      "Iteration 8803, loss = 0.16091945\n",
      "Iteration 8804, loss = 0.16090904\n",
      "Iteration 8805, loss = 0.16089859\n",
      "Iteration 8806, loss = 0.16089316\n",
      "Iteration 8807, loss = 0.16088607\n",
      "Iteration 8808, loss = 0.16087698\n",
      "Iteration 8809, loss = 0.16086568\n",
      "Iteration 8810, loss = 0.16085603\n",
      "Iteration 8811, loss = 0.16084569\n",
      "Iteration 8812, loss = 0.16083969\n",
      "Iteration 8813, loss = 0.16083357\n",
      "Iteration 8814, loss = 0.16082265\n",
      "Iteration 8815, loss = 0.16081594\n",
      "Iteration 8816, loss = 0.16081018\n",
      "Iteration 8817, loss = 0.16080108\n",
      "Iteration 8818, loss = 0.16079299\n",
      "Iteration 8819, loss = 0.16078314\n",
      "Iteration 8820, loss = 0.16077271\n",
      "Iteration 8821, loss = 0.16076596\n",
      "Iteration 8822, loss = 0.16075637\n",
      "Iteration 8823, loss = 0.16074404\n",
      "Iteration 8824, loss = 0.16073973\n",
      "Iteration 8825, loss = 0.16073027\n",
      "Iteration 8826, loss = 0.16071895\n",
      "Iteration 8827, loss = 0.16071634\n",
      "Iteration 8828, loss = 0.16071305\n",
      "Iteration 8829, loss = 0.16070343\n",
      "Iteration 8830, loss = 0.16069066\n",
      "Iteration 8831, loss = 0.16068322\n",
      "Iteration 8832, loss = 0.16067750\n",
      "Iteration 8833, loss = 0.16066979\n",
      "Iteration 8834, loss = 0.16066059\n",
      "Iteration 8835, loss = 0.16064964\n",
      "Iteration 8836, loss = 0.16064425\n",
      "Iteration 8837, loss = 0.16063991\n",
      "Iteration 8838, loss = 0.16063039\n",
      "Iteration 8839, loss = 0.16061661\n",
      "Iteration 8840, loss = 0.16060763\n",
      "Iteration 8841, loss = 0.16060305\n",
      "Iteration 8842, loss = 0.16059428\n",
      "Iteration 8843, loss = 0.16058636\n",
      "Iteration 8844, loss = 0.16057725\n",
      "Iteration 8845, loss = 0.16056871\n",
      "Iteration 8846, loss = 0.16055933\n",
      "Iteration 8847, loss = 0.16054982\n",
      "Iteration 8848, loss = 0.16054261\n",
      "Iteration 8849, loss = 0.16053282\n",
      "Iteration 8850, loss = 0.16052526\n",
      "Iteration 8851, loss = 0.16052034\n",
      "Iteration 8852, loss = 0.16051135\n",
      "Iteration 8853, loss = 0.16050577\n",
      "Iteration 8854, loss = 0.16049401\n",
      "Iteration 8855, loss = 0.16048804\n",
      "Iteration 8856, loss = 0.16047606\n",
      "Iteration 8857, loss = 0.16046424\n",
      "Iteration 8858, loss = 0.16045891\n",
      "Iteration 8859, loss = 0.16045666\n",
      "Iteration 8860, loss = 0.16044955\n",
      "Iteration 8861, loss = 0.16043486\n",
      "Iteration 8862, loss = 0.16042273\n",
      "Iteration 8863, loss = 0.16041727\n",
      "Iteration 8864, loss = 0.16041212\n",
      "Iteration 8865, loss = 0.16040165\n",
      "Iteration 8866, loss = 0.16039484\n",
      "Iteration 8867, loss = 0.16038435\n",
      "Iteration 8868, loss = 0.16037391\n",
      "Iteration 8869, loss = 0.16037040\n",
      "Iteration 8870, loss = 0.16035933\n",
      "Iteration 8871, loss = 0.16035179\n",
      "Iteration 8872, loss = 0.16035032\n",
      "Iteration 8873, loss = 0.16034174\n",
      "Iteration 8874, loss = 0.16033093\n",
      "Iteration 8875, loss = 0.16031966\n",
      "Iteration 8876, loss = 0.16031121\n",
      "Iteration 8877, loss = 0.16030226\n",
      "Iteration 8878, loss = 0.16029502\n",
      "Iteration 8879, loss = 0.16028847\n",
      "Iteration 8880, loss = 0.16027747\n",
      "Iteration 8881, loss = 0.16027319\n",
      "Iteration 8882, loss = 0.16026484\n",
      "Iteration 8883, loss = 0.16025685\n",
      "Iteration 8884, loss = 0.16025017\n",
      "Iteration 8885, loss = 0.16024427\n",
      "Iteration 8886, loss = 0.16023242\n",
      "Iteration 8887, loss = 0.16021668\n",
      "Iteration 8888, loss = 0.16021021\n",
      "Iteration 8889, loss = 0.16020396\n",
      "Iteration 8890, loss = 0.16019971\n",
      "Iteration 8891, loss = 0.16018972\n",
      "Iteration 8892, loss = 0.16018158\n",
      "Iteration 8893, loss = 0.16016954\n",
      "Iteration 8894, loss = 0.16016225\n",
      "Iteration 8895, loss = 0.16015438\n",
      "Iteration 8896, loss = 0.16014854\n",
      "Iteration 8897, loss = 0.16013988\n",
      "Iteration 8898, loss = 0.16012936\n",
      "Iteration 8899, loss = 0.16011982\n",
      "Iteration 8900, loss = 0.16011195\n",
      "Iteration 8901, loss = 0.16010800\n",
      "Iteration 8902, loss = 0.16009890\n",
      "Iteration 8903, loss = 0.16008990\n",
      "Iteration 8904, loss = 0.16008666\n",
      "Iteration 8905, loss = 0.16007784\n",
      "Iteration 8906, loss = 0.16006856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8907, loss = 0.16005783\n",
      "Iteration 8908, loss = 0.16005213\n",
      "Iteration 8909, loss = 0.16004394\n",
      "Iteration 8910, loss = 0.16003376\n",
      "Iteration 8911, loss = 0.16002500\n",
      "Iteration 8912, loss = 0.16001803\n",
      "Iteration 8913, loss = 0.16001020\n",
      "Iteration 8914, loss = 0.16000157\n",
      "Iteration 8915, loss = 0.15999447\n",
      "Iteration 8916, loss = 0.15998815\n",
      "Iteration 8917, loss = 0.15997480\n",
      "Iteration 8918, loss = 0.15996323\n",
      "Iteration 8919, loss = 0.15995770\n",
      "Iteration 8920, loss = 0.15995345\n",
      "Iteration 8921, loss = 0.15994483\n",
      "Iteration 8922, loss = 0.15993386\n",
      "Iteration 8923, loss = 0.15992997\n",
      "Iteration 8924, loss = 0.15992255\n",
      "Iteration 8925, loss = 0.15991378\n",
      "Iteration 8926, loss = 0.15990397\n",
      "Iteration 8927, loss = 0.15989925\n",
      "Iteration 8928, loss = 0.15989431\n",
      "Iteration 8929, loss = 0.15988504\n",
      "Iteration 8930, loss = 0.15987401\n",
      "Iteration 8931, loss = 0.15986482\n",
      "Iteration 8932, loss = 0.15986282\n",
      "Iteration 8933, loss = 0.15985341\n",
      "Iteration 8934, loss = 0.15985025\n",
      "Iteration 8935, loss = 0.15984269\n",
      "Iteration 8936, loss = 0.15983226\n",
      "Iteration 8937, loss = 0.15982190\n",
      "Iteration 8938, loss = 0.15981112\n",
      "Iteration 8939, loss = 0.15980490\n",
      "Iteration 8940, loss = 0.15979795\n",
      "Iteration 8941, loss = 0.15978750\n",
      "Iteration 8942, loss = 0.15977826\n",
      "Iteration 8943, loss = 0.15976960\n",
      "Iteration 8944, loss = 0.15976090\n",
      "Iteration 8945, loss = 0.15975514\n",
      "Iteration 8946, loss = 0.15975029\n",
      "Iteration 8947, loss = 0.15973781\n",
      "Iteration 8948, loss = 0.15972782\n",
      "Iteration 8949, loss = 0.15972446\n",
      "Iteration 8950, loss = 0.15971407\n",
      "Iteration 8951, loss = 0.15970969\n",
      "Iteration 8952, loss = 0.15970148\n",
      "Iteration 8953, loss = 0.15969044\n",
      "Iteration 8954, loss = 0.15968173\n",
      "Iteration 8955, loss = 0.15967377\n",
      "Iteration 8956, loss = 0.15966746\n",
      "Iteration 8957, loss = 0.15965978\n",
      "Iteration 8958, loss = 0.15965289\n",
      "Iteration 8959, loss = 0.15964541\n",
      "Iteration 8960, loss = 0.15963698\n",
      "Iteration 8961, loss = 0.15962741\n",
      "Iteration 8962, loss = 0.15961925\n",
      "Iteration 8963, loss = 0.15961396\n",
      "Iteration 8964, loss = 0.15960817\n",
      "Iteration 8965, loss = 0.15960094\n",
      "Iteration 8966, loss = 0.15959091\n",
      "Iteration 8967, loss = 0.15958604\n",
      "Iteration 8968, loss = 0.15958039\n",
      "Iteration 8969, loss = 0.15957003\n",
      "Iteration 8970, loss = 0.15956178\n",
      "Iteration 8971, loss = 0.15955245\n",
      "Iteration 8972, loss = 0.15954677\n",
      "Iteration 8973, loss = 0.15954049\n",
      "Iteration 8974, loss = 0.15952867\n",
      "Iteration 8975, loss = 0.15952214\n",
      "Iteration 8976, loss = 0.15951731\n",
      "Iteration 8977, loss = 0.15951214\n",
      "Iteration 8978, loss = 0.15950223\n",
      "Iteration 8979, loss = 0.15949466\n",
      "Iteration 8980, loss = 0.15948672\n",
      "Iteration 8981, loss = 0.15948286\n",
      "Iteration 8982, loss = 0.15947379\n",
      "Iteration 8983, loss = 0.15945892\n",
      "Iteration 8984, loss = 0.15945626\n",
      "Iteration 8985, loss = 0.15945056\n",
      "Iteration 8986, loss = 0.15944288\n",
      "Iteration 8987, loss = 0.15943102\n",
      "Iteration 8988, loss = 0.15942420\n",
      "Iteration 8989, loss = 0.15941279\n",
      "Iteration 8990, loss = 0.15940641\n",
      "Iteration 8991, loss = 0.15940393\n",
      "Iteration 8992, loss = 0.15939611\n",
      "Iteration 8993, loss = 0.15938129\n",
      "Iteration 8994, loss = 0.15938038\n",
      "Iteration 8995, loss = 0.15937217\n",
      "Iteration 8996, loss = 0.15936605\n",
      "Iteration 8997, loss = 0.15935883\n",
      "Iteration 8998, loss = 0.15934939\n",
      "Iteration 8999, loss = 0.15933798\n",
      "Iteration 9000, loss = 0.15933509\n",
      "Iteration 9001, loss = 0.15932899\n",
      "Iteration 9002, loss = 0.15932050\n",
      "Iteration 9003, loss = 0.15930998\n",
      "Iteration 9004, loss = 0.15930390\n",
      "Iteration 9005, loss = 0.15929733\n",
      "Iteration 9006, loss = 0.15928731\n",
      "Iteration 9007, loss = 0.15928014\n",
      "Iteration 9008, loss = 0.15928021\n",
      "Iteration 9009, loss = 0.15927257\n",
      "Iteration 9010, loss = 0.15926380\n",
      "Iteration 9011, loss = 0.15925033\n",
      "Iteration 9012, loss = 0.15924347\n",
      "Iteration 9013, loss = 0.15923758\n",
      "Iteration 9014, loss = 0.15923354\n",
      "Iteration 9015, loss = 0.15922788\n",
      "Iteration 9016, loss = 0.15922206\n",
      "Iteration 9017, loss = 0.15920865\n",
      "Iteration 9018, loss = 0.15919944\n",
      "Iteration 9019, loss = 0.15919077\n",
      "Iteration 9020, loss = 0.15918038\n",
      "Iteration 9021, loss = 0.15917769\n",
      "Iteration 9022, loss = 0.15916782\n",
      "Iteration 9023, loss = 0.15915659\n",
      "Iteration 9024, loss = 0.15914832\n",
      "Iteration 9025, loss = 0.15913916\n",
      "Iteration 9026, loss = 0.15913158\n",
      "Iteration 9027, loss = 0.15912864\n",
      "Iteration 9028, loss = 0.15912371\n",
      "Iteration 9029, loss = 0.15911452\n",
      "Iteration 9030, loss = 0.15910586\n",
      "Iteration 9031, loss = 0.15910143\n",
      "Iteration 9032, loss = 0.15909741\n",
      "Iteration 9033, loss = 0.15909174\n",
      "Iteration 9034, loss = 0.15907606\n",
      "Iteration 9035, loss = 0.15907016\n",
      "Iteration 9036, loss = 0.15906524\n",
      "Iteration 9037, loss = 0.15905601\n",
      "Iteration 9038, loss = 0.15904514\n",
      "Iteration 9039, loss = 0.15904633\n",
      "Iteration 9040, loss = 0.15904018\n",
      "Iteration 9041, loss = 0.15902720\n",
      "Iteration 9042, loss = 0.15901553\n",
      "Iteration 9043, loss = 0.15901176\n",
      "Iteration 9044, loss = 0.15900521\n",
      "Iteration 9045, loss = 0.15899798\n",
      "Iteration 9046, loss = 0.15899028\n",
      "Iteration 9047, loss = 0.15898374\n",
      "Iteration 9048, loss = 0.15897410\n",
      "Iteration 9049, loss = 0.15896411\n",
      "Iteration 9050, loss = 0.15896093\n",
      "Iteration 9051, loss = 0.15895342\n",
      "Iteration 9052, loss = 0.15894763\n",
      "Iteration 9053, loss = 0.15893672\n",
      "Iteration 9054, loss = 0.15892779\n",
      "Iteration 9055, loss = 0.15891965\n",
      "Iteration 9056, loss = 0.15891176\n",
      "Iteration 9057, loss = 0.15890889\n",
      "Iteration 9058, loss = 0.15889691\n",
      "Iteration 9059, loss = 0.15889401\n",
      "Iteration 9060, loss = 0.15888569\n",
      "Iteration 9061, loss = 0.15887572\n",
      "Iteration 9062, loss = 0.15886658\n",
      "Iteration 9063, loss = 0.15886359\n",
      "Iteration 9064, loss = 0.15885640\n",
      "Iteration 9065, loss = 0.15885265\n",
      "Iteration 9066, loss = 0.15884542\n",
      "Iteration 9067, loss = 0.15883383\n",
      "Iteration 9068, loss = 0.15882495\n",
      "Iteration 9069, loss = 0.15881875\n",
      "Iteration 9070, loss = 0.15881675\n",
      "Iteration 9071, loss = 0.15881747\n",
      "Iteration 9072, loss = 0.15880097\n",
      "Iteration 9073, loss = 0.15878558\n",
      "Iteration 9074, loss = 0.15878080\n",
      "Iteration 9075, loss = 0.15877446\n",
      "Iteration 9076, loss = 0.15876775\n",
      "Iteration 9077, loss = 0.15875837\n",
      "Iteration 9078, loss = 0.15874890\n",
      "Iteration 9079, loss = 0.15874012\n",
      "Iteration 9080, loss = 0.15873402\n",
      "Iteration 9081, loss = 0.15872398\n",
      "Iteration 9082, loss = 0.15871969\n",
      "Iteration 9083, loss = 0.15871508\n",
      "Iteration 9084, loss = 0.15870816\n",
      "Iteration 9085, loss = 0.15869728\n",
      "Iteration 9086, loss = 0.15869125\n",
      "Iteration 9087, loss = 0.15868114\n",
      "Iteration 9088, loss = 0.15867735\n",
      "Iteration 9089, loss = 0.15867159\n",
      "Iteration 9090, loss = 0.15866169\n",
      "Iteration 9091, loss = 0.15864971\n",
      "Iteration 9092, loss = 0.15864763\n",
      "Iteration 9093, loss = 0.15864299\n",
      "Iteration 9094, loss = 0.15863228\n",
      "Iteration 9095, loss = 0.15862428\n",
      "Iteration 9096, loss = 0.15861654\n",
      "Iteration 9097, loss = 0.15861155\n",
      "Iteration 9098, loss = 0.15860306\n",
      "Iteration 9099, loss = 0.15859913\n",
      "Iteration 9100, loss = 0.15858614\n",
      "Iteration 9101, loss = 0.15857575\n",
      "Iteration 9102, loss = 0.15857027\n",
      "Iteration 9103, loss = 0.15856480\n",
      "Iteration 9104, loss = 0.15855999\n",
      "Iteration 9105, loss = 0.15855733\n",
      "Iteration 9106, loss = 0.15854795\n",
      "Iteration 9107, loss = 0.15853827\n",
      "Iteration 9108, loss = 0.15853111\n",
      "Iteration 9109, loss = 0.15852879\n",
      "Iteration 9110, loss = 0.15852678\n",
      "Iteration 9111, loss = 0.15851601\n",
      "Iteration 9112, loss = 0.15850480\n",
      "Iteration 9113, loss = 0.15850170\n",
      "Iteration 9114, loss = 0.15849472\n",
      "Iteration 9115, loss = 0.15848983\n",
      "Iteration 9116, loss = 0.15847552\n",
      "Iteration 9117, loss = 0.15847123\n",
      "Iteration 9118, loss = 0.15846033\n",
      "Iteration 9119, loss = 0.15845332\n",
      "Iteration 9120, loss = 0.15845100\n",
      "Iteration 9121, loss = 0.15844155\n",
      "Iteration 9122, loss = 0.15842871\n",
      "Iteration 9123, loss = 0.15842642\n",
      "Iteration 9124, loss = 0.15841867\n",
      "Iteration 9125, loss = 0.15840726\n",
      "Iteration 9126, loss = 0.15839949\n",
      "Iteration 9127, loss = 0.15838964\n",
      "Iteration 9128, loss = 0.15838570\n",
      "Iteration 9129, loss = 0.15837847\n",
      "Iteration 9130, loss = 0.15836861\n",
      "Iteration 9131, loss = 0.15836047\n",
      "Iteration 9132, loss = 0.15835587\n",
      "Iteration 9133, loss = 0.15835107\n",
      "Iteration 9134, loss = 0.15834087\n",
      "Iteration 9135, loss = 0.15833656\n",
      "Iteration 9136, loss = 0.15833295\n",
      "Iteration 9137, loss = 0.15832276\n",
      "Iteration 9138, loss = 0.15831610\n",
      "Iteration 9139, loss = 0.15830930\n",
      "Iteration 9140, loss = 0.15830129\n",
      "Iteration 9141, loss = 0.15829325\n",
      "Iteration 9142, loss = 0.15828464\n",
      "Iteration 9143, loss = 0.15827921\n",
      "Iteration 9144, loss = 0.15827393\n",
      "Iteration 9145, loss = 0.15827028\n",
      "Iteration 9146, loss = 0.15825729\n",
      "Iteration 9147, loss = 0.15825176\n",
      "Iteration 9148, loss = 0.15824604\n",
      "Iteration 9149, loss = 0.15823864\n",
      "Iteration 9150, loss = 0.15822929\n",
      "Iteration 9151, loss = 0.15822437\n",
      "Iteration 9152, loss = 0.15821449\n",
      "Iteration 9153, loss = 0.15821094\n",
      "Iteration 9154, loss = 0.15821145\n",
      "Iteration 9155, loss = 0.15819951\n",
      "Iteration 9156, loss = 0.15818793\n",
      "Iteration 9157, loss = 0.15818556\n",
      "Iteration 9158, loss = 0.15817904\n",
      "Iteration 9159, loss = 0.15816845\n",
      "Iteration 9160, loss = 0.15816534\n",
      "Iteration 9161, loss = 0.15815784\n",
      "Iteration 9162, loss = 0.15814386\n",
      "Iteration 9163, loss = 0.15814243\n",
      "Iteration 9164, loss = 0.15814020\n",
      "Iteration 9165, loss = 0.15813221\n",
      "Iteration 9166, loss = 0.15811773\n",
      "Iteration 9167, loss = 0.15810744\n",
      "Iteration 9168, loss = 0.15810468\n",
      "Iteration 9169, loss = 0.15809792\n",
      "Iteration 9170, loss = 0.15808930\n",
      "Iteration 9171, loss = 0.15808080\n",
      "Iteration 9172, loss = 0.15807689\n",
      "Iteration 9173, loss = 0.15807048\n",
      "Iteration 9174, loss = 0.15806122\n",
      "Iteration 9175, loss = 0.15805198\n",
      "Iteration 9176, loss = 0.15804696\n",
      "Iteration 9177, loss = 0.15804034\n",
      "Iteration 9178, loss = 0.15803515\n",
      "Iteration 9179, loss = 0.15802925\n",
      "Iteration 9180, loss = 0.15802371\n",
      "Iteration 9181, loss = 0.15801037\n",
      "Iteration 9182, loss = 0.15800463\n",
      "Iteration 9183, loss = 0.15800352\n",
      "Iteration 9184, loss = 0.15799694\n",
      "Iteration 9185, loss = 0.15798155\n",
      "Iteration 9186, loss = 0.15797877\n",
      "Iteration 9187, loss = 0.15797431\n",
      "Iteration 9188, loss = 0.15797134\n",
      "Iteration 9189, loss = 0.15796240\n",
      "Iteration 9190, loss = 0.15794949\n",
      "Iteration 9191, loss = 0.15794489\n",
      "Iteration 9192, loss = 0.15794279\n",
      "Iteration 9193, loss = 0.15793044\n",
      "Iteration 9194, loss = 0.15792009\n",
      "Iteration 9195, loss = 0.15791594\n",
      "Iteration 9196, loss = 0.15790690\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9197, loss = 0.15789832\n",
      "Iteration 9198, loss = 0.15789795\n",
      "Iteration 9199, loss = 0.15789023\n",
      "Iteration 9200, loss = 0.15788137\n",
      "Iteration 9201, loss = 0.15787455\n",
      "Iteration 9202, loss = 0.15786730\n",
      "Iteration 9203, loss = 0.15786013\n",
      "Iteration 9204, loss = 0.15784966\n",
      "Iteration 9205, loss = 0.15784817\n",
      "Iteration 9206, loss = 0.15784090\n",
      "Iteration 9207, loss = 0.15783504\n",
      "Iteration 9208, loss = 0.15782745\n",
      "Iteration 9209, loss = 0.15782453\n",
      "Iteration 9210, loss = 0.15781248\n",
      "Iteration 9211, loss = 0.15780164\n",
      "Iteration 9212, loss = 0.15780071\n",
      "Iteration 9213, loss = 0.15779757\n",
      "Iteration 9214, loss = 0.15778588\n",
      "Iteration 9215, loss = 0.15777676\n",
      "Iteration 9216, loss = 0.15777312\n",
      "Iteration 9217, loss = 0.15776623\n",
      "Iteration 9218, loss = 0.15775954\n",
      "Iteration 9219, loss = 0.15775311\n",
      "Iteration 9220, loss = 0.15774647\n",
      "Iteration 9221, loss = 0.15773532\n",
      "Iteration 9222, loss = 0.15772955\n",
      "Iteration 9223, loss = 0.15772547\n",
      "Iteration 9224, loss = 0.15771911\n",
      "Iteration 9225, loss = 0.15771011\n",
      "Iteration 9226, loss = 0.15770219\n",
      "Iteration 9227, loss = 0.15769354\n",
      "Iteration 9228, loss = 0.15768927\n",
      "Iteration 9229, loss = 0.15768420\n",
      "Iteration 9230, loss = 0.15767926\n",
      "Iteration 9231, loss = 0.15767009\n",
      "Iteration 9232, loss = 0.15766739\n",
      "Iteration 9233, loss = 0.15766148\n",
      "Iteration 9234, loss = 0.15764781\n",
      "Iteration 9235, loss = 0.15764131\n",
      "Iteration 9236, loss = 0.15763705\n",
      "Iteration 9237, loss = 0.15763265\n",
      "Iteration 9238, loss = 0.15762237\n",
      "Iteration 9239, loss = 0.15761444\n",
      "Iteration 9240, loss = 0.15760525\n",
      "Iteration 9241, loss = 0.15760237\n",
      "Iteration 9242, loss = 0.15759674\n",
      "Iteration 9243, loss = 0.15758789\n",
      "Iteration 9244, loss = 0.15757872\n",
      "Iteration 9245, loss = 0.15757446\n",
      "Iteration 9246, loss = 0.15757092\n",
      "Iteration 9247, loss = 0.15755793\n",
      "Iteration 9248, loss = 0.15755281\n",
      "Iteration 9249, loss = 0.15754728\n",
      "Iteration 9250, loss = 0.15754161\n",
      "Iteration 9251, loss = 0.15753357\n",
      "Iteration 9252, loss = 0.15752576\n",
      "Iteration 9253, loss = 0.15751487\n",
      "Iteration 9254, loss = 0.15750861\n",
      "Iteration 9255, loss = 0.15750414\n",
      "Iteration 9256, loss = 0.15749879\n",
      "Iteration 9257, loss = 0.15748864\n",
      "Iteration 9258, loss = 0.15747714\n",
      "Iteration 9259, loss = 0.15747622\n",
      "Iteration 9260, loss = 0.15746656\n",
      "Iteration 9261, loss = 0.15746681\n",
      "Iteration 9262, loss = 0.15746290\n",
      "Iteration 9263, loss = 0.15745069\n",
      "Iteration 9264, loss = 0.15744331\n",
      "Iteration 9265, loss = 0.15743708\n",
      "Iteration 9266, loss = 0.15743602\n",
      "Iteration 9267, loss = 0.15742795\n",
      "Iteration 9268, loss = 0.15741878\n",
      "Iteration 9269, loss = 0.15741570\n",
      "Iteration 9270, loss = 0.15741015\n",
      "Iteration 9271, loss = 0.15740188\n",
      "Iteration 9272, loss = 0.15739101\n",
      "Iteration 9273, loss = 0.15738284\n",
      "Iteration 9274, loss = 0.15737625\n",
      "Iteration 9275, loss = 0.15737510\n",
      "Iteration 9276, loss = 0.15736883\n",
      "Iteration 9277, loss = 0.15735601\n",
      "Iteration 9278, loss = 0.15734893\n",
      "Iteration 9279, loss = 0.15734798\n",
      "Iteration 9280, loss = 0.15734273\n",
      "Iteration 9281, loss = 0.15733491\n",
      "Iteration 9282, loss = 0.15732505\n",
      "Iteration 9283, loss = 0.15732247\n",
      "Iteration 9284, loss = 0.15731628\n",
      "Iteration 9285, loss = 0.15730748\n",
      "Iteration 9286, loss = 0.15729843\n",
      "Iteration 9287, loss = 0.15728653\n",
      "Iteration 9288, loss = 0.15728478\n",
      "Iteration 9289, loss = 0.15728297\n",
      "Iteration 9290, loss = 0.15726951\n",
      "Iteration 9291, loss = 0.15725817\n",
      "Iteration 9292, loss = 0.15725821\n",
      "Iteration 9293, loss = 0.15725373\n",
      "Iteration 9294, loss = 0.15724586\n",
      "Iteration 9295, loss = 0.15724349\n",
      "Iteration 9296, loss = 0.15723307\n",
      "Iteration 9297, loss = 0.15722156\n",
      "Iteration 9298, loss = 0.15721523\n",
      "Iteration 9299, loss = 0.15720920\n",
      "Iteration 9300, loss = 0.15719856\n",
      "Iteration 9301, loss = 0.15718992\n",
      "Iteration 9302, loss = 0.15718459\n",
      "Iteration 9303, loss = 0.15718225\n",
      "Iteration 9304, loss = 0.15717521\n",
      "Iteration 9305, loss = 0.15716571\n",
      "Iteration 9306, loss = 0.15716227\n",
      "Iteration 9307, loss = 0.15715806\n",
      "Iteration 9308, loss = 0.15714674\n",
      "Iteration 9309, loss = 0.15713861\n",
      "Iteration 9310, loss = 0.15713309\n",
      "Iteration 9311, loss = 0.15712750\n",
      "Iteration 9312, loss = 0.15712075\n",
      "Iteration 9313, loss = 0.15711031\n",
      "Iteration 9314, loss = 0.15710521\n",
      "Iteration 9315, loss = 0.15709931\n",
      "Iteration 9316, loss = 0.15709312\n",
      "Iteration 9317, loss = 0.15708898\n",
      "Iteration 9318, loss = 0.15708024\n",
      "Iteration 9319, loss = 0.15707081\n",
      "Iteration 9320, loss = 0.15706642\n",
      "Iteration 9321, loss = 0.15706328\n",
      "Iteration 9322, loss = 0.15705390\n",
      "Iteration 9323, loss = 0.15704506\n",
      "Iteration 9324, loss = 0.15703281\n",
      "Iteration 9325, loss = 0.15703181\n",
      "Iteration 9326, loss = 0.15702511\n",
      "Iteration 9327, loss = 0.15701534\n",
      "Iteration 9328, loss = 0.15701003\n",
      "Iteration 9329, loss = 0.15700513\n",
      "Iteration 9330, loss = 0.15700242\n",
      "Iteration 9331, loss = 0.15699120\n",
      "Iteration 9332, loss = 0.15698426\n",
      "Iteration 9333, loss = 0.15697836\n",
      "Iteration 9334, loss = 0.15697072\n",
      "Iteration 9335, loss = 0.15697035\n",
      "Iteration 9336, loss = 0.15696209\n",
      "Iteration 9337, loss = 0.15695402\n",
      "Iteration 9338, loss = 0.15694417\n",
      "Iteration 9339, loss = 0.15694521\n",
      "Iteration 9340, loss = 0.15693844\n",
      "Iteration 9341, loss = 0.15693156\n",
      "Iteration 9342, loss = 0.15692208\n",
      "Iteration 9343, loss = 0.15691410\n",
      "Iteration 9344, loss = 0.15690604\n",
      "Iteration 9345, loss = 0.15690048\n",
      "Iteration 9346, loss = 0.15689558\n",
      "Iteration 9347, loss = 0.15688708\n",
      "Iteration 9348, loss = 0.15688018\n",
      "Iteration 9349, loss = 0.15687375\n",
      "Iteration 9350, loss = 0.15686927\n",
      "Iteration 9351, loss = 0.15686224\n",
      "Iteration 9352, loss = 0.15685378\n",
      "Iteration 9353, loss = 0.15684753\n",
      "Iteration 9354, loss = 0.15684013\n",
      "Iteration 9355, loss = 0.15683516\n",
      "Iteration 9356, loss = 0.15683029\n",
      "Iteration 9357, loss = 0.15682385\n",
      "Iteration 9358, loss = 0.15681815\n",
      "Iteration 9359, loss = 0.15680775\n",
      "Iteration 9360, loss = 0.15680029\n",
      "Iteration 9361, loss = 0.15679620\n",
      "Iteration 9362, loss = 0.15679054\n",
      "Iteration 9363, loss = 0.15678424\n",
      "Iteration 9364, loss = 0.15677324\n",
      "Iteration 9365, loss = 0.15677155\n",
      "Iteration 9366, loss = 0.15676690\n",
      "Iteration 9367, loss = 0.15675765\n",
      "Iteration 9368, loss = 0.15675004\n",
      "Iteration 9369, loss = 0.15674575\n",
      "Iteration 9370, loss = 0.15674172\n",
      "Iteration 9371, loss = 0.15673496\n",
      "Iteration 9372, loss = 0.15672976\n",
      "Iteration 9373, loss = 0.15672413\n",
      "Iteration 9374, loss = 0.15671198\n",
      "Iteration 9375, loss = 0.15670517\n",
      "Iteration 9376, loss = 0.15670372\n",
      "Iteration 9377, loss = 0.15670026\n",
      "Iteration 9378, loss = 0.15669236\n",
      "Iteration 9379, loss = 0.15667666\n",
      "Iteration 9380, loss = 0.15667454\n",
      "Iteration 9381, loss = 0.15667065\n",
      "Iteration 9382, loss = 0.15666608\n",
      "Iteration 9383, loss = 0.15666181\n",
      "Iteration 9384, loss = 0.15665643\n",
      "Iteration 9385, loss = 0.15664723\n",
      "Iteration 9386, loss = 0.15663298\n",
      "Iteration 9387, loss = 0.15663276\n",
      "Iteration 9388, loss = 0.15663572\n",
      "Iteration 9389, loss = 0.15662935\n",
      "Iteration 9390, loss = 0.15661771\n",
      "Iteration 9391, loss = 0.15660504\n",
      "Iteration 9392, loss = 0.15659398\n",
      "Iteration 9393, loss = 0.15659057\n",
      "Iteration 9394, loss = 0.15658610\n",
      "Iteration 9395, loss = 0.15657920\n",
      "Iteration 9396, loss = 0.15657467\n",
      "Iteration 9397, loss = 0.15656592\n",
      "Iteration 9398, loss = 0.15655651\n",
      "Iteration 9399, loss = 0.15655153\n",
      "Iteration 9400, loss = 0.15655158\n",
      "Iteration 9401, loss = 0.15654618\n",
      "Iteration 9402, loss = 0.15653333\n",
      "Iteration 9403, loss = 0.15652351\n",
      "Iteration 9404, loss = 0.15652266\n",
      "Iteration 9405, loss = 0.15651542\n",
      "Iteration 9406, loss = 0.15650837\n",
      "Iteration 9407, loss = 0.15650103\n",
      "Iteration 9408, loss = 0.15649719\n",
      "Iteration 9409, loss = 0.15649214\n",
      "Iteration 9410, loss = 0.15648221\n",
      "Iteration 9411, loss = 0.15647162\n",
      "Iteration 9412, loss = 0.15646488\n",
      "Iteration 9413, loss = 0.15646131\n",
      "Iteration 9414, loss = 0.15645184\n",
      "Iteration 9415, loss = 0.15645024\n",
      "Iteration 9416, loss = 0.15644105\n",
      "Iteration 9417, loss = 0.15643467\n",
      "Iteration 9418, loss = 0.15642968\n",
      "Iteration 9419, loss = 0.15642190\n",
      "Iteration 9420, loss = 0.15641632\n",
      "Iteration 9421, loss = 0.15641002\n",
      "Iteration 9422, loss = 0.15640415\n",
      "Iteration 9423, loss = 0.15639767\n",
      "Iteration 9424, loss = 0.15638970\n",
      "Iteration 9425, loss = 0.15638121\n",
      "Iteration 9426, loss = 0.15637851\n",
      "Iteration 9427, loss = 0.15637209\n",
      "Iteration 9428, loss = 0.15636039\n",
      "Iteration 9429, loss = 0.15635158\n",
      "Iteration 9430, loss = 0.15635093\n",
      "Iteration 9431, loss = 0.15634129\n",
      "Iteration 9432, loss = 0.15633570\n",
      "Iteration 9433, loss = 0.15633129\n",
      "Iteration 9434, loss = 0.15632363\n",
      "Iteration 9435, loss = 0.15631560\n",
      "Iteration 9436, loss = 0.15631530\n",
      "Iteration 9437, loss = 0.15630910\n",
      "Iteration 9438, loss = 0.15630068\n",
      "Iteration 9439, loss = 0.15628909\n",
      "Iteration 9440, loss = 0.15628694\n",
      "Iteration 9441, loss = 0.15628306\n",
      "Iteration 9442, loss = 0.15627726\n",
      "Iteration 9443, loss = 0.15627218\n",
      "Iteration 9444, loss = 0.15626017\n",
      "Iteration 9445, loss = 0.15625400\n",
      "Iteration 9446, loss = 0.15624774\n",
      "Iteration 9447, loss = 0.15624519\n",
      "Iteration 9448, loss = 0.15623590\n",
      "Iteration 9449, loss = 0.15622563\n",
      "Iteration 9450, loss = 0.15621867\n",
      "Iteration 9451, loss = 0.15621606\n",
      "Iteration 9452, loss = 0.15620779\n",
      "Iteration 9453, loss = 0.15620088\n",
      "Iteration 9454, loss = 0.15619353\n",
      "Iteration 9455, loss = 0.15618707\n",
      "Iteration 9456, loss = 0.15618535\n",
      "Iteration 9457, loss = 0.15617687\n",
      "Iteration 9458, loss = 0.15616930\n",
      "Iteration 9459, loss = 0.15616337\n",
      "Iteration 9460, loss = 0.15615664\n",
      "Iteration 9461, loss = 0.15615282\n",
      "Iteration 9462, loss = 0.15614765\n",
      "Iteration 9463, loss = 0.15614069\n",
      "Iteration 9464, loss = 0.15613418\n",
      "Iteration 9465, loss = 0.15613206\n",
      "Iteration 9466, loss = 0.15612712\n",
      "Iteration 9467, loss = 0.15611625\n",
      "Iteration 9468, loss = 0.15611119\n",
      "Iteration 9469, loss = 0.15610534\n",
      "Iteration 9470, loss = 0.15610020\n",
      "Iteration 9471, loss = 0.15609423\n",
      "Iteration 9472, loss = 0.15608386\n",
      "Iteration 9473, loss = 0.15607862\n",
      "Iteration 9474, loss = 0.15607244\n",
      "Iteration 9475, loss = 0.15606584\n",
      "Iteration 9476, loss = 0.15605921\n",
      "Iteration 9477, loss = 0.15605194\n",
      "Iteration 9478, loss = 0.15604762\n",
      "Iteration 9479, loss = 0.15603774\n",
      "Iteration 9480, loss = 0.15603548\n",
      "Iteration 9481, loss = 0.15602906\n",
      "Iteration 9482, loss = 0.15602245\n",
      "Iteration 9483, loss = 0.15601470\n",
      "Iteration 9484, loss = 0.15601116\n",
      "Iteration 9485, loss = 0.15600699\n",
      "Iteration 9486, loss = 0.15600126\n",
      "Iteration 9487, loss = 0.15599180\n",
      "Iteration 9488, loss = 0.15598739\n",
      "Iteration 9489, loss = 0.15598027\n",
      "Iteration 9490, loss = 0.15596754\n",
      "Iteration 9491, loss = 0.15596519\n",
      "Iteration 9492, loss = 0.15596263\n",
      "Iteration 9493, loss = 0.15595258\n",
      "Iteration 9494, loss = 0.15594523\n",
      "Iteration 9495, loss = 0.15594012\n",
      "Iteration 9496, loss = 0.15593430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9497, loss = 0.15592616\n",
      "Iteration 9498, loss = 0.15592034\n",
      "Iteration 9499, loss = 0.15591538\n",
      "Iteration 9500, loss = 0.15590978\n",
      "Iteration 9501, loss = 0.15590223\n",
      "Iteration 9502, loss = 0.15589531\n",
      "Iteration 9503, loss = 0.15588540\n",
      "Iteration 9504, loss = 0.15588351\n",
      "Iteration 9505, loss = 0.15588025\n",
      "Iteration 9506, loss = 0.15587338\n",
      "Iteration 9507, loss = 0.15586912\n",
      "Iteration 9508, loss = 0.15586560\n",
      "Iteration 9509, loss = 0.15585901\n",
      "Iteration 9510, loss = 0.15585082\n",
      "Iteration 9511, loss = 0.15584894\n",
      "Iteration 9512, loss = 0.15584090\n",
      "Iteration 9513, loss = 0.15583184\n",
      "Iteration 9514, loss = 0.15582548\n",
      "Iteration 9515, loss = 0.15582237\n",
      "Iteration 9516, loss = 0.15580994\n",
      "Iteration 9517, loss = 0.15580762\n",
      "Iteration 9518, loss = 0.15579963\n",
      "Iteration 9519, loss = 0.15579151\n",
      "Iteration 9520, loss = 0.15578652\n",
      "Iteration 9521, loss = 0.15578151\n",
      "Iteration 9522, loss = 0.15577688\n",
      "Iteration 9523, loss = 0.15576643\n",
      "Iteration 9524, loss = 0.15575745\n",
      "Iteration 9525, loss = 0.15575306\n",
      "Iteration 9526, loss = 0.15575226\n",
      "Iteration 9527, loss = 0.15574763\n",
      "Iteration 9528, loss = 0.15574084\n",
      "Iteration 9529, loss = 0.15573336\n",
      "Iteration 9530, loss = 0.15572568\n",
      "Iteration 9531, loss = 0.15571556\n",
      "Iteration 9532, loss = 0.15570610\n",
      "Iteration 9533, loss = 0.15570411\n",
      "Iteration 9534, loss = 0.15569917\n",
      "Iteration 9535, loss = 0.15569013\n",
      "Iteration 9536, loss = 0.15568121\n",
      "Iteration 9537, loss = 0.15567547\n",
      "Iteration 9538, loss = 0.15567053\n",
      "Iteration 9539, loss = 0.15566720\n",
      "Iteration 9540, loss = 0.15566055\n",
      "Iteration 9541, loss = 0.15564803\n",
      "Iteration 9542, loss = 0.15564318\n",
      "Iteration 9543, loss = 0.15564578\n",
      "Iteration 9544, loss = 0.15563987\n",
      "Iteration 9545, loss = 0.15562770\n",
      "Iteration 9546, loss = 0.15561400\n",
      "Iteration 9547, loss = 0.15561379\n",
      "Iteration 9548, loss = 0.15561258\n",
      "Iteration 9549, loss = 0.15560668\n",
      "Iteration 9550, loss = 0.15559789\n",
      "Iteration 9551, loss = 0.15558883\n",
      "Iteration 9552, loss = 0.15558054\n",
      "Iteration 9553, loss = 0.15558388\n",
      "Iteration 9554, loss = 0.15557512\n",
      "Iteration 9555, loss = 0.15556379\n",
      "Iteration 9556, loss = 0.15555981\n",
      "Iteration 9557, loss = 0.15555532\n",
      "Iteration 9558, loss = 0.15555273\n",
      "Iteration 9559, loss = 0.15554224\n",
      "Iteration 9560, loss = 0.15553813\n",
      "Iteration 9561, loss = 0.15553598\n",
      "Iteration 9562, loss = 0.15552457\n",
      "Iteration 9563, loss = 0.15551358\n",
      "Iteration 9564, loss = 0.15550457\n",
      "Iteration 9565, loss = 0.15550101\n",
      "Iteration 9566, loss = 0.15549619\n",
      "Iteration 9567, loss = 0.15549034\n",
      "Iteration 9568, loss = 0.15548491\n",
      "Iteration 9569, loss = 0.15547432\n",
      "Iteration 9570, loss = 0.15546687\n",
      "Iteration 9571, loss = 0.15546405\n",
      "Iteration 9572, loss = 0.15546042\n",
      "Iteration 9573, loss = 0.15545364\n",
      "Iteration 9574, loss = 0.15544625\n",
      "Iteration 9575, loss = 0.15543917\n",
      "Iteration 9576, loss = 0.15543456\n",
      "Iteration 9577, loss = 0.15543099\n",
      "Iteration 9578, loss = 0.15542212\n",
      "Iteration 9579, loss = 0.15541275\n",
      "Iteration 9580, loss = 0.15540779\n",
      "Iteration 9581, loss = 0.15540586\n",
      "Iteration 9582, loss = 0.15539645\n",
      "Iteration 9583, loss = 0.15539036\n",
      "Iteration 9584, loss = 0.15538563\n",
      "Iteration 9585, loss = 0.15537784\n",
      "Iteration 9586, loss = 0.15537379\n",
      "Iteration 9587, loss = 0.15536733\n",
      "Iteration 9588, loss = 0.15535570\n",
      "Iteration 9589, loss = 0.15535190\n",
      "Iteration 9590, loss = 0.15534534\n",
      "Iteration 9591, loss = 0.15534115\n",
      "Iteration 9592, loss = 0.15533736\n",
      "Iteration 9593, loss = 0.15532840\n",
      "Iteration 9594, loss = 0.15532207\n",
      "Iteration 9595, loss = 0.15531783\n",
      "Iteration 9596, loss = 0.15531228\n",
      "Iteration 9597, loss = 0.15530348\n",
      "Iteration 9598, loss = 0.15529794\n",
      "Iteration 9599, loss = 0.15529188\n",
      "Iteration 9600, loss = 0.15528432\n",
      "Iteration 9601, loss = 0.15527981\n",
      "Iteration 9602, loss = 0.15527872\n",
      "Iteration 9603, loss = 0.15527109\n",
      "Iteration 9604, loss = 0.15526448\n",
      "Iteration 9605, loss = 0.15525706\n",
      "Iteration 9606, loss = 0.15524929\n",
      "Iteration 9607, loss = 0.15524257\n",
      "Iteration 9608, loss = 0.15523724\n",
      "Iteration 9609, loss = 0.15522972\n",
      "Iteration 9610, loss = 0.15521966\n",
      "Iteration 9611, loss = 0.15521821\n",
      "Iteration 9612, loss = 0.15521648\n",
      "Iteration 9613, loss = 0.15520644\n",
      "Iteration 9614, loss = 0.15520169\n",
      "Iteration 9615, loss = 0.15519297\n",
      "Iteration 9616, loss = 0.15518836\n",
      "Iteration 9617, loss = 0.15518242\n",
      "Iteration 9618, loss = 0.15518031\n",
      "Iteration 9619, loss = 0.15517004\n",
      "Iteration 9620, loss = 0.15516487\n",
      "Iteration 9621, loss = 0.15516103\n",
      "Iteration 9622, loss = 0.15515554\n",
      "Iteration 9623, loss = 0.15514687\n",
      "Iteration 9624, loss = 0.15513972\n",
      "Iteration 9625, loss = 0.15513604\n",
      "Iteration 9626, loss = 0.15512627\n",
      "Iteration 9627, loss = 0.15511843\n",
      "Iteration 9628, loss = 0.15511747\n",
      "Iteration 9629, loss = 0.15511536\n",
      "Iteration 9630, loss = 0.15510615\n",
      "Iteration 9631, loss = 0.15509958\n",
      "Iteration 9632, loss = 0.15509255\n",
      "Iteration 9633, loss = 0.15508032\n",
      "Iteration 9634, loss = 0.15507778\n",
      "Iteration 9635, loss = 0.15506782\n",
      "Iteration 9636, loss = 0.15506447\n",
      "Iteration 9637, loss = 0.15505991\n",
      "Iteration 9638, loss = 0.15504768\n",
      "Iteration 9639, loss = 0.15504681\n",
      "Iteration 9640, loss = 0.15504167\n",
      "Iteration 9641, loss = 0.15503134\n",
      "Iteration 9642, loss = 0.15502375\n",
      "Iteration 9643, loss = 0.15501835\n",
      "Iteration 9644, loss = 0.15501351\n",
      "Iteration 9645, loss = 0.15500768\n",
      "Iteration 9646, loss = 0.15500157\n",
      "Iteration 9647, loss = 0.15499562\n",
      "Iteration 9648, loss = 0.15499085\n",
      "Iteration 9649, loss = 0.15498563\n",
      "Iteration 9650, loss = 0.15498164\n",
      "Iteration 9651, loss = 0.15497575\n",
      "Iteration 9652, loss = 0.15496694\n",
      "Iteration 9653, loss = 0.15496406\n",
      "Iteration 9654, loss = 0.15495574\n",
      "Iteration 9655, loss = 0.15494977\n",
      "Iteration 9656, loss = 0.15494150\n",
      "Iteration 9657, loss = 0.15493632\n",
      "Iteration 9658, loss = 0.15493057\n",
      "Iteration 9659, loss = 0.15492952\n",
      "Iteration 9660, loss = 0.15492245\n",
      "Iteration 9661, loss = 0.15491458\n",
      "Iteration 9662, loss = 0.15490428\n",
      "Iteration 9663, loss = 0.15490638\n",
      "Iteration 9664, loss = 0.15490090\n",
      "Iteration 9665, loss = 0.15489178\n",
      "Iteration 9666, loss = 0.15488357\n",
      "Iteration 9667, loss = 0.15487892\n",
      "Iteration 9668, loss = 0.15487193\n",
      "Iteration 9669, loss = 0.15487142\n",
      "Iteration 9670, loss = 0.15486698\n",
      "Iteration 9671, loss = 0.15485832\n",
      "Iteration 9672, loss = 0.15484519\n",
      "Iteration 9673, loss = 0.15484034\n",
      "Iteration 9674, loss = 0.15483695\n",
      "Iteration 9675, loss = 0.15483033\n",
      "Iteration 9676, loss = 0.15481745\n",
      "Iteration 9677, loss = 0.15481557\n",
      "Iteration 9678, loss = 0.15481607\n",
      "Iteration 9679, loss = 0.15481111\n",
      "Iteration 9680, loss = 0.15480047\n",
      "Iteration 9681, loss = 0.15479397\n",
      "Iteration 9682, loss = 0.15478931\n",
      "Iteration 9683, loss = 0.15478146\n",
      "Iteration 9684, loss = 0.15477566\n",
      "Iteration 9685, loss = 0.15476943\n",
      "Iteration 9686, loss = 0.15476174\n",
      "Iteration 9687, loss = 0.15475589\n",
      "Iteration 9688, loss = 0.15475190\n",
      "Iteration 9689, loss = 0.15474389\n",
      "Iteration 9690, loss = 0.15473529\n",
      "Iteration 9691, loss = 0.15472847\n",
      "Iteration 9692, loss = 0.15472384\n",
      "Iteration 9693, loss = 0.15472077\n",
      "Iteration 9694, loss = 0.15471005\n",
      "Iteration 9695, loss = 0.15470669\n",
      "Iteration 9696, loss = 0.15470202\n",
      "Iteration 9697, loss = 0.15469606\n",
      "Iteration 9698, loss = 0.15469132\n",
      "Iteration 9699, loss = 0.15468400\n",
      "Iteration 9700, loss = 0.15467713\n",
      "Iteration 9701, loss = 0.15467440\n",
      "Iteration 9702, loss = 0.15466890\n",
      "Iteration 9703, loss = 0.15466258\n",
      "Iteration 9704, loss = 0.15465408\n",
      "Iteration 9705, loss = 0.15464993\n",
      "Iteration 9706, loss = 0.15464506\n",
      "Iteration 9707, loss = 0.15463566\n",
      "Iteration 9708, loss = 0.15462668\n",
      "Iteration 9709, loss = 0.15462164\n",
      "Iteration 9710, loss = 0.15461750\n",
      "Iteration 9711, loss = 0.15461289\n",
      "Iteration 9712, loss = 0.15460151\n",
      "Iteration 9713, loss = 0.15460143\n",
      "Iteration 9714, loss = 0.15459497\n",
      "Iteration 9715, loss = 0.15458800\n",
      "Iteration 9716, loss = 0.15458169\n",
      "Iteration 9717, loss = 0.15457695\n",
      "Iteration 9718, loss = 0.15456791\n",
      "Iteration 9719, loss = 0.15456539\n",
      "Iteration 9720, loss = 0.15455700\n",
      "Iteration 9721, loss = 0.15454977\n",
      "Iteration 9722, loss = 0.15454391\n",
      "Iteration 9723, loss = 0.15454158\n",
      "Iteration 9724, loss = 0.15453192\n",
      "Iteration 9725, loss = 0.15452516\n",
      "Iteration 9726, loss = 0.15451829\n",
      "Iteration 9727, loss = 0.15451444\n",
      "Iteration 9728, loss = 0.15450754\n",
      "Iteration 9729, loss = 0.15450224\n",
      "Iteration 9730, loss = 0.15449631\n",
      "Iteration 9731, loss = 0.15449212\n",
      "Iteration 9732, loss = 0.15448627\n",
      "Iteration 9733, loss = 0.15447986\n",
      "Iteration 9734, loss = 0.15447349\n",
      "Iteration 9735, loss = 0.15446879\n",
      "Iteration 9736, loss = 0.15446285\n",
      "Iteration 9737, loss = 0.15445451\n",
      "Iteration 9738, loss = 0.15444570\n",
      "Iteration 9739, loss = 0.15443744\n",
      "Iteration 9740, loss = 0.15442931\n",
      "Iteration 9741, loss = 0.15442723\n",
      "Iteration 9742, loss = 0.15442319\n",
      "Iteration 9743, loss = 0.15441263\n",
      "Iteration 9744, loss = 0.15440631\n",
      "Iteration 9745, loss = 0.15439869\n",
      "Iteration 9746, loss = 0.15438916\n",
      "Iteration 9747, loss = 0.15438100\n",
      "Iteration 9748, loss = 0.15436928\n",
      "Iteration 9749, loss = 0.15436852\n",
      "Iteration 9750, loss = 0.15435968\n",
      "Iteration 9751, loss = 0.15434934\n",
      "Iteration 9752, loss = 0.15434348\n",
      "Iteration 9753, loss = 0.15433679\n",
      "Iteration 9754, loss = 0.15432671\n",
      "Iteration 9755, loss = 0.15432048\n",
      "Iteration 9756, loss = 0.15431176\n",
      "Iteration 9757, loss = 0.15430651\n",
      "Iteration 9758, loss = 0.15430032\n",
      "Iteration 9759, loss = 0.15429384\n",
      "Iteration 9760, loss = 0.15428437\n",
      "Iteration 9761, loss = 0.15427677\n",
      "Iteration 9762, loss = 0.15427191\n",
      "Iteration 9763, loss = 0.15426638\n",
      "Iteration 9764, loss = 0.15426086\n",
      "Iteration 9765, loss = 0.15425197\n",
      "Iteration 9766, loss = 0.15424504\n",
      "Iteration 9767, loss = 0.15423905\n",
      "Iteration 9768, loss = 0.15423621\n",
      "Iteration 9769, loss = 0.15422700\n",
      "Iteration 9770, loss = 0.15421848\n",
      "Iteration 9771, loss = 0.15421055\n",
      "Iteration 9772, loss = 0.15420725\n",
      "Iteration 9773, loss = 0.15420192\n",
      "Iteration 9774, loss = 0.15419188\n",
      "Iteration 9775, loss = 0.15418808\n",
      "Iteration 9776, loss = 0.15417770\n",
      "Iteration 9777, loss = 0.15416796\n",
      "Iteration 9778, loss = 0.15416300\n",
      "Iteration 9779, loss = 0.15415810\n",
      "Iteration 9780, loss = 0.15415209\n",
      "Iteration 9781, loss = 0.15414512\n",
      "Iteration 9782, loss = 0.15413653\n",
      "Iteration 9783, loss = 0.15412451\n",
      "Iteration 9784, loss = 0.15412215\n",
      "Iteration 9785, loss = 0.15411422\n",
      "Iteration 9786, loss = 0.15410882\n",
      "Iteration 9787, loss = 0.15409920\n",
      "Iteration 9788, loss = 0.15408866\n",
      "Iteration 9789, loss = 0.15408002\n",
      "Iteration 9790, loss = 0.15407380\n",
      "Iteration 9791, loss = 0.15407111\n",
      "Iteration 9792, loss = 0.15406510\n",
      "Iteration 9793, loss = 0.15405700\n",
      "Iteration 9794, loss = 0.15404806\n",
      "Iteration 9795, loss = 0.15404090\n",
      "Iteration 9796, loss = 0.15403236\n",
      "Iteration 9797, loss = 0.15403404\n",
      "Iteration 9798, loss = 0.15402505\n",
      "Iteration 9799, loss = 0.15401942\n",
      "Iteration 9800, loss = 0.15401111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9801, loss = 0.15400670\n",
      "Iteration 9802, loss = 0.15400106\n",
      "Iteration 9803, loss = 0.15399236\n",
      "Iteration 9804, loss = 0.15398350\n",
      "Iteration 9805, loss = 0.15397428\n",
      "Iteration 9806, loss = 0.15397319\n",
      "Iteration 9807, loss = 0.15396260\n",
      "Iteration 9808, loss = 0.15395661\n",
      "Iteration 9809, loss = 0.15395121\n",
      "Iteration 9810, loss = 0.15395112\n",
      "Iteration 9811, loss = 0.15394318\n",
      "Iteration 9812, loss = 0.15393438\n",
      "Iteration 9813, loss = 0.15392327\n",
      "Iteration 9814, loss = 0.15392013\n",
      "Iteration 9815, loss = 0.15391706\n",
      "Iteration 9816, loss = 0.15390980\n",
      "Iteration 9817, loss = 0.15390047\n",
      "Iteration 9818, loss = 0.15388917\n",
      "Iteration 9819, loss = 0.15388459\n",
      "Iteration 9820, loss = 0.15388101\n",
      "Iteration 9821, loss = 0.15387374\n",
      "Iteration 9822, loss = 0.15386383\n",
      "Iteration 9823, loss = 0.15385895\n",
      "Iteration 9824, loss = 0.15385294\n",
      "Iteration 9825, loss = 0.15384323\n",
      "Iteration 9826, loss = 0.15383879\n",
      "Iteration 9827, loss = 0.15382933\n",
      "Iteration 9828, loss = 0.15382725\n",
      "Iteration 9829, loss = 0.15382110\n",
      "Iteration 9830, loss = 0.15381120\n",
      "Iteration 9831, loss = 0.15380495\n",
      "Iteration 9832, loss = 0.15380291\n",
      "Iteration 9833, loss = 0.15379370\n",
      "Iteration 9834, loss = 0.15378645\n",
      "Iteration 9835, loss = 0.15378181\n",
      "Iteration 9836, loss = 0.15377704\n",
      "Iteration 9837, loss = 0.15376650\n",
      "Iteration 9838, loss = 0.15375725\n",
      "Iteration 9839, loss = 0.15375478\n",
      "Iteration 9840, loss = 0.15375222\n",
      "Iteration 9841, loss = 0.15374638\n",
      "Iteration 9842, loss = 0.15373746\n",
      "Iteration 9843, loss = 0.15373423\n",
      "Iteration 9844, loss = 0.15372897\n",
      "Iteration 9845, loss = 0.15372283\n",
      "Iteration 9846, loss = 0.15371440\n",
      "Iteration 9847, loss = 0.15370483\n",
      "Iteration 9848, loss = 0.15370018\n",
      "Iteration 9849, loss = 0.15369482\n",
      "Iteration 9850, loss = 0.15369479\n",
      "Iteration 9851, loss = 0.15368111\n",
      "Iteration 9852, loss = 0.15367523\n",
      "Iteration 9853, loss = 0.15367524\n",
      "Iteration 9854, loss = 0.15367141\n",
      "Iteration 9855, loss = 0.15366325\n",
      "Iteration 9856, loss = 0.15365705\n",
      "Iteration 9857, loss = 0.15364632\n",
      "Iteration 9858, loss = 0.15364398\n",
      "Iteration 9859, loss = 0.15363826\n",
      "Iteration 9860, loss = 0.15363347\n",
      "Iteration 9861, loss = 0.15362845\n",
      "Iteration 9862, loss = 0.15361913\n",
      "Iteration 9863, loss = 0.15361346\n",
      "Iteration 9864, loss = 0.15360802\n",
      "Iteration 9865, loss = 0.15360374\n",
      "Iteration 9866, loss = 0.15359472\n",
      "Iteration 9867, loss = 0.15359198\n",
      "Iteration 9868, loss = 0.15358797\n",
      "Iteration 9869, loss = 0.15358146\n",
      "Iteration 9870, loss = 0.15357430\n",
      "Iteration 9871, loss = 0.15356816\n",
      "Iteration 9872, loss = 0.15356442\n",
      "Iteration 9873, loss = 0.15355832\n",
      "Iteration 9874, loss = 0.15354981\n",
      "Iteration 9875, loss = 0.15354358\n",
      "Iteration 9876, loss = 0.15353874\n",
      "Iteration 9877, loss = 0.15353099\n",
      "Iteration 9878, loss = 0.15352479\n",
      "Iteration 9879, loss = 0.15352241\n",
      "Iteration 9880, loss = 0.15351479\n",
      "Iteration 9881, loss = 0.15350805\n",
      "Iteration 9882, loss = 0.15350151\n",
      "Iteration 9883, loss = 0.15349837\n",
      "Iteration 9884, loss = 0.15349208\n",
      "Iteration 9885, loss = 0.15348560\n",
      "Iteration 9886, loss = 0.15347681\n",
      "Iteration 9887, loss = 0.15347184\n",
      "Iteration 9888, loss = 0.15346859\n",
      "Iteration 9889, loss = 0.15346715\n",
      "Iteration 9890, loss = 0.15345636\n",
      "Iteration 9891, loss = 0.15344950\n",
      "Iteration 9892, loss = 0.15344284\n",
      "Iteration 9893, loss = 0.15343924\n",
      "Iteration 9894, loss = 0.15343574\n",
      "Iteration 9895, loss = 0.15342537\n",
      "Iteration 9896, loss = 0.15342268\n",
      "Iteration 9897, loss = 0.15341951\n",
      "Iteration 9898, loss = 0.15341327\n",
      "Iteration 9899, loss = 0.15340609\n",
      "Iteration 9900, loss = 0.15339598\n",
      "Iteration 9901, loss = 0.15339589\n",
      "Iteration 9902, loss = 0.15339446\n",
      "Iteration 9903, loss = 0.15339116\n",
      "Iteration 9904, loss = 0.15338012\n",
      "Iteration 9905, loss = 0.15336689\n",
      "Iteration 9906, loss = 0.15336660\n",
      "Iteration 9907, loss = 0.15336434\n",
      "Iteration 9908, loss = 0.15335635\n",
      "Iteration 9909, loss = 0.15334785\n",
      "Iteration 9910, loss = 0.15334339\n",
      "Iteration 9911, loss = 0.15333616\n",
      "Iteration 9912, loss = 0.15332905\n",
      "Iteration 9913, loss = 0.15332583\n",
      "Iteration 9914, loss = 0.15332406\n",
      "Iteration 9915, loss = 0.15331754\n",
      "Iteration 9916, loss = 0.15330790\n",
      "Iteration 9917, loss = 0.15329626\n",
      "Iteration 9918, loss = 0.15329512\n",
      "Iteration 9919, loss = 0.15329016\n",
      "Iteration 9920, loss = 0.15328156\n",
      "Iteration 9921, loss = 0.15327734\n",
      "Iteration 9922, loss = 0.15327236\n",
      "Iteration 9923, loss = 0.15326276\n",
      "Iteration 9924, loss = 0.15325625\n",
      "Iteration 9925, loss = 0.15325315\n",
      "Iteration 9926, loss = 0.15324657\n",
      "Iteration 9927, loss = 0.15324285\n",
      "Iteration 9928, loss = 0.15323756\n",
      "Iteration 9929, loss = 0.15322620\n",
      "Iteration 9930, loss = 0.15322196\n",
      "Iteration 9931, loss = 0.15322098\n",
      "Iteration 9932, loss = 0.15321229\n",
      "Iteration 9933, loss = 0.15320034\n",
      "Iteration 9934, loss = 0.15319494\n",
      "Iteration 9935, loss = 0.15319010\n",
      "Iteration 9936, loss = 0.15318661\n",
      "Iteration 9937, loss = 0.15317846\n",
      "Iteration 9938, loss = 0.15317308\n",
      "Iteration 9939, loss = 0.15316949\n",
      "Iteration 9940, loss = 0.15316285\n",
      "Iteration 9941, loss = 0.15315914\n",
      "Iteration 9942, loss = 0.15315446\n",
      "Iteration 9943, loss = 0.15314747\n",
      "Iteration 9944, loss = 0.15314058\n",
      "Iteration 9945, loss = 0.15313601\n",
      "Iteration 9946, loss = 0.15313271\n",
      "Iteration 9947, loss = 0.15312893\n",
      "Iteration 9948, loss = 0.15312442\n",
      "Iteration 9949, loss = 0.15311884\n",
      "Iteration 9950, loss = 0.15311139\n",
      "Iteration 9951, loss = 0.15310151\n",
      "Iteration 9952, loss = 0.15309601\n",
      "Iteration 9953, loss = 0.15309233\n",
      "Iteration 9954, loss = 0.15308678\n",
      "Iteration 9955, loss = 0.15307892\n",
      "Iteration 9956, loss = 0.15307189\n",
      "Iteration 9957, loss = 0.15306747\n",
      "Iteration 9958, loss = 0.15306247\n",
      "Iteration 9959, loss = 0.15305562\n",
      "Iteration 9960, loss = 0.15305071\n",
      "Iteration 9961, loss = 0.15304321\n",
      "Iteration 9962, loss = 0.15303566\n",
      "Iteration 9963, loss = 0.15303308\n",
      "Iteration 9964, loss = 0.15303040\n",
      "Iteration 9965, loss = 0.15302183\n",
      "Iteration 9966, loss = 0.15301746\n",
      "Iteration 9967, loss = 0.15301087\n",
      "Iteration 9968, loss = 0.15300360\n",
      "Iteration 9969, loss = 0.15299791\n",
      "Iteration 9970, loss = 0.15299683\n",
      "Iteration 9971, loss = 0.15299250\n",
      "Iteration 9972, loss = 0.15298418\n",
      "Iteration 9973, loss = 0.15297562\n",
      "Iteration 9974, loss = 0.15297088\n",
      "Iteration 9975, loss = 0.15296859\n",
      "Iteration 9976, loss = 0.15295876\n",
      "Iteration 9977, loss = 0.15295114\n",
      "Iteration 9978, loss = 0.15294515\n",
      "Iteration 9979, loss = 0.15294198\n",
      "Iteration 9980, loss = 0.15293342\n",
      "Iteration 9981, loss = 0.15292739\n",
      "Iteration 9982, loss = 0.15292494\n",
      "Iteration 9983, loss = 0.15291464\n",
      "Iteration 9984, loss = 0.15291004\n",
      "Iteration 9985, loss = 0.15290173\n",
      "Iteration 9986, loss = 0.15289818\n",
      "Iteration 9987, loss = 0.15288933\n",
      "Iteration 9988, loss = 0.15288928\n",
      "Iteration 9989, loss = 0.15288388\n",
      "Iteration 9990, loss = 0.15287471\n",
      "Iteration 9991, loss = 0.15287233\n",
      "Iteration 9992, loss = 0.15286829\n",
      "Iteration 9993, loss = 0.15286363\n",
      "Iteration 9994, loss = 0.15285232\n",
      "Iteration 9995, loss = 0.15284870\n",
      "Iteration 9996, loss = 0.15284744\n",
      "Iteration 9997, loss = 0.15284361\n",
      "Iteration 9998, loss = 0.15283329\n",
      "Iteration 9999, loss = 0.15282394\n",
      "Iteration 10000, loss = 0.15282155\n",
      "Iteration 10001, loss = 0.15281646\n",
      "Iteration 10002, loss = 0.15281085\n",
      "Iteration 10003, loss = 0.15279940\n",
      "Iteration 10004, loss = 0.15279421\n",
      "Iteration 10005, loss = 0.15278753\n",
      "Iteration 10006, loss = 0.15278403\n",
      "Iteration 10007, loss = 0.15277892\n",
      "Iteration 10008, loss = 0.15277047\n",
      "Iteration 10009, loss = 0.15276780\n",
      "Iteration 10010, loss = 0.15276208\n",
      "Iteration 10011, loss = 0.15275482\n",
      "Iteration 10012, loss = 0.15275647\n",
      "Iteration 10013, loss = 0.15274729\n",
      "Iteration 10014, loss = 0.15274075\n",
      "Iteration 10015, loss = 0.15273300\n",
      "Iteration 10016, loss = 0.15272598\n",
      "Iteration 10017, loss = 0.15272245\n",
      "Iteration 10018, loss = 0.15271997\n",
      "Iteration 10019, loss = 0.15270926\n",
      "Iteration 10020, loss = 0.15270307\n",
      "Iteration 10021, loss = 0.15269903\n",
      "Iteration 10022, loss = 0.15269332\n",
      "Iteration 10023, loss = 0.15268616\n",
      "Iteration 10024, loss = 0.15268151\n",
      "Iteration 10025, loss = 0.15267777\n",
      "Iteration 10026, loss = 0.15266927\n",
      "Iteration 10027, loss = 0.15266048\n",
      "Iteration 10028, loss = 0.15265742\n",
      "Iteration 10029, loss = 0.15265400\n",
      "Iteration 10030, loss = 0.15265012\n",
      "Iteration 10031, loss = 0.15263945\n",
      "Iteration 10032, loss = 0.15264014\n",
      "Iteration 10033, loss = 0.15263411\n",
      "Iteration 10034, loss = 0.15262854\n",
      "Iteration 10035, loss = 0.15262191\n",
      "Iteration 10036, loss = 0.15261804\n",
      "Iteration 10037, loss = 0.15260957\n",
      "Iteration 10038, loss = 0.15260678\n",
      "Iteration 10039, loss = 0.15260018\n",
      "Iteration 10040, loss = 0.15259024\n",
      "Iteration 10041, loss = 0.15258671\n",
      "Iteration 10042, loss = 0.15258223\n",
      "Iteration 10043, loss = 0.15257335\n",
      "Iteration 10044, loss = 0.15256751\n",
      "Iteration 10045, loss = 0.15256432\n",
      "Iteration 10046, loss = 0.15255545\n",
      "Iteration 10047, loss = 0.15255097\n",
      "Iteration 10048, loss = 0.15254803\n",
      "Iteration 10049, loss = 0.15254385\n",
      "Iteration 10050, loss = 0.15253754\n",
      "Iteration 10051, loss = 0.15253249\n",
      "Iteration 10052, loss = 0.15252798\n",
      "Iteration 10053, loss = 0.15252064\n",
      "Iteration 10054, loss = 0.15251464\n",
      "Iteration 10055, loss = 0.15251075\n",
      "Iteration 10056, loss = 0.15250378\n",
      "Iteration 10057, loss = 0.15250060\n",
      "Iteration 10058, loss = 0.15249922\n",
      "Iteration 10059, loss = 0.15248878\n",
      "Iteration 10060, loss = 0.15247978\n",
      "Iteration 10061, loss = 0.15247779\n",
      "Iteration 10062, loss = 0.15246966\n",
      "Iteration 10063, loss = 0.15246556\n",
      "Iteration 10064, loss = 0.15245698\n",
      "Iteration 10065, loss = 0.15244989\n",
      "Iteration 10066, loss = 0.15244770\n",
      "Iteration 10067, loss = 0.15243944\n",
      "Iteration 10068, loss = 0.15243486\n",
      "Iteration 10069, loss = 0.15242788\n",
      "Iteration 10070, loss = 0.15242297\n",
      "Iteration 10071, loss = 0.15241518\n",
      "Iteration 10072, loss = 0.15241358\n",
      "Iteration 10073, loss = 0.15241020\n",
      "Iteration 10074, loss = 0.15240539\n",
      "Iteration 10075, loss = 0.15239439\n",
      "Iteration 10076, loss = 0.15239062\n",
      "Iteration 10077, loss = 0.15238924\n",
      "Iteration 10078, loss = 0.15238406\n",
      "Iteration 10079, loss = 0.15237329\n",
      "Iteration 10080, loss = 0.15237157\n",
      "Iteration 10081, loss = 0.15236479\n",
      "Iteration 10082, loss = 0.15235935\n",
      "Iteration 10083, loss = 0.15235402\n",
      "Iteration 10084, loss = 0.15235223\n",
      "Iteration 10085, loss = 0.15234799\n",
      "Iteration 10086, loss = 0.15234172\n",
      "Iteration 10087, loss = 0.15233251\n",
      "Iteration 10088, loss = 0.15232122\n",
      "Iteration 10089, loss = 0.15232267\n",
      "Iteration 10090, loss = 0.15231722\n",
      "Iteration 10091, loss = 0.15231523\n",
      "Iteration 10092, loss = 0.15230602\n",
      "Iteration 10093, loss = 0.15229519\n",
      "Iteration 10094, loss = 0.15228822\n",
      "Iteration 10095, loss = 0.15228811\n",
      "Iteration 10096, loss = 0.15228579\n",
      "Iteration 10097, loss = 0.15227894\n",
      "Iteration 10098, loss = 0.15226653\n",
      "Iteration 10099, loss = 0.15226538\n",
      "Iteration 10100, loss = 0.15226452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10101, loss = 0.15226102\n",
      "Iteration 10102, loss = 0.15225173\n",
      "Iteration 10103, loss = 0.15224329\n",
      "Iteration 10104, loss = 0.15223904\n",
      "Iteration 10105, loss = 0.15223059\n",
      "Iteration 10106, loss = 0.15222805\n",
      "Iteration 10107, loss = 0.15222098\n",
      "Iteration 10108, loss = 0.15221433\n",
      "Iteration 10109, loss = 0.15220728\n",
      "Iteration 10110, loss = 0.15220244\n",
      "Iteration 10111, loss = 0.15219382\n",
      "Iteration 10112, loss = 0.15218756\n",
      "Iteration 10113, loss = 0.15218559\n",
      "Iteration 10114, loss = 0.15217854\n",
      "Iteration 10115, loss = 0.15216941\n",
      "Iteration 10116, loss = 0.15217070\n",
      "Iteration 10117, loss = 0.15216484\n",
      "Iteration 10118, loss = 0.15215730\n",
      "Iteration 10119, loss = 0.15215217\n",
      "Iteration 10120, loss = 0.15214884\n",
      "Iteration 10121, loss = 0.15214429\n",
      "Iteration 10122, loss = 0.15213436\n",
      "Iteration 10123, loss = 0.15213502\n",
      "Iteration 10124, loss = 0.15213267\n",
      "Iteration 10125, loss = 0.15212525\n",
      "Iteration 10126, loss = 0.15211828\n",
      "Iteration 10127, loss = 0.15211001\n",
      "Iteration 10128, loss = 0.15210578\n",
      "Iteration 10129, loss = 0.15210357\n",
      "Iteration 10130, loss = 0.15209888\n",
      "Iteration 10131, loss = 0.15208945\n",
      "Iteration 10132, loss = 0.15207851\n",
      "Iteration 10133, loss = 0.15207685\n",
      "Iteration 10134, loss = 0.15207230\n",
      "Iteration 10135, loss = 0.15206641\n",
      "Iteration 10136, loss = 0.15206022\n",
      "Iteration 10137, loss = 0.15205413\n",
      "Iteration 10138, loss = 0.15204909\n",
      "Iteration 10139, loss = 0.15204538\n",
      "Iteration 10140, loss = 0.15203942\n",
      "Iteration 10141, loss = 0.15203080\n",
      "Iteration 10142, loss = 0.15202464\n",
      "Iteration 10143, loss = 0.15201878\n",
      "Iteration 10144, loss = 0.15201257\n",
      "Iteration 10145, loss = 0.15200402\n",
      "Iteration 10146, loss = 0.15199754\n",
      "Iteration 10147, loss = 0.15199591\n",
      "Iteration 10148, loss = 0.15198848\n",
      "Iteration 10149, loss = 0.15198424\n",
      "Iteration 10150, loss = 0.15197958\n",
      "Iteration 10151, loss = 0.15197397\n",
      "Iteration 10152, loss = 0.15196698\n",
      "Iteration 10153, loss = 0.15196168\n",
      "Iteration 10154, loss = 0.15196139\n",
      "Iteration 10155, loss = 0.15195585\n",
      "Iteration 10156, loss = 0.15194825\n",
      "Iteration 10157, loss = 0.15193945\n",
      "Iteration 10158, loss = 0.15193583\n",
      "Iteration 10159, loss = 0.15193525\n",
      "Iteration 10160, loss = 0.15192846\n",
      "Iteration 10161, loss = 0.15192013\n",
      "Iteration 10162, loss = 0.15191754\n",
      "Iteration 10163, loss = 0.15191373\n",
      "Iteration 10164, loss = 0.15190595\n",
      "Iteration 10165, loss = 0.15190029\n",
      "Iteration 10166, loss = 0.15189782\n",
      "Iteration 10167, loss = 0.15189492\n",
      "Iteration 10168, loss = 0.15189088\n",
      "Iteration 10169, loss = 0.15187992\n",
      "Iteration 10170, loss = 0.15187288\n",
      "Iteration 10171, loss = 0.15187147\n",
      "Iteration 10172, loss = 0.15186934\n",
      "Iteration 10173, loss = 0.15186089\n",
      "Iteration 10174, loss = 0.15185554\n",
      "Iteration 10175, loss = 0.15184793\n",
      "Iteration 10176, loss = 0.15183976\n",
      "Iteration 10177, loss = 0.15183762\n",
      "Iteration 10178, loss = 0.15183111\n",
      "Iteration 10179, loss = 0.15182653\n",
      "Iteration 10180, loss = 0.15181916\n",
      "Iteration 10181, loss = 0.15181392\n",
      "Iteration 10182, loss = 0.15180871\n",
      "Iteration 10183, loss = 0.15180539\n",
      "Iteration 10184, loss = 0.15179730\n",
      "Iteration 10185, loss = 0.15179129\n",
      "Iteration 10186, loss = 0.15178653\n",
      "Iteration 10187, loss = 0.15177640\n",
      "Iteration 10188, loss = 0.15177187\n",
      "Iteration 10189, loss = 0.15176953\n",
      "Iteration 10190, loss = 0.15176057\n",
      "Iteration 10191, loss = 0.15175563\n",
      "Iteration 10192, loss = 0.15175160\n",
      "Iteration 10193, loss = 0.15174712\n",
      "Iteration 10194, loss = 0.15173766\n",
      "Iteration 10195, loss = 0.15173710\n",
      "Iteration 10196, loss = 0.15173304\n",
      "Iteration 10197, loss = 0.15172666\n",
      "Iteration 10198, loss = 0.15171563\n",
      "Iteration 10199, loss = 0.15171330\n",
      "Iteration 10200, loss = 0.15171044\n",
      "Iteration 10201, loss = 0.15170749\n",
      "Iteration 10202, loss = 0.15169621\n",
      "Iteration 10203, loss = 0.15169280\n",
      "Iteration 10204, loss = 0.15169201\n",
      "Iteration 10205, loss = 0.15168704\n",
      "Iteration 10206, loss = 0.15167455\n",
      "Iteration 10207, loss = 0.15167333\n",
      "Iteration 10208, loss = 0.15166935\n",
      "Iteration 10209, loss = 0.15166303\n",
      "Iteration 10210, loss = 0.15165373\n",
      "Iteration 10211, loss = 0.15164748\n",
      "Iteration 10212, loss = 0.15164353\n",
      "Iteration 10213, loss = 0.15163833\n",
      "Iteration 10214, loss = 0.15163301\n",
      "Iteration 10215, loss = 0.15162785\n",
      "Iteration 10216, loss = 0.15162026\n",
      "Iteration 10217, loss = 0.15162024\n",
      "Iteration 10218, loss = 0.15161765\n",
      "Iteration 10219, loss = 0.15160946\n",
      "Iteration 10220, loss = 0.15160093\n",
      "Iteration 10221, loss = 0.15159513\n",
      "Iteration 10222, loss = 0.15159009\n",
      "Iteration 10223, loss = 0.15158429\n",
      "Iteration 10224, loss = 0.15157509\n",
      "Iteration 10225, loss = 0.15157268\n",
      "Iteration 10226, loss = 0.15156536\n",
      "Iteration 10227, loss = 0.15156260\n",
      "Iteration 10228, loss = 0.15155928\n",
      "Iteration 10229, loss = 0.15155195\n",
      "Iteration 10230, loss = 0.15154403\n",
      "Iteration 10231, loss = 0.15154235\n",
      "Iteration 10232, loss = 0.15153604\n",
      "Iteration 10233, loss = 0.15153018\n",
      "Iteration 10234, loss = 0.15152507\n",
      "Iteration 10235, loss = 0.15152243\n",
      "Iteration 10236, loss = 0.15151477\n",
      "Iteration 10237, loss = 0.15151208\n",
      "Iteration 10238, loss = 0.15150716\n",
      "Iteration 10239, loss = 0.15150014\n",
      "Iteration 10240, loss = 0.15149665\n",
      "Iteration 10241, loss = 0.15149045\n",
      "Iteration 10242, loss = 0.15148328\n",
      "Iteration 10243, loss = 0.15147457\n",
      "Iteration 10244, loss = 0.15147143\n",
      "Iteration 10245, loss = 0.15146678\n",
      "Iteration 10246, loss = 0.15146205\n",
      "Iteration 10247, loss = 0.15146019\n",
      "Iteration 10248, loss = 0.15145363\n",
      "Iteration 10249, loss = 0.15144767\n",
      "Iteration 10250, loss = 0.15144737\n",
      "Iteration 10251, loss = 0.15144442\n",
      "Iteration 10252, loss = 0.15143765\n",
      "Iteration 10253, loss = 0.15142900\n",
      "Iteration 10254, loss = 0.15142396\n",
      "Iteration 10255, loss = 0.15141832\n",
      "Iteration 10256, loss = 0.15141183\n",
      "Iteration 10257, loss = 0.15140485\n",
      "Iteration 10258, loss = 0.15140135\n",
      "Iteration 10259, loss = 0.15139778\n",
      "Iteration 10260, loss = 0.15138821\n",
      "Iteration 10261, loss = 0.15138132\n",
      "Iteration 10262, loss = 0.15138032\n",
      "Iteration 10263, loss = 0.15137686\n",
      "Iteration 10264, loss = 0.15137142\n",
      "Iteration 10265, loss = 0.15136059\n",
      "Iteration 10266, loss = 0.15135324\n",
      "Iteration 10267, loss = 0.15135189\n",
      "Iteration 10268, loss = 0.15134735\n",
      "Iteration 10269, loss = 0.15134237\n",
      "Iteration 10270, loss = 0.15133827\n",
      "Iteration 10271, loss = 0.15133462\n",
      "Iteration 10272, loss = 0.15132885\n",
      "Iteration 10273, loss = 0.15132086\n",
      "Iteration 10274, loss = 0.15132051\n",
      "Iteration 10275, loss = 0.15131522\n",
      "Iteration 10276, loss = 0.15130968\n",
      "Iteration 10277, loss = 0.15129934\n",
      "Iteration 10278, loss = 0.15129145\n",
      "Iteration 10279, loss = 0.15128816\n",
      "Iteration 10280, loss = 0.15127948\n",
      "Iteration 10281, loss = 0.15127388\n",
      "Iteration 10282, loss = 0.15127391\n",
      "Iteration 10283, loss = 0.15126924\n",
      "Iteration 10284, loss = 0.15126656\n",
      "Iteration 10285, loss = 0.15126257\n",
      "Iteration 10286, loss = 0.15125445\n",
      "Iteration 10287, loss = 0.15124606\n",
      "Iteration 10288, loss = 0.15124406\n",
      "Iteration 10289, loss = 0.15123923\n",
      "Iteration 10290, loss = 0.15123411\n",
      "Iteration 10291, loss = 0.15122661\n",
      "Iteration 10292, loss = 0.15121520\n",
      "Iteration 10293, loss = 0.15121342\n",
      "Iteration 10294, loss = 0.15120964\n",
      "Iteration 10295, loss = 0.15120637\n",
      "Iteration 10296, loss = 0.15119838\n",
      "Iteration 10297, loss = 0.15119094\n",
      "Iteration 10298, loss = 0.15118414\n",
      "Iteration 10299, loss = 0.15118024\n",
      "Iteration 10300, loss = 0.15117663\n",
      "Iteration 10301, loss = 0.15117025\n",
      "Iteration 10302, loss = 0.15115960\n",
      "Iteration 10303, loss = 0.15116027\n",
      "Iteration 10304, loss = 0.15115377\n",
      "Iteration 10305, loss = 0.15115162\n",
      "Iteration 10306, loss = 0.15114602\n",
      "Iteration 10307, loss = 0.15113833\n",
      "Iteration 10308, loss = 0.15113470\n",
      "Iteration 10309, loss = 0.15113256\n",
      "Iteration 10310, loss = 0.15112906\n",
      "Iteration 10311, loss = 0.15112339\n",
      "Iteration 10312, loss = 0.15111532\n",
      "Iteration 10313, loss = 0.15110782\n",
      "Iteration 10314, loss = 0.15110770\n",
      "Iteration 10315, loss = 0.15110319\n",
      "Iteration 10316, loss = 0.15109377\n",
      "Iteration 10317, loss = 0.15108376\n",
      "Iteration 10318, loss = 0.15108002\n",
      "Iteration 10319, loss = 0.15108578\n",
      "Iteration 10320, loss = 0.15107916\n",
      "Iteration 10321, loss = 0.15106753\n",
      "Iteration 10322, loss = 0.15105868\n",
      "Iteration 10323, loss = 0.15105206\n",
      "Iteration 10324, loss = 0.15105162\n",
      "Iteration 10325, loss = 0.15104559\n",
      "Iteration 10326, loss = 0.15103971\n",
      "Iteration 10327, loss = 0.15103189\n",
      "Iteration 10328, loss = 0.15102700\n",
      "Iteration 10329, loss = 0.15102003\n",
      "Iteration 10330, loss = 0.15101947\n",
      "Iteration 10331, loss = 0.15101324\n",
      "Iteration 10332, loss = 0.15100619\n",
      "Iteration 10333, loss = 0.15099974\n",
      "Iteration 10334, loss = 0.15099901\n",
      "Iteration 10335, loss = 0.15099374\n",
      "Iteration 10336, loss = 0.15098640\n",
      "Iteration 10337, loss = 0.15098273\n",
      "Iteration 10338, loss = 0.15097669\n",
      "Iteration 10339, loss = 0.15097287\n",
      "Iteration 10340, loss = 0.15096382\n",
      "Iteration 10341, loss = 0.15095867\n",
      "Iteration 10342, loss = 0.15095287\n",
      "Iteration 10343, loss = 0.15095487\n",
      "Iteration 10344, loss = 0.15094438\n",
      "Iteration 10345, loss = 0.15093344\n",
      "Iteration 10346, loss = 0.15093183\n",
      "Iteration 10347, loss = 0.15092738\n",
      "Iteration 10348, loss = 0.15092014\n",
      "Iteration 10349, loss = 0.15091703\n",
      "Iteration 10350, loss = 0.15090980\n",
      "Iteration 10351, loss = 0.15090527\n",
      "Iteration 10352, loss = 0.15090074\n",
      "Iteration 10353, loss = 0.15089540\n",
      "Iteration 10354, loss = 0.15088822\n",
      "Iteration 10355, loss = 0.15088716\n",
      "Iteration 10356, loss = 0.15087961\n",
      "Iteration 10357, loss = 0.15086900\n",
      "Iteration 10358, loss = 0.15086747\n",
      "Iteration 10359, loss = 0.15086276\n",
      "Iteration 10360, loss = 0.15085760\n",
      "Iteration 10361, loss = 0.15085261\n",
      "Iteration 10362, loss = 0.15085011\n",
      "Iteration 10363, loss = 0.15084412\n",
      "Iteration 10364, loss = 0.15083790\n",
      "Iteration 10365, loss = 0.15083070\n",
      "Iteration 10366, loss = 0.15082462\n",
      "Iteration 10367, loss = 0.15082141\n",
      "Iteration 10368, loss = 0.15081451\n",
      "Iteration 10369, loss = 0.15080743\n",
      "Iteration 10370, loss = 0.15080718\n",
      "Iteration 10371, loss = 0.15080210\n",
      "Iteration 10372, loss = 0.15080327\n",
      "Iteration 10373, loss = 0.15079510\n",
      "Iteration 10374, loss = 0.15078655\n",
      "Iteration 10375, loss = 0.15078377\n",
      "Iteration 10376, loss = 0.15077864\n",
      "Iteration 10377, loss = 0.15077411\n",
      "Iteration 10378, loss = 0.15076846\n",
      "Iteration 10379, loss = 0.15075624\n",
      "Iteration 10380, loss = 0.15075779\n",
      "Iteration 10381, loss = 0.15075612\n",
      "Iteration 10382, loss = 0.15074709\n",
      "Iteration 10383, loss = 0.15073870\n",
      "Iteration 10384, loss = 0.15073426\n",
      "Iteration 10385, loss = 0.15073548\n",
      "Iteration 10386, loss = 0.15073362\n",
      "Iteration 10387, loss = 0.15072531\n",
      "Iteration 10388, loss = 0.15071481\n",
      "Iteration 10389, loss = 0.15071224\n",
      "Iteration 10390, loss = 0.15070703\n",
      "Iteration 10391, loss = 0.15069667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10392, loss = 0.15069129\n",
      "Iteration 10393, loss = 0.15068962\n",
      "Iteration 10394, loss = 0.15068496\n",
      "Iteration 10395, loss = 0.15067731\n",
      "Iteration 10396, loss = 0.15066757\n",
      "Iteration 10397, loss = 0.15066692\n",
      "Iteration 10398, loss = 0.15066318\n",
      "Iteration 10399, loss = 0.15065448\n",
      "Iteration 10400, loss = 0.15064687\n",
      "Iteration 10401, loss = 0.15064168\n",
      "Iteration 10402, loss = 0.15063568\n",
      "Iteration 10403, loss = 0.15063271\n",
      "Iteration 10404, loss = 0.15062372\n",
      "Iteration 10405, loss = 0.15062016\n",
      "Iteration 10406, loss = 0.15061862\n",
      "Iteration 10407, loss = 0.15061357\n",
      "Iteration 10408, loss = 0.15060940\n",
      "Iteration 10409, loss = 0.15060183\n",
      "Iteration 10410, loss = 0.15059578\n",
      "Iteration 10411, loss = 0.15059234\n",
      "Iteration 10412, loss = 0.15058354\n",
      "Iteration 10413, loss = 0.15057851\n",
      "Iteration 10414, loss = 0.15057579\n",
      "Iteration 10415, loss = 0.15057611\n",
      "Iteration 10416, loss = 0.15056977\n",
      "Iteration 10417, loss = 0.15056106\n",
      "Iteration 10418, loss = 0.15055733\n",
      "Iteration 10419, loss = 0.15054522\n",
      "Iteration 10420, loss = 0.15054126\n",
      "Iteration 10421, loss = 0.15054025\n",
      "Iteration 10422, loss = 0.15053346\n",
      "Iteration 10423, loss = 0.15052634\n",
      "Iteration 10424, loss = 0.15051752\n",
      "Iteration 10425, loss = 0.15051528\n",
      "Iteration 10426, loss = 0.15051283\n",
      "Iteration 10427, loss = 0.15050468\n",
      "Iteration 10428, loss = 0.15049779\n",
      "Iteration 10429, loss = 0.15049088\n",
      "Iteration 10430, loss = 0.15048630\n",
      "Iteration 10431, loss = 0.15048001\n",
      "Iteration 10432, loss = 0.15047455\n",
      "Iteration 10433, loss = 0.15047413\n",
      "Iteration 10434, loss = 0.15046801\n",
      "Iteration 10435, loss = 0.15045961\n",
      "Iteration 10436, loss = 0.15045283\n",
      "Iteration 10437, loss = 0.15045020\n",
      "Iteration 10438, loss = 0.15044495\n",
      "Iteration 10439, loss = 0.15044307\n",
      "Iteration 10440, loss = 0.15043435\n",
      "Iteration 10441, loss = 0.15043151\n",
      "Iteration 10442, loss = 0.15042591\n",
      "Iteration 10443, loss = 0.15042021\n",
      "Iteration 10444, loss = 0.15041348\n",
      "Iteration 10445, loss = 0.15041133\n",
      "Iteration 10446, loss = 0.15040606\n",
      "Iteration 10447, loss = 0.15039863\n",
      "Iteration 10448, loss = 0.15039406\n",
      "Iteration 10449, loss = 0.15038652\n",
      "Iteration 10450, loss = 0.15038049\n",
      "Iteration 10451, loss = 0.15036808\n",
      "Iteration 10452, loss = 0.15036944\n",
      "Iteration 10453, loss = 0.15036452\n",
      "Iteration 10454, loss = 0.15035573\n",
      "Iteration 10455, loss = 0.15035155\n",
      "Iteration 10456, loss = 0.15034912\n",
      "Iteration 10457, loss = 0.15034356\n",
      "Iteration 10458, loss = 0.15033728\n",
      "Iteration 10459, loss = 0.15033407\n",
      "Iteration 10460, loss = 0.15032809\n",
      "Iteration 10461, loss = 0.15032216\n",
      "Iteration 10462, loss = 0.15031303\n",
      "Iteration 10463, loss = 0.15030651\n",
      "Iteration 10464, loss = 0.15030165\n",
      "Iteration 10465, loss = 0.15029529\n",
      "Iteration 10466, loss = 0.15029132\n",
      "Iteration 10467, loss = 0.15027955\n",
      "Iteration 10468, loss = 0.15026922\n",
      "Iteration 10469, loss = 0.15026420\n",
      "Iteration 10470, loss = 0.15025580\n",
      "Iteration 10471, loss = 0.15024950\n",
      "Iteration 10472, loss = 0.15024242\n",
      "Iteration 10473, loss = 0.15023591\n",
      "Iteration 10474, loss = 0.15022865\n",
      "Iteration 10475, loss = 0.15022113\n",
      "Iteration 10476, loss = 0.15021319\n",
      "Iteration 10477, loss = 0.15020785\n",
      "Iteration 10478, loss = 0.15019931\n",
      "Iteration 10479, loss = 0.15019243\n",
      "Iteration 10480, loss = 0.15018613\n",
      "Iteration 10481, loss = 0.15017902\n",
      "Iteration 10482, loss = 0.15016776\n",
      "Iteration 10483, loss = 0.15016201\n",
      "Iteration 10484, loss = 0.15015567\n",
      "Iteration 10485, loss = 0.15014661\n",
      "Iteration 10486, loss = 0.15013201\n",
      "Iteration 10487, loss = 0.15012526\n",
      "Iteration 10488, loss = 0.15011906\n",
      "Iteration 10489, loss = 0.15010663\n",
      "Iteration 10490, loss = 0.15010191\n",
      "Iteration 10491, loss = 0.15008915\n",
      "Iteration 10492, loss = 0.15008074\n",
      "Iteration 10493, loss = 0.15007644\n",
      "Iteration 10494, loss = 0.15006365\n",
      "Iteration 10495, loss = 0.15004900\n",
      "Iteration 10496, loss = 0.15003435\n",
      "Iteration 10497, loss = 0.15002605\n",
      "Iteration 10498, loss = 0.15001449\n",
      "Iteration 10499, loss = 0.15000008\n",
      "Iteration 10500, loss = 0.14998645\n",
      "Iteration 10501, loss = 0.14997910\n",
      "Iteration 10502, loss = 0.14996682\n",
      "Iteration 10503, loss = 0.14995121\n",
      "Iteration 10504, loss = 0.14993682\n",
      "Iteration 10505, loss = 0.14992463\n",
      "Iteration 10506, loss = 0.14991353\n",
      "Iteration 10507, loss = 0.14989713\n",
      "Iteration 10508, loss = 0.14987961\n",
      "Iteration 10509, loss = 0.14986421\n",
      "Iteration 10510, loss = 0.14985109\n",
      "Iteration 10511, loss = 0.14983815\n",
      "Iteration 10512, loss = 0.14982125\n",
      "Iteration 10513, loss = 0.14981348\n",
      "Iteration 10514, loss = 0.14980344\n",
      "Iteration 10515, loss = 0.14978807\n",
      "Iteration 10516, loss = 0.14977461\n",
      "Iteration 10517, loss = 0.14976489\n",
      "Iteration 10518, loss = 0.14975449\n",
      "Iteration 10519, loss = 0.14973961\n",
      "Iteration 10520, loss = 0.14972677\n",
      "Iteration 10521, loss = 0.14971618\n",
      "Iteration 10522, loss = 0.14970095\n",
      "Iteration 10523, loss = 0.14968398\n",
      "Iteration 10524, loss = 0.14966678\n",
      "Iteration 10525, loss = 0.14966282\n",
      "Iteration 10526, loss = 0.14964815\n",
      "Iteration 10527, loss = 0.14962850\n",
      "Iteration 10528, loss = 0.14961556\n",
      "Iteration 10529, loss = 0.14959656\n",
      "Iteration 10530, loss = 0.14957840\n",
      "Iteration 10531, loss = 0.14956650\n",
      "Iteration 10532, loss = 0.14954856\n",
      "Iteration 10533, loss = 0.14953061\n",
      "Iteration 10534, loss = 0.14950987\n",
      "Iteration 10535, loss = 0.14949548\n",
      "Iteration 10536, loss = 0.14948164\n",
      "Iteration 10537, loss = 0.14946740\n",
      "Iteration 10538, loss = 0.14944868\n",
      "Iteration 10539, loss = 0.14942313\n",
      "Iteration 10540, loss = 0.14940970\n",
      "Iteration 10541, loss = 0.14939604\n",
      "Iteration 10542, loss = 0.14937696\n",
      "Iteration 10543, loss = 0.14935568\n",
      "Iteration 10544, loss = 0.14934296\n",
      "Iteration 10545, loss = 0.14932934\n",
      "Iteration 10546, loss = 0.14932372\n",
      "Iteration 10547, loss = 0.14931145\n",
      "Iteration 10548, loss = 0.14930152\n",
      "Iteration 10549, loss = 0.14928570\n",
      "Iteration 10550, loss = 0.14927932\n",
      "Iteration 10551, loss = 0.14926630\n",
      "Iteration 10552, loss = 0.14925618\n",
      "Iteration 10553, loss = 0.14924496\n",
      "Iteration 10554, loss = 0.14923198\n",
      "Iteration 10555, loss = 0.14921472\n",
      "Iteration 10556, loss = 0.14920790\n",
      "Iteration 10557, loss = 0.14919926\n",
      "Iteration 10558, loss = 0.14918476\n",
      "Iteration 10559, loss = 0.14916856\n",
      "Iteration 10560, loss = 0.14915522\n",
      "Iteration 10561, loss = 0.14914543\n",
      "Iteration 10562, loss = 0.14913211\n",
      "Iteration 10563, loss = 0.14912080\n",
      "Iteration 10564, loss = 0.14911135\n",
      "Iteration 10565, loss = 0.14909910\n",
      "Iteration 10566, loss = 0.14907988\n",
      "Iteration 10567, loss = 0.14906918\n",
      "Iteration 10568, loss = 0.14905688\n",
      "Iteration 10569, loss = 0.14904917\n",
      "Iteration 10570, loss = 0.14903901\n",
      "Iteration 10571, loss = 0.14902454\n",
      "Iteration 10572, loss = 0.14900995\n",
      "Iteration 10573, loss = 0.14899230\n",
      "Iteration 10574, loss = 0.14898468\n",
      "Iteration 10575, loss = 0.14897531\n",
      "Iteration 10576, loss = 0.14896610\n",
      "Iteration 10577, loss = 0.14895215\n",
      "Iteration 10578, loss = 0.14893431\n",
      "Iteration 10579, loss = 0.14891465\n",
      "Iteration 10580, loss = 0.14890960\n",
      "Iteration 10581, loss = 0.14889462\n",
      "Iteration 10582, loss = 0.14887927\n",
      "Iteration 10583, loss = 0.14886466\n",
      "Iteration 10584, loss = 0.14885239\n",
      "Iteration 10585, loss = 0.14884497\n",
      "Iteration 10586, loss = 0.14883033\n",
      "Iteration 10587, loss = 0.14881140\n",
      "Iteration 10588, loss = 0.14880156\n",
      "Iteration 10589, loss = 0.14878984\n",
      "Iteration 10590, loss = 0.14877720\n",
      "Iteration 10591, loss = 0.14876594\n",
      "Iteration 10592, loss = 0.14875206\n",
      "Iteration 10593, loss = 0.14874617\n",
      "Iteration 10594, loss = 0.14873298\n",
      "Iteration 10595, loss = 0.14871333\n",
      "Iteration 10596, loss = 0.14870197\n",
      "Iteration 10597, loss = 0.14869398\n",
      "Iteration 10598, loss = 0.14868285\n",
      "Iteration 10599, loss = 0.14866461\n",
      "Iteration 10600, loss = 0.14865423\n",
      "Iteration 10601, loss = 0.14864163\n",
      "Iteration 10602, loss = 0.14862942\n",
      "Iteration 10603, loss = 0.14861594\n",
      "Iteration 10604, loss = 0.14859803\n",
      "Iteration 10605, loss = 0.14858287\n",
      "Iteration 10606, loss = 0.14857455\n",
      "Iteration 10607, loss = 0.14856151\n",
      "Iteration 10608, loss = 0.14854837\n",
      "Iteration 10609, loss = 0.14854148\n",
      "Iteration 10610, loss = 0.14852660\n",
      "Iteration 10611, loss = 0.14851129\n",
      "Iteration 10612, loss = 0.14849748\n",
      "Iteration 10613, loss = 0.14848299\n",
      "Iteration 10614, loss = 0.14846851\n",
      "Iteration 10615, loss = 0.14845809\n",
      "Iteration 10616, loss = 0.14844528\n",
      "Iteration 10617, loss = 0.14843543\n",
      "Iteration 10618, loss = 0.14842160\n",
      "Iteration 10619, loss = 0.14840812\n",
      "Iteration 10620, loss = 0.14839725\n",
      "Iteration 10621, loss = 0.14838499\n",
      "Iteration 10622, loss = 0.14836995\n",
      "Iteration 10623, loss = 0.14835752\n",
      "Iteration 10624, loss = 0.14834306\n",
      "Iteration 10625, loss = 0.14833320\n",
      "Iteration 10626, loss = 0.14832033\n",
      "Iteration 10627, loss = 0.14830498\n",
      "Iteration 10628, loss = 0.14829247\n",
      "Iteration 10629, loss = 0.14827687\n",
      "Iteration 10630, loss = 0.14826576\n",
      "Iteration 10631, loss = 0.14825207\n",
      "Iteration 10632, loss = 0.14823803\n",
      "Iteration 10633, loss = 0.14823046\n",
      "Iteration 10634, loss = 0.14821790\n",
      "Iteration 10635, loss = 0.14820373\n",
      "Iteration 10636, loss = 0.14818931\n",
      "Iteration 10637, loss = 0.14817355\n",
      "Iteration 10638, loss = 0.14816287\n",
      "Iteration 10639, loss = 0.14814499\n",
      "Iteration 10640, loss = 0.14813084\n",
      "Iteration 10641, loss = 0.14811732\n",
      "Iteration 10642, loss = 0.14810571\n",
      "Iteration 10643, loss = 0.14809411\n",
      "Iteration 10644, loss = 0.14808168\n",
      "Iteration 10645, loss = 0.14806880\n",
      "Iteration 10646, loss = 0.14805188\n",
      "Iteration 10647, loss = 0.14803077\n",
      "Iteration 10648, loss = 0.14802193\n",
      "Iteration 10649, loss = 0.14802099\n",
      "Iteration 10650, loss = 0.14799617\n",
      "Iteration 10651, loss = 0.14798186\n",
      "Iteration 10652, loss = 0.14797221\n",
      "Iteration 10653, loss = 0.14795985\n",
      "Iteration 10654, loss = 0.14794624\n",
      "Iteration 10655, loss = 0.14793106\n",
      "Iteration 10656, loss = 0.14791814\n",
      "Iteration 10657, loss = 0.14790406\n",
      "Iteration 10658, loss = 0.14788936\n",
      "Iteration 10659, loss = 0.14787318\n",
      "Iteration 10660, loss = 0.14785640\n",
      "Iteration 10661, loss = 0.14785150\n",
      "Iteration 10662, loss = 0.14783571\n",
      "Iteration 10663, loss = 0.14781946\n",
      "Iteration 10664, loss = 0.14780711\n",
      "Iteration 10665, loss = 0.14779567\n",
      "Iteration 10666, loss = 0.14778642\n",
      "Iteration 10667, loss = 0.14776790\n",
      "Iteration 10668, loss = 0.14775105\n",
      "Iteration 10669, loss = 0.14773860\n",
      "Iteration 10670, loss = 0.14772465\n",
      "Iteration 10671, loss = 0.14770526\n",
      "Iteration 10672, loss = 0.14770032\n",
      "Iteration 10673, loss = 0.14768506\n",
      "Iteration 10674, loss = 0.14767103\n",
      "Iteration 10675, loss = 0.14765878\n",
      "Iteration 10676, loss = 0.14764387\n",
      "Iteration 10677, loss = 0.14762782\n",
      "Iteration 10678, loss = 0.14761447\n",
      "Iteration 10679, loss = 0.14759887\n",
      "Iteration 10680, loss = 0.14758759\n",
      "Iteration 10681, loss = 0.14757506\n",
      "Iteration 10682, loss = 0.14755841\n",
      "Iteration 10683, loss = 0.14754007\n",
      "Iteration 10684, loss = 0.14753040\n",
      "Iteration 10685, loss = 0.14751787\n",
      "Iteration 10686, loss = 0.14749971\n",
      "Iteration 10687, loss = 0.14748488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10688, loss = 0.14747017\n",
      "Iteration 10689, loss = 0.14745830\n",
      "Iteration 10690, loss = 0.14744433\n",
      "Iteration 10691, loss = 0.14743072\n",
      "Iteration 10692, loss = 0.14741360\n",
      "Iteration 10693, loss = 0.14740116\n",
      "Iteration 10694, loss = 0.14738711\n",
      "Iteration 10695, loss = 0.14737856\n",
      "Iteration 10696, loss = 0.14736275\n",
      "Iteration 10697, loss = 0.14734728\n",
      "Iteration 10698, loss = 0.14733889\n",
      "Iteration 10699, loss = 0.14732308\n",
      "Iteration 10700, loss = 0.14730336\n",
      "Iteration 10701, loss = 0.14728608\n",
      "Iteration 10702, loss = 0.14727553\n",
      "Iteration 10703, loss = 0.14725741\n",
      "Iteration 10704, loss = 0.14724435\n",
      "Iteration 10705, loss = 0.14723169\n",
      "Iteration 10706, loss = 0.14721222\n",
      "Iteration 10707, loss = 0.14720127\n",
      "Iteration 10708, loss = 0.14719015\n",
      "Iteration 10709, loss = 0.14717103\n",
      "Iteration 10710, loss = 0.14715467\n",
      "Iteration 10711, loss = 0.14714688\n",
      "Iteration 10712, loss = 0.14713432\n",
      "Iteration 10713, loss = 0.14711961\n",
      "Iteration 10714, loss = 0.14710557\n",
      "Iteration 10715, loss = 0.14708850\n",
      "Iteration 10716, loss = 0.14707906\n",
      "Iteration 10717, loss = 0.14706484\n",
      "Iteration 10718, loss = 0.14704515\n",
      "Iteration 10719, loss = 0.14703151\n",
      "Iteration 10720, loss = 0.14701770\n",
      "Iteration 10721, loss = 0.14699963\n",
      "Iteration 10722, loss = 0.14698581\n",
      "Iteration 10723, loss = 0.14697238\n",
      "Iteration 10724, loss = 0.14695253\n",
      "Iteration 10725, loss = 0.14693697\n",
      "Iteration 10726, loss = 0.14692327\n",
      "Iteration 10727, loss = 0.14690913\n",
      "Iteration 10728, loss = 0.14689149\n",
      "Iteration 10729, loss = 0.14687432\n",
      "Iteration 10730, loss = 0.14686688\n",
      "Iteration 10731, loss = 0.14684918\n",
      "Iteration 10732, loss = 0.14683330\n",
      "Iteration 10733, loss = 0.14682168\n",
      "Iteration 10734, loss = 0.14680450\n",
      "Iteration 10735, loss = 0.14679034\n",
      "Iteration 10736, loss = 0.14677846\n",
      "Iteration 10737, loss = 0.14676729\n",
      "Iteration 10738, loss = 0.14674975\n",
      "Iteration 10739, loss = 0.14673684\n",
      "Iteration 10740, loss = 0.14671931\n",
      "Iteration 10741, loss = 0.14670293\n",
      "Iteration 10742, loss = 0.14668790\n",
      "Iteration 10743, loss = 0.14667635\n",
      "Iteration 10744, loss = 0.14665802\n",
      "Iteration 10745, loss = 0.14664556\n",
      "Iteration 10746, loss = 0.14663220\n",
      "Iteration 10747, loss = 0.14661640\n",
      "Iteration 10748, loss = 0.14660629\n",
      "Iteration 10749, loss = 0.14658955\n",
      "Iteration 10750, loss = 0.14657215\n",
      "Iteration 10751, loss = 0.14655537\n",
      "Iteration 10752, loss = 0.14654089\n",
      "Iteration 10753, loss = 0.14652367\n",
      "Iteration 10754, loss = 0.14651127\n",
      "Iteration 10755, loss = 0.14649110\n",
      "Iteration 10756, loss = 0.14647577\n",
      "Iteration 10757, loss = 0.14646381\n",
      "Iteration 10758, loss = 0.14645021\n",
      "Iteration 10759, loss = 0.14643166\n",
      "Iteration 10760, loss = 0.14641569\n",
      "Iteration 10761, loss = 0.14640248\n",
      "Iteration 10762, loss = 0.14638835\n",
      "Iteration 10763, loss = 0.14637398\n",
      "Iteration 10764, loss = 0.14635408\n",
      "Iteration 10765, loss = 0.14633980\n",
      "Iteration 10766, loss = 0.14632699\n",
      "Iteration 10767, loss = 0.14631568\n",
      "Iteration 10768, loss = 0.14630091\n",
      "Iteration 10769, loss = 0.14628397\n",
      "Iteration 10770, loss = 0.14626804\n",
      "Iteration 10771, loss = 0.14625063\n",
      "Iteration 10772, loss = 0.14623899\n",
      "Iteration 10773, loss = 0.14622504\n",
      "Iteration 10774, loss = 0.14620586\n",
      "Iteration 10775, loss = 0.14619067\n",
      "Iteration 10776, loss = 0.14617729\n",
      "Iteration 10777, loss = 0.14615539\n",
      "Iteration 10778, loss = 0.14614099\n",
      "Iteration 10779, loss = 0.14612392\n",
      "Iteration 10780, loss = 0.14611102\n",
      "Iteration 10781, loss = 0.14609617\n",
      "Iteration 10782, loss = 0.14608205\n",
      "Iteration 10783, loss = 0.14606463\n",
      "Iteration 10784, loss = 0.14604777\n",
      "Iteration 10785, loss = 0.14603293\n",
      "Iteration 10786, loss = 0.14601825\n",
      "Iteration 10787, loss = 0.14600346\n",
      "Iteration 10788, loss = 0.14598582\n",
      "Iteration 10789, loss = 0.14597154\n",
      "Iteration 10790, loss = 0.14595789\n",
      "Iteration 10791, loss = 0.14593853\n",
      "Iteration 10792, loss = 0.14592527\n",
      "Iteration 10793, loss = 0.14591448\n",
      "Iteration 10794, loss = 0.14589568\n",
      "Iteration 10795, loss = 0.14588165\n",
      "Iteration 10796, loss = 0.14587296\n",
      "Iteration 10797, loss = 0.14585710\n",
      "Iteration 10798, loss = 0.14583803\n",
      "Iteration 10799, loss = 0.14581903\n",
      "Iteration 10800, loss = 0.14580040\n",
      "Iteration 10801, loss = 0.14578973\n",
      "Iteration 10802, loss = 0.14577348\n",
      "Iteration 10803, loss = 0.14575788\n",
      "Iteration 10804, loss = 0.14573975\n",
      "Iteration 10805, loss = 0.14572737\n",
      "Iteration 10806, loss = 0.14571668\n",
      "Iteration 10807, loss = 0.14570085\n",
      "Iteration 10808, loss = 0.14568089\n",
      "Iteration 10809, loss = 0.14566562\n",
      "Iteration 10810, loss = 0.14564966\n",
      "Iteration 10811, loss = 0.14563277\n",
      "Iteration 10812, loss = 0.14562185\n",
      "Iteration 10813, loss = 0.14560332\n",
      "Iteration 10814, loss = 0.14558964\n",
      "Iteration 10815, loss = 0.14557728\n",
      "Iteration 10816, loss = 0.14556142\n",
      "Iteration 10817, loss = 0.14554476\n",
      "Iteration 10818, loss = 0.14552804\n",
      "Iteration 10819, loss = 0.14551528\n",
      "Iteration 10820, loss = 0.14550020\n",
      "Iteration 10821, loss = 0.14548289\n",
      "Iteration 10822, loss = 0.14546762\n",
      "Iteration 10823, loss = 0.14544643\n",
      "Iteration 10824, loss = 0.14543145\n",
      "Iteration 10825, loss = 0.14542501\n",
      "Iteration 10826, loss = 0.14540915\n",
      "Iteration 10827, loss = 0.14538661\n",
      "Iteration 10828, loss = 0.14537093\n",
      "Iteration 10829, loss = 0.14536076\n",
      "Iteration 10830, loss = 0.14534936\n",
      "Iteration 10831, loss = 0.14533394\n",
      "Iteration 10832, loss = 0.14531434\n",
      "Iteration 10833, loss = 0.14529700\n",
      "Iteration 10834, loss = 0.14528422\n",
      "Iteration 10835, loss = 0.14526936\n",
      "Iteration 10836, loss = 0.14525344\n",
      "Iteration 10837, loss = 0.14523570\n",
      "Iteration 10838, loss = 0.14521240\n",
      "Iteration 10839, loss = 0.14519692\n",
      "Iteration 10840, loss = 0.14518663\n",
      "Iteration 10841, loss = 0.14516822\n",
      "Iteration 10842, loss = 0.14515258\n",
      "Iteration 10843, loss = 0.14513835\n",
      "Iteration 10844, loss = 0.14511999\n",
      "Iteration 10845, loss = 0.14510425\n",
      "Iteration 10846, loss = 0.14508582\n",
      "Iteration 10847, loss = 0.14507296\n",
      "Iteration 10848, loss = 0.14505399\n",
      "Iteration 10849, loss = 0.14503943\n",
      "Iteration 10850, loss = 0.14502399\n",
      "Iteration 10851, loss = 0.14500987\n",
      "Iteration 10852, loss = 0.14499854\n",
      "Iteration 10853, loss = 0.14498023\n",
      "Iteration 10854, loss = 0.14496156\n",
      "Iteration 10855, loss = 0.14494130\n",
      "Iteration 10856, loss = 0.14492953\n",
      "Iteration 10857, loss = 0.14491197\n",
      "Iteration 10858, loss = 0.14489769\n",
      "Iteration 10859, loss = 0.14488219\n",
      "Iteration 10860, loss = 0.14486620\n",
      "Iteration 10861, loss = 0.14485223\n",
      "Iteration 10862, loss = 0.14483693\n",
      "Iteration 10863, loss = 0.14481958\n",
      "Iteration 10864, loss = 0.14480522\n",
      "Iteration 10865, loss = 0.14479504\n",
      "Iteration 10866, loss = 0.14477461\n",
      "Iteration 10867, loss = 0.14475646\n",
      "Iteration 10868, loss = 0.14474496\n",
      "Iteration 10869, loss = 0.14473367\n",
      "Iteration 10870, loss = 0.14471158\n",
      "Iteration 10871, loss = 0.14469127\n",
      "Iteration 10872, loss = 0.14467351\n",
      "Iteration 10873, loss = 0.14466177\n",
      "Iteration 10874, loss = 0.14464579\n",
      "Iteration 10875, loss = 0.14463081\n",
      "Iteration 10876, loss = 0.14461889\n",
      "Iteration 10877, loss = 0.14460190\n",
      "Iteration 10878, loss = 0.14458711\n",
      "Iteration 10879, loss = 0.14457001\n",
      "Iteration 10880, loss = 0.14455456\n",
      "Iteration 10881, loss = 0.14454008\n",
      "Iteration 10882, loss = 0.14452633\n",
      "Iteration 10883, loss = 0.14450914\n",
      "Iteration 10884, loss = 0.14449026\n",
      "Iteration 10885, loss = 0.14447477\n",
      "Iteration 10886, loss = 0.14445880\n",
      "Iteration 10887, loss = 0.14444552\n",
      "Iteration 10888, loss = 0.14442490\n",
      "Iteration 10889, loss = 0.14440640\n",
      "Iteration 10890, loss = 0.14439603\n",
      "Iteration 10891, loss = 0.14438413\n",
      "Iteration 10892, loss = 0.14436787\n",
      "Iteration 10893, loss = 0.14434419\n",
      "Iteration 10894, loss = 0.14432907\n",
      "Iteration 10895, loss = 0.14431360\n",
      "Iteration 10896, loss = 0.14429676\n",
      "Iteration 10897, loss = 0.14427975\n",
      "Iteration 10898, loss = 0.14425884\n",
      "Iteration 10899, loss = 0.14425009\n",
      "Iteration 10900, loss = 0.14423463\n",
      "Iteration 10901, loss = 0.14422181\n",
      "Iteration 10902, loss = 0.14419992\n",
      "Iteration 10903, loss = 0.14418410\n",
      "Iteration 10904, loss = 0.14417456\n",
      "Iteration 10905, loss = 0.14415651\n",
      "Iteration 10906, loss = 0.14413581\n",
      "Iteration 10907, loss = 0.14411593\n",
      "Iteration 10908, loss = 0.14410510\n",
      "Iteration 10909, loss = 0.14408881\n",
      "Iteration 10910, loss = 0.14406888\n",
      "Iteration 10911, loss = 0.14405221\n",
      "Iteration 10912, loss = 0.14403624\n",
      "Iteration 10913, loss = 0.14401937\n",
      "Iteration 10914, loss = 0.14400045\n",
      "Iteration 10915, loss = 0.14397972\n",
      "Iteration 10916, loss = 0.14396576\n",
      "Iteration 10917, loss = 0.14395020\n",
      "Iteration 10918, loss = 0.14392951\n",
      "Iteration 10919, loss = 0.14391776\n",
      "Iteration 10920, loss = 0.14390134\n",
      "Iteration 10921, loss = 0.14388182\n",
      "Iteration 10922, loss = 0.14386378\n",
      "Iteration 10923, loss = 0.14385193\n",
      "Iteration 10924, loss = 0.14383366\n",
      "Iteration 10925, loss = 0.14381853\n",
      "Iteration 10926, loss = 0.14380316\n",
      "Iteration 10927, loss = 0.14378496\n",
      "Iteration 10928, loss = 0.14377183\n",
      "Iteration 10929, loss = 0.14375086\n",
      "Iteration 10930, loss = 0.14373147\n",
      "Iteration 10931, loss = 0.14371546\n",
      "Iteration 10932, loss = 0.14369799\n",
      "Iteration 10933, loss = 0.14368308\n",
      "Iteration 10934, loss = 0.14366181\n",
      "Iteration 10935, loss = 0.14364463\n",
      "Iteration 10936, loss = 0.14363133\n",
      "Iteration 10937, loss = 0.14362070\n",
      "Iteration 10938, loss = 0.14360425\n",
      "Iteration 10939, loss = 0.14358311\n",
      "Iteration 10940, loss = 0.14356074\n",
      "Iteration 10941, loss = 0.14354215\n",
      "Iteration 10942, loss = 0.14352931\n",
      "Iteration 10943, loss = 0.14351882\n",
      "Iteration 10944, loss = 0.14349337\n",
      "Iteration 10945, loss = 0.14347360\n",
      "Iteration 10946, loss = 0.14346400\n",
      "Iteration 10947, loss = 0.14344524\n",
      "Iteration 10948, loss = 0.14342809\n",
      "Iteration 10949, loss = 0.14341016\n",
      "Iteration 10950, loss = 0.14339717\n",
      "Iteration 10951, loss = 0.14338040\n",
      "Iteration 10952, loss = 0.14336535\n",
      "Iteration 10953, loss = 0.14334703\n",
      "Iteration 10954, loss = 0.14332221\n",
      "Iteration 10955, loss = 0.14330798\n",
      "Iteration 10956, loss = 0.14329415\n",
      "Iteration 10957, loss = 0.14327886\n",
      "Iteration 10958, loss = 0.14326145\n",
      "Iteration 10959, loss = 0.14324140\n",
      "Iteration 10960, loss = 0.14322131\n",
      "Iteration 10961, loss = 0.14320388\n",
      "Iteration 10962, loss = 0.14318715\n",
      "Iteration 10963, loss = 0.14317191\n",
      "Iteration 10964, loss = 0.14315089\n",
      "Iteration 10965, loss = 0.14313026\n",
      "Iteration 10966, loss = 0.14311513\n",
      "Iteration 10967, loss = 0.14310942\n",
      "Iteration 10968, loss = 0.14308893\n",
      "Iteration 10969, loss = 0.14306787\n",
      "Iteration 10970, loss = 0.14305208\n",
      "Iteration 10971, loss = 0.14304003\n",
      "Iteration 10972, loss = 0.14302205\n",
      "Iteration 10973, loss = 0.14300614\n",
      "Iteration 10974, loss = 0.14298865\n",
      "Iteration 10975, loss = 0.14296735\n",
      "Iteration 10976, loss = 0.14294834\n",
      "Iteration 10977, loss = 0.14293417\n",
      "Iteration 10978, loss = 0.14292194\n",
      "Iteration 10979, loss = 0.14290088\n",
      "Iteration 10980, loss = 0.14287880\n",
      "Iteration 10981, loss = 0.14285896\n",
      "Iteration 10982, loss = 0.14285035\n",
      "Iteration 10983, loss = 0.14283502\n",
      "Iteration 10984, loss = 0.14281367\n",
      "Iteration 10985, loss = 0.14279411\n",
      "Iteration 10986, loss = 0.14278036\n",
      "Iteration 10987, loss = 0.14276087\n",
      "Iteration 10988, loss = 0.14274470\n",
      "Iteration 10989, loss = 0.14272659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10990, loss = 0.14270749\n",
      "Iteration 10991, loss = 0.14269501\n",
      "Iteration 10992, loss = 0.14267401\n",
      "Iteration 10993, loss = 0.14265507\n",
      "Iteration 10994, loss = 0.14264188\n",
      "Iteration 10995, loss = 0.14262599\n",
      "Iteration 10996, loss = 0.14260687\n",
      "Iteration 10997, loss = 0.14258798\n",
      "Iteration 10998, loss = 0.14257598\n",
      "Iteration 10999, loss = 0.14256202\n",
      "Iteration 11000, loss = 0.14254262\n",
      "Iteration 11001, loss = 0.14252235\n",
      "Iteration 11002, loss = 0.14250665\n",
      "Iteration 11003, loss = 0.14249342\n",
      "Iteration 11004, loss = 0.14247511\n",
      "Iteration 11005, loss = 0.14245218\n",
      "Iteration 11006, loss = 0.14243523\n",
      "Iteration 11007, loss = 0.14242581\n",
      "Iteration 11008, loss = 0.14240463\n",
      "Iteration 11009, loss = 0.14238552\n",
      "Iteration 11010, loss = 0.14237014\n",
      "Iteration 11011, loss = 0.14235653\n",
      "Iteration 11012, loss = 0.14234087\n",
      "Iteration 11013, loss = 0.14232400\n",
      "Iteration 11014, loss = 0.14230471\n",
      "Iteration 11015, loss = 0.14228392\n",
      "Iteration 11016, loss = 0.14227021\n",
      "Iteration 11017, loss = 0.14225078\n",
      "Iteration 11018, loss = 0.14223198\n",
      "Iteration 11019, loss = 0.14220946\n",
      "Iteration 11020, loss = 0.14219719\n",
      "Iteration 11021, loss = 0.14218457\n",
      "Iteration 11022, loss = 0.14216796\n",
      "Iteration 11023, loss = 0.14214881\n",
      "Iteration 11024, loss = 0.14212659\n",
      "Iteration 11025, loss = 0.14211931\n",
      "Iteration 11026, loss = 0.14210342\n",
      "Iteration 11027, loss = 0.14208310\n",
      "Iteration 11028, loss = 0.14206531\n",
      "Iteration 11029, loss = 0.14204536\n",
      "Iteration 11030, loss = 0.14202995\n",
      "Iteration 11031, loss = 0.14201725\n",
      "Iteration 11032, loss = 0.14200099\n",
      "Iteration 11033, loss = 0.14197975\n",
      "Iteration 11034, loss = 0.14196176\n",
      "Iteration 11035, loss = 0.14194516\n",
      "Iteration 11036, loss = 0.14193114\n",
      "Iteration 11037, loss = 0.14191319\n",
      "Iteration 11038, loss = 0.14190008\n",
      "Iteration 11039, loss = 0.14188243\n",
      "Iteration 11040, loss = 0.14186032\n",
      "Iteration 11041, loss = 0.14184010\n",
      "Iteration 11042, loss = 0.14182918\n",
      "Iteration 11043, loss = 0.14181299\n",
      "Iteration 11044, loss = 0.14179192\n",
      "Iteration 11045, loss = 0.14177558\n",
      "Iteration 11046, loss = 0.14175173\n",
      "Iteration 11047, loss = 0.14174174\n",
      "Iteration 11048, loss = 0.14172967\n",
      "Iteration 11049, loss = 0.14171223\n",
      "Iteration 11050, loss = 0.14168869\n",
      "Iteration 11051, loss = 0.14167126\n",
      "Iteration 11052, loss = 0.14166017\n",
      "Iteration 11053, loss = 0.14164545\n",
      "Iteration 11054, loss = 0.14162520\n",
      "Iteration 11055, loss = 0.14161027\n",
      "Iteration 11056, loss = 0.14159357\n",
      "Iteration 11057, loss = 0.14157175\n",
      "Iteration 11058, loss = 0.14155480\n",
      "Iteration 11059, loss = 0.14153954\n",
      "Iteration 11060, loss = 0.14152098\n",
      "Iteration 11061, loss = 0.14150087\n",
      "Iteration 11062, loss = 0.14147766\n",
      "Iteration 11063, loss = 0.14146310\n",
      "Iteration 11064, loss = 0.14145157\n",
      "Iteration 11065, loss = 0.14143344\n",
      "Iteration 11066, loss = 0.14141246\n",
      "Iteration 11067, loss = 0.14140086\n",
      "Iteration 11068, loss = 0.14138301\n",
      "Iteration 11069, loss = 0.14136654\n",
      "Iteration 11070, loss = 0.14134633\n",
      "Iteration 11071, loss = 0.14133378\n",
      "Iteration 11072, loss = 0.14131899\n",
      "Iteration 11073, loss = 0.14130247\n",
      "Iteration 11074, loss = 0.14128079\n",
      "Iteration 11075, loss = 0.14126197\n",
      "Iteration 11076, loss = 0.14124719\n",
      "Iteration 11077, loss = 0.14123347\n",
      "Iteration 11078, loss = 0.14122154\n",
      "Iteration 11079, loss = 0.14119986\n",
      "Iteration 11080, loss = 0.14117881\n",
      "Iteration 11081, loss = 0.14115893\n",
      "Iteration 11082, loss = 0.14114604\n",
      "Iteration 11083, loss = 0.14113036\n",
      "Iteration 11084, loss = 0.14111158\n",
      "Iteration 11085, loss = 0.14109413\n",
      "Iteration 11086, loss = 0.14107158\n",
      "Iteration 11087, loss = 0.14105862\n",
      "Iteration 11088, loss = 0.14104427\n",
      "Iteration 11089, loss = 0.14102310\n",
      "Iteration 11090, loss = 0.14100855\n",
      "Iteration 11091, loss = 0.14099181\n",
      "Iteration 11092, loss = 0.14097440\n",
      "Iteration 11093, loss = 0.14095562\n",
      "Iteration 11094, loss = 0.14093909\n",
      "Iteration 11095, loss = 0.14092301\n",
      "Iteration 11096, loss = 0.14090331\n",
      "Iteration 11097, loss = 0.14089021\n",
      "Iteration 11098, loss = 0.14086776\n",
      "Iteration 11099, loss = 0.14085107\n",
      "Iteration 11100, loss = 0.14083707\n",
      "Iteration 11101, loss = 0.14081969\n",
      "Iteration 11102, loss = 0.14080271\n",
      "Iteration 11103, loss = 0.14078196\n",
      "Iteration 11104, loss = 0.14076444\n",
      "Iteration 11105, loss = 0.14075075\n",
      "Iteration 11106, loss = 0.14073546\n",
      "Iteration 11107, loss = 0.14072108\n",
      "Iteration 11108, loss = 0.14069841\n",
      "Iteration 11109, loss = 0.14068468\n",
      "Iteration 11110, loss = 0.14066559\n",
      "Iteration 11111, loss = 0.14065148\n",
      "Iteration 11112, loss = 0.14063587\n",
      "Iteration 11113, loss = 0.14062109\n",
      "Iteration 11114, loss = 0.14060250\n",
      "Iteration 11115, loss = 0.14058423\n",
      "Iteration 11116, loss = 0.14056729\n",
      "Iteration 11117, loss = 0.14055404\n",
      "Iteration 11118, loss = 0.14053499\n",
      "Iteration 11119, loss = 0.14051799\n",
      "Iteration 11120, loss = 0.14049679\n",
      "Iteration 11121, loss = 0.14047707\n",
      "Iteration 11122, loss = 0.14046056\n",
      "Iteration 11123, loss = 0.14045419\n",
      "Iteration 11124, loss = 0.14043342\n",
      "Iteration 11125, loss = 0.14041042\n",
      "Iteration 11126, loss = 0.14039519\n",
      "Iteration 11127, loss = 0.14037836\n",
      "Iteration 11128, loss = 0.14036119\n",
      "Iteration 11129, loss = 0.14034347\n",
      "Iteration 11130, loss = 0.14032625\n",
      "Iteration 11131, loss = 0.14031154\n",
      "Iteration 11132, loss = 0.14029319\n",
      "Iteration 11133, loss = 0.14027657\n",
      "Iteration 11134, loss = 0.14025717\n",
      "Iteration 11135, loss = 0.14024057\n",
      "Iteration 11136, loss = 0.14023010\n",
      "Iteration 11137, loss = 0.14021413\n",
      "Iteration 11138, loss = 0.14019394\n",
      "Iteration 11139, loss = 0.14018081\n",
      "Iteration 11140, loss = 0.14016525\n",
      "Iteration 11141, loss = 0.14014621\n",
      "Iteration 11142, loss = 0.14012846\n",
      "Iteration 11143, loss = 0.14011192\n",
      "Iteration 11144, loss = 0.14009447\n",
      "Iteration 11145, loss = 0.14008118\n",
      "Iteration 11146, loss = 0.14006837\n",
      "Iteration 11147, loss = 0.14004567\n",
      "Iteration 11148, loss = 0.14002123\n",
      "Iteration 11149, loss = 0.14000869\n",
      "Iteration 11150, loss = 0.13999284\n",
      "Iteration 11151, loss = 0.13997938\n",
      "Iteration 11152, loss = 0.13996490\n",
      "Iteration 11153, loss = 0.13994159\n",
      "Iteration 11154, loss = 0.13992224\n",
      "Iteration 11155, loss = 0.13991062\n",
      "Iteration 11156, loss = 0.13989203\n",
      "Iteration 11157, loss = 0.13987396\n",
      "Iteration 11158, loss = 0.13985537\n",
      "Iteration 11159, loss = 0.13984253\n",
      "Iteration 11160, loss = 0.13982649\n",
      "Iteration 11161, loss = 0.13980711\n",
      "Iteration 11162, loss = 0.13978899\n",
      "Iteration 11163, loss = 0.13977609\n",
      "Iteration 11164, loss = 0.13975738\n",
      "Iteration 11165, loss = 0.13973979\n",
      "Iteration 11166, loss = 0.13972443\n",
      "Iteration 11167, loss = 0.13971071\n",
      "Iteration 11168, loss = 0.13969090\n",
      "Iteration 11169, loss = 0.13967250\n",
      "Iteration 11170, loss = 0.13965768\n",
      "Iteration 11171, loss = 0.13964226\n",
      "Iteration 11172, loss = 0.13962309\n",
      "Iteration 11173, loss = 0.13960908\n",
      "Iteration 11174, loss = 0.13959332\n",
      "Iteration 11175, loss = 0.13957699\n",
      "Iteration 11176, loss = 0.13955941\n",
      "Iteration 11177, loss = 0.13953918\n",
      "Iteration 11178, loss = 0.13951958\n",
      "Iteration 11179, loss = 0.13950279\n",
      "Iteration 11180, loss = 0.13948913\n",
      "Iteration 11181, loss = 0.13947363\n",
      "Iteration 11182, loss = 0.13945356\n",
      "Iteration 11183, loss = 0.13944121\n",
      "Iteration 11184, loss = 0.13942139\n",
      "Iteration 11185, loss = 0.13940169\n",
      "Iteration 11186, loss = 0.13938729\n",
      "Iteration 11187, loss = 0.13936899\n",
      "Iteration 11188, loss = 0.13935555\n",
      "Iteration 11189, loss = 0.13934013\n",
      "Iteration 11190, loss = 0.13932225\n",
      "Iteration 11191, loss = 0.13930531\n",
      "Iteration 11192, loss = 0.13928564\n",
      "Iteration 11193, loss = 0.13927284\n",
      "Iteration 11194, loss = 0.13925684\n",
      "Iteration 11195, loss = 0.13923822\n",
      "Iteration 11196, loss = 0.13922156\n",
      "Iteration 11197, loss = 0.13920660\n",
      "Iteration 11198, loss = 0.13919121\n",
      "Iteration 11199, loss = 0.13917291\n",
      "Iteration 11200, loss = 0.13916072\n",
      "Iteration 11201, loss = 0.13914323\n",
      "Iteration 11202, loss = 0.13912293\n",
      "Iteration 11203, loss = 0.13910455\n",
      "Iteration 11204, loss = 0.13909022\n",
      "Iteration 11205, loss = 0.13907198\n",
      "Iteration 11206, loss = 0.13905565\n",
      "Iteration 11207, loss = 0.13903887\n",
      "Iteration 11208, loss = 0.13902598\n",
      "Iteration 11209, loss = 0.13900677\n",
      "Iteration 11210, loss = 0.13898841\n",
      "Iteration 11211, loss = 0.13897159\n",
      "Iteration 11212, loss = 0.13895311\n",
      "Iteration 11213, loss = 0.13893868\n",
      "Iteration 11214, loss = 0.13892133\n",
      "Iteration 11215, loss = 0.13890851\n",
      "Iteration 11216, loss = 0.13888956\n",
      "Iteration 11217, loss = 0.13887103\n",
      "Iteration 11218, loss = 0.13885361\n",
      "Iteration 11219, loss = 0.13883568\n",
      "Iteration 11220, loss = 0.13882147\n",
      "Iteration 11221, loss = 0.13880419\n",
      "Iteration 11222, loss = 0.13878500\n",
      "Iteration 11223, loss = 0.13876917\n",
      "Iteration 11224, loss = 0.13875322\n",
      "Iteration 11225, loss = 0.13873436\n",
      "Iteration 11226, loss = 0.13871586\n",
      "Iteration 11227, loss = 0.13870240\n",
      "Iteration 11228, loss = 0.13868449\n",
      "Iteration 11229, loss = 0.13866740\n",
      "Iteration 11230, loss = 0.13865464\n",
      "Iteration 11231, loss = 0.13864030\n",
      "Iteration 11232, loss = 0.13862189\n",
      "Iteration 11233, loss = 0.13860691\n",
      "Iteration 11234, loss = 0.13859791\n",
      "Iteration 11235, loss = 0.13857618\n",
      "Iteration 11236, loss = 0.13855256\n",
      "Iteration 11237, loss = 0.13853735\n",
      "Iteration 11238, loss = 0.13851828\n",
      "Iteration 11239, loss = 0.13850486\n",
      "Iteration 11240, loss = 0.13849001\n",
      "Iteration 11241, loss = 0.13846933\n",
      "Iteration 11242, loss = 0.13845355\n",
      "Iteration 11243, loss = 0.13844453\n",
      "Iteration 11244, loss = 0.13842777\n",
      "Iteration 11245, loss = 0.13840768\n",
      "Iteration 11246, loss = 0.13839462\n",
      "Iteration 11247, loss = 0.13837838\n",
      "Iteration 11248, loss = 0.13836387\n",
      "Iteration 11249, loss = 0.13834362\n",
      "Iteration 11250, loss = 0.13832537\n",
      "Iteration 11251, loss = 0.13831247\n",
      "Iteration 11252, loss = 0.13829755\n",
      "Iteration 11253, loss = 0.13827771\n",
      "Iteration 11254, loss = 0.13826027\n",
      "Iteration 11255, loss = 0.13824559\n",
      "Iteration 11256, loss = 0.13823080\n",
      "Iteration 11257, loss = 0.13821342\n",
      "Iteration 11258, loss = 0.13819447\n",
      "Iteration 11259, loss = 0.13817476\n",
      "Iteration 11260, loss = 0.13816331\n",
      "Iteration 11261, loss = 0.13814622\n",
      "Iteration 11262, loss = 0.13812390\n",
      "Iteration 11263, loss = 0.13810997\n",
      "Iteration 11264, loss = 0.13809657\n",
      "Iteration 11265, loss = 0.13807908\n",
      "Iteration 11266, loss = 0.13805838\n",
      "Iteration 11267, loss = 0.13804501\n",
      "Iteration 11268, loss = 0.13802764\n",
      "Iteration 11269, loss = 0.13801259\n",
      "Iteration 11270, loss = 0.13799414\n",
      "Iteration 11271, loss = 0.13797393\n",
      "Iteration 11272, loss = 0.13795848\n",
      "Iteration 11273, loss = 0.13794397\n",
      "Iteration 11274, loss = 0.13792317\n",
      "Iteration 11275, loss = 0.13790791\n",
      "Iteration 11276, loss = 0.13789335\n",
      "Iteration 11277, loss = 0.13787845\n",
      "Iteration 11278, loss = 0.13785914\n",
      "Iteration 11279, loss = 0.13784458\n",
      "Iteration 11280, loss = 0.13782439\n",
      "Iteration 11281, loss = 0.13781039\n",
      "Iteration 11282, loss = 0.13779228\n",
      "Iteration 11283, loss = 0.13777718\n",
      "Iteration 11284, loss = 0.13776005\n",
      "Iteration 11285, loss = 0.13774844\n",
      "Iteration 11286, loss = 0.13773308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11287, loss = 0.13771818\n",
      "Iteration 11288, loss = 0.13770357\n",
      "Iteration 11289, loss = 0.13768480\n",
      "Iteration 11290, loss = 0.13766765\n",
      "Iteration 11291, loss = 0.13765464\n",
      "Iteration 11292, loss = 0.13763611\n",
      "Iteration 11293, loss = 0.13761445\n",
      "Iteration 11294, loss = 0.13759728\n",
      "Iteration 11295, loss = 0.13758357\n",
      "Iteration 11296, loss = 0.13756942\n",
      "Iteration 11297, loss = 0.13755152\n",
      "Iteration 11298, loss = 0.13753501\n",
      "Iteration 11299, loss = 0.13751881\n",
      "Iteration 11300, loss = 0.13750372\n",
      "Iteration 11301, loss = 0.13749046\n",
      "Iteration 11302, loss = 0.13747140\n",
      "Iteration 11303, loss = 0.13745611\n",
      "Iteration 11304, loss = 0.13743260\n",
      "Iteration 11305, loss = 0.13742374\n",
      "Iteration 11306, loss = 0.13741340\n",
      "Iteration 11307, loss = 0.13739414\n",
      "Iteration 11308, loss = 0.13737609\n",
      "Iteration 11309, loss = 0.13736029\n",
      "Iteration 11310, loss = 0.13734530\n",
      "Iteration 11311, loss = 0.13732881\n",
      "Iteration 11312, loss = 0.13731173\n",
      "Iteration 11313, loss = 0.13729176\n",
      "Iteration 11314, loss = 0.13727442\n",
      "Iteration 11315, loss = 0.13725241\n",
      "Iteration 11316, loss = 0.13724034\n",
      "Iteration 11317, loss = 0.13722611\n",
      "Iteration 11318, loss = 0.13721147\n",
      "Iteration 11319, loss = 0.13719346\n",
      "Iteration 11320, loss = 0.13717143\n",
      "Iteration 11321, loss = 0.13715526\n",
      "Iteration 11322, loss = 0.13714179\n",
      "Iteration 11323, loss = 0.13712277\n",
      "Iteration 11324, loss = 0.13710500\n",
      "Iteration 11325, loss = 0.13709038\n",
      "Iteration 11326, loss = 0.13707180\n",
      "Iteration 11327, loss = 0.13705922\n",
      "Iteration 11328, loss = 0.13703789\n",
      "Iteration 11329, loss = 0.13702255\n",
      "Iteration 11330, loss = 0.13700946\n",
      "Iteration 11331, loss = 0.13699514\n",
      "Iteration 11332, loss = 0.13698147\n",
      "Iteration 11333, loss = 0.13696239\n",
      "Iteration 11334, loss = 0.13694459\n",
      "Iteration 11335, loss = 0.13692310\n",
      "Iteration 11336, loss = 0.13690929\n",
      "Iteration 11337, loss = 0.13689225\n",
      "Iteration 11338, loss = 0.13687573\n",
      "Iteration 11339, loss = 0.13686329\n",
      "Iteration 11340, loss = 0.13684230\n",
      "Iteration 11341, loss = 0.13682824\n",
      "Iteration 11342, loss = 0.13681907\n",
      "Iteration 11343, loss = 0.13680030\n",
      "Iteration 11344, loss = 0.13678621\n",
      "Iteration 11345, loss = 0.13677069\n",
      "Iteration 11346, loss = 0.13675136\n",
      "Iteration 11347, loss = 0.13673159\n",
      "Iteration 11348, loss = 0.13671737\n",
      "Iteration 11349, loss = 0.13669434\n",
      "Iteration 11350, loss = 0.13667962\n",
      "Iteration 11351, loss = 0.13665900\n",
      "Iteration 11352, loss = 0.13664319\n",
      "Iteration 11353, loss = 0.13662639\n",
      "Iteration 11354, loss = 0.13661022\n",
      "Iteration 11355, loss = 0.13659299\n",
      "Iteration 11356, loss = 0.13657996\n",
      "Iteration 11357, loss = 0.13655642\n",
      "Iteration 11358, loss = 0.13654442\n",
      "Iteration 11359, loss = 0.13652991\n",
      "Iteration 11360, loss = 0.13651148\n",
      "Iteration 11361, loss = 0.13649268\n",
      "Iteration 11362, loss = 0.13647894\n",
      "Iteration 11363, loss = 0.13646068\n",
      "Iteration 11364, loss = 0.13644412\n",
      "Iteration 11365, loss = 0.13642905\n",
      "Iteration 11366, loss = 0.13641506\n",
      "Iteration 11367, loss = 0.13639636\n",
      "Iteration 11368, loss = 0.13637439\n",
      "Iteration 11369, loss = 0.13636727\n",
      "Iteration 11370, loss = 0.13635900\n",
      "Iteration 11371, loss = 0.13634062\n",
      "Iteration 11372, loss = 0.13632252\n",
      "Iteration 11373, loss = 0.13630071\n",
      "Iteration 11374, loss = 0.13628890\n",
      "Iteration 11375, loss = 0.13627518\n",
      "Iteration 11376, loss = 0.13625946\n",
      "Iteration 11377, loss = 0.13623984\n",
      "Iteration 11378, loss = 0.13622201\n",
      "Iteration 11379, loss = 0.13620639\n",
      "Iteration 11380, loss = 0.13619552\n",
      "Iteration 11381, loss = 0.13618026\n",
      "Iteration 11382, loss = 0.13615982\n",
      "Iteration 11383, loss = 0.13613979\n",
      "Iteration 11384, loss = 0.13612646\n",
      "Iteration 11385, loss = 0.13611224\n",
      "Iteration 11386, loss = 0.13609892\n",
      "Iteration 11387, loss = 0.13607884\n",
      "Iteration 11388, loss = 0.13606383\n",
      "Iteration 11389, loss = 0.13604603\n",
      "Iteration 11390, loss = 0.13602991\n",
      "Iteration 11391, loss = 0.13600642\n",
      "Iteration 11392, loss = 0.13599085\n",
      "Iteration 11393, loss = 0.13597624\n",
      "Iteration 11394, loss = 0.13595720\n",
      "Iteration 11395, loss = 0.13594101\n",
      "Iteration 11396, loss = 0.13592636\n",
      "Iteration 11397, loss = 0.13591021\n",
      "Iteration 11398, loss = 0.13589996\n",
      "Iteration 11399, loss = 0.13588021\n",
      "Iteration 11400, loss = 0.13586397\n",
      "Iteration 11401, loss = 0.13584711\n",
      "Iteration 11402, loss = 0.13582812\n",
      "Iteration 11403, loss = 0.13581416\n",
      "Iteration 11404, loss = 0.13579675\n",
      "Iteration 11405, loss = 0.13577807\n",
      "Iteration 11406, loss = 0.13576337\n",
      "Iteration 11407, loss = 0.13574374\n",
      "Iteration 11408, loss = 0.13573583\n",
      "Iteration 11409, loss = 0.13571580\n",
      "Iteration 11410, loss = 0.13570139\n",
      "Iteration 11411, loss = 0.13568483\n",
      "Iteration 11412, loss = 0.13566865\n",
      "Iteration 11413, loss = 0.13564566\n",
      "Iteration 11414, loss = 0.13563625\n",
      "Iteration 11415, loss = 0.13562284\n",
      "Iteration 11416, loss = 0.13560298\n",
      "Iteration 11417, loss = 0.13559037\n",
      "Iteration 11418, loss = 0.13557320\n",
      "Iteration 11419, loss = 0.13555595\n",
      "Iteration 11420, loss = 0.13554348\n",
      "Iteration 11421, loss = 0.13552875\n",
      "Iteration 11422, loss = 0.13550917\n",
      "Iteration 11423, loss = 0.13549046\n",
      "Iteration 11424, loss = 0.13547077\n",
      "Iteration 11425, loss = 0.13545515\n",
      "Iteration 11426, loss = 0.13544145\n",
      "Iteration 11427, loss = 0.13542532\n",
      "Iteration 11428, loss = 0.13540784\n",
      "Iteration 11429, loss = 0.13539073\n",
      "Iteration 11430, loss = 0.13537430\n",
      "Iteration 11431, loss = 0.13536325\n",
      "Iteration 11432, loss = 0.13534439\n",
      "Iteration 11433, loss = 0.13532450\n",
      "Iteration 11434, loss = 0.13530826\n",
      "Iteration 11435, loss = 0.13529824\n",
      "Iteration 11436, loss = 0.13528411\n",
      "Iteration 11437, loss = 0.13526461\n",
      "Iteration 11438, loss = 0.13524540\n",
      "Iteration 11439, loss = 0.13523086\n",
      "Iteration 11440, loss = 0.13521406\n",
      "Iteration 11441, loss = 0.13520256\n",
      "Iteration 11442, loss = 0.13518448\n",
      "Iteration 11443, loss = 0.13516362\n",
      "Iteration 11444, loss = 0.13515030\n",
      "Iteration 11445, loss = 0.13513251\n",
      "Iteration 11446, loss = 0.13511551\n",
      "Iteration 11447, loss = 0.13510458\n",
      "Iteration 11448, loss = 0.13509235\n",
      "Iteration 11449, loss = 0.13507127\n",
      "Iteration 11450, loss = 0.13505012\n",
      "Iteration 11451, loss = 0.13503746\n",
      "Iteration 11452, loss = 0.13502533\n",
      "Iteration 11453, loss = 0.13500657\n",
      "Iteration 11454, loss = 0.13498795\n",
      "Iteration 11455, loss = 0.13497273\n",
      "Iteration 11456, loss = 0.13495885\n",
      "Iteration 11457, loss = 0.13494096\n",
      "Iteration 11458, loss = 0.13492762\n",
      "Iteration 11459, loss = 0.13491368\n",
      "Iteration 11460, loss = 0.13489631\n",
      "Iteration 11461, loss = 0.13487991\n",
      "Iteration 11462, loss = 0.13486335\n",
      "Iteration 11463, loss = 0.13484507\n",
      "Iteration 11464, loss = 0.13482838\n",
      "Iteration 11465, loss = 0.13481519\n",
      "Iteration 11466, loss = 0.13479978\n",
      "Iteration 11467, loss = 0.13478144\n",
      "Iteration 11468, loss = 0.13476811\n",
      "Iteration 11469, loss = 0.13475421\n",
      "Iteration 11470, loss = 0.13474456\n",
      "Iteration 11471, loss = 0.13472592\n",
      "Iteration 11472, loss = 0.13470402\n",
      "Iteration 11473, loss = 0.13468698\n",
      "Iteration 11474, loss = 0.13467454\n",
      "Iteration 11475, loss = 0.13465780\n",
      "Iteration 11476, loss = 0.13464255\n",
      "Iteration 11477, loss = 0.13462244\n",
      "Iteration 11478, loss = 0.13460593\n",
      "Iteration 11479, loss = 0.13458814\n",
      "Iteration 11480, loss = 0.13457610\n",
      "Iteration 11481, loss = 0.13456292\n",
      "Iteration 11482, loss = 0.13454365\n",
      "Iteration 11483, loss = 0.13452698\n",
      "Iteration 11484, loss = 0.13451312\n",
      "Iteration 11485, loss = 0.13449396\n",
      "Iteration 11486, loss = 0.13447919\n",
      "Iteration 11487, loss = 0.13446340\n",
      "Iteration 11488, loss = 0.13444911\n",
      "Iteration 11489, loss = 0.13443510\n",
      "Iteration 11490, loss = 0.13442204\n",
      "Iteration 11491, loss = 0.13440672\n",
      "Iteration 11492, loss = 0.13439069\n",
      "Iteration 11493, loss = 0.13437467\n",
      "Iteration 11494, loss = 0.13435618\n",
      "Iteration 11495, loss = 0.13434074\n",
      "Iteration 11496, loss = 0.13432322\n",
      "Iteration 11497, loss = 0.13430444\n",
      "Iteration 11498, loss = 0.13428906\n",
      "Iteration 11499, loss = 0.13427348\n",
      "Iteration 11500, loss = 0.13425571\n",
      "Iteration 11501, loss = 0.13423739\n",
      "Iteration 11502, loss = 0.13422371\n",
      "Iteration 11503, loss = 0.13420380\n",
      "Iteration 11504, loss = 0.13419833\n",
      "Iteration 11505, loss = 0.13418161\n",
      "Iteration 11506, loss = 0.13416229\n",
      "Iteration 11507, loss = 0.13414572\n",
      "Iteration 11508, loss = 0.13413196\n",
      "Iteration 11509, loss = 0.13411455\n",
      "Iteration 11510, loss = 0.13409455\n",
      "Iteration 11511, loss = 0.13408314\n",
      "Iteration 11512, loss = 0.13406674\n",
      "Iteration 11513, loss = 0.13405523\n",
      "Iteration 11514, loss = 0.13404069\n",
      "Iteration 11515, loss = 0.13402360\n",
      "Iteration 11516, loss = 0.13400552\n",
      "Iteration 11517, loss = 0.13399126\n",
      "Iteration 11518, loss = 0.13397247\n",
      "Iteration 11519, loss = 0.13395804\n",
      "Iteration 11520, loss = 0.13394769\n",
      "Iteration 11521, loss = 0.13392944\n",
      "Iteration 11522, loss = 0.13390796\n",
      "Iteration 11523, loss = 0.13389659\n",
      "Iteration 11524, loss = 0.13388515\n",
      "Iteration 11525, loss = 0.13386716\n",
      "Iteration 11526, loss = 0.13384922\n",
      "Iteration 11527, loss = 0.13383838\n",
      "Iteration 11528, loss = 0.13382265\n",
      "Iteration 11529, loss = 0.13380534\n",
      "Iteration 11530, loss = 0.13379348\n",
      "Iteration 11531, loss = 0.13377885\n",
      "Iteration 11532, loss = 0.13376646\n",
      "Iteration 11533, loss = 0.13375028\n",
      "Iteration 11534, loss = 0.13372892\n",
      "Iteration 11535, loss = 0.13371456\n",
      "Iteration 11536, loss = 0.13370556\n",
      "Iteration 11537, loss = 0.13368440\n",
      "Iteration 11538, loss = 0.13366194\n",
      "Iteration 11539, loss = 0.13364826\n",
      "Iteration 11540, loss = 0.13362940\n",
      "Iteration 11541, loss = 0.13361921\n",
      "Iteration 11542, loss = 0.13361074\n",
      "Iteration 11543, loss = 0.13358688\n",
      "Iteration 11544, loss = 0.13357078\n",
      "Iteration 11545, loss = 0.13355244\n",
      "Iteration 11546, loss = 0.13354122\n",
      "Iteration 11547, loss = 0.13352733\n",
      "Iteration 11548, loss = 0.13350863\n",
      "Iteration 11549, loss = 0.13349407\n",
      "Iteration 11550, loss = 0.13347804\n",
      "Iteration 11551, loss = 0.13346629\n",
      "Iteration 11552, loss = 0.13344768\n",
      "Iteration 11553, loss = 0.13343106\n",
      "Iteration 11554, loss = 0.13341898\n",
      "Iteration 11555, loss = 0.13340151\n",
      "Iteration 11556, loss = 0.13338613\n",
      "Iteration 11557, loss = 0.13337180\n",
      "Iteration 11558, loss = 0.13335776\n",
      "Iteration 11559, loss = 0.13334123\n",
      "Iteration 11560, loss = 0.13333077\n",
      "Iteration 11561, loss = 0.13332016\n",
      "Iteration 11562, loss = 0.13330035\n",
      "Iteration 11563, loss = 0.13328041\n",
      "Iteration 11564, loss = 0.13326414\n",
      "Iteration 11565, loss = 0.13324138\n",
      "Iteration 11566, loss = 0.13322999\n",
      "Iteration 11567, loss = 0.13321983\n",
      "Iteration 11568, loss = 0.13320594\n",
      "Iteration 11569, loss = 0.13318142\n",
      "Iteration 11570, loss = 0.13316357\n",
      "Iteration 11571, loss = 0.13315206\n",
      "Iteration 11572, loss = 0.13313936\n",
      "Iteration 11573, loss = 0.13312858\n",
      "Iteration 11574, loss = 0.13311467\n",
      "Iteration 11575, loss = 0.13309673\n",
      "Iteration 11576, loss = 0.13307250\n",
      "Iteration 11577, loss = 0.13306231\n",
      "Iteration 11578, loss = 0.13304703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11579, loss = 0.13303708\n",
      "Iteration 11580, loss = 0.13302114\n",
      "Iteration 11581, loss = 0.13300287\n",
      "Iteration 11582, loss = 0.13298210\n",
      "Iteration 11583, loss = 0.13296748\n",
      "Iteration 11584, loss = 0.13295276\n",
      "Iteration 11585, loss = 0.13293641\n",
      "Iteration 11586, loss = 0.13291689\n",
      "Iteration 11587, loss = 0.13290604\n",
      "Iteration 11588, loss = 0.13288750\n",
      "Iteration 11589, loss = 0.13287691\n",
      "Iteration 11590, loss = 0.13285672\n",
      "Iteration 11591, loss = 0.13284356\n",
      "Iteration 11592, loss = 0.13282939\n",
      "Iteration 11593, loss = 0.13281597\n",
      "Iteration 11594, loss = 0.13280421\n",
      "Iteration 11595, loss = 0.13278671\n",
      "Iteration 11596, loss = 0.13277041\n",
      "Iteration 11597, loss = 0.13275360\n",
      "Iteration 11598, loss = 0.13273981\n",
      "Iteration 11599, loss = 0.13272231\n",
      "Iteration 11600, loss = 0.13270218\n",
      "Iteration 11601, loss = 0.13269079\n",
      "Iteration 11602, loss = 0.13267955\n",
      "Iteration 11603, loss = 0.13266672\n",
      "Iteration 11604, loss = 0.13265252\n",
      "Iteration 11605, loss = 0.13262812\n",
      "Iteration 11606, loss = 0.13261296\n",
      "Iteration 11607, loss = 0.13260489\n",
      "Iteration 11608, loss = 0.13259091\n",
      "Iteration 11609, loss = 0.13256775\n",
      "Iteration 11610, loss = 0.13254852\n",
      "Iteration 11611, loss = 0.13253502\n",
      "Iteration 11612, loss = 0.13252488\n",
      "Iteration 11613, loss = 0.13250529\n",
      "Iteration 11614, loss = 0.13249261\n",
      "Iteration 11615, loss = 0.13247901\n",
      "Iteration 11616, loss = 0.13245950\n",
      "Iteration 11617, loss = 0.13244550\n",
      "Iteration 11618, loss = 0.13243240\n",
      "Iteration 11619, loss = 0.13241615\n",
      "Iteration 11620, loss = 0.13240689\n",
      "Iteration 11621, loss = 0.13238512\n",
      "Iteration 11622, loss = 0.13236764\n",
      "Iteration 11623, loss = 0.13235737\n",
      "Iteration 11624, loss = 0.13234271\n",
      "Iteration 11625, loss = 0.13232488\n",
      "Iteration 11626, loss = 0.13230890\n",
      "Iteration 11627, loss = 0.13229171\n",
      "Iteration 11628, loss = 0.13228053\n",
      "Iteration 11629, loss = 0.13226929\n",
      "Iteration 11630, loss = 0.13225149\n",
      "Iteration 11631, loss = 0.13223656\n",
      "Iteration 11632, loss = 0.13221933\n",
      "Iteration 11633, loss = 0.13220444\n",
      "Iteration 11634, loss = 0.13218874\n",
      "Iteration 11635, loss = 0.13217291\n",
      "Iteration 11636, loss = 0.13215980\n",
      "Iteration 11637, loss = 0.13214463\n",
      "Iteration 11638, loss = 0.13213212\n",
      "Iteration 11639, loss = 0.13211908\n",
      "Iteration 11640, loss = 0.13210088\n",
      "Iteration 11641, loss = 0.13208239\n",
      "Iteration 11642, loss = 0.13206713\n",
      "Iteration 11643, loss = 0.13205174\n",
      "Iteration 11644, loss = 0.13204094\n",
      "Iteration 11645, loss = 0.13202543\n",
      "Iteration 11646, loss = 0.13200676\n",
      "Iteration 11647, loss = 0.13199118\n",
      "Iteration 11648, loss = 0.13197961\n",
      "Iteration 11649, loss = 0.13196353\n",
      "Iteration 11650, loss = 0.13194943\n",
      "Iteration 11651, loss = 0.13193809\n",
      "Iteration 11652, loss = 0.13191632\n",
      "Iteration 11653, loss = 0.13190324\n",
      "Iteration 11654, loss = 0.13189063\n",
      "Iteration 11655, loss = 0.13187318\n",
      "Iteration 11656, loss = 0.13186276\n",
      "Iteration 11657, loss = 0.13184754\n",
      "Iteration 11658, loss = 0.13182588\n",
      "Iteration 11659, loss = 0.13181291\n",
      "Iteration 11660, loss = 0.13179981\n",
      "Iteration 11661, loss = 0.13178690\n",
      "Iteration 11662, loss = 0.13177717\n",
      "Iteration 11663, loss = 0.13175741\n",
      "Iteration 11664, loss = 0.13173760\n",
      "Iteration 11665, loss = 0.13172332\n",
      "Iteration 11666, loss = 0.13170289\n",
      "Iteration 11667, loss = 0.13169936\n",
      "Iteration 11668, loss = 0.13168542\n",
      "Iteration 11669, loss = 0.13166232\n",
      "Iteration 11670, loss = 0.13164185\n",
      "Iteration 11671, loss = 0.13162433\n",
      "Iteration 11672, loss = 0.13161815\n",
      "Iteration 11673, loss = 0.13160615\n",
      "Iteration 11674, loss = 0.13159029\n",
      "Iteration 11675, loss = 0.13157174\n",
      "Iteration 11676, loss = 0.13156126\n",
      "Iteration 11677, loss = 0.13153990\n",
      "Iteration 11678, loss = 0.13152897\n",
      "Iteration 11679, loss = 0.13152313\n",
      "Iteration 11680, loss = 0.13150449\n",
      "Iteration 11681, loss = 0.13148952\n",
      "Iteration 11682, loss = 0.13147296\n",
      "Iteration 11683, loss = 0.13145492\n",
      "Iteration 11684, loss = 0.13143745\n",
      "Iteration 11685, loss = 0.13142463\n",
      "Iteration 11686, loss = 0.13141023\n",
      "Iteration 11687, loss = 0.13139071\n",
      "Iteration 11688, loss = 0.13137513\n",
      "Iteration 11689, loss = 0.13135737\n",
      "Iteration 11690, loss = 0.13134756\n",
      "Iteration 11691, loss = 0.13132700\n",
      "Iteration 11692, loss = 0.13130543\n",
      "Iteration 11693, loss = 0.13129536\n",
      "Iteration 11694, loss = 0.13127758\n",
      "Iteration 11695, loss = 0.13126233\n",
      "Iteration 11696, loss = 0.13124752\n",
      "Iteration 11697, loss = 0.13123473\n",
      "Iteration 11698, loss = 0.13122171\n",
      "Iteration 11699, loss = 0.13120552\n",
      "Iteration 11700, loss = 0.13118994\n",
      "Iteration 11701, loss = 0.13117296\n",
      "Iteration 11702, loss = 0.13116012\n",
      "Iteration 11703, loss = 0.13114252\n",
      "Iteration 11704, loss = 0.13112579\n",
      "Iteration 11705, loss = 0.13111529\n",
      "Iteration 11706, loss = 0.13110248\n",
      "Iteration 11707, loss = 0.13108572\n",
      "Iteration 11708, loss = 0.13107206\n",
      "Iteration 11709, loss = 0.13105445\n",
      "Iteration 11710, loss = 0.13104229\n",
      "Iteration 11711, loss = 0.13102571\n",
      "Iteration 11712, loss = 0.13100905\n",
      "Iteration 11713, loss = 0.13099722\n",
      "Iteration 11714, loss = 0.13098449\n",
      "Iteration 11715, loss = 0.13096287\n",
      "Iteration 11716, loss = 0.13095243\n",
      "Iteration 11717, loss = 0.13094318\n",
      "Iteration 11718, loss = 0.13092957\n",
      "Iteration 11719, loss = 0.13090780\n",
      "Iteration 11720, loss = 0.13089439\n",
      "Iteration 11721, loss = 0.13087847\n",
      "Iteration 11722, loss = 0.13085642\n",
      "Iteration 11723, loss = 0.13084509\n",
      "Iteration 11724, loss = 0.13083228\n",
      "Iteration 11725, loss = 0.13081286\n",
      "Iteration 11726, loss = 0.13079716\n",
      "Iteration 11727, loss = 0.13077897\n",
      "Iteration 11728, loss = 0.13076843\n",
      "Iteration 11729, loss = 0.13075361\n",
      "Iteration 11730, loss = 0.13073876\n",
      "Iteration 11731, loss = 0.13072530\n",
      "Iteration 11732, loss = 0.13071012\n",
      "Iteration 11733, loss = 0.13069750\n",
      "Iteration 11734, loss = 0.13068224\n",
      "Iteration 11735, loss = 0.13066627\n",
      "Iteration 11736, loss = 0.13064766\n",
      "Iteration 11737, loss = 0.13063931\n",
      "Iteration 11738, loss = 0.13061703\n",
      "Iteration 11739, loss = 0.13060040\n",
      "Iteration 11740, loss = 0.13059080\n",
      "Iteration 11741, loss = 0.13057436\n",
      "Iteration 11742, loss = 0.13056184\n",
      "Iteration 11743, loss = 0.13054543\n",
      "Iteration 11744, loss = 0.13053230\n",
      "Iteration 11745, loss = 0.13051920\n",
      "Iteration 11746, loss = 0.13050076\n",
      "Iteration 11747, loss = 0.13048579\n",
      "Iteration 11748, loss = 0.13047479\n",
      "Iteration 11749, loss = 0.13045709\n",
      "Iteration 11750, loss = 0.13043828\n",
      "Iteration 11751, loss = 0.13042491\n",
      "Iteration 11752, loss = 0.13041236\n",
      "Iteration 11753, loss = 0.13040112\n",
      "Iteration 11754, loss = 0.13037970\n",
      "Iteration 11755, loss = 0.13037102\n",
      "Iteration 11756, loss = 0.13036094\n",
      "Iteration 11757, loss = 0.13034472\n",
      "Iteration 11758, loss = 0.13032476\n",
      "Iteration 11759, loss = 0.13030789\n",
      "Iteration 11760, loss = 0.13029691\n",
      "Iteration 11761, loss = 0.13028580\n",
      "Iteration 11762, loss = 0.13026743\n",
      "Iteration 11763, loss = 0.13024589\n",
      "Iteration 11764, loss = 0.13023386\n",
      "Iteration 11765, loss = 0.13022155\n",
      "Iteration 11766, loss = 0.13020395\n",
      "Iteration 11767, loss = 0.13018758\n",
      "Iteration 11768, loss = 0.13017644\n",
      "Iteration 11769, loss = 0.13015609\n",
      "Iteration 11770, loss = 0.13014397\n",
      "Iteration 11771, loss = 0.13013464\n",
      "Iteration 11772, loss = 0.13011621\n",
      "Iteration 11773, loss = 0.13010140\n",
      "Iteration 11774, loss = 0.13008250\n",
      "Iteration 11775, loss = 0.13007197\n",
      "Iteration 11776, loss = 0.13005853\n",
      "Iteration 11777, loss = 0.13004005\n",
      "Iteration 11778, loss = 0.13002934\n",
      "Iteration 11779, loss = 0.13001352\n",
      "Iteration 11780, loss = 0.13000200\n",
      "Iteration 11781, loss = 0.12998814\n",
      "Iteration 11782, loss = 0.12997387\n",
      "Iteration 11783, loss = 0.12995775\n",
      "Iteration 11784, loss = 0.12994471\n",
      "Iteration 11785, loss = 0.12993177\n",
      "Iteration 11786, loss = 0.12991522\n",
      "Iteration 11787, loss = 0.12990201\n",
      "Iteration 11788, loss = 0.12988372\n",
      "Iteration 11789, loss = 0.12987217\n",
      "Iteration 11790, loss = 0.12986165\n",
      "Iteration 11791, loss = 0.12984505\n",
      "Iteration 11792, loss = 0.12983060\n",
      "Iteration 11793, loss = 0.12981343\n",
      "Iteration 11794, loss = 0.12979575\n",
      "Iteration 11795, loss = 0.12978657\n",
      "Iteration 11796, loss = 0.12978264\n",
      "Iteration 11797, loss = 0.12976308\n",
      "Iteration 11798, loss = 0.12974082\n",
      "Iteration 11799, loss = 0.12972118\n",
      "Iteration 11800, loss = 0.12971161\n",
      "Iteration 11801, loss = 0.12969543\n",
      "Iteration 11802, loss = 0.12967403\n",
      "Iteration 11803, loss = 0.12966719\n",
      "Iteration 11804, loss = 0.12965358\n",
      "Iteration 11805, loss = 0.12963960\n",
      "Iteration 11806, loss = 0.12961835\n",
      "Iteration 11807, loss = 0.12960597\n",
      "Iteration 11808, loss = 0.12959434\n",
      "Iteration 11809, loss = 0.12958347\n",
      "Iteration 11810, loss = 0.12956395\n",
      "Iteration 11811, loss = 0.12954392\n",
      "Iteration 11812, loss = 0.12953556\n",
      "Iteration 11813, loss = 0.12952010\n",
      "Iteration 11814, loss = 0.12950582\n",
      "Iteration 11815, loss = 0.12949041\n",
      "Iteration 11816, loss = 0.12947408\n",
      "Iteration 11817, loss = 0.12946119\n",
      "Iteration 11818, loss = 0.12944623\n",
      "Iteration 11819, loss = 0.12943079\n",
      "Iteration 11820, loss = 0.12941999\n",
      "Iteration 11821, loss = 0.12940489\n",
      "Iteration 11822, loss = 0.12938709\n",
      "Iteration 11823, loss = 0.12937178\n",
      "Iteration 11824, loss = 0.12935842\n",
      "Iteration 11825, loss = 0.12934623\n",
      "Iteration 11826, loss = 0.12932849\n",
      "Iteration 11827, loss = 0.12931213\n",
      "Iteration 11828, loss = 0.12930090\n",
      "Iteration 11829, loss = 0.12928448\n",
      "Iteration 11830, loss = 0.12926866\n",
      "Iteration 11831, loss = 0.12925359\n",
      "Iteration 11832, loss = 0.12924421\n",
      "Iteration 11833, loss = 0.12922756\n",
      "Iteration 11834, loss = 0.12921729\n",
      "Iteration 11835, loss = 0.12920077\n",
      "Iteration 11836, loss = 0.12917961\n",
      "Iteration 11837, loss = 0.12916853\n",
      "Iteration 11838, loss = 0.12915811\n",
      "Iteration 11839, loss = 0.12914719\n",
      "Iteration 11840, loss = 0.12912995\n",
      "Iteration 11841, loss = 0.12910819\n",
      "Iteration 11842, loss = 0.12909878\n",
      "Iteration 11843, loss = 0.12909007\n",
      "Iteration 11844, loss = 0.12907572\n",
      "Iteration 11845, loss = 0.12905880\n",
      "Iteration 11846, loss = 0.12904532\n",
      "Iteration 11847, loss = 0.12902623\n",
      "Iteration 11848, loss = 0.12902210\n",
      "Iteration 11849, loss = 0.12901065\n",
      "Iteration 11850, loss = 0.12899609\n",
      "Iteration 11851, loss = 0.12898279\n",
      "Iteration 11852, loss = 0.12896769\n",
      "Iteration 11853, loss = 0.12895435\n",
      "Iteration 11854, loss = 0.12894022\n",
      "Iteration 11855, loss = 0.12892570\n",
      "Iteration 11856, loss = 0.12890565\n",
      "Iteration 11857, loss = 0.12888400\n",
      "Iteration 11858, loss = 0.12887252\n",
      "Iteration 11859, loss = 0.12886433\n",
      "Iteration 11860, loss = 0.12884673\n",
      "Iteration 11861, loss = 0.12882694\n",
      "Iteration 11862, loss = 0.12881174\n",
      "Iteration 11863, loss = 0.12880179\n",
      "Iteration 11864, loss = 0.12878468\n",
      "Iteration 11865, loss = 0.12876794\n",
      "Iteration 11866, loss = 0.12875431\n",
      "Iteration 11867, loss = 0.12873823\n",
      "Iteration 11868, loss = 0.12872234\n",
      "Iteration 11869, loss = 0.12870403\n",
      "Iteration 11870, loss = 0.12869450\n",
      "Iteration 11871, loss = 0.12867736\n",
      "Iteration 11872, loss = 0.12866580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11873, loss = 0.12865360\n",
      "Iteration 11874, loss = 0.12863693\n",
      "Iteration 11875, loss = 0.12862459\n",
      "Iteration 11876, loss = 0.12861119\n",
      "Iteration 11877, loss = 0.12859730\n",
      "Iteration 11878, loss = 0.12858334\n",
      "Iteration 11879, loss = 0.12857328\n",
      "Iteration 11880, loss = 0.12855901\n",
      "Iteration 11881, loss = 0.12854336\n",
      "Iteration 11882, loss = 0.12853124\n",
      "Iteration 11883, loss = 0.12851711\n",
      "Iteration 11884, loss = 0.12850356\n",
      "Iteration 11885, loss = 0.12848068\n",
      "Iteration 11886, loss = 0.12846763\n",
      "Iteration 11887, loss = 0.12845456\n",
      "Iteration 11888, loss = 0.12844246\n",
      "Iteration 11889, loss = 0.12842932\n",
      "Iteration 11890, loss = 0.12841349\n",
      "Iteration 11891, loss = 0.12839794\n",
      "Iteration 11892, loss = 0.12837792\n",
      "Iteration 11893, loss = 0.12836893\n",
      "Iteration 11894, loss = 0.12835952\n",
      "Iteration 11895, loss = 0.12834609\n",
      "Iteration 11896, loss = 0.12833101\n",
      "Iteration 11897, loss = 0.12831512\n",
      "Iteration 11898, loss = 0.12830002\n",
      "Iteration 11899, loss = 0.12828750\n",
      "Iteration 11900, loss = 0.12827349\n",
      "Iteration 11901, loss = 0.12826229\n",
      "Iteration 11902, loss = 0.12824901\n",
      "Iteration 11903, loss = 0.12823791\n",
      "Iteration 11904, loss = 0.12822096\n",
      "Iteration 11905, loss = 0.12820347\n",
      "Iteration 11906, loss = 0.12818916\n",
      "Iteration 11907, loss = 0.12818032\n",
      "Iteration 11908, loss = 0.12816367\n",
      "Iteration 11909, loss = 0.12814224\n",
      "Iteration 11910, loss = 0.12812469\n",
      "Iteration 11911, loss = 0.12811991\n",
      "Iteration 11912, loss = 0.12810706\n",
      "Iteration 11913, loss = 0.12808949\n",
      "Iteration 11914, loss = 0.12807349\n",
      "Iteration 11915, loss = 0.12806211\n",
      "Iteration 11916, loss = 0.12805097\n",
      "Iteration 11917, loss = 0.12803293\n",
      "Iteration 11918, loss = 0.12801544\n",
      "Iteration 11919, loss = 0.12800557\n",
      "Iteration 11920, loss = 0.12799516\n",
      "Iteration 11921, loss = 0.12797268\n",
      "Iteration 11922, loss = 0.12796143\n",
      "Iteration 11923, loss = 0.12794776\n",
      "Iteration 11924, loss = 0.12793510\n",
      "Iteration 11925, loss = 0.12791544\n",
      "Iteration 11926, loss = 0.12790733\n",
      "Iteration 11927, loss = 0.12789196\n",
      "Iteration 11928, loss = 0.12787757\n",
      "Iteration 11929, loss = 0.12786140\n",
      "Iteration 11930, loss = 0.12784942\n",
      "Iteration 11931, loss = 0.12782964\n",
      "Iteration 11932, loss = 0.12781073\n",
      "Iteration 11933, loss = 0.12780579\n",
      "Iteration 11934, loss = 0.12779440\n",
      "Iteration 11935, loss = 0.12777815\n",
      "Iteration 11936, loss = 0.12776272\n",
      "Iteration 11937, loss = 0.12774874\n",
      "Iteration 11938, loss = 0.12774196\n",
      "Iteration 11939, loss = 0.12772945\n",
      "Iteration 11940, loss = 0.12771047\n",
      "Iteration 11941, loss = 0.12769498\n",
      "Iteration 11942, loss = 0.12768238\n",
      "Iteration 11943, loss = 0.12766891\n",
      "Iteration 11944, loss = 0.12765519\n",
      "Iteration 11945, loss = 0.12763540\n",
      "Iteration 11946, loss = 0.12762660\n",
      "Iteration 11947, loss = 0.12761522\n",
      "Iteration 11948, loss = 0.12760347\n",
      "Iteration 11949, loss = 0.12758633\n",
      "Iteration 11950, loss = 0.12756581\n",
      "Iteration 11951, loss = 0.12755988\n",
      "Iteration 11952, loss = 0.12754212\n",
      "Iteration 11953, loss = 0.12753189\n",
      "Iteration 11954, loss = 0.12751262\n",
      "Iteration 11955, loss = 0.12750285\n",
      "Iteration 11956, loss = 0.12748779\n",
      "Iteration 11957, loss = 0.12747392\n",
      "Iteration 11958, loss = 0.12745888\n",
      "Iteration 11959, loss = 0.12744319\n",
      "Iteration 11960, loss = 0.12742914\n",
      "Iteration 11961, loss = 0.12742063\n",
      "Iteration 11962, loss = 0.12740370\n",
      "Iteration 11963, loss = 0.12738580\n",
      "Iteration 11964, loss = 0.12736916\n",
      "Iteration 11965, loss = 0.12735908\n",
      "Iteration 11966, loss = 0.12734964\n",
      "Iteration 11967, loss = 0.12733126\n",
      "Iteration 11968, loss = 0.12732064\n",
      "Iteration 11969, loss = 0.12730842\n",
      "Iteration 11970, loss = 0.12729482\n",
      "Iteration 11971, loss = 0.12728445\n",
      "Iteration 11972, loss = 0.12726412\n",
      "Iteration 11973, loss = 0.12724311\n",
      "Iteration 11974, loss = 0.12723994\n",
      "Iteration 11975, loss = 0.12722600\n",
      "Iteration 11976, loss = 0.12721032\n",
      "Iteration 11977, loss = 0.12719759\n",
      "Iteration 11978, loss = 0.12717947\n",
      "Iteration 11979, loss = 0.12716299\n",
      "Iteration 11980, loss = 0.12715221\n",
      "Iteration 11981, loss = 0.12714000\n",
      "Iteration 11982, loss = 0.12712179\n",
      "Iteration 11983, loss = 0.12709839\n",
      "Iteration 11984, loss = 0.12709112\n",
      "Iteration 11985, loss = 0.12708110\n",
      "Iteration 11986, loss = 0.12706063\n",
      "Iteration 11987, loss = 0.12705006\n",
      "Iteration 11988, loss = 0.12703144\n",
      "Iteration 11989, loss = 0.12701940\n",
      "Iteration 11990, loss = 0.12700699\n",
      "Iteration 11991, loss = 0.12699374\n",
      "Iteration 11992, loss = 0.12697935\n",
      "Iteration 11993, loss = 0.12696597\n",
      "Iteration 11994, loss = 0.12695260\n",
      "Iteration 11995, loss = 0.12693832\n",
      "Iteration 11996, loss = 0.12692813\n",
      "Iteration 11997, loss = 0.12691265\n",
      "Iteration 11998, loss = 0.12689509\n",
      "Iteration 11999, loss = 0.12688666\n",
      "Iteration 12000, loss = 0.12687461\n",
      "Iteration 12001, loss = 0.12685953\n",
      "Iteration 12002, loss = 0.12684448\n",
      "Iteration 12003, loss = 0.12682783\n",
      "Iteration 12004, loss = 0.12681271\n",
      "Iteration 12005, loss = 0.12680020\n",
      "Iteration 12006, loss = 0.12678405\n",
      "Iteration 12007, loss = 0.12677315\n",
      "Iteration 12008, loss = 0.12675762\n",
      "Iteration 12009, loss = 0.12674937\n",
      "Iteration 12010, loss = 0.12673897\n",
      "Iteration 12011, loss = 0.12672096\n",
      "Iteration 12012, loss = 0.12670362\n",
      "Iteration 12013, loss = 0.12669795\n",
      "Iteration 12014, loss = 0.12668687\n",
      "Iteration 12015, loss = 0.12667394\n",
      "Iteration 12016, loss = 0.12665571\n",
      "Iteration 12017, loss = 0.12663629\n",
      "Iteration 12018, loss = 0.12662565\n",
      "Iteration 12019, loss = 0.12661918\n",
      "Iteration 12020, loss = 0.12660692\n",
      "Iteration 12021, loss = 0.12658609\n",
      "Iteration 12022, loss = 0.12657373\n",
      "Iteration 12023, loss = 0.12655902\n",
      "Iteration 12024, loss = 0.12654775\n",
      "Iteration 12025, loss = 0.12653420\n",
      "Iteration 12026, loss = 0.12651923\n",
      "Iteration 12027, loss = 0.12650835\n",
      "Iteration 12028, loss = 0.12649394\n",
      "Iteration 12029, loss = 0.12647953\n",
      "Iteration 12030, loss = 0.12646092\n",
      "Iteration 12031, loss = 0.12643939\n",
      "Iteration 12032, loss = 0.12643181\n",
      "Iteration 12033, loss = 0.12642603\n",
      "Iteration 12034, loss = 0.12640636\n",
      "Iteration 12035, loss = 0.12639429\n",
      "Iteration 12036, loss = 0.12638157\n",
      "Iteration 12037, loss = 0.12636961\n",
      "Iteration 12038, loss = 0.12635036\n",
      "Iteration 12039, loss = 0.12633574\n",
      "Iteration 12040, loss = 0.12632016\n",
      "Iteration 12041, loss = 0.12631571\n",
      "Iteration 12042, loss = 0.12630311\n",
      "Iteration 12043, loss = 0.12628267\n",
      "Iteration 12044, loss = 0.12626956\n",
      "Iteration 12045, loss = 0.12625799\n",
      "Iteration 12046, loss = 0.12624348\n",
      "Iteration 12047, loss = 0.12622932\n",
      "Iteration 12048, loss = 0.12621295\n",
      "Iteration 12049, loss = 0.12619606\n",
      "Iteration 12050, loss = 0.12618367\n",
      "Iteration 12051, loss = 0.12616642\n",
      "Iteration 12052, loss = 0.12615716\n",
      "Iteration 12053, loss = 0.12614605\n",
      "Iteration 12054, loss = 0.12613244\n",
      "Iteration 12055, loss = 0.12611545\n",
      "Iteration 12056, loss = 0.12610227\n",
      "Iteration 12057, loss = 0.12609468\n",
      "Iteration 12058, loss = 0.12607882\n",
      "Iteration 12059, loss = 0.12606311\n",
      "Iteration 12060, loss = 0.12605172\n",
      "Iteration 12061, loss = 0.12604382\n",
      "Iteration 12062, loss = 0.12602888\n",
      "Iteration 12063, loss = 0.12601480\n",
      "Iteration 12064, loss = 0.12599463\n",
      "Iteration 12065, loss = 0.12598584\n",
      "Iteration 12066, loss = 0.12596772\n",
      "Iteration 12067, loss = 0.12595863\n",
      "Iteration 12068, loss = 0.12594593\n",
      "Iteration 12069, loss = 0.12592763\n",
      "Iteration 12070, loss = 0.12591387\n",
      "Iteration 12071, loss = 0.12590476\n",
      "Iteration 12072, loss = 0.12589006\n",
      "Iteration 12073, loss = 0.12588071\n",
      "Iteration 12074, loss = 0.12586497\n",
      "Iteration 12075, loss = 0.12584888\n",
      "Iteration 12076, loss = 0.12584306\n",
      "Iteration 12077, loss = 0.12583113\n",
      "Iteration 12078, loss = 0.12581387\n",
      "Iteration 12079, loss = 0.12580247\n",
      "Iteration 12080, loss = 0.12579167\n",
      "Iteration 12081, loss = 0.12577617\n",
      "Iteration 12082, loss = 0.12576562\n",
      "Iteration 12083, loss = 0.12574586\n",
      "Iteration 12084, loss = 0.12573375\n",
      "Iteration 12085, loss = 0.12572108\n",
      "Iteration 12086, loss = 0.12570579\n",
      "Iteration 12087, loss = 0.12568800\n",
      "Iteration 12088, loss = 0.12567707\n",
      "Iteration 12089, loss = 0.12566084\n",
      "Iteration 12090, loss = 0.12564726\n",
      "Iteration 12091, loss = 0.12563663\n",
      "Iteration 12092, loss = 0.12562027\n",
      "Iteration 12093, loss = 0.12560653\n",
      "Iteration 12094, loss = 0.12559509\n",
      "Iteration 12095, loss = 0.12558155\n",
      "Iteration 12096, loss = 0.12557468\n",
      "Iteration 12097, loss = 0.12555994\n",
      "Iteration 12098, loss = 0.12554532\n",
      "Iteration 12099, loss = 0.12553116\n",
      "Iteration 12100, loss = 0.12552029\n",
      "Iteration 12101, loss = 0.12550764\n",
      "Iteration 12102, loss = 0.12549532\n",
      "Iteration 12103, loss = 0.12547885\n",
      "Iteration 12104, loss = 0.12546438\n",
      "Iteration 12105, loss = 0.12545038\n",
      "Iteration 12106, loss = 0.12543701\n",
      "Iteration 12107, loss = 0.12542621\n",
      "Iteration 12108, loss = 0.12540538\n",
      "Iteration 12109, loss = 0.12539628\n",
      "Iteration 12110, loss = 0.12538637\n",
      "Iteration 12111, loss = 0.12536833\n",
      "Iteration 12112, loss = 0.12535812\n",
      "Iteration 12113, loss = 0.12534175\n",
      "Iteration 12114, loss = 0.12532208\n",
      "Iteration 12115, loss = 0.12531615\n",
      "Iteration 12116, loss = 0.12530570\n",
      "Iteration 12117, loss = 0.12529400\n",
      "Iteration 12118, loss = 0.12527559\n",
      "Iteration 12119, loss = 0.12526355\n",
      "Iteration 12120, loss = 0.12525355\n",
      "Iteration 12121, loss = 0.12523825\n",
      "Iteration 12122, loss = 0.12522115\n",
      "Iteration 12123, loss = 0.12520833\n",
      "Iteration 12124, loss = 0.12519591\n",
      "Iteration 12125, loss = 0.12518125\n",
      "Iteration 12126, loss = 0.12516652\n",
      "Iteration 12127, loss = 0.12515193\n",
      "Iteration 12128, loss = 0.12514200\n",
      "Iteration 12129, loss = 0.12512517\n",
      "Iteration 12130, loss = 0.12511267\n",
      "Iteration 12131, loss = 0.12509981\n",
      "Iteration 12132, loss = 0.12509056\n",
      "Iteration 12133, loss = 0.12508026\n",
      "Iteration 12134, loss = 0.12506718\n",
      "Iteration 12135, loss = 0.12505372\n",
      "Iteration 12136, loss = 0.12504143\n",
      "Iteration 12137, loss = 0.12502531\n",
      "Iteration 12138, loss = 0.12501341\n",
      "Iteration 12139, loss = 0.12500342\n",
      "Iteration 12140, loss = 0.12498605\n",
      "Iteration 12141, loss = 0.12496517\n",
      "Iteration 12142, loss = 0.12495911\n",
      "Iteration 12143, loss = 0.12494497\n",
      "Iteration 12144, loss = 0.12493560\n",
      "Iteration 12145, loss = 0.12492595\n",
      "Iteration 12146, loss = 0.12491107\n",
      "Iteration 12147, loss = 0.12489054\n",
      "Iteration 12148, loss = 0.12487124\n",
      "Iteration 12149, loss = 0.12485863\n",
      "Iteration 12150, loss = 0.12485050\n",
      "Iteration 12151, loss = 0.12483332\n",
      "Iteration 12152, loss = 0.12482319\n",
      "Iteration 12153, loss = 0.12481518\n",
      "Iteration 12154, loss = 0.12480267\n",
      "Iteration 12155, loss = 0.12478732\n",
      "Iteration 12156, loss = 0.12477512\n",
      "Iteration 12157, loss = 0.12476355\n",
      "Iteration 12158, loss = 0.12475363\n",
      "Iteration 12159, loss = 0.12473699\n",
      "Iteration 12160, loss = 0.12471840\n",
      "Iteration 12161, loss = 0.12470772\n",
      "Iteration 12162, loss = 0.12469709\n",
      "Iteration 12163, loss = 0.12468346\n",
      "Iteration 12164, loss = 0.12466406\n",
      "Iteration 12165, loss = 0.12465488\n",
      "Iteration 12166, loss = 0.12464302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12167, loss = 0.12463268\n",
      "Iteration 12168, loss = 0.12461540\n",
      "Iteration 12169, loss = 0.12460721\n",
      "Iteration 12170, loss = 0.12459542\n",
      "Iteration 12171, loss = 0.12458429\n",
      "Iteration 12172, loss = 0.12456732\n",
      "Iteration 12173, loss = 0.12455734\n",
      "Iteration 12174, loss = 0.12454866\n",
      "Iteration 12175, loss = 0.12453143\n",
      "Iteration 12176, loss = 0.12451693\n",
      "Iteration 12177, loss = 0.12450419\n",
      "Iteration 12178, loss = 0.12448668\n",
      "Iteration 12179, loss = 0.12447395\n",
      "Iteration 12180, loss = 0.12445837\n",
      "Iteration 12181, loss = 0.12444566\n",
      "Iteration 12182, loss = 0.12444246\n",
      "Iteration 12183, loss = 0.12442904\n",
      "Iteration 12184, loss = 0.12440965\n",
      "Iteration 12185, loss = 0.12440078\n",
      "Iteration 12186, loss = 0.12438473\n",
      "Iteration 12187, loss = 0.12437229\n",
      "Iteration 12188, loss = 0.12436013\n",
      "Iteration 12189, loss = 0.12434720\n",
      "Iteration 12190, loss = 0.12433208\n",
      "Iteration 12191, loss = 0.12431701\n",
      "Iteration 12192, loss = 0.12431357\n",
      "Iteration 12193, loss = 0.12430185\n",
      "Iteration 12194, loss = 0.12428166\n",
      "Iteration 12195, loss = 0.12426526\n",
      "Iteration 12196, loss = 0.12425568\n",
      "Iteration 12197, loss = 0.12425118\n",
      "Iteration 12198, loss = 0.12423733\n",
      "Iteration 12199, loss = 0.12422061\n",
      "Iteration 12200, loss = 0.12420704\n",
      "Iteration 12201, loss = 0.12419704\n",
      "Iteration 12202, loss = 0.12418732\n",
      "Iteration 12203, loss = 0.12417762\n",
      "Iteration 12204, loss = 0.12416123\n",
      "Iteration 12205, loss = 0.12414342\n",
      "Iteration 12206, loss = 0.12412718\n",
      "Iteration 12207, loss = 0.12411328\n",
      "Iteration 12208, loss = 0.12410214\n",
      "Iteration 12209, loss = 0.12409295\n",
      "Iteration 12210, loss = 0.12407794\n",
      "Iteration 12211, loss = 0.12406013\n",
      "Iteration 12212, loss = 0.12404781\n",
      "Iteration 12213, loss = 0.12403897\n",
      "Iteration 12214, loss = 0.12402835\n",
      "Iteration 12215, loss = 0.12401316\n",
      "Iteration 12216, loss = 0.12400306\n",
      "Iteration 12217, loss = 0.12398663\n",
      "Iteration 12218, loss = 0.12397322\n",
      "Iteration 12219, loss = 0.12396935\n",
      "Iteration 12220, loss = 0.12395426\n",
      "Iteration 12221, loss = 0.12393473\n",
      "Iteration 12222, loss = 0.12392464\n",
      "Iteration 12223, loss = 0.12391314\n",
      "Iteration 12224, loss = 0.12389741\n",
      "Iteration 12225, loss = 0.12388164\n",
      "Iteration 12226, loss = 0.12387157\n",
      "Iteration 12227, loss = 0.12385778\n",
      "Iteration 12228, loss = 0.12384553\n",
      "Iteration 12229, loss = 0.12383822\n",
      "Iteration 12230, loss = 0.12382326\n",
      "Iteration 12231, loss = 0.12380753\n",
      "Iteration 12232, loss = 0.12380046\n",
      "Iteration 12233, loss = 0.12378663\n",
      "Iteration 12234, loss = 0.12376688\n",
      "Iteration 12235, loss = 0.12375908\n",
      "Iteration 12236, loss = 0.12374616\n",
      "Iteration 12237, loss = 0.12373893\n",
      "Iteration 12238, loss = 0.12373091\n",
      "Iteration 12239, loss = 0.12371549\n",
      "Iteration 12240, loss = 0.12369656\n",
      "Iteration 12241, loss = 0.12368463\n",
      "Iteration 12242, loss = 0.12367555\n",
      "Iteration 12243, loss = 0.12366472\n",
      "Iteration 12244, loss = 0.12365232\n",
      "Iteration 12245, loss = 0.12362998\n",
      "Iteration 12246, loss = 0.12362489\n",
      "Iteration 12247, loss = 0.12361617\n",
      "Iteration 12248, loss = 0.12360192\n",
      "Iteration 12249, loss = 0.12358829\n",
      "Iteration 12250, loss = 0.12357899\n",
      "Iteration 12251, loss = 0.12356250\n",
      "Iteration 12252, loss = 0.12354953\n",
      "Iteration 12253, loss = 0.12353453\n",
      "Iteration 12254, loss = 0.12352496\n",
      "Iteration 12255, loss = 0.12351914\n",
      "Iteration 12256, loss = 0.12350402\n",
      "Iteration 12257, loss = 0.12348372\n",
      "Iteration 12258, loss = 0.12347585\n",
      "Iteration 12259, loss = 0.12347359\n",
      "Iteration 12260, loss = 0.12346170\n",
      "Iteration 12261, loss = 0.12344825\n",
      "Iteration 12262, loss = 0.12342742\n",
      "Iteration 12263, loss = 0.12341270\n",
      "Iteration 12264, loss = 0.12340116\n",
      "Iteration 12265, loss = 0.12339301\n",
      "Iteration 12266, loss = 0.12337926\n",
      "Iteration 12267, loss = 0.12336113\n",
      "Iteration 12268, loss = 0.12335041\n",
      "Iteration 12269, loss = 0.12333865\n",
      "Iteration 12270, loss = 0.12332361\n",
      "Iteration 12271, loss = 0.12331139\n",
      "Iteration 12272, loss = 0.12329761\n",
      "Iteration 12273, loss = 0.12328514\n",
      "Iteration 12274, loss = 0.12327873\n",
      "Iteration 12275, loss = 0.12325767\n",
      "Iteration 12276, loss = 0.12324885\n",
      "Iteration 12277, loss = 0.12324296\n",
      "Iteration 12278, loss = 0.12323110\n",
      "Iteration 12279, loss = 0.12321174\n",
      "Iteration 12280, loss = 0.12320654\n",
      "Iteration 12281, loss = 0.12319723\n",
      "Iteration 12282, loss = 0.12318735\n",
      "Iteration 12283, loss = 0.12316724\n",
      "Iteration 12284, loss = 0.12314546\n",
      "Iteration 12285, loss = 0.12313891\n",
      "Iteration 12286, loss = 0.12312882\n",
      "Iteration 12287, loss = 0.12311421\n",
      "Iteration 12288, loss = 0.12310088\n",
      "Iteration 12289, loss = 0.12308582\n",
      "Iteration 12290, loss = 0.12307637\n",
      "Iteration 12291, loss = 0.12306709\n",
      "Iteration 12292, loss = 0.12305818\n",
      "Iteration 12293, loss = 0.12304638\n",
      "Iteration 12294, loss = 0.12303078\n",
      "Iteration 12295, loss = 0.12301975\n",
      "Iteration 12296, loss = 0.12300340\n",
      "Iteration 12297, loss = 0.12298887\n",
      "Iteration 12298, loss = 0.12298304\n",
      "Iteration 12299, loss = 0.12296935\n",
      "Iteration 12300, loss = 0.12295283\n",
      "Iteration 12301, loss = 0.12294537\n",
      "Iteration 12302, loss = 0.12293447\n",
      "Iteration 12303, loss = 0.12292263\n",
      "Iteration 12304, loss = 0.12290975\n",
      "Iteration 12305, loss = 0.12290364\n",
      "Iteration 12306, loss = 0.12288697\n",
      "Iteration 12307, loss = 0.12287503\n",
      "Iteration 12308, loss = 0.12286111\n",
      "Iteration 12309, loss = 0.12285017\n",
      "Iteration 12310, loss = 0.12283901\n",
      "Iteration 12311, loss = 0.12282240\n",
      "Iteration 12312, loss = 0.12281835\n",
      "Iteration 12313, loss = 0.12280533\n",
      "Iteration 12314, loss = 0.12278832\n",
      "Iteration 12315, loss = 0.12277996\n",
      "Iteration 12316, loss = 0.12276327\n",
      "Iteration 12317, loss = 0.12275110\n",
      "Iteration 12318, loss = 0.12274073\n",
      "Iteration 12319, loss = 0.12273084\n",
      "Iteration 12320, loss = 0.12271673\n",
      "Iteration 12321, loss = 0.12269744\n",
      "Iteration 12322, loss = 0.12268827\n",
      "Iteration 12323, loss = 0.12268074\n",
      "Iteration 12324, loss = 0.12267200\n",
      "Iteration 12325, loss = 0.12265304\n",
      "Iteration 12326, loss = 0.12263429\n",
      "Iteration 12327, loss = 0.12262664\n",
      "Iteration 12328, loss = 0.12262119\n",
      "Iteration 12329, loss = 0.12260218\n",
      "Iteration 12330, loss = 0.12259245\n",
      "Iteration 12331, loss = 0.12258546\n",
      "Iteration 12332, loss = 0.12257056\n",
      "Iteration 12333, loss = 0.12255263\n",
      "Iteration 12334, loss = 0.12254019\n",
      "Iteration 12335, loss = 0.12252305\n",
      "Iteration 12336, loss = 0.12251633\n",
      "Iteration 12337, loss = 0.12250939\n",
      "Iteration 12338, loss = 0.12249137\n",
      "Iteration 12339, loss = 0.12248545\n",
      "Iteration 12340, loss = 0.12247893\n",
      "Iteration 12341, loss = 0.12246363\n",
      "Iteration 12342, loss = 0.12244857\n",
      "Iteration 12343, loss = 0.12243122\n",
      "Iteration 12344, loss = 0.12242043\n",
      "Iteration 12345, loss = 0.12240936\n",
      "Iteration 12346, loss = 0.12239759\n",
      "Iteration 12347, loss = 0.12238034\n",
      "Iteration 12348, loss = 0.12236841\n",
      "Iteration 12349, loss = 0.12236072\n",
      "Iteration 12350, loss = 0.12234975\n",
      "Iteration 12351, loss = 0.12233919\n",
      "Iteration 12352, loss = 0.12232334\n",
      "Iteration 12353, loss = 0.12230941\n",
      "Iteration 12354, loss = 0.12229705\n",
      "Iteration 12355, loss = 0.12228676\n",
      "Iteration 12356, loss = 0.12227670\n",
      "Iteration 12357, loss = 0.12226957\n",
      "Iteration 12358, loss = 0.12225451\n",
      "Iteration 12359, loss = 0.12224032\n",
      "Iteration 12360, loss = 0.12222438\n",
      "Iteration 12361, loss = 0.12221721\n",
      "Iteration 12362, loss = 0.12220634\n",
      "Iteration 12363, loss = 0.12218967\n",
      "Iteration 12364, loss = 0.12217792\n",
      "Iteration 12365, loss = 0.12216383\n",
      "Iteration 12366, loss = 0.12215020\n",
      "Iteration 12367, loss = 0.12213442\n",
      "Iteration 12368, loss = 0.12212892\n",
      "Iteration 12369, loss = 0.12212411\n",
      "Iteration 12370, loss = 0.12210689\n",
      "Iteration 12371, loss = 0.12209134\n",
      "Iteration 12372, loss = 0.12208324\n",
      "Iteration 12373, loss = 0.12206663\n",
      "Iteration 12374, loss = 0.12205563\n",
      "Iteration 12375, loss = 0.12203951\n",
      "Iteration 12376, loss = 0.12202423\n",
      "Iteration 12377, loss = 0.12200806\n",
      "Iteration 12378, loss = 0.12199707\n",
      "Iteration 12379, loss = 0.12198886\n",
      "Iteration 12380, loss = 0.12197357\n",
      "Iteration 12381, loss = 0.12195519\n",
      "Iteration 12382, loss = 0.12194524\n",
      "Iteration 12383, loss = 0.12193377\n",
      "Iteration 12384, loss = 0.12192151\n",
      "Iteration 12385, loss = 0.12191093\n",
      "Iteration 12386, loss = 0.12189686\n",
      "Iteration 12387, loss = 0.12188773\n",
      "Iteration 12388, loss = 0.12187796\n",
      "Iteration 12389, loss = 0.12186515\n",
      "Iteration 12390, loss = 0.12186053\n",
      "Iteration 12391, loss = 0.12184968\n",
      "Iteration 12392, loss = 0.12183252\n",
      "Iteration 12393, loss = 0.12181819\n",
      "Iteration 12394, loss = 0.12180281\n",
      "Iteration 12395, loss = 0.12179053\n",
      "Iteration 12396, loss = 0.12178134\n",
      "Iteration 12397, loss = 0.12177261\n",
      "Iteration 12398, loss = 0.12175810\n",
      "Iteration 12399, loss = 0.12174258\n",
      "Iteration 12400, loss = 0.12173176\n",
      "Iteration 12401, loss = 0.12171933\n",
      "Iteration 12402, loss = 0.12170803\n",
      "Iteration 12403, loss = 0.12169957\n",
      "Iteration 12404, loss = 0.12168217\n",
      "Iteration 12405, loss = 0.12166340\n",
      "Iteration 12406, loss = 0.12165933\n",
      "Iteration 12407, loss = 0.12164506\n",
      "Iteration 12408, loss = 0.12162695\n",
      "Iteration 12409, loss = 0.12161572\n",
      "Iteration 12410, loss = 0.12160522\n",
      "Iteration 12411, loss = 0.12159186\n",
      "Iteration 12412, loss = 0.12158280\n",
      "Iteration 12413, loss = 0.12156620\n",
      "Iteration 12414, loss = 0.12155408\n",
      "Iteration 12415, loss = 0.12154317\n",
      "Iteration 12416, loss = 0.12153515\n",
      "Iteration 12417, loss = 0.12152042\n",
      "Iteration 12418, loss = 0.12150258\n",
      "Iteration 12419, loss = 0.12149440\n",
      "Iteration 12420, loss = 0.12148199\n",
      "Iteration 12421, loss = 0.12147563\n",
      "Iteration 12422, loss = 0.12146081\n",
      "Iteration 12423, loss = 0.12144707\n",
      "Iteration 12424, loss = 0.12143654\n",
      "Iteration 12425, loss = 0.12142057\n",
      "Iteration 12426, loss = 0.12141156\n",
      "Iteration 12427, loss = 0.12139747\n",
      "Iteration 12428, loss = 0.12138532\n",
      "Iteration 12429, loss = 0.12137725\n",
      "Iteration 12430, loss = 0.12135866\n",
      "Iteration 12431, loss = 0.12134807\n",
      "Iteration 12432, loss = 0.12134106\n",
      "Iteration 12433, loss = 0.12132777\n",
      "Iteration 12434, loss = 0.12131149\n",
      "Iteration 12435, loss = 0.12129610\n",
      "Iteration 12436, loss = 0.12129126\n",
      "Iteration 12437, loss = 0.12128318\n",
      "Iteration 12438, loss = 0.12126793\n",
      "Iteration 12439, loss = 0.12125376\n",
      "Iteration 12440, loss = 0.12124058\n",
      "Iteration 12441, loss = 0.12123173\n",
      "Iteration 12442, loss = 0.12121955\n",
      "Iteration 12443, loss = 0.12120700\n",
      "Iteration 12444, loss = 0.12119247\n",
      "Iteration 12445, loss = 0.12117879\n",
      "Iteration 12446, loss = 0.12116465\n",
      "Iteration 12447, loss = 0.12115349\n",
      "Iteration 12448, loss = 0.12114159\n",
      "Iteration 12449, loss = 0.12113318\n",
      "Iteration 12450, loss = 0.12112138\n",
      "Iteration 12451, loss = 0.12110716\n",
      "Iteration 12452, loss = 0.12108565\n",
      "Iteration 12453, loss = 0.12108079\n",
      "Iteration 12454, loss = 0.12107005\n",
      "Iteration 12455, loss = 0.12105223\n",
      "Iteration 12456, loss = 0.12104115\n",
      "Iteration 12457, loss = 0.12102987\n",
      "Iteration 12458, loss = 0.12101660\n",
      "Iteration 12459, loss = 0.12100342\n",
      "Iteration 12460, loss = 0.12099260\n",
      "Iteration 12461, loss = 0.12097730\n",
      "Iteration 12462, loss = 0.12096201\n",
      "Iteration 12463, loss = 0.12095615\n",
      "Iteration 12464, loss = 0.12094566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12465, loss = 0.12092987\n",
      "Iteration 12466, loss = 0.12091597\n",
      "Iteration 12467, loss = 0.12090772\n",
      "Iteration 12468, loss = 0.12089515\n",
      "Iteration 12469, loss = 0.12088016\n",
      "Iteration 12470, loss = 0.12086696\n",
      "Iteration 12471, loss = 0.12086224\n",
      "Iteration 12472, loss = 0.12084640\n",
      "Iteration 12473, loss = 0.12083196\n",
      "Iteration 12474, loss = 0.12081431\n",
      "Iteration 12475, loss = 0.12080868\n",
      "Iteration 12476, loss = 0.12079733\n",
      "Iteration 12477, loss = 0.12078291\n",
      "Iteration 12478, loss = 0.12077330\n",
      "Iteration 12479, loss = 0.12076717\n",
      "Iteration 12480, loss = 0.12075952\n",
      "Iteration 12481, loss = 0.12074779\n",
      "Iteration 12482, loss = 0.12073155\n",
      "Iteration 12483, loss = 0.12071424\n",
      "Iteration 12484, loss = 0.12070456\n",
      "Iteration 12485, loss = 0.12069194\n",
      "Iteration 12486, loss = 0.12067963\n",
      "Iteration 12487, loss = 0.12066411\n",
      "Iteration 12488, loss = 0.12065662\n",
      "Iteration 12489, loss = 0.12064823\n",
      "Iteration 12490, loss = 0.12063497\n",
      "Iteration 12491, loss = 0.12062245\n",
      "Iteration 12492, loss = 0.12061014\n",
      "Iteration 12493, loss = 0.12060087\n",
      "Iteration 12494, loss = 0.12058660\n",
      "Iteration 12495, loss = 0.12057613\n",
      "Iteration 12496, loss = 0.12056240\n",
      "Iteration 12497, loss = 0.12055024\n",
      "Iteration 12498, loss = 0.12054169\n",
      "Iteration 12499, loss = 0.12053015\n",
      "Iteration 12500, loss = 0.12051870\n",
      "Iteration 12501, loss = 0.12051108\n",
      "Iteration 12502, loss = 0.12049437\n",
      "Iteration 12503, loss = 0.12047369\n",
      "Iteration 12504, loss = 0.12047123\n",
      "Iteration 12505, loss = 0.12046066\n",
      "Iteration 12506, loss = 0.12045102\n",
      "Iteration 12507, loss = 0.12043499\n",
      "Iteration 12508, loss = 0.12041489\n",
      "Iteration 12509, loss = 0.12040269\n",
      "Iteration 12510, loss = 0.12039720\n",
      "Iteration 12511, loss = 0.12037984\n",
      "Iteration 12512, loss = 0.12036504\n",
      "Iteration 12513, loss = 0.12035522\n",
      "Iteration 12514, loss = 0.12034657\n",
      "Iteration 12515, loss = 0.12032897\n",
      "Iteration 12516, loss = 0.12031501\n",
      "Iteration 12517, loss = 0.12030718\n",
      "Iteration 12518, loss = 0.12029881\n",
      "Iteration 12519, loss = 0.12028270\n",
      "Iteration 12520, loss = 0.12027440\n",
      "Iteration 12521, loss = 0.12026689\n",
      "Iteration 12522, loss = 0.12025700\n",
      "Iteration 12523, loss = 0.12023985\n",
      "Iteration 12524, loss = 0.12022802\n",
      "Iteration 12525, loss = 0.12021518\n",
      "Iteration 12526, loss = 0.12020293\n",
      "Iteration 12527, loss = 0.12019040\n",
      "Iteration 12528, loss = 0.12018424\n",
      "Iteration 12529, loss = 0.12016976\n",
      "Iteration 12530, loss = 0.12015166\n",
      "Iteration 12531, loss = 0.12014169\n",
      "Iteration 12532, loss = 0.12012960\n",
      "Iteration 12533, loss = 0.12011340\n",
      "Iteration 12534, loss = 0.12010283\n",
      "Iteration 12535, loss = 0.12009485\n",
      "Iteration 12536, loss = 0.12008156\n",
      "Iteration 12537, loss = 0.12007123\n",
      "Iteration 12538, loss = 0.12006038\n",
      "Iteration 12539, loss = 0.12004579\n",
      "Iteration 12540, loss = 0.12003568\n",
      "Iteration 12541, loss = 0.12003218\n",
      "Iteration 12542, loss = 0.12002273\n",
      "Iteration 12543, loss = 0.12000947\n",
      "Iteration 12544, loss = 0.11998954\n",
      "Iteration 12545, loss = 0.11997573\n",
      "Iteration 12546, loss = 0.11996999\n",
      "Iteration 12547, loss = 0.11995869\n",
      "Iteration 12548, loss = 0.11994831\n",
      "Iteration 12549, loss = 0.11992730\n",
      "Iteration 12550, loss = 0.11991648\n",
      "Iteration 12551, loss = 0.11990395\n",
      "Iteration 12552, loss = 0.11989845\n",
      "Iteration 12553, loss = 0.11988626\n",
      "Iteration 12554, loss = 0.11987250\n",
      "Iteration 12555, loss = 0.11985869\n",
      "Iteration 12556, loss = 0.11984960\n",
      "Iteration 12557, loss = 0.11983530\n",
      "Iteration 12558, loss = 0.11982424\n",
      "Iteration 12559, loss = 0.11981689\n",
      "Iteration 12560, loss = 0.11980729\n",
      "Iteration 12561, loss = 0.11979609\n",
      "Iteration 12562, loss = 0.11978592\n",
      "Iteration 12563, loss = 0.11977591\n",
      "Iteration 12564, loss = 0.11976196\n",
      "Iteration 12565, loss = 0.11974999\n",
      "Iteration 12566, loss = 0.11973754\n",
      "Iteration 12567, loss = 0.11972905\n",
      "Iteration 12568, loss = 0.11972027\n",
      "Iteration 12569, loss = 0.11970627\n",
      "Iteration 12570, loss = 0.11969396\n",
      "Iteration 12571, loss = 0.11968501\n",
      "Iteration 12572, loss = 0.11967289\n",
      "Iteration 12573, loss = 0.11966090\n",
      "Iteration 12574, loss = 0.11965163\n",
      "Iteration 12575, loss = 0.11963255\n",
      "Iteration 12576, loss = 0.11961387\n",
      "Iteration 12577, loss = 0.11960593\n",
      "Iteration 12578, loss = 0.11959873\n",
      "Iteration 12579, loss = 0.11958788\n",
      "Iteration 12580, loss = 0.11957212\n",
      "Iteration 12581, loss = 0.11955599\n",
      "Iteration 12582, loss = 0.11954901\n",
      "Iteration 12583, loss = 0.11953910\n",
      "Iteration 12584, loss = 0.11952216\n",
      "Iteration 12585, loss = 0.11951488\n",
      "Iteration 12586, loss = 0.11949946\n",
      "Iteration 12587, loss = 0.11948640\n",
      "Iteration 12588, loss = 0.11947913\n",
      "Iteration 12589, loss = 0.11946836\n",
      "Iteration 12590, loss = 0.11945937\n",
      "Iteration 12591, loss = 0.11944638\n",
      "Iteration 12592, loss = 0.11943735\n",
      "Iteration 12593, loss = 0.11942397\n",
      "Iteration 12594, loss = 0.11940710\n",
      "Iteration 12595, loss = 0.11939733\n",
      "Iteration 12596, loss = 0.11938533\n",
      "Iteration 12597, loss = 0.11937456\n",
      "Iteration 12598, loss = 0.11936839\n",
      "Iteration 12599, loss = 0.11935729\n",
      "Iteration 12600, loss = 0.11934482\n",
      "Iteration 12601, loss = 0.11933055\n",
      "Iteration 12602, loss = 0.11932126\n",
      "Iteration 12603, loss = 0.11931012\n",
      "Iteration 12604, loss = 0.11929948\n",
      "Iteration 12605, loss = 0.11928702\n",
      "Iteration 12606, loss = 0.11927524\n",
      "Iteration 12607, loss = 0.11926297\n",
      "Iteration 12608, loss = 0.11924834\n",
      "Iteration 12609, loss = 0.11923533\n",
      "Iteration 12610, loss = 0.11923334\n",
      "Iteration 12611, loss = 0.11921993\n",
      "Iteration 12612, loss = 0.11919847\n",
      "Iteration 12613, loss = 0.11919351\n",
      "Iteration 12614, loss = 0.11918165\n",
      "Iteration 12615, loss = 0.11917153\n",
      "Iteration 12616, loss = 0.11916999\n",
      "Iteration 12617, loss = 0.11916005\n",
      "Iteration 12618, loss = 0.11914192\n",
      "Iteration 12619, loss = 0.11913223\n",
      "Iteration 12620, loss = 0.11911975\n",
      "Iteration 12621, loss = 0.11910889\n",
      "Iteration 12622, loss = 0.11909966\n",
      "Iteration 12623, loss = 0.11908294\n",
      "Iteration 12624, loss = 0.11906158\n",
      "Iteration 12625, loss = 0.11905850\n",
      "Iteration 12626, loss = 0.11905628\n",
      "Iteration 12627, loss = 0.11903640\n",
      "Iteration 12628, loss = 0.11902424\n",
      "Iteration 12629, loss = 0.11901210\n",
      "Iteration 12630, loss = 0.11900385\n",
      "Iteration 12631, loss = 0.11898969\n",
      "Iteration 12632, loss = 0.11897915\n",
      "Iteration 12633, loss = 0.11896867\n",
      "Iteration 12634, loss = 0.11895478\n",
      "Iteration 12635, loss = 0.11894799\n",
      "Iteration 12636, loss = 0.11893714\n",
      "Iteration 12637, loss = 0.11893153\n",
      "Iteration 12638, loss = 0.11892252\n",
      "Iteration 12639, loss = 0.11891018\n",
      "Iteration 12640, loss = 0.11889307\n",
      "Iteration 12641, loss = 0.11888099\n",
      "Iteration 12642, loss = 0.11886834\n",
      "Iteration 12643, loss = 0.11885525\n",
      "Iteration 12644, loss = 0.11885399\n",
      "Iteration 12645, loss = 0.11884403\n",
      "Iteration 12646, loss = 0.11882891\n",
      "Iteration 12647, loss = 0.11881434\n",
      "Iteration 12648, loss = 0.11880322\n",
      "Iteration 12649, loss = 0.11879485\n",
      "Iteration 12650, loss = 0.11878628\n",
      "Iteration 12651, loss = 0.11877076\n",
      "Iteration 12652, loss = 0.11875789\n",
      "Iteration 12653, loss = 0.11874922\n",
      "Iteration 12654, loss = 0.11873855\n",
      "Iteration 12655, loss = 0.11873227\n",
      "Iteration 12656, loss = 0.11871421\n",
      "Iteration 12657, loss = 0.11869933\n",
      "Iteration 12658, loss = 0.11869471\n",
      "Iteration 12659, loss = 0.11868806\n",
      "Iteration 12660, loss = 0.11867394\n",
      "Iteration 12661, loss = 0.11866234\n",
      "Iteration 12662, loss = 0.11865867\n",
      "Iteration 12663, loss = 0.11864690\n",
      "Iteration 12664, loss = 0.11863022\n",
      "Iteration 12665, loss = 0.11861222\n",
      "Iteration 12666, loss = 0.11860105\n",
      "Iteration 12667, loss = 0.11860340\n",
      "Iteration 12668, loss = 0.11859377\n",
      "Iteration 12669, loss = 0.11857692\n",
      "Iteration 12670, loss = 0.11856126\n",
      "Iteration 12671, loss = 0.11854617\n",
      "Iteration 12672, loss = 0.11853936\n",
      "Iteration 12673, loss = 0.11853317\n",
      "Iteration 12674, loss = 0.11851943\n",
      "Iteration 12675, loss = 0.11850484\n",
      "Iteration 12676, loss = 0.11848616\n",
      "Iteration 12677, loss = 0.11847368\n",
      "Iteration 12678, loss = 0.11847328\n",
      "Iteration 12679, loss = 0.11846108\n",
      "Iteration 12680, loss = 0.11844771\n",
      "Iteration 12681, loss = 0.11843856\n",
      "Iteration 12682, loss = 0.11841898\n",
      "Iteration 12683, loss = 0.11841090\n",
      "Iteration 12684, loss = 0.11840488\n",
      "Iteration 12685, loss = 0.11839586\n",
      "Iteration 12686, loss = 0.11838941\n",
      "Iteration 12687, loss = 0.11837869\n",
      "Iteration 12688, loss = 0.11836219\n",
      "Iteration 12689, loss = 0.11834489\n",
      "Iteration 12690, loss = 0.11833704\n",
      "Iteration 12691, loss = 0.11832614\n",
      "Iteration 12692, loss = 0.11831069\n",
      "Iteration 12693, loss = 0.11829743\n",
      "Iteration 12694, loss = 0.11828935\n",
      "Iteration 12695, loss = 0.11828199\n",
      "Iteration 12696, loss = 0.11826963\n",
      "Iteration 12697, loss = 0.11825766\n",
      "Iteration 12698, loss = 0.11824190\n",
      "Iteration 12699, loss = 0.11823192\n",
      "Iteration 12700, loss = 0.11822231\n",
      "Iteration 12701, loss = 0.11821358\n",
      "Iteration 12702, loss = 0.11820032\n",
      "Iteration 12703, loss = 0.11819226\n",
      "Iteration 12704, loss = 0.11817938\n",
      "Iteration 12705, loss = 0.11817266\n",
      "Iteration 12706, loss = 0.11816266\n",
      "Iteration 12707, loss = 0.11814918\n",
      "Iteration 12708, loss = 0.11814427\n",
      "Iteration 12709, loss = 0.11812780\n",
      "Iteration 12710, loss = 0.11811963\n",
      "Iteration 12711, loss = 0.11811285\n",
      "Iteration 12712, loss = 0.11809111\n",
      "Iteration 12713, loss = 0.11807848\n",
      "Iteration 12714, loss = 0.11808143\n",
      "Iteration 12715, loss = 0.11807203\n",
      "Iteration 12716, loss = 0.11805249\n",
      "Iteration 12717, loss = 0.11804057\n",
      "Iteration 12718, loss = 0.11803009\n",
      "Iteration 12719, loss = 0.11802345\n",
      "Iteration 12720, loss = 0.11800889\n",
      "Iteration 12721, loss = 0.11799126\n",
      "Iteration 12722, loss = 0.11798083\n",
      "Iteration 12723, loss = 0.11797259\n",
      "Iteration 12724, loss = 0.11796509\n",
      "Iteration 12725, loss = 0.11795276\n",
      "Iteration 12726, loss = 0.11793883\n",
      "Iteration 12727, loss = 0.11792662\n",
      "Iteration 12728, loss = 0.11791153\n",
      "Iteration 12729, loss = 0.11790393\n",
      "Iteration 12730, loss = 0.11789611\n",
      "Iteration 12731, loss = 0.11788320\n",
      "Iteration 12732, loss = 0.11787207\n",
      "Iteration 12733, loss = 0.11785863\n",
      "Iteration 12734, loss = 0.11784170\n",
      "Iteration 12735, loss = 0.11783253\n",
      "Iteration 12736, loss = 0.11782244\n",
      "Iteration 12737, loss = 0.11781429\n",
      "Iteration 12738, loss = 0.11779841\n",
      "Iteration 12739, loss = 0.11779353\n",
      "Iteration 12740, loss = 0.11778695\n",
      "Iteration 12741, loss = 0.11777379\n",
      "Iteration 12742, loss = 0.11776290\n",
      "Iteration 12743, loss = 0.11775298\n",
      "Iteration 12744, loss = 0.11773934\n",
      "Iteration 12745, loss = 0.11772756\n",
      "Iteration 12746, loss = 0.11771937\n",
      "Iteration 12747, loss = 0.11770766\n",
      "Iteration 12748, loss = 0.11769888\n",
      "Iteration 12749, loss = 0.11768654\n",
      "Iteration 12750, loss = 0.11767418\n",
      "Iteration 12751, loss = 0.11766474\n",
      "Iteration 12752, loss = 0.11765402\n",
      "Iteration 12753, loss = 0.11764900\n",
      "Iteration 12754, loss = 0.11763495\n",
      "Iteration 12755, loss = 0.11761437\n",
      "Iteration 12756, loss = 0.11761028\n",
      "Iteration 12757, loss = 0.11760851\n",
      "Iteration 12758, loss = 0.11759502\n",
      "Iteration 12759, loss = 0.11757449\n",
      "Iteration 12760, loss = 0.11756190\n",
      "Iteration 12761, loss = 0.11755757\n",
      "Iteration 12762, loss = 0.11754093\n",
      "Iteration 12763, loss = 0.11753131\n",
      "Iteration 12764, loss = 0.11752186\n",
      "Iteration 12765, loss = 0.11750869\n",
      "Iteration 12766, loss = 0.11749733\n",
      "Iteration 12767, loss = 0.11748865\n",
      "Iteration 12768, loss = 0.11747521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12769, loss = 0.11746435\n",
      "Iteration 12770, loss = 0.11745543\n",
      "Iteration 12771, loss = 0.11744497\n",
      "Iteration 12772, loss = 0.11743219\n",
      "Iteration 12773, loss = 0.11742293\n",
      "Iteration 12774, loss = 0.11741338\n",
      "Iteration 12775, loss = 0.11740488\n",
      "Iteration 12776, loss = 0.11739169\n",
      "Iteration 12777, loss = 0.11738108\n",
      "Iteration 12778, loss = 0.11737324\n",
      "Iteration 12779, loss = 0.11735973\n",
      "Iteration 12780, loss = 0.11734825\n",
      "Iteration 12781, loss = 0.11733898\n",
      "Iteration 12782, loss = 0.11733417\n",
      "Iteration 12783, loss = 0.11732042\n",
      "Iteration 12784, loss = 0.11730487\n",
      "Iteration 12785, loss = 0.11729648\n",
      "Iteration 12786, loss = 0.11728540\n",
      "Iteration 12787, loss = 0.11727729\n",
      "Iteration 12788, loss = 0.11726187\n",
      "Iteration 12789, loss = 0.11725047\n",
      "Iteration 12790, loss = 0.11724319\n",
      "Iteration 12791, loss = 0.11722582\n",
      "Iteration 12792, loss = 0.11721735\n",
      "Iteration 12793, loss = 0.11721164\n",
      "Iteration 12794, loss = 0.11719405\n",
      "Iteration 12795, loss = 0.11718930\n",
      "Iteration 12796, loss = 0.11717845\n",
      "Iteration 12797, loss = 0.11716931\n",
      "Iteration 12798, loss = 0.11716231\n",
      "Iteration 12799, loss = 0.11715185\n",
      "Iteration 12800, loss = 0.11713908\n",
      "Iteration 12801, loss = 0.11712077\n",
      "Iteration 12802, loss = 0.11711364\n",
      "Iteration 12803, loss = 0.11710634\n",
      "Iteration 12804, loss = 0.11709629\n",
      "Iteration 12805, loss = 0.11708303\n",
      "Iteration 12806, loss = 0.11706753\n",
      "Iteration 12807, loss = 0.11706296\n",
      "Iteration 12808, loss = 0.11705146\n",
      "Iteration 12809, loss = 0.11703980\n",
      "Iteration 12810, loss = 0.11702850\n",
      "Iteration 12811, loss = 0.11701617\n",
      "Iteration 12812, loss = 0.11700521\n",
      "Iteration 12813, loss = 0.11699731\n",
      "Iteration 12814, loss = 0.11698387\n",
      "Iteration 12815, loss = 0.11696949\n",
      "Iteration 12816, loss = 0.11696987\n",
      "Iteration 12817, loss = 0.11695642\n",
      "Iteration 12818, loss = 0.11693968\n",
      "Iteration 12819, loss = 0.11693251\n",
      "Iteration 12820, loss = 0.11692346\n",
      "Iteration 12821, loss = 0.11691125\n",
      "Iteration 12822, loss = 0.11690388\n",
      "Iteration 12823, loss = 0.11689511\n",
      "Iteration 12824, loss = 0.11687873\n",
      "Iteration 12825, loss = 0.11686597\n",
      "Iteration 12826, loss = 0.11685431\n",
      "Iteration 12827, loss = 0.11684853\n",
      "Iteration 12828, loss = 0.11683663\n",
      "Iteration 12829, loss = 0.11682007\n",
      "Iteration 12830, loss = 0.11680881\n",
      "Iteration 12831, loss = 0.11680009\n",
      "Iteration 12832, loss = 0.11678695\n",
      "Iteration 12833, loss = 0.11678067\n",
      "Iteration 12834, loss = 0.11676917\n",
      "Iteration 12835, loss = 0.11675918\n",
      "Iteration 12836, loss = 0.11675021\n",
      "Iteration 12837, loss = 0.11674583\n",
      "Iteration 12838, loss = 0.11673679\n",
      "Iteration 12839, loss = 0.11672280\n",
      "Iteration 12840, loss = 0.11670959\n",
      "Iteration 12841, loss = 0.11670362\n",
      "Iteration 12842, loss = 0.11669482\n",
      "Iteration 12843, loss = 0.11668155\n",
      "Iteration 12844, loss = 0.11666899\n",
      "Iteration 12845, loss = 0.11665871\n",
      "Iteration 12846, loss = 0.11665153\n",
      "Iteration 12847, loss = 0.11663602\n",
      "Iteration 12848, loss = 0.11662094\n",
      "Iteration 12849, loss = 0.11660865\n",
      "Iteration 12850, loss = 0.11660198\n",
      "Iteration 12851, loss = 0.11659194\n",
      "Iteration 12852, loss = 0.11657502\n",
      "Iteration 12853, loss = 0.11656299\n",
      "Iteration 12854, loss = 0.11655881\n",
      "Iteration 12855, loss = 0.11654843\n",
      "Iteration 12856, loss = 0.11653841\n",
      "Iteration 12857, loss = 0.11652757\n",
      "Iteration 12858, loss = 0.11651352\n",
      "Iteration 12859, loss = 0.11650308\n",
      "Iteration 12860, loss = 0.11648878\n",
      "Iteration 12861, loss = 0.11647539\n",
      "Iteration 12862, loss = 0.11647231\n",
      "Iteration 12863, loss = 0.11646336\n",
      "Iteration 12864, loss = 0.11644861\n",
      "Iteration 12865, loss = 0.11643861\n",
      "Iteration 12866, loss = 0.11642832\n",
      "Iteration 12867, loss = 0.11641963\n",
      "Iteration 12868, loss = 0.11641098\n",
      "Iteration 12869, loss = 0.11639770\n",
      "Iteration 12870, loss = 0.11638922\n",
      "Iteration 12871, loss = 0.11637088\n",
      "Iteration 12872, loss = 0.11636218\n",
      "Iteration 12873, loss = 0.11636174\n",
      "Iteration 12874, loss = 0.11634994\n",
      "Iteration 12875, loss = 0.11633459\n",
      "Iteration 12876, loss = 0.11631686\n",
      "Iteration 12877, loss = 0.11631445\n",
      "Iteration 12878, loss = 0.11630540\n",
      "Iteration 12879, loss = 0.11629147\n",
      "Iteration 12880, loss = 0.11627991\n",
      "Iteration 12881, loss = 0.11626834\n",
      "Iteration 12882, loss = 0.11625821\n",
      "Iteration 12883, loss = 0.11624261\n",
      "Iteration 12884, loss = 0.11623381\n",
      "Iteration 12885, loss = 0.11622433\n",
      "Iteration 12886, loss = 0.11621393\n",
      "Iteration 12887, loss = 0.11620563\n",
      "Iteration 12888, loss = 0.11619298\n",
      "Iteration 12889, loss = 0.11618164\n",
      "Iteration 12890, loss = 0.11616638\n",
      "Iteration 12891, loss = 0.11616124\n",
      "Iteration 12892, loss = 0.11615541\n",
      "Iteration 12893, loss = 0.11613605\n",
      "Iteration 12894, loss = 0.11612911\n",
      "Iteration 12895, loss = 0.11612413\n",
      "Iteration 12896, loss = 0.11610997\n",
      "Iteration 12897, loss = 0.11609232\n",
      "Iteration 12898, loss = 0.11608414\n",
      "Iteration 12899, loss = 0.11606961\n",
      "Iteration 12900, loss = 0.11606529\n",
      "Iteration 12901, loss = 0.11605115\n",
      "Iteration 12902, loss = 0.11603853\n",
      "Iteration 12903, loss = 0.11603359\n",
      "Iteration 12904, loss = 0.11602768\n",
      "Iteration 12905, loss = 0.11601395\n",
      "Iteration 12906, loss = 0.11600007\n",
      "Iteration 12907, loss = 0.11599535\n",
      "Iteration 12908, loss = 0.11598916\n",
      "Iteration 12909, loss = 0.11598036\n",
      "Iteration 12910, loss = 0.11596973\n",
      "Iteration 12911, loss = 0.11595909\n",
      "Iteration 12912, loss = 0.11594689\n",
      "Iteration 12913, loss = 0.11593339\n",
      "Iteration 12914, loss = 0.11591764\n",
      "Iteration 12915, loss = 0.11590863\n",
      "Iteration 12916, loss = 0.11590909\n",
      "Iteration 12917, loss = 0.11588723\n",
      "Iteration 12918, loss = 0.11587459\n",
      "Iteration 12919, loss = 0.11587146\n",
      "Iteration 12920, loss = 0.11585739\n",
      "Iteration 12921, loss = 0.11585294\n",
      "Iteration 12922, loss = 0.11584926\n",
      "Iteration 12923, loss = 0.11583220\n",
      "Iteration 12924, loss = 0.11581747\n",
      "Iteration 12925, loss = 0.11581336\n",
      "Iteration 12926, loss = 0.11580338\n",
      "Iteration 12927, loss = 0.11578762\n",
      "Iteration 12928, loss = 0.11577843\n",
      "Iteration 12929, loss = 0.11576347\n",
      "Iteration 12930, loss = 0.11575435\n",
      "Iteration 12931, loss = 0.11574687\n",
      "Iteration 12932, loss = 0.11573571\n",
      "Iteration 12933, loss = 0.11572372\n",
      "Iteration 12934, loss = 0.11571448\n",
      "Iteration 12935, loss = 0.11570170\n",
      "Iteration 12936, loss = 0.11568812\n",
      "Iteration 12937, loss = 0.11568307\n",
      "Iteration 12938, loss = 0.11566813\n",
      "Iteration 12939, loss = 0.11566000\n",
      "Iteration 12940, loss = 0.11564838\n",
      "Iteration 12941, loss = 0.11563531\n",
      "Iteration 12942, loss = 0.11562670\n",
      "Iteration 12943, loss = 0.11561629\n",
      "Iteration 12944, loss = 0.11559803\n",
      "Iteration 12945, loss = 0.11559296\n",
      "Iteration 12946, loss = 0.11558564\n",
      "Iteration 12947, loss = 0.11556909\n",
      "Iteration 12948, loss = 0.11555787\n",
      "Iteration 12949, loss = 0.11554779\n",
      "Iteration 12950, loss = 0.11554617\n",
      "Iteration 12951, loss = 0.11553431\n",
      "Iteration 12952, loss = 0.11551352\n",
      "Iteration 12953, loss = 0.11550391\n",
      "Iteration 12954, loss = 0.11550010\n",
      "Iteration 12955, loss = 0.11549090\n",
      "Iteration 12956, loss = 0.11547749\n",
      "Iteration 12957, loss = 0.11546925\n",
      "Iteration 12958, loss = 0.11545620\n",
      "Iteration 12959, loss = 0.11544092\n",
      "Iteration 12960, loss = 0.11543086\n",
      "Iteration 12961, loss = 0.11542203\n",
      "Iteration 12962, loss = 0.11540598\n",
      "Iteration 12963, loss = 0.11539379\n",
      "Iteration 12964, loss = 0.11538633\n",
      "Iteration 12965, loss = 0.11537356\n",
      "Iteration 12966, loss = 0.11537171\n",
      "Iteration 12967, loss = 0.11536581\n",
      "Iteration 12968, loss = 0.11535641\n",
      "Iteration 12969, loss = 0.11533928\n",
      "Iteration 12970, loss = 0.11533088\n",
      "Iteration 12971, loss = 0.11532070\n",
      "Iteration 12972, loss = 0.11531317\n",
      "Iteration 12973, loss = 0.11530303\n",
      "Iteration 12974, loss = 0.11528998\n",
      "Iteration 12975, loss = 0.11527908\n",
      "Iteration 12976, loss = 0.11526762\n",
      "Iteration 12977, loss = 0.11525593\n",
      "Iteration 12978, loss = 0.11524413\n",
      "Iteration 12979, loss = 0.11523396\n",
      "Iteration 12980, loss = 0.11522308\n",
      "Iteration 12981, loss = 0.11521675\n",
      "Iteration 12982, loss = 0.11520732\n",
      "Iteration 12983, loss = 0.11519024\n",
      "Iteration 12984, loss = 0.11517324\n",
      "Iteration 12985, loss = 0.11515593\n",
      "Iteration 12986, loss = 0.11514433\n",
      "Iteration 12987, loss = 0.11512900\n",
      "Iteration 12988, loss = 0.11511602\n",
      "Iteration 12989, loss = 0.11510026\n",
      "Iteration 12990, loss = 0.11507654\n",
      "Iteration 12991, loss = 0.11506492\n",
      "Iteration 12992, loss = 0.11505085\n",
      "Iteration 12993, loss = 0.11503505\n",
      "Iteration 12994, loss = 0.11501031\n",
      "Iteration 12995, loss = 0.11499601\n",
      "Iteration 12996, loss = 0.11498411\n",
      "Iteration 12997, loss = 0.11496697\n",
      "Iteration 12998, loss = 0.11494701\n",
      "Iteration 12999, loss = 0.11492734\n",
      "Iteration 13000, loss = 0.11491316\n",
      "Iteration 13001, loss = 0.11489815\n",
      "Iteration 13002, loss = 0.11488691\n",
      "Iteration 13003, loss = 0.11486776\n",
      "Iteration 13004, loss = 0.11484337\n",
      "Iteration 13005, loss = 0.11482242\n",
      "Iteration 13006, loss = 0.11480768\n",
      "Iteration 13007, loss = 0.11479612\n",
      "Iteration 13008, loss = 0.11477933\n",
      "Iteration 13009, loss = 0.11475744\n",
      "Iteration 13010, loss = 0.11473584\n",
      "Iteration 13011, loss = 0.11471861\n",
      "Iteration 13012, loss = 0.11470706\n",
      "Iteration 13013, loss = 0.11468816\n",
      "Iteration 13014, loss = 0.11466610\n",
      "Iteration 13015, loss = 0.11464519\n",
      "Iteration 13016, loss = 0.11463183\n",
      "Iteration 13017, loss = 0.11462067\n",
      "Iteration 13018, loss = 0.11460026\n",
      "Iteration 13019, loss = 0.11458329\n",
      "Iteration 13020, loss = 0.11456404\n",
      "Iteration 13021, loss = 0.11454251\n",
      "Iteration 13022, loss = 0.11453065\n",
      "Iteration 13023, loss = 0.11451667\n",
      "Iteration 13024, loss = 0.11450183\n",
      "Iteration 13025, loss = 0.11448050\n",
      "Iteration 13026, loss = 0.11445703\n",
      "Iteration 13027, loss = 0.11444632\n",
      "Iteration 13028, loss = 0.11443233\n",
      "Iteration 13029, loss = 0.11441579\n",
      "Iteration 13030, loss = 0.11440191\n",
      "Iteration 13031, loss = 0.11438641\n",
      "Iteration 13032, loss = 0.11436338\n",
      "Iteration 13033, loss = 0.11434541\n",
      "Iteration 13034, loss = 0.11433217\n",
      "Iteration 13035, loss = 0.11431301\n",
      "Iteration 13036, loss = 0.11429329\n",
      "Iteration 13037, loss = 0.11427587\n",
      "Iteration 13038, loss = 0.11425972\n",
      "Iteration 13039, loss = 0.11424301\n",
      "Iteration 13040, loss = 0.11422966\n",
      "Iteration 13041, loss = 0.11421366\n",
      "Iteration 13042, loss = 0.11419860\n",
      "Iteration 13043, loss = 0.11418340\n",
      "Iteration 13044, loss = 0.11416223\n",
      "Iteration 13045, loss = 0.11414904\n",
      "Iteration 13046, loss = 0.11413892\n",
      "Iteration 13047, loss = 0.11412545\n",
      "Iteration 13048, loss = 0.11410509\n",
      "Iteration 13049, loss = 0.11409319\n",
      "Iteration 13050, loss = 0.11408194\n",
      "Iteration 13051, loss = 0.11406965\n",
      "Iteration 13052, loss = 0.11405167\n",
      "Iteration 13053, loss = 0.11403310\n",
      "Iteration 13054, loss = 0.11401382\n",
      "Iteration 13055, loss = 0.11399969\n",
      "Iteration 13056, loss = 0.11398471\n",
      "Iteration 13057, loss = 0.11397653\n",
      "Iteration 13058, loss = 0.11396143\n",
      "Iteration 13059, loss = 0.11393949\n",
      "Iteration 13060, loss = 0.11392482\n",
      "Iteration 13061, loss = 0.11391065\n",
      "Iteration 13062, loss = 0.11389599\n",
      "Iteration 13063, loss = 0.11388981\n",
      "Iteration 13064, loss = 0.11387145\n",
      "Iteration 13065, loss = 0.11385125\n",
      "Iteration 13066, loss = 0.11383868\n",
      "Iteration 13067, loss = 0.11382066\n",
      "Iteration 13068, loss = 0.11380714\n",
      "Iteration 13069, loss = 0.11379395\n",
      "Iteration 13070, loss = 0.11378051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13071, loss = 0.11376811\n",
      "Iteration 13072, loss = 0.11375193\n",
      "Iteration 13073, loss = 0.11373140\n",
      "Iteration 13074, loss = 0.11372300\n",
      "Iteration 13075, loss = 0.11371440\n",
      "Iteration 13076, loss = 0.11369697\n",
      "Iteration 13077, loss = 0.11367994\n",
      "Iteration 13078, loss = 0.11366386\n",
      "Iteration 13079, loss = 0.11364917\n",
      "Iteration 13080, loss = 0.11363421\n",
      "Iteration 13081, loss = 0.11361488\n",
      "Iteration 13082, loss = 0.11360431\n",
      "Iteration 13083, loss = 0.11358841\n",
      "Iteration 13084, loss = 0.11357221\n",
      "Iteration 13085, loss = 0.11356088\n",
      "Iteration 13086, loss = 0.11354083\n",
      "Iteration 13087, loss = 0.11352553\n",
      "Iteration 13088, loss = 0.11352080\n",
      "Iteration 13089, loss = 0.11350501\n",
      "Iteration 13090, loss = 0.11348899\n",
      "Iteration 13091, loss = 0.11346936\n",
      "Iteration 13092, loss = 0.11345514\n",
      "Iteration 13093, loss = 0.11344524\n",
      "Iteration 13094, loss = 0.11342833\n",
      "Iteration 13095, loss = 0.11341074\n",
      "Iteration 13096, loss = 0.11339177\n",
      "Iteration 13097, loss = 0.11337428\n",
      "Iteration 13098, loss = 0.11336333\n",
      "Iteration 13099, loss = 0.11335307\n",
      "Iteration 13100, loss = 0.11333946\n",
      "Iteration 13101, loss = 0.11332156\n",
      "Iteration 13102, loss = 0.11330345\n",
      "Iteration 13103, loss = 0.11328843\n",
      "Iteration 13104, loss = 0.11327641\n",
      "Iteration 13105, loss = 0.11325933\n",
      "Iteration 13106, loss = 0.11324590\n",
      "Iteration 13107, loss = 0.11322801\n",
      "Iteration 13108, loss = 0.11321372\n",
      "Iteration 13109, loss = 0.11320714\n",
      "Iteration 13110, loss = 0.11319386\n",
      "Iteration 13111, loss = 0.11317415\n",
      "Iteration 13112, loss = 0.11315732\n",
      "Iteration 13113, loss = 0.11313822\n",
      "Iteration 13114, loss = 0.11311986\n",
      "Iteration 13115, loss = 0.11311258\n",
      "Iteration 13116, loss = 0.11310114\n",
      "Iteration 13117, loss = 0.11308353\n",
      "Iteration 13118, loss = 0.11306831\n",
      "Iteration 13119, loss = 0.11305381\n",
      "Iteration 13120, loss = 0.11303294\n",
      "Iteration 13121, loss = 0.11301951\n",
      "Iteration 13122, loss = 0.11300585\n",
      "Iteration 13123, loss = 0.11298990\n",
      "Iteration 13124, loss = 0.11297386\n",
      "Iteration 13125, loss = 0.11296495\n",
      "Iteration 13126, loss = 0.11295193\n",
      "Iteration 13127, loss = 0.11293685\n",
      "Iteration 13128, loss = 0.11291686\n",
      "Iteration 13129, loss = 0.11290221\n",
      "Iteration 13130, loss = 0.11289243\n",
      "Iteration 13131, loss = 0.11287737\n",
      "Iteration 13132, loss = 0.11285935\n",
      "Iteration 13133, loss = 0.11284968\n",
      "Iteration 13134, loss = 0.11283996\n",
      "Iteration 13135, loss = 0.11282400\n",
      "Iteration 13136, loss = 0.11280782\n",
      "Iteration 13137, loss = 0.11279596\n",
      "Iteration 13138, loss = 0.11278262\n",
      "Iteration 13139, loss = 0.11276745\n",
      "Iteration 13140, loss = 0.11275651\n",
      "Iteration 13141, loss = 0.11273847\n",
      "Iteration 13142, loss = 0.11272375\n",
      "Iteration 13143, loss = 0.11271098\n",
      "Iteration 13144, loss = 0.11269421\n",
      "Iteration 13145, loss = 0.11268387\n",
      "Iteration 13146, loss = 0.11266427\n",
      "Iteration 13147, loss = 0.11265635\n",
      "Iteration 13148, loss = 0.11264538\n",
      "Iteration 13149, loss = 0.11262559\n",
      "Iteration 13150, loss = 0.11261273\n",
      "Iteration 13151, loss = 0.11259642\n",
      "Iteration 13152, loss = 0.11258377\n",
      "Iteration 13153, loss = 0.11257054\n",
      "Iteration 13154, loss = 0.11255807\n",
      "Iteration 13155, loss = 0.11254522\n",
      "Iteration 13156, loss = 0.11253235\n",
      "Iteration 13157, loss = 0.11252175\n",
      "Iteration 13158, loss = 0.11251164\n",
      "Iteration 13159, loss = 0.11249522\n",
      "Iteration 13160, loss = 0.11248217\n",
      "Iteration 13161, loss = 0.11246907\n",
      "Iteration 13162, loss = 0.11245754\n",
      "Iteration 13163, loss = 0.11244288\n",
      "Iteration 13164, loss = 0.11241986\n",
      "Iteration 13165, loss = 0.11241742\n",
      "Iteration 13166, loss = 0.11241239\n",
      "Iteration 13167, loss = 0.11239102\n",
      "Iteration 13168, loss = 0.11237724\n",
      "Iteration 13169, loss = 0.11236747\n",
      "Iteration 13170, loss = 0.11235520\n",
      "Iteration 13171, loss = 0.11234550\n",
      "Iteration 13172, loss = 0.11232865\n",
      "Iteration 13173, loss = 0.11230165\n",
      "Iteration 13174, loss = 0.11227870\n",
      "Iteration 13175, loss = 0.11225323\n",
      "Iteration 13176, loss = 0.11222540\n",
      "Iteration 13177, loss = 0.11219499\n",
      "Iteration 13178, loss = 0.11216967\n",
      "Iteration 13179, loss = 0.11214699\n",
      "Iteration 13180, loss = 0.11212457\n",
      "Iteration 13181, loss = 0.11209474\n",
      "Iteration 13182, loss = 0.11206825\n",
      "Iteration 13183, loss = 0.11204126\n",
      "Iteration 13184, loss = 0.11201561\n",
      "Iteration 13185, loss = 0.11198471\n",
      "Iteration 13186, loss = 0.11195776\n",
      "Iteration 13187, loss = 0.11193352\n",
      "Iteration 13188, loss = 0.11189411\n",
      "Iteration 13189, loss = 0.11186281\n",
      "Iteration 13190, loss = 0.11183844\n",
      "Iteration 13191, loss = 0.11181358\n",
      "Iteration 13192, loss = 0.11177985\n",
      "Iteration 13193, loss = 0.11174960\n",
      "Iteration 13194, loss = 0.11171863\n",
      "Iteration 13195, loss = 0.11168319\n",
      "Iteration 13196, loss = 0.11165938\n",
      "Iteration 13197, loss = 0.11163672\n",
      "Iteration 13198, loss = 0.11160123\n",
      "Iteration 13199, loss = 0.11157150\n",
      "Iteration 13200, loss = 0.11154460\n",
      "Iteration 13201, loss = 0.11151476\n",
      "Iteration 13202, loss = 0.11149148\n",
      "Iteration 13203, loss = 0.11146644\n",
      "Iteration 13204, loss = 0.11143296\n",
      "Iteration 13205, loss = 0.11140326\n",
      "Iteration 13206, loss = 0.11137490\n",
      "Iteration 13207, loss = 0.11134213\n",
      "Iteration 13208, loss = 0.11132334\n",
      "Iteration 13209, loss = 0.11129357\n",
      "Iteration 13210, loss = 0.11126722\n",
      "Iteration 13211, loss = 0.11124417\n",
      "Iteration 13212, loss = 0.11121724\n",
      "Iteration 13213, loss = 0.11118553\n",
      "Iteration 13214, loss = 0.11115649\n",
      "Iteration 13215, loss = 0.11113056\n",
      "Iteration 13216, loss = 0.11110203\n",
      "Iteration 13217, loss = 0.11107347\n",
      "Iteration 13218, loss = 0.11104095\n",
      "Iteration 13219, loss = 0.11102040\n",
      "Iteration 13220, loss = 0.11099796\n",
      "Iteration 13221, loss = 0.11097242\n",
      "Iteration 13222, loss = 0.11094397\n",
      "Iteration 13223, loss = 0.11091661\n",
      "Iteration 13224, loss = 0.11088593\n",
      "Iteration 13225, loss = 0.11086722\n",
      "Iteration 13226, loss = 0.11084234\n",
      "Iteration 13227, loss = 0.11081386\n",
      "Iteration 13228, loss = 0.11078580\n",
      "Iteration 13229, loss = 0.11076044\n",
      "Iteration 13230, loss = 0.11073041\n",
      "Iteration 13231, loss = 0.11070826\n",
      "Iteration 13232, loss = 0.11068555\n",
      "Iteration 13233, loss = 0.11066318\n",
      "Iteration 13234, loss = 0.11063566\n",
      "Iteration 13235, loss = 0.11061125\n",
      "Iteration 13236, loss = 0.11058752\n",
      "Iteration 13237, loss = 0.11056419\n",
      "Iteration 13238, loss = 0.11054368\n",
      "Iteration 13239, loss = 0.11051664\n",
      "Iteration 13240, loss = 0.11049469\n",
      "Iteration 13241, loss = 0.11047650\n",
      "Iteration 13242, loss = 0.11045518\n",
      "Iteration 13243, loss = 0.11043187\n",
      "Iteration 13244, loss = 0.11041293\n",
      "Iteration 13245, loss = 0.11039339\n",
      "Iteration 13246, loss = 0.11038448\n",
      "Iteration 13247, loss = 0.11037406\n",
      "Iteration 13248, loss = 0.11035295\n",
      "Iteration 13249, loss = 0.11033845\n",
      "Iteration 13250, loss = 0.11032967\n",
      "Iteration 13251, loss = 0.11031871\n",
      "Iteration 13252, loss = 0.11030311\n",
      "Iteration 13253, loss = 0.11028417\n",
      "Iteration 13254, loss = 0.11026727\n",
      "Iteration 13255, loss = 0.11025271\n",
      "Iteration 13256, loss = 0.11024241\n",
      "Iteration 13257, loss = 0.11023142\n",
      "Iteration 13258, loss = 0.11021404\n",
      "Iteration 13259, loss = 0.11020276\n",
      "Iteration 13260, loss = 0.11019214\n",
      "Iteration 13261, loss = 0.11017560\n",
      "Iteration 13262, loss = 0.11015767\n",
      "Iteration 13263, loss = 0.11015734\n",
      "Iteration 13264, loss = 0.11014339\n",
      "Iteration 13265, loss = 0.11012753\n",
      "Iteration 13266, loss = 0.11011574\n",
      "Iteration 13267, loss = 0.11010240\n",
      "Iteration 13268, loss = 0.11009717\n",
      "Iteration 13269, loss = 0.11008746\n",
      "Iteration 13270, loss = 0.11007110\n",
      "Iteration 13271, loss = 0.11005508\n",
      "Iteration 13272, loss = 0.11004091\n",
      "Iteration 13273, loss = 0.11002374\n",
      "Iteration 13274, loss = 0.11001127\n",
      "Iteration 13275, loss = 0.10999714\n",
      "Iteration 13276, loss = 0.10998669\n",
      "Iteration 13277, loss = 0.10997201\n",
      "Iteration 13278, loss = 0.10995716\n",
      "Iteration 13279, loss = 0.10994461\n",
      "Iteration 13280, loss = 0.10993269\n",
      "Iteration 13281, loss = 0.10992288\n",
      "Iteration 13282, loss = 0.10990936\n",
      "Iteration 13283, loss = 0.10989413\n",
      "Iteration 13284, loss = 0.10988385\n",
      "Iteration 13285, loss = 0.10986755\n",
      "Iteration 13286, loss = 0.10985250\n",
      "Iteration 13287, loss = 0.10984425\n",
      "Iteration 13288, loss = 0.10983105\n",
      "Iteration 13289, loss = 0.10981949\n",
      "Iteration 13290, loss = 0.10980744\n",
      "Iteration 13291, loss = 0.10979450\n",
      "Iteration 13292, loss = 0.10977982\n",
      "Iteration 13293, loss = 0.10977146\n",
      "Iteration 13294, loss = 0.10975744\n",
      "Iteration 13295, loss = 0.10974069\n",
      "Iteration 13296, loss = 0.10973227\n",
      "Iteration 13297, loss = 0.10972325\n",
      "Iteration 13298, loss = 0.10970758\n",
      "Iteration 13299, loss = 0.10969495\n",
      "Iteration 13300, loss = 0.10968760\n",
      "Iteration 13301, loss = 0.10967471\n",
      "Iteration 13302, loss = 0.10965865\n",
      "Iteration 13303, loss = 0.10964968\n",
      "Iteration 13304, loss = 0.10963134\n",
      "Iteration 13305, loss = 0.10961641\n",
      "Iteration 13306, loss = 0.10960770\n",
      "Iteration 13307, loss = 0.10960018\n",
      "Iteration 13308, loss = 0.10958659\n",
      "Iteration 13309, loss = 0.10957270\n",
      "Iteration 13310, loss = 0.10955762\n",
      "Iteration 13311, loss = 0.10954407\n",
      "Iteration 13312, loss = 0.10953297\n",
      "Iteration 13313, loss = 0.10952143\n",
      "Iteration 13314, loss = 0.10951394\n",
      "Iteration 13315, loss = 0.10949616\n",
      "Iteration 13316, loss = 0.10947859\n",
      "Iteration 13317, loss = 0.10946693\n",
      "Iteration 13318, loss = 0.10945890\n",
      "Iteration 13319, loss = 0.10945095\n",
      "Iteration 13320, loss = 0.10943444\n",
      "Iteration 13321, loss = 0.10941966\n",
      "Iteration 13322, loss = 0.10941307\n",
      "Iteration 13323, loss = 0.10940460\n",
      "Iteration 13324, loss = 0.10939090\n",
      "Iteration 13325, loss = 0.10937094\n",
      "Iteration 13326, loss = 0.10935986\n",
      "Iteration 13327, loss = 0.10935295\n",
      "Iteration 13328, loss = 0.10933484\n",
      "Iteration 13329, loss = 0.10932442\n",
      "Iteration 13330, loss = 0.10930787\n",
      "Iteration 13331, loss = 0.10929734\n",
      "Iteration 13332, loss = 0.10928623\n",
      "Iteration 13333, loss = 0.10927912\n",
      "Iteration 13334, loss = 0.10926285\n",
      "Iteration 13335, loss = 0.10924839\n",
      "Iteration 13336, loss = 0.10923200\n",
      "Iteration 13337, loss = 0.10921705\n",
      "Iteration 13338, loss = 0.10920288\n",
      "Iteration 13339, loss = 0.10920323\n",
      "Iteration 13340, loss = 0.10919408\n",
      "Iteration 13341, loss = 0.10917084\n",
      "Iteration 13342, loss = 0.10915972\n",
      "Iteration 13343, loss = 0.10914920\n",
      "Iteration 13344, loss = 0.10913856\n",
      "Iteration 13345, loss = 0.10912922\n",
      "Iteration 13346, loss = 0.10911130\n",
      "Iteration 13347, loss = 0.10909849\n",
      "Iteration 13348, loss = 0.10908801\n",
      "Iteration 13349, loss = 0.10907643\n",
      "Iteration 13350, loss = 0.10906272\n",
      "Iteration 13351, loss = 0.10904708\n",
      "Iteration 13352, loss = 0.10903728\n",
      "Iteration 13353, loss = 0.10902695\n",
      "Iteration 13354, loss = 0.10901044\n",
      "Iteration 13355, loss = 0.10899545\n",
      "Iteration 13356, loss = 0.10898680\n",
      "Iteration 13357, loss = 0.10897912\n",
      "Iteration 13358, loss = 0.10896775\n",
      "Iteration 13359, loss = 0.10895267\n",
      "Iteration 13360, loss = 0.10893619\n",
      "Iteration 13361, loss = 0.10892948\n",
      "Iteration 13362, loss = 0.10891511\n",
      "Iteration 13363, loss = 0.10890215\n",
      "Iteration 13364, loss = 0.10888973\n",
      "Iteration 13365, loss = 0.10887921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13366, loss = 0.10886518\n",
      "Iteration 13367, loss = 0.10885549\n",
      "Iteration 13368, loss = 0.10884384\n",
      "Iteration 13369, loss = 0.10882846\n",
      "Iteration 13370, loss = 0.10881725\n",
      "Iteration 13371, loss = 0.10880546\n",
      "Iteration 13372, loss = 0.10879029\n",
      "Iteration 13373, loss = 0.10878090\n",
      "Iteration 13374, loss = 0.10877138\n",
      "Iteration 13375, loss = 0.10875992\n",
      "Iteration 13376, loss = 0.10874996\n",
      "Iteration 13377, loss = 0.10873603\n",
      "Iteration 13378, loss = 0.10872299\n",
      "Iteration 13379, loss = 0.10871492\n",
      "Iteration 13380, loss = 0.10870345\n",
      "Iteration 13381, loss = 0.10868990\n",
      "Iteration 13382, loss = 0.10867576\n",
      "Iteration 13383, loss = 0.10867044\n",
      "Iteration 13384, loss = 0.10865954\n",
      "Iteration 13385, loss = 0.10864778\n",
      "Iteration 13386, loss = 0.10863356\n",
      "Iteration 13387, loss = 0.10861480\n",
      "Iteration 13388, loss = 0.10861334\n",
      "Iteration 13389, loss = 0.10860231\n",
      "Iteration 13390, loss = 0.10858191\n",
      "Iteration 13391, loss = 0.10856856\n",
      "Iteration 13392, loss = 0.10856450\n",
      "Iteration 13393, loss = 0.10855490\n",
      "Iteration 13394, loss = 0.10854471\n",
      "Iteration 13395, loss = 0.10852693\n",
      "Iteration 13396, loss = 0.10851142\n",
      "Iteration 13397, loss = 0.10850409\n",
      "Iteration 13398, loss = 0.10849029\n",
      "Iteration 13399, loss = 0.10847522\n",
      "Iteration 13400, loss = 0.10846240\n",
      "Iteration 13401, loss = 0.10845651\n",
      "Iteration 13402, loss = 0.10845047\n",
      "Iteration 13403, loss = 0.10843116\n",
      "Iteration 13404, loss = 0.10841718\n",
      "Iteration 13405, loss = 0.10840828\n",
      "Iteration 13406, loss = 0.10840121\n",
      "Iteration 13407, loss = 0.10838813\n",
      "Iteration 13408, loss = 0.10836230\n",
      "Iteration 13409, loss = 0.10836293\n",
      "Iteration 13410, loss = 0.10835837\n",
      "Iteration 13411, loss = 0.10834454\n",
      "Iteration 13412, loss = 0.10832855\n",
      "Iteration 13413, loss = 0.10831501\n",
      "Iteration 13414, loss = 0.10830709\n",
      "Iteration 13415, loss = 0.10828929\n",
      "Iteration 13416, loss = 0.10827652\n",
      "Iteration 13417, loss = 0.10827090\n",
      "Iteration 13418, loss = 0.10825839\n",
      "Iteration 13419, loss = 0.10824764\n",
      "Iteration 13420, loss = 0.10823747\n",
      "Iteration 13421, loss = 0.10822524\n",
      "Iteration 13422, loss = 0.10821404\n",
      "Iteration 13423, loss = 0.10819661\n",
      "Iteration 13424, loss = 0.10818474\n",
      "Iteration 13425, loss = 0.10817567\n",
      "Iteration 13426, loss = 0.10815985\n",
      "Iteration 13427, loss = 0.10814984\n",
      "Iteration 13428, loss = 0.10813957\n",
      "Iteration 13429, loss = 0.10812172\n",
      "Iteration 13430, loss = 0.10811221\n",
      "Iteration 13431, loss = 0.10810670\n",
      "Iteration 13432, loss = 0.10809687\n",
      "Iteration 13433, loss = 0.10808280\n",
      "Iteration 13434, loss = 0.10807195\n",
      "Iteration 13435, loss = 0.10805648\n",
      "Iteration 13436, loss = 0.10804811\n",
      "Iteration 13437, loss = 0.10803451\n",
      "Iteration 13438, loss = 0.10801280\n",
      "Iteration 13439, loss = 0.10797918\n",
      "Iteration 13440, loss = 0.10794341\n",
      "Iteration 13441, loss = 0.10789785\n",
      "Iteration 13442, loss = 0.10785041\n",
      "Iteration 13443, loss = 0.10780067\n",
      "Iteration 13444, loss = 0.10775074\n",
      "Iteration 13445, loss = 0.10771718\n",
      "Iteration 13446, loss = 0.10769789\n",
      "Iteration 13447, loss = 0.10767858\n",
      "Iteration 13448, loss = 0.10766124\n",
      "Iteration 13449, loss = 0.10764141\n",
      "Iteration 13450, loss = 0.10762150\n",
      "Iteration 13451, loss = 0.10759905\n",
      "Iteration 13452, loss = 0.10757391\n",
      "Iteration 13453, loss = 0.10755515\n",
      "Iteration 13454, loss = 0.10753653\n",
      "Iteration 13455, loss = 0.10751442\n",
      "Iteration 13456, loss = 0.10749146\n",
      "Iteration 13457, loss = 0.10746265\n",
      "Iteration 13458, loss = 0.10744426\n",
      "Iteration 13459, loss = 0.10742739\n",
      "Iteration 13460, loss = 0.10741798\n",
      "Iteration 13461, loss = 0.10739743\n",
      "Iteration 13462, loss = 0.10736079\n",
      "Iteration 13463, loss = 0.10733197\n",
      "Iteration 13464, loss = 0.10731416\n",
      "Iteration 13465, loss = 0.10728907\n",
      "Iteration 13466, loss = 0.10726589\n",
      "Iteration 13467, loss = 0.10724558\n",
      "Iteration 13468, loss = 0.10722386\n",
      "Iteration 13469, loss = 0.10720418\n",
      "Iteration 13470, loss = 0.10718113\n",
      "Iteration 13471, loss = 0.10715996\n",
      "Iteration 13472, loss = 0.10714098\n",
      "Iteration 13473, loss = 0.10711721\n",
      "Iteration 13474, loss = 0.10709224\n",
      "Iteration 13475, loss = 0.10706594\n",
      "Iteration 13476, loss = 0.10705258\n",
      "Iteration 13477, loss = 0.10703083\n",
      "Iteration 13478, loss = 0.10700608\n",
      "Iteration 13479, loss = 0.10698319\n",
      "Iteration 13480, loss = 0.10696243\n",
      "Iteration 13481, loss = 0.10694038\n",
      "Iteration 13482, loss = 0.10691764\n",
      "Iteration 13483, loss = 0.10690516\n",
      "Iteration 13484, loss = 0.10689222\n",
      "Iteration 13485, loss = 0.10687886\n",
      "Iteration 13486, loss = 0.10686722\n",
      "Iteration 13487, loss = 0.10685606\n",
      "Iteration 13488, loss = 0.10684875\n",
      "Iteration 13489, loss = 0.10683076\n",
      "Iteration 13490, loss = 0.10682053\n",
      "Iteration 13491, loss = 0.10681113\n",
      "Iteration 13492, loss = 0.10679395\n",
      "Iteration 13493, loss = 0.10677998\n",
      "Iteration 13494, loss = 0.10675999\n",
      "Iteration 13495, loss = 0.10673536\n",
      "Iteration 13496, loss = 0.10671883\n",
      "Iteration 13497, loss = 0.10672141\n",
      "Iteration 13498, loss = 0.10670915\n",
      "Iteration 13499, loss = 0.10669335\n",
      "Iteration 13500, loss = 0.10667399\n",
      "Iteration 13501, loss = 0.10665799\n",
      "Iteration 13502, loss = 0.10663835\n",
      "Iteration 13503, loss = 0.10662248\n",
      "Iteration 13504, loss = 0.10661232\n",
      "Iteration 13505, loss = 0.10659655\n",
      "Iteration 13506, loss = 0.10658890\n",
      "Iteration 13507, loss = 0.10657078\n",
      "Iteration 13508, loss = 0.10655605\n",
      "Iteration 13509, loss = 0.10653634\n",
      "Iteration 13510, loss = 0.10652613\n",
      "Iteration 13511, loss = 0.10651395\n",
      "Iteration 13512, loss = 0.10649999\n",
      "Iteration 13513, loss = 0.10647678\n",
      "Iteration 13514, loss = 0.10645958\n",
      "Iteration 13515, loss = 0.10645024\n",
      "Iteration 13516, loss = 0.10643599\n",
      "Iteration 13517, loss = 0.10642872\n",
      "Iteration 13518, loss = 0.10641378\n",
      "Iteration 13519, loss = 0.10639410\n",
      "Iteration 13520, loss = 0.10637792\n",
      "Iteration 13521, loss = 0.10636531\n",
      "Iteration 13522, loss = 0.10635167\n",
      "Iteration 13523, loss = 0.10633222\n",
      "Iteration 13524, loss = 0.10632063\n",
      "Iteration 13525, loss = 0.10630415\n",
      "Iteration 13526, loss = 0.10629525\n",
      "Iteration 13527, loss = 0.10628250\n",
      "Iteration 13528, loss = 0.10627002\n",
      "Iteration 13529, loss = 0.10625717\n",
      "Iteration 13530, loss = 0.10624053\n",
      "Iteration 13531, loss = 0.10622218\n",
      "Iteration 13532, loss = 0.10621254\n",
      "Iteration 13533, loss = 0.10619405\n",
      "Iteration 13534, loss = 0.10618012\n",
      "Iteration 13535, loss = 0.10616519\n",
      "Iteration 13536, loss = 0.10615157\n",
      "Iteration 13537, loss = 0.10614048\n",
      "Iteration 13538, loss = 0.10612994\n",
      "Iteration 13539, loss = 0.10611761\n",
      "Iteration 13540, loss = 0.10609636\n",
      "Iteration 13541, loss = 0.10607351\n",
      "Iteration 13542, loss = 0.10606902\n",
      "Iteration 13543, loss = 0.10606187\n",
      "Iteration 13544, loss = 0.10604263\n",
      "Iteration 13545, loss = 0.10602211\n",
      "Iteration 13546, loss = 0.10601152\n",
      "Iteration 13547, loss = 0.10600215\n",
      "Iteration 13548, loss = 0.10599335\n",
      "Iteration 13549, loss = 0.10597737\n",
      "Iteration 13550, loss = 0.10595943\n",
      "Iteration 13551, loss = 0.10594079\n",
      "Iteration 13552, loss = 0.10592370\n",
      "Iteration 13553, loss = 0.10590906\n",
      "Iteration 13554, loss = 0.10589757\n",
      "Iteration 13555, loss = 0.10588633\n",
      "Iteration 13556, loss = 0.10587642\n",
      "Iteration 13557, loss = 0.10586014\n",
      "Iteration 13558, loss = 0.10584917\n",
      "Iteration 13559, loss = 0.10583799\n",
      "Iteration 13560, loss = 0.10581904\n",
      "Iteration 13561, loss = 0.10580069\n",
      "Iteration 13562, loss = 0.10578287\n",
      "Iteration 13563, loss = 0.10577473\n",
      "Iteration 13564, loss = 0.10576481\n",
      "Iteration 13565, loss = 0.10575314\n",
      "Iteration 13566, loss = 0.10574084\n",
      "Iteration 13567, loss = 0.10572386\n",
      "Iteration 13568, loss = 0.10570698\n",
      "Iteration 13569, loss = 0.10569409\n",
      "Iteration 13570, loss = 0.10567869\n",
      "Iteration 13571, loss = 0.10566665\n",
      "Iteration 13572, loss = 0.10565615\n",
      "Iteration 13573, loss = 0.10564035\n",
      "Iteration 13574, loss = 0.10562707\n",
      "Iteration 13575, loss = 0.10560519\n",
      "Iteration 13576, loss = 0.10557011\n",
      "Iteration 13577, loss = 0.10553148\n",
      "Iteration 13578, loss = 0.10548759\n",
      "Iteration 13579, loss = 0.10543928\n",
      "Iteration 13580, loss = 0.10538693\n",
      "Iteration 13581, loss = 0.10534192\n",
      "Iteration 13582, loss = 0.10528629\n",
      "Iteration 13583, loss = 0.10522785\n",
      "Iteration 13584, loss = 0.10517506\n",
      "Iteration 13585, loss = 0.10511949\n",
      "Iteration 13586, loss = 0.10505918\n",
      "Iteration 13587, loss = 0.10499819\n",
      "Iteration 13588, loss = 0.10493639\n",
      "Iteration 13589, loss = 0.10487448\n",
      "Iteration 13590, loss = 0.10481471\n",
      "Iteration 13591, loss = 0.10475896\n",
      "Iteration 13592, loss = 0.10470515\n",
      "Iteration 13593, loss = 0.10464116\n",
      "Iteration 13594, loss = 0.10457861\n",
      "Iteration 13595, loss = 0.10451629\n",
      "Iteration 13596, loss = 0.10446291\n",
      "Iteration 13597, loss = 0.10440743\n",
      "Iteration 13598, loss = 0.10435513\n",
      "Iteration 13599, loss = 0.10429645\n",
      "Iteration 13600, loss = 0.10423690\n",
      "Iteration 13601, loss = 0.10418324\n",
      "Iteration 13602, loss = 0.10413145\n",
      "Iteration 13603, loss = 0.10408014\n",
      "Iteration 13604, loss = 0.10402558\n",
      "Iteration 13605, loss = 0.10397048\n",
      "Iteration 13606, loss = 0.10394548\n",
      "Iteration 13607, loss = 0.10392180\n",
      "Iteration 13608, loss = 0.10389600\n",
      "Iteration 13609, loss = 0.10387204\n",
      "Iteration 13610, loss = 0.10384872\n",
      "Iteration 13611, loss = 0.10382284\n",
      "Iteration 13612, loss = 0.10379743\n",
      "Iteration 13613, loss = 0.10377563\n",
      "Iteration 13614, loss = 0.10374874\n",
      "Iteration 13615, loss = 0.10372525\n",
      "Iteration 13616, loss = 0.10369961\n",
      "Iteration 13617, loss = 0.10367795\n",
      "Iteration 13618, loss = 0.10365135\n",
      "Iteration 13619, loss = 0.10362879\n",
      "Iteration 13620, loss = 0.10360461\n",
      "Iteration 13621, loss = 0.10357722\n",
      "Iteration 13622, loss = 0.10355175\n",
      "Iteration 13623, loss = 0.10353021\n",
      "Iteration 13624, loss = 0.10350949\n",
      "Iteration 13625, loss = 0.10348292\n",
      "Iteration 13626, loss = 0.10345177\n",
      "Iteration 13627, loss = 0.10343443\n",
      "Iteration 13628, loss = 0.10341080\n",
      "Iteration 13629, loss = 0.10338778\n",
      "Iteration 13630, loss = 0.10336344\n",
      "Iteration 13631, loss = 0.10333715\n",
      "Iteration 13632, loss = 0.10331218\n",
      "Iteration 13633, loss = 0.10329014\n",
      "Iteration 13634, loss = 0.10326848\n",
      "Iteration 13635, loss = 0.10324427\n",
      "Iteration 13636, loss = 0.10321749\n",
      "Iteration 13637, loss = 0.10319381\n",
      "Iteration 13638, loss = 0.10317456\n",
      "Iteration 13639, loss = 0.10315376\n",
      "Iteration 13640, loss = 0.10312975\n",
      "Iteration 13641, loss = 0.10310212\n",
      "Iteration 13642, loss = 0.10308113\n",
      "Iteration 13643, loss = 0.10305753\n",
      "Iteration 13644, loss = 0.10303512\n",
      "Iteration 13645, loss = 0.10301520\n",
      "Iteration 13646, loss = 0.10299354\n",
      "Iteration 13647, loss = 0.10297460\n",
      "Iteration 13648, loss = 0.10294941\n",
      "Iteration 13649, loss = 0.10292608\n",
      "Iteration 13650, loss = 0.10290648\n",
      "Iteration 13651, loss = 0.10288664\n",
      "Iteration 13652, loss = 0.10285823\n",
      "Iteration 13653, loss = 0.10283369\n",
      "Iteration 13654, loss = 0.10281539\n",
      "Iteration 13655, loss = 0.10279244\n",
      "Iteration 13656, loss = 0.10276668\n",
      "Iteration 13657, loss = 0.10275031\n",
      "Iteration 13658, loss = 0.10272710\n",
      "Iteration 13659, loss = 0.10270168\n",
      "Iteration 13660, loss = 0.10267781\n",
      "Iteration 13661, loss = 0.10265888\n",
      "Iteration 13662, loss = 0.10263780\n",
      "Iteration 13663, loss = 0.10261888\n",
      "Iteration 13664, loss = 0.10259640\n",
      "Iteration 13665, loss = 0.10257442\n",
      "Iteration 13666, loss = 0.10255084\n",
      "Iteration 13667, loss = 0.10253406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13668, loss = 0.10252428\n",
      "Iteration 13669, loss = 0.10251071\n",
      "Iteration 13670, loss = 0.10249363\n",
      "Iteration 13671, loss = 0.10247652\n",
      "Iteration 13672, loss = 0.10245098\n",
      "Iteration 13673, loss = 0.10242865\n",
      "Iteration 13674, loss = 0.10241286\n",
      "Iteration 13675, loss = 0.10239492\n",
      "Iteration 13676, loss = 0.10237800\n",
      "Iteration 13677, loss = 0.10235772\n",
      "Iteration 13678, loss = 0.10234416\n",
      "Iteration 13679, loss = 0.10232556\n",
      "Iteration 13680, loss = 0.10230575\n",
      "Iteration 13681, loss = 0.10228849\n",
      "Iteration 13682, loss = 0.10227194\n",
      "Iteration 13683, loss = 0.10225414\n",
      "Iteration 13684, loss = 0.10223072\n",
      "Iteration 13685, loss = 0.10221233\n",
      "Iteration 13686, loss = 0.10219452\n",
      "Iteration 13687, loss = 0.10218852\n",
      "Iteration 13688, loss = 0.10216707\n",
      "Iteration 13689, loss = 0.10214535\n",
      "Iteration 13690, loss = 0.10212558\n",
      "Iteration 13691, loss = 0.10210927\n",
      "Iteration 13692, loss = 0.10209982\n",
      "Iteration 13693, loss = 0.10208568\n",
      "Iteration 13694, loss = 0.10206564\n",
      "Iteration 13695, loss = 0.10204641\n",
      "Iteration 13696, loss = 0.10203022\n",
      "Iteration 13697, loss = 0.10201522\n",
      "Iteration 13698, loss = 0.10199614\n",
      "Iteration 13699, loss = 0.10198100\n",
      "Iteration 13700, loss = 0.10196479\n",
      "Iteration 13701, loss = 0.10194982\n",
      "Iteration 13702, loss = 0.10192754\n",
      "Iteration 13703, loss = 0.10191307\n",
      "Iteration 13704, loss = 0.10190016\n",
      "Iteration 13705, loss = 0.10188384\n",
      "Iteration 13706, loss = 0.10186052\n",
      "Iteration 13707, loss = 0.10184305\n",
      "Iteration 13708, loss = 0.10183324\n",
      "Iteration 13709, loss = 0.10182003\n",
      "Iteration 13710, loss = 0.10180609\n",
      "Iteration 13711, loss = 0.10179094\n",
      "Iteration 13712, loss = 0.10177140\n",
      "Iteration 13713, loss = 0.10175683\n",
      "Iteration 13714, loss = 0.10173954\n",
      "Iteration 13715, loss = 0.10172217\n",
      "Iteration 13716, loss = 0.10170153\n",
      "Iteration 13717, loss = 0.10169264\n",
      "Iteration 13718, loss = 0.10167706\n",
      "Iteration 13719, loss = 0.10165622\n",
      "Iteration 13720, loss = 0.10163818\n",
      "Iteration 13721, loss = 0.10162374\n",
      "Iteration 13722, loss = 0.10161218\n",
      "Iteration 13723, loss = 0.10159892\n",
      "Iteration 13724, loss = 0.10158010\n",
      "Iteration 13725, loss = 0.10156486\n",
      "Iteration 13726, loss = 0.10155334\n",
      "Iteration 13727, loss = 0.10153795\n",
      "Iteration 13728, loss = 0.10151672\n",
      "Iteration 13729, loss = 0.10149974\n",
      "Iteration 13730, loss = 0.10149147\n",
      "Iteration 13731, loss = 0.10147201\n",
      "Iteration 13732, loss = 0.10145735\n",
      "Iteration 13733, loss = 0.10144109\n",
      "Iteration 13734, loss = 0.10142599\n",
      "Iteration 13735, loss = 0.10141824\n",
      "Iteration 13736, loss = 0.10139763\n",
      "Iteration 13737, loss = 0.10138058\n",
      "Iteration 13738, loss = 0.10136524\n",
      "Iteration 13739, loss = 0.10135055\n",
      "Iteration 13740, loss = 0.10133691\n",
      "Iteration 13741, loss = 0.10131622\n",
      "Iteration 13742, loss = 0.10130390\n",
      "Iteration 13743, loss = 0.10128963\n",
      "Iteration 13744, loss = 0.10127519\n",
      "Iteration 13745, loss = 0.10126222\n",
      "Iteration 13746, loss = 0.10125019\n",
      "Iteration 13747, loss = 0.10123602\n",
      "Iteration 13748, loss = 0.10121811\n",
      "Iteration 13749, loss = 0.10120339\n",
      "Iteration 13750, loss = 0.10119394\n",
      "Iteration 13751, loss = 0.10117411\n",
      "Iteration 13752, loss = 0.10115951\n",
      "Iteration 13753, loss = 0.10114481\n",
      "Iteration 13754, loss = 0.10112924\n",
      "Iteration 13755, loss = 0.10111592\n",
      "Iteration 13756, loss = 0.10110023\n",
      "Iteration 13757, loss = 0.10109193\n",
      "Iteration 13758, loss = 0.10107622\n",
      "Iteration 13759, loss = 0.10106060\n",
      "Iteration 13760, loss = 0.10105032\n",
      "Iteration 13761, loss = 0.10103491\n",
      "Iteration 13762, loss = 0.10102197\n",
      "Iteration 13763, loss = 0.10100981\n",
      "Iteration 13764, loss = 0.10099394\n",
      "Iteration 13765, loss = 0.10098177\n",
      "Iteration 13766, loss = 0.10096184\n",
      "Iteration 13767, loss = 0.10094474\n",
      "Iteration 13768, loss = 0.10093220\n",
      "Iteration 13769, loss = 0.10091585\n",
      "Iteration 13770, loss = 0.10090641\n",
      "Iteration 13771, loss = 0.10089610\n",
      "Iteration 13772, loss = 0.10088247\n",
      "Iteration 13773, loss = 0.10086937\n",
      "Iteration 13774, loss = 0.10084931\n",
      "Iteration 13775, loss = 0.10083506\n",
      "Iteration 13776, loss = 0.10083087\n",
      "Iteration 13777, loss = 0.10081525\n",
      "Iteration 13778, loss = 0.10079728\n",
      "Iteration 13779, loss = 0.10078109\n",
      "Iteration 13780, loss = 0.10077377\n",
      "Iteration 13781, loss = 0.10076361\n",
      "Iteration 13782, loss = 0.10074807\n",
      "Iteration 13783, loss = 0.10073023\n",
      "Iteration 13784, loss = 0.10071461\n",
      "Iteration 13785, loss = 0.10070280\n",
      "Iteration 13786, loss = 0.10068795\n",
      "Iteration 13787, loss = 0.10067236\n",
      "Iteration 13788, loss = 0.10066493\n",
      "Iteration 13789, loss = 0.10064858\n",
      "Iteration 13790, loss = 0.10063566\n",
      "Iteration 13791, loss = 0.10062264\n",
      "Iteration 13792, loss = 0.10060942\n",
      "Iteration 13793, loss = 0.10059710\n",
      "Iteration 13794, loss = 0.10058472\n",
      "Iteration 13795, loss = 0.10057241\n",
      "Iteration 13796, loss = 0.10056418\n",
      "Iteration 13797, loss = 0.10054611\n",
      "Iteration 13798, loss = 0.10052805\n",
      "Iteration 13799, loss = 0.10051810\n",
      "Iteration 13800, loss = 0.10050721\n",
      "Iteration 13801, loss = 0.10048649\n",
      "Iteration 13802, loss = 0.10047647\n",
      "Iteration 13803, loss = 0.10046617\n",
      "Iteration 13804, loss = 0.10045294\n",
      "Iteration 13805, loss = 0.10043650\n",
      "Iteration 13806, loss = 0.10042398\n",
      "Iteration 13807, loss = 0.10040629\n",
      "Iteration 13808, loss = 0.10039868\n",
      "Iteration 13809, loss = 0.10038778\n",
      "Iteration 13810, loss = 0.10037539\n",
      "Iteration 13811, loss = 0.10036049\n",
      "Iteration 13812, loss = 0.10034686\n",
      "Iteration 13813, loss = 0.10033030\n",
      "Iteration 13814, loss = 0.10031943\n",
      "Iteration 13815, loss = 0.10030662\n",
      "Iteration 13816, loss = 0.10029577\n",
      "Iteration 13817, loss = 0.10028583\n",
      "Iteration 13818, loss = 0.10026824\n",
      "Iteration 13819, loss = 0.10025377\n",
      "Iteration 13820, loss = 0.10024417\n",
      "Iteration 13821, loss = 0.10023029\n",
      "Iteration 13822, loss = 0.10021620\n",
      "Iteration 13823, loss = 0.10019047\n",
      "Iteration 13824, loss = 0.10017079\n",
      "Iteration 13825, loss = 0.10013945\n",
      "Iteration 13826, loss = 0.10009831\n",
      "Iteration 13827, loss = 0.10006081\n",
      "Iteration 13828, loss = 0.10001798\n",
      "Iteration 13829, loss = 0.09997462\n",
      "Iteration 13830, loss = 0.09992507\n",
      "Iteration 13831, loss = 0.09987539\n",
      "Iteration 13832, loss = 0.09982632\n",
      "Iteration 13833, loss = 0.09977506\n",
      "Iteration 13834, loss = 0.09972100\n",
      "Iteration 13835, loss = 0.09966904\n",
      "Iteration 13836, loss = 0.09962022\n",
      "Iteration 13837, loss = 0.09956533\n",
      "Iteration 13838, loss = 0.09950787\n",
      "Iteration 13839, loss = 0.09945358\n",
      "Iteration 13840, loss = 0.09940254\n",
      "Iteration 13841, loss = 0.09935396\n",
      "Iteration 13842, loss = 0.09935749\n",
      "Iteration 13843, loss = 0.09935206\n",
      "Iteration 13844, loss = 0.09934765\n",
      "Iteration 13845, loss = 0.09933299\n",
      "Iteration 13846, loss = 0.09931711\n",
      "Iteration 13847, loss = 0.09930186\n",
      "Iteration 13848, loss = 0.09927905\n",
      "Iteration 13849, loss = 0.09925880\n",
      "Iteration 13850, loss = 0.09923409\n",
      "Iteration 13851, loss = 0.09920514\n",
      "Iteration 13852, loss = 0.09916957\n",
      "Iteration 13853, loss = 0.09913382\n",
      "Iteration 13854, loss = 0.09910002\n",
      "Iteration 13855, loss = 0.09905776\n",
      "Iteration 13856, loss = 0.09900864\n",
      "Iteration 13857, loss = 0.09899118\n",
      "Iteration 13858, loss = 0.09897847\n",
      "Iteration 13859, loss = 0.09896141\n",
      "Iteration 13860, loss = 0.09893995\n",
      "Iteration 13861, loss = 0.09890957\n",
      "Iteration 13862, loss = 0.09887670\n",
      "Iteration 13863, loss = 0.09884442\n",
      "Iteration 13864, loss = 0.09881213\n",
      "Iteration 13865, loss = 0.09876600\n",
      "Iteration 13866, loss = 0.09873711\n",
      "Iteration 13867, loss = 0.09872167\n",
      "Iteration 13868, loss = 0.09869694\n",
      "Iteration 13869, loss = 0.09867342\n",
      "Iteration 13870, loss = 0.09864680\n",
      "Iteration 13871, loss = 0.09860679\n",
      "Iteration 13872, loss = 0.09857232\n",
      "Iteration 13873, loss = 0.09854349\n",
      "Iteration 13874, loss = 0.09854221\n",
      "Iteration 13875, loss = 0.09853193\n",
      "Iteration 13876, loss = 0.09851017\n",
      "Iteration 13877, loss = 0.09849357\n",
      "Iteration 13878, loss = 0.09847854\n",
      "Iteration 13879, loss = 0.09846611\n",
      "Iteration 13880, loss = 0.09844963\n",
      "Iteration 13881, loss = 0.09843166\n",
      "Iteration 13882, loss = 0.09841249\n",
      "Iteration 13883, loss = 0.09839725\n",
      "Iteration 13884, loss = 0.09839433\n",
      "Iteration 13885, loss = 0.09837929\n",
      "Iteration 13886, loss = 0.09835793\n",
      "Iteration 13887, loss = 0.09834202\n",
      "Iteration 13888, loss = 0.09833368\n",
      "Iteration 13889, loss = 0.09832140\n",
      "Iteration 13890, loss = 0.09830488\n",
      "Iteration 13891, loss = 0.09828816\n",
      "Iteration 13892, loss = 0.09826687\n",
      "Iteration 13893, loss = 0.09825998\n",
      "Iteration 13894, loss = 0.09824656\n",
      "Iteration 13895, loss = 0.09823351\n",
      "Iteration 13896, loss = 0.09822215\n",
      "Iteration 13897, loss = 0.09820532\n",
      "Iteration 13898, loss = 0.09819225\n",
      "Iteration 13899, loss = 0.09818207\n",
      "Iteration 13900, loss = 0.09817008\n",
      "Iteration 13901, loss = 0.09815016\n",
      "Iteration 13902, loss = 0.09813689\n",
      "Iteration 13903, loss = 0.09812537\n",
      "Iteration 13904, loss = 0.09811129\n",
      "Iteration 13905, loss = 0.09809650\n",
      "Iteration 13906, loss = 0.09808845\n",
      "Iteration 13907, loss = 0.09807545\n",
      "Iteration 13908, loss = 0.09806118\n",
      "Iteration 13909, loss = 0.09804652\n",
      "Iteration 13910, loss = 0.09803290\n",
      "Iteration 13911, loss = 0.09802301\n",
      "Iteration 13912, loss = 0.09800659\n",
      "Iteration 13913, loss = 0.09799214\n",
      "Iteration 13914, loss = 0.09798193\n",
      "Iteration 13915, loss = 0.09796724\n",
      "Iteration 13916, loss = 0.09795509\n",
      "Iteration 13917, loss = 0.09793989\n",
      "Iteration 13918, loss = 0.09792504\n",
      "Iteration 13919, loss = 0.09791185\n",
      "Iteration 13920, loss = 0.09789967\n",
      "Iteration 13921, loss = 0.09788607\n",
      "Iteration 13922, loss = 0.09786898\n",
      "Iteration 13923, loss = 0.09785804\n",
      "Iteration 13924, loss = 0.09784067\n",
      "Iteration 13925, loss = 0.09783035\n",
      "Iteration 13926, loss = 0.09781966\n",
      "Iteration 13927, loss = 0.09780666\n",
      "Iteration 13928, loss = 0.09779034\n",
      "Iteration 13929, loss = 0.09777893\n",
      "Iteration 13930, loss = 0.09777046\n",
      "Iteration 13931, loss = 0.09775829\n",
      "Iteration 13932, loss = 0.09774465\n",
      "Iteration 13933, loss = 0.09773198\n",
      "Iteration 13934, loss = 0.09772029\n",
      "Iteration 13935, loss = 0.09770660\n",
      "Iteration 13936, loss = 0.09769907\n",
      "Iteration 13937, loss = 0.09767665\n",
      "Iteration 13938, loss = 0.09766824\n",
      "Iteration 13939, loss = 0.09766089\n",
      "Iteration 13940, loss = 0.09764420\n",
      "Iteration 13941, loss = 0.09762875\n",
      "Iteration 13942, loss = 0.09761468\n",
      "Iteration 13943, loss = 0.09760578\n",
      "Iteration 13944, loss = 0.09759280\n",
      "Iteration 13945, loss = 0.09757854\n",
      "Iteration 13946, loss = 0.09756670\n",
      "Iteration 13947, loss = 0.09754639\n",
      "Iteration 13948, loss = 0.09753022\n",
      "Iteration 13949, loss = 0.09752669\n",
      "Iteration 13950, loss = 0.09751639\n",
      "Iteration 13951, loss = 0.09749739\n",
      "Iteration 13952, loss = 0.09748253\n",
      "Iteration 13953, loss = 0.09747009\n",
      "Iteration 13954, loss = 0.09745787\n",
      "Iteration 13955, loss = 0.09744384\n",
      "Iteration 13956, loss = 0.09743322\n",
      "Iteration 13957, loss = 0.09742644\n",
      "Iteration 13958, loss = 0.09740842\n",
      "Iteration 13959, loss = 0.09738493\n",
      "Iteration 13960, loss = 0.09737550\n",
      "Iteration 13961, loss = 0.09736445\n",
      "Iteration 13962, loss = 0.09734418\n",
      "Iteration 13963, loss = 0.09733627\n",
      "Iteration 13964, loss = 0.09731967\n",
      "Iteration 13965, loss = 0.09730127\n",
      "Iteration 13966, loss = 0.09729213\n",
      "Iteration 13967, loss = 0.09727667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13968, loss = 0.09726335\n",
      "Iteration 13969, loss = 0.09725514\n",
      "Iteration 13970, loss = 0.09724328\n",
      "Iteration 13971, loss = 0.09722982\n",
      "Iteration 13972, loss = 0.09721614\n",
      "Iteration 13973, loss = 0.09720455\n",
      "Iteration 13974, loss = 0.09719364\n",
      "Iteration 13975, loss = 0.09717897\n",
      "Iteration 13976, loss = 0.09716543\n",
      "Iteration 13977, loss = 0.09716050\n",
      "Iteration 13978, loss = 0.09714523\n",
      "Iteration 13979, loss = 0.09712447\n",
      "Iteration 13980, loss = 0.09711714\n",
      "Iteration 13981, loss = 0.09710694\n",
      "Iteration 13982, loss = 0.09709296\n",
      "Iteration 13983, loss = 0.09708132\n",
      "Iteration 13984, loss = 0.09706796\n",
      "Iteration 13985, loss = 0.09705591\n",
      "Iteration 13986, loss = 0.09704307\n",
      "Iteration 13987, loss = 0.09702672\n",
      "Iteration 13988, loss = 0.09701291\n",
      "Iteration 13989, loss = 0.09700145\n",
      "Iteration 13990, loss = 0.09698976\n",
      "Iteration 13991, loss = 0.09696792\n",
      "Iteration 13992, loss = 0.09695919\n",
      "Iteration 13993, loss = 0.09694994\n",
      "Iteration 13994, loss = 0.09693307\n",
      "Iteration 13995, loss = 0.09691755\n",
      "Iteration 13996, loss = 0.09691197\n",
      "Iteration 13997, loss = 0.09690462\n",
      "Iteration 13998, loss = 0.09689048\n",
      "Iteration 13999, loss = 0.09687624\n",
      "Iteration 14000, loss = 0.09686158\n",
      "Iteration 14001, loss = 0.09685328\n",
      "Iteration 14002, loss = 0.09683796\n",
      "Iteration 14003, loss = 0.09682285\n",
      "Iteration 14004, loss = 0.09681006\n",
      "Iteration 14005, loss = 0.09679203\n",
      "Iteration 14006, loss = 0.09679282\n",
      "Iteration 14007, loss = 0.09677949\n",
      "Iteration 14008, loss = 0.09675859\n",
      "Iteration 14009, loss = 0.09674262\n",
      "Iteration 14010, loss = 0.09673198\n",
      "Iteration 14011, loss = 0.09672488\n",
      "Iteration 14012, loss = 0.09671341\n",
      "Iteration 14013, loss = 0.09670082\n",
      "Iteration 14014, loss = 0.09668415\n",
      "Iteration 14015, loss = 0.09666936\n",
      "Iteration 14016, loss = 0.09665670\n",
      "Iteration 14017, loss = 0.09664152\n",
      "Iteration 14018, loss = 0.09662797\n",
      "Iteration 14019, loss = 0.09661295\n",
      "Iteration 14020, loss = 0.09660714\n",
      "Iteration 14021, loss = 0.09659644\n",
      "Iteration 14022, loss = 0.09657691\n",
      "Iteration 14023, loss = 0.09656548\n",
      "Iteration 14024, loss = 0.09655270\n",
      "Iteration 14025, loss = 0.09654057\n",
      "Iteration 14026, loss = 0.09653072\n",
      "Iteration 14027, loss = 0.09651447\n",
      "Iteration 14028, loss = 0.09650350\n",
      "Iteration 14029, loss = 0.09649049\n",
      "Iteration 14030, loss = 0.09647918\n",
      "Iteration 14031, loss = 0.09646597\n",
      "Iteration 14032, loss = 0.09645194\n",
      "Iteration 14033, loss = 0.09643715\n",
      "Iteration 14034, loss = 0.09642851\n",
      "Iteration 14035, loss = 0.09641790\n",
      "Iteration 14036, loss = 0.09640816\n",
      "Iteration 14037, loss = 0.09639100\n",
      "Iteration 14038, loss = 0.09637602\n",
      "Iteration 14039, loss = 0.09636387\n",
      "Iteration 14040, loss = 0.09635711\n",
      "Iteration 14041, loss = 0.09634487\n",
      "Iteration 14042, loss = 0.09633069\n",
      "Iteration 14043, loss = 0.09631432\n",
      "Iteration 14044, loss = 0.09629884\n",
      "Iteration 14045, loss = 0.09628651\n",
      "Iteration 14046, loss = 0.09627216\n",
      "Iteration 14047, loss = 0.09625913\n",
      "Iteration 14048, loss = 0.09624688\n",
      "Iteration 14049, loss = 0.09623624\n",
      "Iteration 14050, loss = 0.09622690\n",
      "Iteration 14051, loss = 0.09621426\n",
      "Iteration 14052, loss = 0.09620016\n",
      "Iteration 14053, loss = 0.09618531\n",
      "Iteration 14054, loss = 0.09617249\n",
      "Iteration 14055, loss = 0.09616388\n",
      "Iteration 14056, loss = 0.09615272\n",
      "Iteration 14057, loss = 0.09613937\n",
      "Iteration 14058, loss = 0.09612277\n",
      "Iteration 14059, loss = 0.09611720\n",
      "Iteration 14060, loss = 0.09610613\n",
      "Iteration 14061, loss = 0.09609239\n",
      "Iteration 14062, loss = 0.09608088\n",
      "Iteration 14063, loss = 0.09606911\n",
      "Iteration 14064, loss = 0.09604704\n",
      "Iteration 14065, loss = 0.09603786\n",
      "Iteration 14066, loss = 0.09602679\n",
      "Iteration 14067, loss = 0.09601493\n",
      "Iteration 14068, loss = 0.09600237\n",
      "Iteration 14069, loss = 0.09598988\n",
      "Iteration 14070, loss = 0.09597988\n",
      "Iteration 14071, loss = 0.09596776\n",
      "Iteration 14072, loss = 0.09595048\n",
      "Iteration 14073, loss = 0.09593836\n",
      "Iteration 14074, loss = 0.09592710\n",
      "Iteration 14075, loss = 0.09591191\n",
      "Iteration 14076, loss = 0.09590596\n",
      "Iteration 14077, loss = 0.09589518\n",
      "Iteration 14078, loss = 0.09587634\n",
      "Iteration 14079, loss = 0.09586343\n",
      "Iteration 14080, loss = 0.09585508\n",
      "Iteration 14081, loss = 0.09584105\n",
      "Iteration 14082, loss = 0.09582946\n",
      "Iteration 14083, loss = 0.09580971\n",
      "Iteration 14084, loss = 0.09579376\n",
      "Iteration 14085, loss = 0.09578501\n",
      "Iteration 14086, loss = 0.09578210\n",
      "Iteration 14087, loss = 0.09576534\n",
      "Iteration 14088, loss = 0.09574618\n",
      "Iteration 14089, loss = 0.09573515\n",
      "Iteration 14090, loss = 0.09572115\n",
      "Iteration 14091, loss = 0.09570837\n",
      "Iteration 14092, loss = 0.09570132\n",
      "Iteration 14093, loss = 0.09568357\n",
      "Iteration 14094, loss = 0.09567484\n",
      "Iteration 14095, loss = 0.09566536\n",
      "Iteration 14096, loss = 0.09565264\n",
      "Iteration 14097, loss = 0.09564143\n",
      "Iteration 14098, loss = 0.09563094\n",
      "Iteration 14099, loss = 0.09561712\n",
      "Iteration 14100, loss = 0.09560314\n",
      "Iteration 14101, loss = 0.09559268\n",
      "Iteration 14102, loss = 0.09558170\n",
      "Iteration 14103, loss = 0.09556487\n",
      "Iteration 14104, loss = 0.09554790\n",
      "Iteration 14105, loss = 0.09553854\n",
      "Iteration 14106, loss = 0.09552773\n",
      "Iteration 14107, loss = 0.09551378\n",
      "Iteration 14108, loss = 0.09550394\n",
      "Iteration 14109, loss = 0.09548698\n",
      "Iteration 14110, loss = 0.09547457\n",
      "Iteration 14111, loss = 0.09547099\n",
      "Iteration 14112, loss = 0.09545451\n",
      "Iteration 14113, loss = 0.09543731\n",
      "Iteration 14114, loss = 0.09543155\n",
      "Iteration 14115, loss = 0.09542638\n",
      "Iteration 14116, loss = 0.09541198\n",
      "Iteration 14117, loss = 0.09539150\n",
      "Iteration 14118, loss = 0.09538308\n",
      "Iteration 14119, loss = 0.09537263\n",
      "Iteration 14120, loss = 0.09536117\n",
      "Iteration 14121, loss = 0.09534414\n",
      "Iteration 14122, loss = 0.09532953\n",
      "Iteration 14123, loss = 0.09532290\n",
      "Iteration 14124, loss = 0.09531786\n",
      "Iteration 14125, loss = 0.09530566\n",
      "Iteration 14126, loss = 0.09528797\n",
      "Iteration 14127, loss = 0.09527212\n",
      "Iteration 14128, loss = 0.09526206\n",
      "Iteration 14129, loss = 0.09525112\n",
      "Iteration 14130, loss = 0.09524418\n",
      "Iteration 14131, loss = 0.09522875\n",
      "Iteration 14132, loss = 0.09520711\n",
      "Iteration 14133, loss = 0.09520025\n",
      "Iteration 14134, loss = 0.09518459\n",
      "Iteration 14135, loss = 0.09517871\n",
      "Iteration 14136, loss = 0.09516770\n",
      "Iteration 14137, loss = 0.09514956\n",
      "Iteration 14138, loss = 0.09513515\n",
      "Iteration 14139, loss = 0.09511961\n",
      "Iteration 14140, loss = 0.09511325\n",
      "Iteration 14141, loss = 0.09510322\n",
      "Iteration 14142, loss = 0.09508829\n",
      "Iteration 14143, loss = 0.09507259\n",
      "Iteration 14144, loss = 0.09507193\n",
      "Iteration 14145, loss = 0.09505999\n",
      "Iteration 14146, loss = 0.09504385\n",
      "Iteration 14147, loss = 0.09503162\n",
      "Iteration 14148, loss = 0.09502162\n",
      "Iteration 14149, loss = 0.09500634\n",
      "Iteration 14150, loss = 0.09499245\n",
      "Iteration 14151, loss = 0.09498455\n",
      "Iteration 14152, loss = 0.09497021\n",
      "Iteration 14153, loss = 0.09495498\n",
      "Iteration 14154, loss = 0.09494492\n",
      "Iteration 14155, loss = 0.09492817\n",
      "Iteration 14156, loss = 0.09491872\n",
      "Iteration 14157, loss = 0.09491194\n",
      "Iteration 14158, loss = 0.09489934\n",
      "Iteration 14159, loss = 0.09488461\n",
      "Iteration 14160, loss = 0.09487443\n",
      "Iteration 14161, loss = 0.09486258\n",
      "Iteration 14162, loss = 0.09485150\n",
      "Iteration 14163, loss = 0.09483803\n",
      "Iteration 14164, loss = 0.09482231\n",
      "Iteration 14165, loss = 0.09481162\n",
      "Iteration 14166, loss = 0.09480355\n",
      "Iteration 14167, loss = 0.09479001\n",
      "Iteration 14168, loss = 0.09477545\n",
      "Iteration 14169, loss = 0.09476281\n",
      "Iteration 14170, loss = 0.09475329\n",
      "Iteration 14171, loss = 0.09474548\n",
      "Iteration 14172, loss = 0.09473231\n",
      "Iteration 14173, loss = 0.09471543\n",
      "Iteration 14174, loss = 0.09470118\n",
      "Iteration 14175, loss = 0.09469104\n",
      "Iteration 14176, loss = 0.09467912\n",
      "Iteration 14177, loss = 0.09466799\n",
      "Iteration 14178, loss = 0.09465689\n",
      "Iteration 14179, loss = 0.09464298\n",
      "Iteration 14180, loss = 0.09463178\n",
      "Iteration 14181, loss = 0.09461373\n",
      "Iteration 14182, loss = 0.09460438\n",
      "Iteration 14183, loss = 0.09459318\n",
      "Iteration 14184, loss = 0.09458111\n",
      "Iteration 14185, loss = 0.09457011\n",
      "Iteration 14186, loss = 0.09455660\n",
      "Iteration 14187, loss = 0.09454655\n",
      "Iteration 14188, loss = 0.09453678\n",
      "Iteration 14189, loss = 0.09452430\n",
      "Iteration 14190, loss = 0.09451167\n",
      "Iteration 14191, loss = 0.09449624\n",
      "Iteration 14192, loss = 0.09448937\n",
      "Iteration 14193, loss = 0.09447295\n",
      "Iteration 14194, loss = 0.09446393\n",
      "Iteration 14195, loss = 0.09445122\n",
      "Iteration 14196, loss = 0.09443925\n",
      "Iteration 14197, loss = 0.09443093\n",
      "Iteration 14198, loss = 0.09441420\n",
      "Iteration 14199, loss = 0.09440173\n",
      "Iteration 14200, loss = 0.09439131\n",
      "Iteration 14201, loss = 0.09438145\n",
      "Iteration 14202, loss = 0.09436624\n",
      "Iteration 14203, loss = 0.09435128\n",
      "Iteration 14204, loss = 0.09434317\n",
      "Iteration 14205, loss = 0.09433360\n",
      "Iteration 14206, loss = 0.09431884\n",
      "Iteration 14207, loss = 0.09431253\n",
      "Iteration 14208, loss = 0.09429814\n",
      "Iteration 14209, loss = 0.09428741\n",
      "Iteration 14210, loss = 0.09427130\n",
      "Iteration 14211, loss = 0.09426468\n",
      "Iteration 14212, loss = 0.09425106\n",
      "Iteration 14213, loss = 0.09424058\n",
      "Iteration 14214, loss = 0.09422976\n",
      "Iteration 14215, loss = 0.09421980\n",
      "Iteration 14216, loss = 0.09420506\n",
      "Iteration 14217, loss = 0.09419938\n",
      "Iteration 14218, loss = 0.09418230\n",
      "Iteration 14219, loss = 0.09416692\n",
      "Iteration 14220, loss = 0.09415277\n",
      "Iteration 14221, loss = 0.09414273\n",
      "Iteration 14222, loss = 0.09413250\n",
      "Iteration 14223, loss = 0.09411596\n",
      "Iteration 14224, loss = 0.09410435\n",
      "Iteration 14225, loss = 0.09409087\n",
      "Iteration 14226, loss = 0.09408255\n",
      "Iteration 14227, loss = 0.09406626\n",
      "Iteration 14228, loss = 0.09405500\n",
      "Iteration 14229, loss = 0.09404556\n",
      "Iteration 14230, loss = 0.09402930\n",
      "Iteration 14231, loss = 0.09401496\n",
      "Iteration 14232, loss = 0.09401102\n",
      "Iteration 14233, loss = 0.09399789\n",
      "Iteration 14234, loss = 0.09398483\n",
      "Iteration 14235, loss = 0.09397351\n",
      "Iteration 14236, loss = 0.09395797\n",
      "Iteration 14237, loss = 0.09394733\n",
      "Iteration 14238, loss = 0.09393721\n",
      "Iteration 14239, loss = 0.09392877\n",
      "Iteration 14240, loss = 0.09391452\n",
      "Iteration 14241, loss = 0.09390293\n",
      "Iteration 14242, loss = 0.09389406\n",
      "Iteration 14243, loss = 0.09388400\n",
      "Iteration 14244, loss = 0.09387413\n",
      "Iteration 14245, loss = 0.09385709\n",
      "Iteration 14246, loss = 0.09384742\n",
      "Iteration 14247, loss = 0.09383833\n",
      "Iteration 14248, loss = 0.09383014\n",
      "Iteration 14249, loss = 0.09381324\n",
      "Iteration 14250, loss = 0.09379999\n",
      "Iteration 14251, loss = 0.09378887\n",
      "Iteration 14252, loss = 0.09378265\n",
      "Iteration 14253, loss = 0.09376472\n",
      "Iteration 14254, loss = 0.09375529\n",
      "Iteration 14255, loss = 0.09374067\n",
      "Iteration 14256, loss = 0.09373270\n",
      "Iteration 14257, loss = 0.09372178\n",
      "Iteration 14258, loss = 0.09370987\n",
      "Iteration 14259, loss = 0.09369725\n",
      "Iteration 14260, loss = 0.09368503\n",
      "Iteration 14261, loss = 0.09367285\n",
      "Iteration 14262, loss = 0.09366641\n",
      "Iteration 14263, loss = 0.09365063\n",
      "Iteration 14264, loss = 0.09363820\n",
      "Iteration 14265, loss = 0.09362264\n",
      "Iteration 14266, loss = 0.09360985\n",
      "Iteration 14267, loss = 0.09360253\n",
      "Iteration 14268, loss = 0.09359067\n",
      "Iteration 14269, loss = 0.09357779\n",
      "Iteration 14270, loss = 0.09356422\n",
      "Iteration 14271, loss = 0.09355247\n",
      "Iteration 14272, loss = 0.09354015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14273, loss = 0.09352688\n",
      "Iteration 14274, loss = 0.09351680\n",
      "Iteration 14275, loss = 0.09350766\n",
      "Iteration 14276, loss = 0.09349841\n",
      "Iteration 14277, loss = 0.09348452\n",
      "Iteration 14278, loss = 0.09346839\n",
      "Iteration 14279, loss = 0.09345935\n",
      "Iteration 14280, loss = 0.09345136\n",
      "Iteration 14281, loss = 0.09343610\n",
      "Iteration 14282, loss = 0.09343136\n",
      "Iteration 14283, loss = 0.09342070\n",
      "Iteration 14284, loss = 0.09340862\n",
      "Iteration 14285, loss = 0.09339546\n",
      "Iteration 14286, loss = 0.09338497\n",
      "Iteration 14287, loss = 0.09337456\n",
      "Iteration 14288, loss = 0.09336005\n",
      "Iteration 14289, loss = 0.09334609\n",
      "Iteration 14290, loss = 0.09333587\n",
      "Iteration 14291, loss = 0.09332352\n",
      "Iteration 14292, loss = 0.09330920\n",
      "Iteration 14293, loss = 0.09330119\n",
      "Iteration 14294, loss = 0.09328947\n",
      "Iteration 14295, loss = 0.09327402\n",
      "Iteration 14296, loss = 0.09326778\n",
      "Iteration 14297, loss = 0.09325745\n",
      "Iteration 14298, loss = 0.09324361\n",
      "Iteration 14299, loss = 0.09322500\n",
      "Iteration 14300, loss = 0.09321829\n",
      "Iteration 14301, loss = 0.09320960\n",
      "Iteration 14302, loss = 0.09319941\n",
      "Iteration 14303, loss = 0.09318800\n",
      "Iteration 14304, loss = 0.09317155\n",
      "Iteration 14305, loss = 0.09315833\n",
      "Iteration 14306, loss = 0.09315000\n",
      "Iteration 14307, loss = 0.09313418\n",
      "Iteration 14308, loss = 0.09312430\n",
      "Iteration 14309, loss = 0.09311363\n",
      "Iteration 14310, loss = 0.09309803\n",
      "Iteration 14311, loss = 0.09309174\n",
      "Iteration 14312, loss = 0.09308153\n",
      "Iteration 14313, loss = 0.09307188\n",
      "Iteration 14314, loss = 0.09305846\n",
      "Iteration 14315, loss = 0.09304804\n",
      "Iteration 14316, loss = 0.09303638\n",
      "Iteration 14317, loss = 0.09302280\n",
      "Iteration 14318, loss = 0.09300732\n",
      "Iteration 14319, loss = 0.09299517\n",
      "Iteration 14320, loss = 0.09298593\n",
      "Iteration 14321, loss = 0.09297385\n",
      "Iteration 14322, loss = 0.09296292\n",
      "Iteration 14323, loss = 0.09295435\n",
      "Iteration 14324, loss = 0.09294353\n",
      "Iteration 14325, loss = 0.09293117\n",
      "Iteration 14326, loss = 0.09291796\n",
      "Iteration 14327, loss = 0.09290883\n",
      "Iteration 14328, loss = 0.09289316\n",
      "Iteration 14329, loss = 0.09288889\n",
      "Iteration 14330, loss = 0.09287980\n",
      "Iteration 14331, loss = 0.09286403\n",
      "Iteration 14332, loss = 0.09285253\n",
      "Iteration 14333, loss = 0.09284261\n",
      "Iteration 14334, loss = 0.09282537\n",
      "Iteration 14335, loss = 0.09281784\n",
      "Iteration 14336, loss = 0.09280789\n",
      "Iteration 14337, loss = 0.09279450\n",
      "Iteration 14338, loss = 0.09278404\n",
      "Iteration 14339, loss = 0.09277082\n",
      "Iteration 14340, loss = 0.09276179\n",
      "Iteration 14341, loss = 0.09274826\n",
      "Iteration 14342, loss = 0.09273492\n",
      "Iteration 14343, loss = 0.09272220\n",
      "Iteration 14344, loss = 0.09271886\n",
      "Iteration 14345, loss = 0.09270497\n",
      "Iteration 14346, loss = 0.09269212\n",
      "Iteration 14347, loss = 0.09267987\n",
      "Iteration 14348, loss = 0.09266843\n",
      "Iteration 14349, loss = 0.09265947\n",
      "Iteration 14350, loss = 0.09264883\n",
      "Iteration 14351, loss = 0.09263712\n",
      "Iteration 14352, loss = 0.09262458\n",
      "Iteration 14353, loss = 0.09261493\n",
      "Iteration 14354, loss = 0.09260415\n",
      "Iteration 14355, loss = 0.09259320\n",
      "Iteration 14356, loss = 0.09258614\n",
      "Iteration 14357, loss = 0.09257219\n",
      "Iteration 14358, loss = 0.09255772\n",
      "Iteration 14359, loss = 0.09254934\n",
      "Iteration 14360, loss = 0.09253518\n",
      "Iteration 14361, loss = 0.09252033\n",
      "Iteration 14362, loss = 0.09251614\n",
      "Iteration 14363, loss = 0.09249583\n",
      "Iteration 14364, loss = 0.09247857\n",
      "Iteration 14365, loss = 0.09247184\n",
      "Iteration 14366, loss = 0.09246183\n",
      "Iteration 14367, loss = 0.09244793\n",
      "Iteration 14368, loss = 0.09243794\n",
      "Iteration 14369, loss = 0.09243012\n",
      "Iteration 14370, loss = 0.09241448\n",
      "Iteration 14371, loss = 0.09240247\n",
      "Iteration 14372, loss = 0.09238868\n",
      "Iteration 14373, loss = 0.09238321\n",
      "Iteration 14374, loss = 0.09237280\n",
      "Iteration 14375, loss = 0.09235678\n",
      "Iteration 14376, loss = 0.09234138\n",
      "Iteration 14377, loss = 0.09232749\n",
      "Iteration 14378, loss = 0.09232421\n",
      "Iteration 14379, loss = 0.09231010\n",
      "Iteration 14380, loss = 0.09229757\n",
      "Iteration 14381, loss = 0.09228602\n",
      "Iteration 14382, loss = 0.09227979\n",
      "Iteration 14383, loss = 0.09226883\n",
      "Iteration 14384, loss = 0.09225610\n",
      "Iteration 14385, loss = 0.09224568\n",
      "Iteration 14386, loss = 0.09223356\n",
      "Iteration 14387, loss = 0.09222193\n",
      "Iteration 14388, loss = 0.09221087\n",
      "Iteration 14389, loss = 0.09219795\n",
      "Iteration 14390, loss = 0.09218894\n",
      "Iteration 14391, loss = 0.09217681\n",
      "Iteration 14392, loss = 0.09216533\n",
      "Iteration 14393, loss = 0.09215762\n",
      "Iteration 14394, loss = 0.09214477\n",
      "Iteration 14395, loss = 0.09213208\n",
      "Iteration 14396, loss = 0.09211973\n",
      "Iteration 14397, loss = 0.09210479\n",
      "Iteration 14398, loss = 0.09209897\n",
      "Iteration 14399, loss = 0.09209016\n",
      "Iteration 14400, loss = 0.09207665\n",
      "Iteration 14401, loss = 0.09206251\n",
      "Iteration 14402, loss = 0.09205500\n",
      "Iteration 14403, loss = 0.09204237\n",
      "Iteration 14404, loss = 0.09203198\n",
      "Iteration 14405, loss = 0.09202067\n",
      "Iteration 14406, loss = 0.09200540\n",
      "Iteration 14407, loss = 0.09200559\n",
      "Iteration 14408, loss = 0.09199288\n",
      "Iteration 14409, loss = 0.09197319\n",
      "Iteration 14410, loss = 0.09196700\n",
      "Iteration 14411, loss = 0.09195676\n",
      "Iteration 14412, loss = 0.09194461\n",
      "Iteration 14413, loss = 0.09193261\n",
      "Iteration 14414, loss = 0.09191946\n",
      "Iteration 14415, loss = 0.09190468\n",
      "Iteration 14416, loss = 0.09189450\n",
      "Iteration 14417, loss = 0.09188331\n",
      "Iteration 14418, loss = 0.09187127\n",
      "Iteration 14419, loss = 0.09185952\n",
      "Iteration 14420, loss = 0.09185374\n",
      "Iteration 14421, loss = 0.09184494\n",
      "Iteration 14422, loss = 0.09183315\n",
      "Iteration 14423, loss = 0.09182086\n",
      "Iteration 14424, loss = 0.09181246\n",
      "Iteration 14425, loss = 0.09179883\n",
      "Iteration 14426, loss = 0.09178744\n",
      "Iteration 14427, loss = 0.09177080\n",
      "Iteration 14428, loss = 0.09175975\n",
      "Iteration 14429, loss = 0.09175042\n",
      "Iteration 14430, loss = 0.09174124\n",
      "Iteration 14431, loss = 0.09172771\n",
      "Iteration 14432, loss = 0.09171718\n",
      "Iteration 14433, loss = 0.09170839\n",
      "Iteration 14434, loss = 0.09169387\n",
      "Iteration 14435, loss = 0.09168779\n",
      "Iteration 14436, loss = 0.09168146\n",
      "Iteration 14437, loss = 0.09166557\n",
      "Iteration 14438, loss = 0.09165402\n",
      "Iteration 14439, loss = 0.09164591\n",
      "Iteration 14440, loss = 0.09163715\n",
      "Iteration 14441, loss = 0.09162296\n",
      "Iteration 14442, loss = 0.09161105\n",
      "Iteration 14443, loss = 0.09159957\n",
      "Iteration 14444, loss = 0.09159081\n",
      "Iteration 14445, loss = 0.09157799\n",
      "Iteration 14446, loss = 0.09157285\n",
      "Iteration 14447, loss = 0.09155578\n",
      "Iteration 14448, loss = 0.09154653\n",
      "Iteration 14449, loss = 0.09153624\n",
      "Iteration 14450, loss = 0.09152509\n",
      "Iteration 14451, loss = 0.09152188\n",
      "Iteration 14452, loss = 0.09150363\n",
      "Iteration 14453, loss = 0.09149325\n",
      "Iteration 14454, loss = 0.09148686\n",
      "Iteration 14455, loss = 0.09147730\n",
      "Iteration 14456, loss = 0.09146545\n",
      "Iteration 14457, loss = 0.09144975\n",
      "Iteration 14458, loss = 0.09143388\n",
      "Iteration 14459, loss = 0.09142441\n",
      "Iteration 14460, loss = 0.09141694\n",
      "Iteration 14461, loss = 0.09140461\n",
      "Iteration 14462, loss = 0.09139176\n",
      "Iteration 14463, loss = 0.09138220\n",
      "Iteration 14464, loss = 0.09137121\n",
      "Iteration 14465, loss = 0.09136568\n",
      "Iteration 14466, loss = 0.09135231\n",
      "Iteration 14467, loss = 0.09134112\n",
      "Iteration 14468, loss = 0.09132311\n",
      "Iteration 14469, loss = 0.09131041\n",
      "Iteration 14470, loss = 0.09131079\n",
      "Iteration 14471, loss = 0.09129977\n",
      "Iteration 14472, loss = 0.09127904\n",
      "Iteration 14473, loss = 0.09126971\n",
      "Iteration 14474, loss = 0.09126119\n",
      "Iteration 14475, loss = 0.09125255\n",
      "Iteration 14476, loss = 0.09124101\n",
      "Iteration 14477, loss = 0.09122585\n",
      "Iteration 14478, loss = 0.09121607\n",
      "Iteration 14479, loss = 0.09120772\n",
      "Iteration 14480, loss = 0.09119975\n",
      "Iteration 14481, loss = 0.09118635\n",
      "Iteration 14482, loss = 0.09117498\n",
      "Iteration 14483, loss = 0.09116630\n",
      "Iteration 14484, loss = 0.09115568\n",
      "Iteration 14485, loss = 0.09114576\n",
      "Iteration 14486, loss = 0.09113321\n",
      "Iteration 14487, loss = 0.09112720\n",
      "Iteration 14488, loss = 0.09111084\n",
      "Iteration 14489, loss = 0.09109714\n",
      "Iteration 14490, loss = 0.09109181\n",
      "Iteration 14491, loss = 0.09107926\n",
      "Iteration 14492, loss = 0.09106500\n",
      "Iteration 14493, loss = 0.09105256\n",
      "Iteration 14494, loss = 0.09104508\n",
      "Iteration 14495, loss = 0.09103400\n",
      "Iteration 14496, loss = 0.09102766\n",
      "Iteration 14497, loss = 0.09101875\n",
      "Iteration 14498, loss = 0.09100542\n",
      "Iteration 14499, loss = 0.09098782\n",
      "Iteration 14500, loss = 0.09098369\n",
      "Iteration 14501, loss = 0.09097025\n",
      "Iteration 14502, loss = 0.09095634\n",
      "Iteration 14503, loss = 0.09094318\n",
      "Iteration 14504, loss = 0.09093133\n",
      "Iteration 14505, loss = 0.09092284\n",
      "Iteration 14506, loss = 0.09090991\n",
      "Iteration 14507, loss = 0.09089633\n",
      "Iteration 14508, loss = 0.09088663\n",
      "Iteration 14509, loss = 0.09087967\n",
      "Iteration 14510, loss = 0.09087921\n",
      "Iteration 14511, loss = 0.09086109\n",
      "Iteration 14512, loss = 0.09084768\n",
      "Iteration 14513, loss = 0.09083803\n",
      "Iteration 14514, loss = 0.09082999\n",
      "Iteration 14515, loss = 0.09081804\n",
      "Iteration 14516, loss = 0.09080606\n",
      "Iteration 14517, loss = 0.09079596\n",
      "Iteration 14518, loss = 0.09078609\n",
      "Iteration 14519, loss = 0.09077026\n",
      "Iteration 14520, loss = 0.09076192\n",
      "Iteration 14521, loss = 0.09075712\n",
      "Iteration 14522, loss = 0.09074536\n",
      "Iteration 14523, loss = 0.09073212\n",
      "Iteration 14524, loss = 0.09072093\n",
      "Iteration 14525, loss = 0.09071069\n",
      "Iteration 14526, loss = 0.09069872\n",
      "Iteration 14527, loss = 0.09068724\n",
      "Iteration 14528, loss = 0.09067710\n",
      "Iteration 14529, loss = 0.09066422\n",
      "Iteration 14530, loss = 0.09065393\n",
      "Iteration 14531, loss = 0.09064350\n",
      "Iteration 14532, loss = 0.09063455\n",
      "Iteration 14533, loss = 0.09062263\n",
      "Iteration 14534, loss = 0.09061160\n",
      "Iteration 14535, loss = 0.09060157\n",
      "Iteration 14536, loss = 0.09058960\n",
      "Iteration 14537, loss = 0.09057713\n",
      "Iteration 14538, loss = 0.09057213\n",
      "Iteration 14539, loss = 0.09056327\n",
      "Iteration 14540, loss = 0.09054662\n",
      "Iteration 14541, loss = 0.09053232\n",
      "Iteration 14542, loss = 0.09052552\n",
      "Iteration 14543, loss = 0.09051672\n",
      "Iteration 14544, loss = 0.09050437\n",
      "Iteration 14545, loss = 0.09048982\n",
      "Iteration 14546, loss = 0.09048432\n",
      "Iteration 14547, loss = 0.09047370\n",
      "Iteration 14548, loss = 0.09046826\n",
      "Iteration 14549, loss = 0.09045429\n",
      "Iteration 14550, loss = 0.09044329\n",
      "Iteration 14551, loss = 0.09043421\n",
      "Iteration 14552, loss = 0.09042697\n",
      "Iteration 14553, loss = 0.09041453\n",
      "Iteration 14554, loss = 0.09040315\n",
      "Iteration 14555, loss = 0.09039311\n",
      "Iteration 14556, loss = 0.09038363\n",
      "Iteration 14557, loss = 0.09036993\n",
      "Iteration 14558, loss = 0.09035636\n",
      "Iteration 14559, loss = 0.09034742\n",
      "Iteration 14560, loss = 0.09034023\n",
      "Iteration 14561, loss = 0.09032842\n",
      "Iteration 14562, loss = 0.09031427\n",
      "Iteration 14563, loss = 0.09030448\n",
      "Iteration 14564, loss = 0.09029227\n",
      "Iteration 14565, loss = 0.09028613\n",
      "Iteration 14566, loss = 0.09027724\n",
      "Iteration 14567, loss = 0.09025758\n",
      "Iteration 14568, loss = 0.09024892\n",
      "Iteration 14569, loss = 0.09024455\n",
      "Iteration 14570, loss = 0.09022989\n",
      "Iteration 14571, loss = 0.09021509\n",
      "Iteration 14572, loss = 0.09020593\n",
      "Iteration 14573, loss = 0.09020273\n",
      "Iteration 14574, loss = 0.09018723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14575, loss = 0.09017768\n",
      "Iteration 14576, loss = 0.09016974\n",
      "Iteration 14577, loss = 0.09015822\n",
      "Iteration 14578, loss = 0.09015150\n",
      "Iteration 14579, loss = 0.09013978\n",
      "Iteration 14580, loss = 0.09012546\n",
      "Iteration 14581, loss = 0.09011322\n",
      "Iteration 14582, loss = 0.09010262\n",
      "Iteration 14583, loss = 0.09009299\n",
      "Iteration 14584, loss = 0.09007870\n",
      "Iteration 14585, loss = 0.09006745\n",
      "Iteration 14586, loss = 0.09006339\n",
      "Iteration 14587, loss = 0.09005269\n",
      "Iteration 14588, loss = 0.09003744\n",
      "Iteration 14589, loss = 0.09002966\n",
      "Iteration 14590, loss = 0.09002047\n",
      "Iteration 14591, loss = 0.09001164\n",
      "Iteration 14592, loss = 0.08999400\n",
      "Iteration 14593, loss = 0.08998273\n",
      "Iteration 14594, loss = 0.08997589\n",
      "Iteration 14595, loss = 0.08996523\n",
      "Iteration 14596, loss = 0.08995638\n",
      "Iteration 14597, loss = 0.08994677\n",
      "Iteration 14598, loss = 0.08993258\n",
      "Iteration 14599, loss = 0.08992335\n",
      "Iteration 14600, loss = 0.08991554\n",
      "Iteration 14601, loss = 0.08990429\n",
      "Iteration 14602, loss = 0.08989219\n",
      "Iteration 14603, loss = 0.08987911\n",
      "Iteration 14604, loss = 0.08987049\n",
      "Iteration 14605, loss = 0.08986003\n",
      "Iteration 14606, loss = 0.08985072\n",
      "Iteration 14607, loss = 0.08983876\n",
      "Iteration 14608, loss = 0.08982851\n",
      "Iteration 14609, loss = 0.08981849\n",
      "Iteration 14610, loss = 0.08980462\n",
      "Iteration 14611, loss = 0.08979191\n",
      "Iteration 14612, loss = 0.08978568\n",
      "Iteration 14613, loss = 0.08978007\n",
      "Iteration 14614, loss = 0.08977226\n",
      "Iteration 14615, loss = 0.08975982\n",
      "Iteration 14616, loss = 0.08974766\n",
      "Iteration 14617, loss = 0.08973929\n",
      "Iteration 14618, loss = 0.08972963\n",
      "Iteration 14619, loss = 0.08971602\n",
      "Iteration 14620, loss = 0.08970565\n",
      "Iteration 14621, loss = 0.08969472\n",
      "Iteration 14622, loss = 0.08968443\n",
      "Iteration 14623, loss = 0.08967686\n",
      "Iteration 14624, loss = 0.08966695\n",
      "Iteration 14625, loss = 0.08965459\n",
      "Iteration 14626, loss = 0.08964430\n",
      "Iteration 14627, loss = 0.08963303\n",
      "Iteration 14628, loss = 0.08962388\n",
      "Iteration 14629, loss = 0.08961172\n",
      "Iteration 14630, loss = 0.08960318\n",
      "Iteration 14631, loss = 0.08959364\n",
      "Iteration 14632, loss = 0.08958259\n",
      "Iteration 14633, loss = 0.08956823\n",
      "Iteration 14634, loss = 0.08955659\n",
      "Iteration 14635, loss = 0.08954966\n",
      "Iteration 14636, loss = 0.08954587\n",
      "Iteration 14637, loss = 0.08954119\n",
      "Iteration 14638, loss = 0.08952375\n",
      "Iteration 14639, loss = 0.08950630\n",
      "Iteration 14640, loss = 0.08950186\n",
      "Iteration 14641, loss = 0.08949326\n",
      "Iteration 14642, loss = 0.08948373\n",
      "Iteration 14643, loss = 0.08946690\n",
      "Iteration 14644, loss = 0.08945492\n",
      "Iteration 14645, loss = 0.08944434\n",
      "Iteration 14646, loss = 0.08943282\n",
      "Iteration 14647, loss = 0.08942685\n",
      "Iteration 14648, loss = 0.08941775\n",
      "Iteration 14649, loss = 0.08940435\n",
      "Iteration 14650, loss = 0.08939283\n",
      "Iteration 14651, loss = 0.08938285\n",
      "Iteration 14652, loss = 0.08937495\n",
      "Iteration 14653, loss = 0.08936692\n",
      "Iteration 14654, loss = 0.08934932\n",
      "Iteration 14655, loss = 0.08933875\n",
      "Iteration 14656, loss = 0.08933510\n",
      "Iteration 14657, loss = 0.08932737\n",
      "Iteration 14658, loss = 0.08931579\n",
      "Iteration 14659, loss = 0.08930416\n",
      "Iteration 14660, loss = 0.08929710\n",
      "Iteration 14661, loss = 0.08928791\n",
      "Iteration 14662, loss = 0.08927580\n",
      "Iteration 14663, loss = 0.08926287\n",
      "Iteration 14664, loss = 0.08925161\n",
      "Iteration 14665, loss = 0.08923907\n",
      "Iteration 14666, loss = 0.08922907\n",
      "Iteration 14667, loss = 0.08921820\n",
      "Iteration 14668, loss = 0.08921046\n",
      "Iteration 14669, loss = 0.08919807\n",
      "Iteration 14670, loss = 0.08919015\n",
      "Iteration 14671, loss = 0.08917699\n",
      "Iteration 14672, loss = 0.08916833\n",
      "Iteration 14673, loss = 0.08916057\n",
      "Iteration 14674, loss = 0.08915156\n",
      "Iteration 14675, loss = 0.08913905\n",
      "Iteration 14676, loss = 0.08912868\n",
      "Iteration 14677, loss = 0.08911545\n",
      "Iteration 14678, loss = 0.08910744\n",
      "Iteration 14679, loss = 0.08909523\n",
      "Iteration 14680, loss = 0.08907934\n",
      "Iteration 14681, loss = 0.08907498\n",
      "Iteration 14682, loss = 0.08906945\n",
      "Iteration 14683, loss = 0.08906065\n",
      "Iteration 14684, loss = 0.08904625\n",
      "Iteration 14685, loss = 0.08903704\n",
      "Iteration 14686, loss = 0.08902881\n",
      "Iteration 14687, loss = 0.08901538\n",
      "Iteration 14688, loss = 0.08900574\n",
      "Iteration 14689, loss = 0.08899340\n",
      "Iteration 14690, loss = 0.08898173\n",
      "Iteration 14691, loss = 0.08897384\n",
      "Iteration 14692, loss = 0.08896544\n",
      "Iteration 14693, loss = 0.08895291\n",
      "Iteration 14694, loss = 0.08894803\n",
      "Iteration 14695, loss = 0.08893782\n",
      "Iteration 14696, loss = 0.08892578\n",
      "Iteration 14697, loss = 0.08891595\n",
      "Iteration 14698, loss = 0.08890698\n",
      "Iteration 14699, loss = 0.08889349\n",
      "Iteration 14700, loss = 0.08888914\n",
      "Iteration 14701, loss = 0.08887236\n",
      "Iteration 14702, loss = 0.08885690\n",
      "Iteration 14703, loss = 0.08885777\n",
      "Iteration 14704, loss = 0.08884804\n",
      "Iteration 14705, loss = 0.08883098\n",
      "Iteration 14706, loss = 0.08882136\n",
      "Iteration 14707, loss = 0.08881235\n",
      "Iteration 14708, loss = 0.08879908\n",
      "Iteration 14709, loss = 0.08879238\n",
      "Iteration 14710, loss = 0.08878467\n",
      "Iteration 14711, loss = 0.08877004\n",
      "Iteration 14712, loss = 0.08876157\n",
      "Iteration 14713, loss = 0.08874747\n",
      "Iteration 14714, loss = 0.08873102\n",
      "Iteration 14715, loss = 0.08872533\n",
      "Iteration 14716, loss = 0.08871650\n",
      "Iteration 14717, loss = 0.08869993\n",
      "Iteration 14718, loss = 0.08869458\n",
      "Iteration 14719, loss = 0.08868745\n",
      "Iteration 14720, loss = 0.08867826\n",
      "Iteration 14721, loss = 0.08866523\n",
      "Iteration 14722, loss = 0.08865517\n",
      "Iteration 14723, loss = 0.08864477\n",
      "Iteration 14724, loss = 0.08863386\n",
      "Iteration 14725, loss = 0.08862099\n",
      "Iteration 14726, loss = 0.08861093\n",
      "Iteration 14727, loss = 0.08860261\n",
      "Iteration 14728, loss = 0.08859892\n",
      "Iteration 14729, loss = 0.08859140\n",
      "Iteration 14730, loss = 0.08857581\n",
      "Iteration 14731, loss = 0.08856597\n",
      "Iteration 14732, loss = 0.08855841\n",
      "Iteration 14733, loss = 0.08854830\n",
      "Iteration 14734, loss = 0.08853518\n",
      "Iteration 14735, loss = 0.08852602\n",
      "Iteration 14736, loss = 0.08851728\n",
      "Iteration 14737, loss = 0.08850691\n",
      "Iteration 14738, loss = 0.08849054\n",
      "Iteration 14739, loss = 0.08848331\n",
      "Iteration 14740, loss = 0.08847651\n",
      "Iteration 14741, loss = 0.08846625\n",
      "Iteration 14742, loss = 0.08845708\n",
      "Iteration 14743, loss = 0.08844288\n",
      "Iteration 14744, loss = 0.08843162\n",
      "Iteration 14745, loss = 0.08842370\n",
      "Iteration 14746, loss = 0.08841485\n",
      "Iteration 14747, loss = 0.08840256\n",
      "Iteration 14748, loss = 0.08839772\n",
      "Iteration 14749, loss = 0.08838450\n",
      "Iteration 14750, loss = 0.08837529\n",
      "Iteration 14751, loss = 0.08836344\n",
      "Iteration 14752, loss = 0.08835471\n",
      "Iteration 14753, loss = 0.08835096\n",
      "Iteration 14754, loss = 0.08834455\n",
      "Iteration 14755, loss = 0.08833089\n",
      "Iteration 14756, loss = 0.08831889\n",
      "Iteration 14757, loss = 0.08830788\n",
      "Iteration 14758, loss = 0.08830406\n",
      "Iteration 14759, loss = 0.08829047\n",
      "Iteration 14760, loss = 0.08828161\n",
      "Iteration 14761, loss = 0.08827278\n",
      "Iteration 14762, loss = 0.08826342\n",
      "Iteration 14763, loss = 0.08825463\n",
      "Iteration 14764, loss = 0.08824327\n",
      "Iteration 14765, loss = 0.08823424\n",
      "Iteration 14766, loss = 0.08822071\n",
      "Iteration 14767, loss = 0.08820627\n",
      "Iteration 14768, loss = 0.08819584\n",
      "Iteration 14769, loss = 0.08818650\n",
      "Iteration 14770, loss = 0.08817138\n",
      "Iteration 14771, loss = 0.08816010\n",
      "Iteration 14772, loss = 0.08815580\n",
      "Iteration 14773, loss = 0.08814605\n",
      "Iteration 14774, loss = 0.08813105\n",
      "Iteration 14775, loss = 0.08812075\n",
      "Iteration 14776, loss = 0.08811503\n",
      "Iteration 14777, loss = 0.08810676\n",
      "Iteration 14778, loss = 0.08809542\n",
      "Iteration 14779, loss = 0.08808654\n",
      "Iteration 14780, loss = 0.08808013\n",
      "Iteration 14781, loss = 0.08807203\n",
      "Iteration 14782, loss = 0.08805768\n",
      "Iteration 14783, loss = 0.08804429\n",
      "Iteration 14784, loss = 0.08803237\n",
      "Iteration 14785, loss = 0.08802094\n",
      "Iteration 14786, loss = 0.08801126\n",
      "Iteration 14787, loss = 0.08800011\n",
      "Iteration 14788, loss = 0.08799338\n",
      "Iteration 14789, loss = 0.08797976\n",
      "Iteration 14790, loss = 0.08797294\n",
      "Iteration 14791, loss = 0.08796404\n",
      "Iteration 14792, loss = 0.08795101\n",
      "Iteration 14793, loss = 0.08794244\n",
      "Iteration 14794, loss = 0.08793120\n",
      "Iteration 14795, loss = 0.08792500\n",
      "Iteration 14796, loss = 0.08791177\n",
      "Iteration 14797, loss = 0.08790514\n",
      "Iteration 14798, loss = 0.08789232\n",
      "Iteration 14799, loss = 0.08788806\n",
      "Iteration 14800, loss = 0.08787627\n",
      "Iteration 14801, loss = 0.08786319\n",
      "Iteration 14802, loss = 0.08785551\n",
      "Iteration 14803, loss = 0.08784453\n",
      "Iteration 14804, loss = 0.08783624\n",
      "Iteration 14805, loss = 0.08782580\n",
      "Iteration 14806, loss = 0.08781692\n",
      "Iteration 14807, loss = 0.08780443\n",
      "Iteration 14808, loss = 0.08779923\n",
      "Iteration 14809, loss = 0.08778742\n",
      "Iteration 14810, loss = 0.08777467\n",
      "Iteration 14811, loss = 0.08777108\n",
      "Iteration 14812, loss = 0.08776052\n",
      "Iteration 14813, loss = 0.08774771\n",
      "Iteration 14814, loss = 0.08773959\n",
      "Iteration 14815, loss = 0.08773319\n",
      "Iteration 14816, loss = 0.08772610\n",
      "Iteration 14817, loss = 0.08771285\n",
      "Iteration 14818, loss = 0.08770071\n",
      "Iteration 14819, loss = 0.08768701\n",
      "Iteration 14820, loss = 0.08768316\n",
      "Iteration 14821, loss = 0.08767340\n",
      "Iteration 14822, loss = 0.08766126\n",
      "Iteration 14823, loss = 0.08764765\n",
      "Iteration 14824, loss = 0.08764022\n",
      "Iteration 14825, loss = 0.08763403\n",
      "Iteration 14826, loss = 0.08762237\n",
      "Iteration 14827, loss = 0.08761469\n",
      "Iteration 14828, loss = 0.08760546\n",
      "Iteration 14829, loss = 0.08759814\n",
      "Iteration 14830, loss = 0.08758648\n",
      "Iteration 14831, loss = 0.08757705\n",
      "Iteration 14832, loss = 0.08756741\n",
      "Iteration 14833, loss = 0.08755517\n",
      "Iteration 14834, loss = 0.08754217\n",
      "Iteration 14835, loss = 0.08753268\n",
      "Iteration 14836, loss = 0.08752384\n",
      "Iteration 14837, loss = 0.08751797\n",
      "Iteration 14838, loss = 0.08750717\n",
      "Iteration 14839, loss = 0.08749521\n",
      "Iteration 14840, loss = 0.08749115\n",
      "Iteration 14841, loss = 0.08748286\n",
      "Iteration 14842, loss = 0.08746919\n",
      "Iteration 14843, loss = 0.08745945\n",
      "Iteration 14844, loss = 0.08745027\n",
      "Iteration 14845, loss = 0.08743883\n",
      "Iteration 14846, loss = 0.08742849\n",
      "Iteration 14847, loss = 0.08742084\n",
      "Iteration 14848, loss = 0.08740830\n",
      "Iteration 14849, loss = 0.08739624\n",
      "Iteration 14850, loss = 0.08738796\n",
      "Iteration 14851, loss = 0.08737759\n",
      "Iteration 14852, loss = 0.08736375\n",
      "Iteration 14853, loss = 0.08735794\n",
      "Iteration 14854, loss = 0.08734835\n",
      "Iteration 14855, loss = 0.08733780\n",
      "Iteration 14856, loss = 0.08732816\n",
      "Iteration 14857, loss = 0.08731880\n",
      "Iteration 14858, loss = 0.08731234\n",
      "Iteration 14859, loss = 0.08730077\n",
      "Iteration 14860, loss = 0.08729086\n",
      "Iteration 14861, loss = 0.08728044\n",
      "Iteration 14862, loss = 0.08727418\n",
      "Iteration 14863, loss = 0.08726022\n",
      "Iteration 14864, loss = 0.08724781\n",
      "Iteration 14865, loss = 0.08723653\n",
      "Iteration 14866, loss = 0.08722457\n",
      "Iteration 14867, loss = 0.08721892\n",
      "Iteration 14868, loss = 0.08721022\n",
      "Iteration 14869, loss = 0.08719920\n",
      "Iteration 14870, loss = 0.08718642\n",
      "Iteration 14871, loss = 0.08717764\n",
      "Iteration 14872, loss = 0.08716910\n",
      "Iteration 14873, loss = 0.08716164\n",
      "Iteration 14874, loss = 0.08715077\n",
      "Iteration 14875, loss = 0.08714201\n",
      "Iteration 14876, loss = 0.08713453\n",
      "Iteration 14877, loss = 0.08712339\n",
      "Iteration 14878, loss = 0.08710932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14879, loss = 0.08710897\n",
      "Iteration 14880, loss = 0.08709824\n",
      "Iteration 14881, loss = 0.08708699\n",
      "Iteration 14882, loss = 0.08707584\n",
      "Iteration 14883, loss = 0.08707038\n",
      "Iteration 14884, loss = 0.08706128\n",
      "Iteration 14885, loss = 0.08705303\n",
      "Iteration 14886, loss = 0.08704029\n",
      "Iteration 14887, loss = 0.08703242\n",
      "Iteration 14888, loss = 0.08701634\n",
      "Iteration 14889, loss = 0.08701380\n",
      "Iteration 14890, loss = 0.08700524\n",
      "Iteration 14891, loss = 0.08699424\n",
      "Iteration 14892, loss = 0.08698262\n",
      "Iteration 14893, loss = 0.08697041\n",
      "Iteration 14894, loss = 0.08696202\n",
      "Iteration 14895, loss = 0.08694972\n",
      "Iteration 14896, loss = 0.08694205\n",
      "Iteration 14897, loss = 0.08693309\n",
      "Iteration 14898, loss = 0.08691794\n",
      "Iteration 14899, loss = 0.08691148\n",
      "Iteration 14900, loss = 0.08690366\n",
      "Iteration 14901, loss = 0.08689320\n",
      "Iteration 14902, loss = 0.08688595\n",
      "Iteration 14903, loss = 0.08687423\n",
      "Iteration 14904, loss = 0.08686482\n",
      "Iteration 14905, loss = 0.08684918\n",
      "Iteration 14906, loss = 0.08683893\n",
      "Iteration 14907, loss = 0.08683765\n",
      "Iteration 14908, loss = 0.08682748\n",
      "Iteration 14909, loss = 0.08681056\n",
      "Iteration 14910, loss = 0.08680429\n",
      "Iteration 14911, loss = 0.08679902\n",
      "Iteration 14912, loss = 0.08678625\n",
      "Iteration 14913, loss = 0.08677447\n",
      "Iteration 14914, loss = 0.08676252\n",
      "Iteration 14915, loss = 0.08675647\n",
      "Iteration 14916, loss = 0.08674431\n",
      "Iteration 14917, loss = 0.08673839\n",
      "Iteration 14918, loss = 0.08672995\n",
      "Iteration 14919, loss = 0.08672401\n",
      "Iteration 14920, loss = 0.08671268\n",
      "Iteration 14921, loss = 0.08670708\n",
      "Iteration 14922, loss = 0.08669885\n",
      "Iteration 14923, loss = 0.08668517\n",
      "Iteration 14924, loss = 0.08666905\n",
      "Iteration 14925, loss = 0.08666537\n",
      "Iteration 14926, loss = 0.08665733\n",
      "Iteration 14927, loss = 0.08664356\n",
      "Iteration 14928, loss = 0.08663393\n",
      "Iteration 14929, loss = 0.08663093\n",
      "Iteration 14930, loss = 0.08662230\n",
      "Iteration 14931, loss = 0.08660751\n",
      "Iteration 14932, loss = 0.08660063\n",
      "Iteration 14933, loss = 0.08659417\n",
      "Iteration 14934, loss = 0.08658306\n",
      "Iteration 14935, loss = 0.08656961\n",
      "Iteration 14936, loss = 0.08655903\n",
      "Iteration 14937, loss = 0.08655090\n",
      "Iteration 14938, loss = 0.08654718\n",
      "Iteration 14939, loss = 0.08653491\n",
      "Iteration 14940, loss = 0.08652401\n",
      "Iteration 14941, loss = 0.08651459\n",
      "Iteration 14942, loss = 0.08650742\n",
      "Iteration 14943, loss = 0.08650225\n",
      "Iteration 14944, loss = 0.08648778\n",
      "Iteration 14945, loss = 0.08647247\n",
      "Iteration 14946, loss = 0.08647041\n",
      "Iteration 14947, loss = 0.08645985\n",
      "Iteration 14948, loss = 0.08644530\n",
      "Iteration 14949, loss = 0.08643957\n",
      "Iteration 14950, loss = 0.08643055\n",
      "Iteration 14951, loss = 0.08642306\n",
      "Iteration 14952, loss = 0.08640705\n",
      "Iteration 14953, loss = 0.08639944\n",
      "Iteration 14954, loss = 0.08639259\n",
      "Iteration 14955, loss = 0.08638091\n",
      "Iteration 14956, loss = 0.08637165\n",
      "Iteration 14957, loss = 0.08636210\n",
      "Iteration 14958, loss = 0.08635193\n",
      "Iteration 14959, loss = 0.08634336\n",
      "Iteration 14960, loss = 0.08632870\n",
      "Iteration 14961, loss = 0.08632535\n",
      "Iteration 14962, loss = 0.08631509\n",
      "Iteration 14963, loss = 0.08630093\n",
      "Iteration 14964, loss = 0.08629234\n",
      "Iteration 14965, loss = 0.08628259\n",
      "Iteration 14966, loss = 0.08627232\n",
      "Iteration 14967, loss = 0.08626782\n",
      "Iteration 14968, loss = 0.08625894\n",
      "Iteration 14969, loss = 0.08624580\n",
      "Iteration 14970, loss = 0.08623494\n",
      "Iteration 14971, loss = 0.08622833\n",
      "Iteration 14972, loss = 0.08622006\n",
      "Iteration 14973, loss = 0.08620837\n",
      "Iteration 14974, loss = 0.08619633\n",
      "Iteration 14975, loss = 0.08618654\n",
      "Iteration 14976, loss = 0.08617935\n",
      "Iteration 14977, loss = 0.08617335\n",
      "Iteration 14978, loss = 0.08615858\n",
      "Iteration 14979, loss = 0.08614593\n",
      "Iteration 14980, loss = 0.08613827\n",
      "Iteration 14981, loss = 0.08613564\n",
      "Iteration 14982, loss = 0.08612301\n",
      "Iteration 14983, loss = 0.08611087\n",
      "Iteration 14984, loss = 0.08610432\n",
      "Iteration 14985, loss = 0.08609913\n",
      "Iteration 14986, loss = 0.08609166\n",
      "Iteration 14987, loss = 0.08607803\n",
      "Iteration 14988, loss = 0.08606798\n",
      "Iteration 14989, loss = 0.08605521\n",
      "Iteration 14990, loss = 0.08604985\n",
      "Iteration 14991, loss = 0.08604053\n",
      "Iteration 14992, loss = 0.08603414\n",
      "Iteration 14993, loss = 0.08602168\n",
      "Iteration 14994, loss = 0.08600881\n",
      "Iteration 14995, loss = 0.08599772\n",
      "Iteration 14996, loss = 0.08598834\n",
      "Iteration 14997, loss = 0.08598134\n",
      "Iteration 14998, loss = 0.08596915\n",
      "Iteration 14999, loss = 0.08595892\n",
      "Iteration 15000, loss = 0.08595019\n",
      "Iteration 15001, loss = 0.08594088\n",
      "Iteration 15002, loss = 0.08593128\n",
      "Iteration 15003, loss = 0.08592331\n",
      "Iteration 15004, loss = 0.08591316\n",
      "Iteration 15005, loss = 0.08590341\n",
      "Iteration 15006, loss = 0.08589185\n",
      "Iteration 15007, loss = 0.08588547\n",
      "Iteration 15008, loss = 0.08587221\n",
      "Iteration 15009, loss = 0.08586735\n",
      "Iteration 15010, loss = 0.08585978\n",
      "Iteration 15011, loss = 0.08584931\n",
      "Iteration 15012, loss = 0.08584203\n",
      "Iteration 15013, loss = 0.08583251\n",
      "Iteration 15014, loss = 0.08582373\n",
      "Iteration 15015, loss = 0.08581393\n",
      "Iteration 15016, loss = 0.08580760\n",
      "Iteration 15017, loss = 0.08579844\n",
      "Iteration 15018, loss = 0.08578687\n",
      "Iteration 15019, loss = 0.08577369\n",
      "Iteration 15020, loss = 0.08576678\n",
      "Iteration 15021, loss = 0.08575642\n",
      "Iteration 15022, loss = 0.08574459\n",
      "Iteration 15023, loss = 0.08573467\n",
      "Iteration 15024, loss = 0.08572443\n",
      "Iteration 15025, loss = 0.08571416\n",
      "Iteration 15026, loss = 0.08570837\n",
      "Iteration 15027, loss = 0.08569552\n",
      "Iteration 15028, loss = 0.08568555\n",
      "Iteration 15029, loss = 0.08568038\n",
      "Iteration 15030, loss = 0.08567249\n",
      "Iteration 15031, loss = 0.08566315\n",
      "Iteration 15032, loss = 0.08564789\n",
      "Iteration 15033, loss = 0.08564329\n",
      "Iteration 15034, loss = 0.08563598\n",
      "Iteration 15035, loss = 0.08562753\n",
      "Iteration 15036, loss = 0.08561578\n",
      "Iteration 15037, loss = 0.08560988\n",
      "Iteration 15038, loss = 0.08560237\n",
      "Iteration 15039, loss = 0.08559722\n",
      "Iteration 15040, loss = 0.08558859\n",
      "Iteration 15041, loss = 0.08557520\n",
      "Iteration 15042, loss = 0.08556414\n",
      "Iteration 15043, loss = 0.08555766\n",
      "Iteration 15044, loss = 0.08555101\n",
      "Iteration 15045, loss = 0.08554268\n",
      "Iteration 15046, loss = 0.08553188\n",
      "Iteration 15047, loss = 0.08551978\n",
      "Iteration 15048, loss = 0.08550631\n",
      "Iteration 15049, loss = 0.08550042\n",
      "Iteration 15050, loss = 0.08549051\n",
      "Iteration 15051, loss = 0.08548221\n",
      "Iteration 15052, loss = 0.08547298\n",
      "Iteration 15053, loss = 0.08545959\n",
      "Iteration 15054, loss = 0.08544932\n",
      "Iteration 15055, loss = 0.08544634\n",
      "Iteration 15056, loss = 0.08543715\n",
      "Iteration 15057, loss = 0.08542856\n",
      "Iteration 15058, loss = 0.08542124\n",
      "Iteration 15059, loss = 0.08540584\n",
      "Iteration 15060, loss = 0.08539695\n",
      "Iteration 15061, loss = 0.08538858\n",
      "Iteration 15062, loss = 0.08538050\n",
      "Iteration 15063, loss = 0.08537159\n",
      "Iteration 15064, loss = 0.08536496\n",
      "Iteration 15065, loss = 0.08535110\n",
      "Iteration 15066, loss = 0.08533731\n",
      "Iteration 15067, loss = 0.08533283\n",
      "Iteration 15068, loss = 0.08532601\n",
      "Iteration 15069, loss = 0.08531725\n",
      "Iteration 15070, loss = 0.08530679\n",
      "Iteration 15071, loss = 0.08530035\n",
      "Iteration 15072, loss = 0.08529267\n",
      "Iteration 15073, loss = 0.08528210\n",
      "Iteration 15074, loss = 0.08526693\n",
      "Iteration 15075, loss = 0.08525682\n",
      "Iteration 15076, loss = 0.08524822\n",
      "Iteration 15077, loss = 0.08523937\n",
      "Iteration 15078, loss = 0.08522798\n",
      "Iteration 15079, loss = 0.08521931\n",
      "Iteration 15080, loss = 0.08521000\n",
      "Iteration 15081, loss = 0.08519949\n",
      "Iteration 15082, loss = 0.08519661\n",
      "Iteration 15083, loss = 0.08518548\n",
      "Iteration 15084, loss = 0.08517587\n",
      "Iteration 15085, loss = 0.08516233\n",
      "Iteration 15086, loss = 0.08515898\n",
      "Iteration 15087, loss = 0.08514912\n",
      "Iteration 15088, loss = 0.08513589\n",
      "Iteration 15089, loss = 0.08513063\n",
      "Iteration 15090, loss = 0.08511926\n",
      "Iteration 15091, loss = 0.08511389\n",
      "Iteration 15092, loss = 0.08510536\n",
      "Iteration 15093, loss = 0.08509441\n",
      "Iteration 15094, loss = 0.08508076\n",
      "Iteration 15095, loss = 0.08507855\n",
      "Iteration 15096, loss = 0.08506812\n",
      "Iteration 15097, loss = 0.08505669\n",
      "Iteration 15098, loss = 0.08504887\n",
      "Iteration 15099, loss = 0.08503590\n",
      "Iteration 15100, loss = 0.08502766\n",
      "Iteration 15101, loss = 0.08501849\n",
      "Iteration 15102, loss = 0.08500809\n",
      "Iteration 15103, loss = 0.08500107\n",
      "Iteration 15104, loss = 0.08498942\n",
      "Iteration 15105, loss = 0.08497756\n",
      "Iteration 15106, loss = 0.08497254\n",
      "Iteration 15107, loss = 0.08496724\n",
      "Iteration 15108, loss = 0.08495789\n",
      "Iteration 15109, loss = 0.08494983\n",
      "Iteration 15110, loss = 0.08493652\n",
      "Iteration 15111, loss = 0.08492620\n",
      "Iteration 15112, loss = 0.08492360\n",
      "Iteration 15113, loss = 0.08490952\n",
      "Iteration 15114, loss = 0.08490731\n",
      "Iteration 15115, loss = 0.08489961\n",
      "Iteration 15116, loss = 0.08488798\n",
      "Iteration 15117, loss = 0.08487946\n",
      "Iteration 15118, loss = 0.08487044\n",
      "Iteration 15119, loss = 0.08486047\n",
      "Iteration 15120, loss = 0.08485087\n",
      "Iteration 15121, loss = 0.08484086\n",
      "Iteration 15122, loss = 0.08483345\n",
      "Iteration 15123, loss = 0.08482414\n",
      "Iteration 15124, loss = 0.08481164\n",
      "Iteration 15125, loss = 0.08480519\n",
      "Iteration 15126, loss = 0.08479759\n",
      "Iteration 15127, loss = 0.08478866\n",
      "Iteration 15128, loss = 0.08477356\n",
      "Iteration 15129, loss = 0.08476242\n",
      "Iteration 15130, loss = 0.08476138\n",
      "Iteration 15131, loss = 0.08475332\n",
      "Iteration 15132, loss = 0.08474207\n",
      "Iteration 15133, loss = 0.08473567\n",
      "Iteration 15134, loss = 0.08473221\n",
      "Iteration 15135, loss = 0.08472019\n",
      "Iteration 15136, loss = 0.08471128\n",
      "Iteration 15137, loss = 0.08470625\n",
      "Iteration 15138, loss = 0.08469354\n",
      "Iteration 15139, loss = 0.08467865\n",
      "Iteration 15140, loss = 0.08467335\n",
      "Iteration 15141, loss = 0.08466285\n",
      "Iteration 15142, loss = 0.08465172\n",
      "Iteration 15143, loss = 0.08463990\n",
      "Iteration 15144, loss = 0.08463606\n",
      "Iteration 15145, loss = 0.08462258\n",
      "Iteration 15146, loss = 0.08461158\n",
      "Iteration 15147, loss = 0.08460326\n",
      "Iteration 15148, loss = 0.08460163\n",
      "Iteration 15149, loss = 0.08458812\n",
      "Iteration 15150, loss = 0.08457927\n",
      "Iteration 15151, loss = 0.08457554\n",
      "Iteration 15152, loss = 0.08456513\n",
      "Iteration 15153, loss = 0.08455735\n",
      "Iteration 15154, loss = 0.08454703\n",
      "Iteration 15155, loss = 0.08453475\n",
      "Iteration 15156, loss = 0.08452461\n",
      "Iteration 15157, loss = 0.08451954\n",
      "Iteration 15158, loss = 0.08450992\n",
      "Iteration 15159, loss = 0.08449412\n",
      "Iteration 15160, loss = 0.08448955\n",
      "Iteration 15161, loss = 0.08448181\n",
      "Iteration 15162, loss = 0.08447290\n",
      "Iteration 15163, loss = 0.08446285\n",
      "Iteration 15164, loss = 0.08445587\n",
      "Iteration 15165, loss = 0.08444448\n",
      "Iteration 15166, loss = 0.08443062\n",
      "Iteration 15167, loss = 0.08442120\n",
      "Iteration 15168, loss = 0.08441399\n",
      "Iteration 15169, loss = 0.08440873\n",
      "Iteration 15170, loss = 0.08439854\n",
      "Iteration 15171, loss = 0.08438885\n",
      "Iteration 15172, loss = 0.08437885\n",
      "Iteration 15173, loss = 0.08437056\n",
      "Iteration 15174, loss = 0.08435982\n",
      "Iteration 15175, loss = 0.08435490\n",
      "Iteration 15176, loss = 0.08434709\n",
      "Iteration 15177, loss = 0.08433462\n",
      "Iteration 15178, loss = 0.08432572\n",
      "Iteration 15179, loss = 0.08431828\n",
      "Iteration 15180, loss = 0.08430965\n",
      "Iteration 15181, loss = 0.08430022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15182, loss = 0.08428833\n",
      "Iteration 15183, loss = 0.08427670\n",
      "Iteration 15184, loss = 0.08427122\n",
      "Iteration 15185, loss = 0.08426513\n",
      "Iteration 15186, loss = 0.08425183\n",
      "Iteration 15187, loss = 0.08424623\n",
      "Iteration 15188, loss = 0.08423765\n",
      "Iteration 15189, loss = 0.08422632\n",
      "Iteration 15190, loss = 0.08421774\n",
      "Iteration 15191, loss = 0.08420600\n",
      "Iteration 15192, loss = 0.08419293\n",
      "Iteration 15193, loss = 0.08418793\n",
      "Iteration 15194, loss = 0.08418354\n",
      "Iteration 15195, loss = 0.08417426\n",
      "Iteration 15196, loss = 0.08416122\n",
      "Iteration 15197, loss = 0.08415703\n",
      "Iteration 15198, loss = 0.08414824\n",
      "Iteration 15199, loss = 0.08414067\n",
      "Iteration 15200, loss = 0.08413414\n",
      "Iteration 15201, loss = 0.08412530\n",
      "Iteration 15202, loss = 0.08411552\n",
      "Iteration 15203, loss = 0.08410444\n",
      "Iteration 15204, loss = 0.08409836\n",
      "Iteration 15205, loss = 0.08408780\n",
      "Iteration 15206, loss = 0.08407847\n",
      "Iteration 15207, loss = 0.08406937\n",
      "Iteration 15208, loss = 0.08406052\n",
      "Iteration 15209, loss = 0.08404773\n",
      "Iteration 15210, loss = 0.08404378\n",
      "Iteration 15211, loss = 0.08403048\n",
      "Iteration 15212, loss = 0.08402116\n",
      "Iteration 15213, loss = 0.08401309\n",
      "Iteration 15214, loss = 0.08400841\n",
      "Iteration 15215, loss = 0.08400104\n",
      "Iteration 15216, loss = 0.08399449\n",
      "Iteration 15217, loss = 0.08398408\n",
      "Iteration 15218, loss = 0.08397243\n",
      "Iteration 15219, loss = 0.08396210\n",
      "Iteration 15220, loss = 0.08395636\n",
      "Iteration 15221, loss = 0.08394772\n",
      "Iteration 15222, loss = 0.08393312\n",
      "Iteration 15223, loss = 0.08392653\n",
      "Iteration 15224, loss = 0.08392099\n",
      "Iteration 15225, loss = 0.08391305\n",
      "Iteration 15226, loss = 0.08390261\n",
      "Iteration 15227, loss = 0.08389274\n",
      "Iteration 15228, loss = 0.08388568\n",
      "Iteration 15229, loss = 0.08387557\n",
      "Iteration 15230, loss = 0.08386710\n",
      "Iteration 15231, loss = 0.08386219\n",
      "Iteration 15232, loss = 0.08384842\n",
      "Iteration 15233, loss = 0.08383710\n",
      "Iteration 15234, loss = 0.08383107\n",
      "Iteration 15235, loss = 0.08382352\n",
      "Iteration 15236, loss = 0.08381372\n",
      "Iteration 15237, loss = 0.08380807\n",
      "Iteration 15238, loss = 0.08379717\n",
      "Iteration 15239, loss = 0.08378945\n",
      "Iteration 15240, loss = 0.08378288\n",
      "Iteration 15241, loss = 0.08377636\n",
      "Iteration 15242, loss = 0.08376524\n",
      "Iteration 15243, loss = 0.08375197\n",
      "Iteration 15244, loss = 0.08374236\n",
      "Iteration 15245, loss = 0.08373917\n",
      "Iteration 15246, loss = 0.08372775\n",
      "Iteration 15247, loss = 0.08371209\n",
      "Iteration 15248, loss = 0.08370754\n",
      "Iteration 15249, loss = 0.08370195\n",
      "Iteration 15250, loss = 0.08369412\n",
      "Iteration 15251, loss = 0.08368516\n",
      "Iteration 15252, loss = 0.08367658\n",
      "Iteration 15253, loss = 0.08366693\n",
      "Iteration 15254, loss = 0.08365560\n",
      "Iteration 15255, loss = 0.08364836\n",
      "Iteration 15256, loss = 0.08364251\n",
      "Iteration 15257, loss = 0.08363454\n",
      "Iteration 15258, loss = 0.08362256\n",
      "Iteration 15259, loss = 0.08361299\n",
      "Iteration 15260, loss = 0.08360448\n",
      "Iteration 15261, loss = 0.08359610\n",
      "Iteration 15262, loss = 0.08358481\n",
      "Iteration 15263, loss = 0.08357465\n",
      "Iteration 15264, loss = 0.08356977\n",
      "Iteration 15265, loss = 0.08355981\n",
      "Iteration 15266, loss = 0.08354915\n",
      "Iteration 15267, loss = 0.08354252\n",
      "Iteration 15268, loss = 0.08353388\n",
      "Iteration 15269, loss = 0.08352817\n",
      "Iteration 15270, loss = 0.08351910\n",
      "Iteration 15271, loss = 0.08350186\n",
      "Iteration 15272, loss = 0.08349590\n",
      "Iteration 15273, loss = 0.08349004\n",
      "Iteration 15274, loss = 0.08348126\n",
      "Iteration 15275, loss = 0.08346993\n",
      "Iteration 15276, loss = 0.08346482\n",
      "Iteration 15277, loss = 0.08345295\n",
      "Iteration 15278, loss = 0.08344526\n",
      "Iteration 15279, loss = 0.08343550\n",
      "Iteration 15280, loss = 0.08342626\n",
      "Iteration 15281, loss = 0.08342315\n",
      "Iteration 15282, loss = 0.08341365\n",
      "Iteration 15283, loss = 0.08340563\n",
      "Iteration 15284, loss = 0.08339802\n",
      "Iteration 15285, loss = 0.08339328\n",
      "Iteration 15286, loss = 0.08338756\n",
      "Iteration 15287, loss = 0.08337824\n",
      "Iteration 15288, loss = 0.08336692\n",
      "Iteration 15289, loss = 0.08335527\n",
      "Iteration 15290, loss = 0.08334411\n",
      "Iteration 15291, loss = 0.08334005\n",
      "Iteration 15292, loss = 0.08333551\n",
      "Iteration 15293, loss = 0.08332429\n",
      "Iteration 15294, loss = 0.08331350\n",
      "Iteration 15295, loss = 0.08330312\n",
      "Iteration 15296, loss = 0.08329392\n",
      "Iteration 15297, loss = 0.08328446\n",
      "Iteration 15298, loss = 0.08327691\n",
      "Iteration 15299, loss = 0.08326723\n",
      "Iteration 15300, loss = 0.08325294\n",
      "Iteration 15301, loss = 0.08324340\n",
      "Iteration 15302, loss = 0.08324258\n",
      "Iteration 15303, loss = 0.08323393\n",
      "Iteration 15304, loss = 0.08322502\n",
      "Iteration 15305, loss = 0.08321328\n",
      "Iteration 15306, loss = 0.08320310\n",
      "Iteration 15307, loss = 0.08319591\n",
      "Iteration 15308, loss = 0.08318727\n",
      "Iteration 15309, loss = 0.08318513\n",
      "Iteration 15310, loss = 0.08317571\n",
      "Iteration 15311, loss = 0.08316630\n",
      "Iteration 15312, loss = 0.08315958\n",
      "Iteration 15313, loss = 0.08314843\n",
      "Iteration 15314, loss = 0.08313917\n",
      "Iteration 15315, loss = 0.08313008\n",
      "Iteration 15316, loss = 0.08311652\n",
      "Iteration 15317, loss = 0.08311493\n",
      "Iteration 15318, loss = 0.08310428\n",
      "Iteration 15319, loss = 0.08309507\n",
      "Iteration 15320, loss = 0.08308636\n",
      "Iteration 15321, loss = 0.08308092\n",
      "Iteration 15322, loss = 0.08307507\n",
      "Iteration 15323, loss = 0.08306350\n",
      "Iteration 15324, loss = 0.08305627\n",
      "Iteration 15325, loss = 0.08304635\n",
      "Iteration 15326, loss = 0.08303568\n",
      "Iteration 15327, loss = 0.08302246\n",
      "Iteration 15328, loss = 0.08301881\n",
      "Iteration 15329, loss = 0.08300938\n",
      "Iteration 15330, loss = 0.08300151\n",
      "Iteration 15331, loss = 0.08299330\n",
      "Iteration 15332, loss = 0.08298427\n",
      "Iteration 15333, loss = 0.08297467\n",
      "Iteration 15334, loss = 0.08296120\n",
      "Iteration 15335, loss = 0.08295938\n",
      "Iteration 15336, loss = 0.08294964\n",
      "Iteration 15337, loss = 0.08293419\n",
      "Iteration 15338, loss = 0.08292759\n",
      "Iteration 15339, loss = 0.08292451\n",
      "Iteration 15340, loss = 0.08291350\n",
      "Iteration 15341, loss = 0.08290323\n",
      "Iteration 15342, loss = 0.08289502\n",
      "Iteration 15343, loss = 0.08288940\n",
      "Iteration 15344, loss = 0.08287897\n",
      "Iteration 15345, loss = 0.08286806\n",
      "Iteration 15346, loss = 0.08286008\n",
      "Iteration 15347, loss = 0.08285210\n",
      "Iteration 15348, loss = 0.08284064\n",
      "Iteration 15349, loss = 0.08283009\n",
      "Iteration 15350, loss = 0.08282302\n",
      "Iteration 15351, loss = 0.08281361\n",
      "Iteration 15352, loss = 0.08281040\n",
      "Iteration 15353, loss = 0.08279977\n",
      "Iteration 15354, loss = 0.08279174\n",
      "Iteration 15355, loss = 0.08278417\n",
      "Iteration 15356, loss = 0.08277835\n",
      "Iteration 15357, loss = 0.08276753\n",
      "Iteration 15358, loss = 0.08275846\n",
      "Iteration 15359, loss = 0.08275190\n",
      "Iteration 15360, loss = 0.08274247\n",
      "Iteration 15361, loss = 0.08273001\n",
      "Iteration 15362, loss = 0.08273187\n",
      "Iteration 15363, loss = 0.08272274\n",
      "Iteration 15364, loss = 0.08270839\n",
      "Iteration 15365, loss = 0.08270138\n",
      "Iteration 15366, loss = 0.08269089\n",
      "Iteration 15367, loss = 0.08268527\n",
      "Iteration 15368, loss = 0.08267364\n",
      "Iteration 15369, loss = 0.08265835\n",
      "Iteration 15370, loss = 0.08265845\n",
      "Iteration 15371, loss = 0.08265124\n",
      "Iteration 15372, loss = 0.08264158\n",
      "Iteration 15373, loss = 0.08263562\n",
      "Iteration 15374, loss = 0.08262693\n",
      "Iteration 15375, loss = 0.08261797\n",
      "Iteration 15376, loss = 0.08260880\n",
      "Iteration 15377, loss = 0.08259885\n",
      "Iteration 15378, loss = 0.08258902\n",
      "Iteration 15379, loss = 0.08257875\n",
      "Iteration 15380, loss = 0.08257405\n",
      "Iteration 15381, loss = 0.08256873\n",
      "Iteration 15382, loss = 0.08256007\n",
      "Iteration 15383, loss = 0.08255420\n",
      "Iteration 15384, loss = 0.08254683\n",
      "Iteration 15385, loss = 0.08253649\n",
      "Iteration 15386, loss = 0.08253192\n",
      "Iteration 15387, loss = 0.08251945\n",
      "Iteration 15388, loss = 0.08251017\n",
      "Iteration 15389, loss = 0.08249781\n",
      "Iteration 15390, loss = 0.08249158\n",
      "Iteration 15391, loss = 0.08249031\n",
      "Iteration 15392, loss = 0.08248344\n",
      "Iteration 15393, loss = 0.08247304\n",
      "Iteration 15394, loss = 0.08245866\n",
      "Iteration 15395, loss = 0.08244844\n",
      "Iteration 15396, loss = 0.08244065\n",
      "Iteration 15397, loss = 0.08243731\n",
      "Iteration 15398, loss = 0.08243023\n",
      "Iteration 15399, loss = 0.08241686\n",
      "Iteration 15400, loss = 0.08240684\n",
      "Iteration 15401, loss = 0.08239599\n",
      "Iteration 15402, loss = 0.08238773\n",
      "Iteration 15403, loss = 0.08237854\n",
      "Iteration 15404, loss = 0.08236814\n",
      "Iteration 15405, loss = 0.08236191\n",
      "Iteration 15406, loss = 0.08235892\n",
      "Iteration 15407, loss = 0.08235077\n",
      "Iteration 15408, loss = 0.08233360\n",
      "Iteration 15409, loss = 0.08232738\n",
      "Iteration 15410, loss = 0.08232322\n",
      "Iteration 15411, loss = 0.08231440\n",
      "Iteration 15412, loss = 0.08230597\n",
      "Iteration 15413, loss = 0.08229402\n",
      "Iteration 15414, loss = 0.08229279\n",
      "Iteration 15415, loss = 0.08228162\n",
      "Iteration 15416, loss = 0.08227150\n",
      "Iteration 15417, loss = 0.08226108\n",
      "Iteration 15418, loss = 0.08225544\n",
      "Iteration 15419, loss = 0.08224854\n",
      "Iteration 15420, loss = 0.08223869\n",
      "Iteration 15421, loss = 0.08223138\n",
      "Iteration 15422, loss = 0.08222260\n",
      "Iteration 15423, loss = 0.08220911\n",
      "Iteration 15424, loss = 0.08220716\n",
      "Iteration 15425, loss = 0.08220001\n",
      "Iteration 15426, loss = 0.08218546\n",
      "Iteration 15427, loss = 0.08217836\n",
      "Iteration 15428, loss = 0.08217120\n",
      "Iteration 15429, loss = 0.08217118\n",
      "Iteration 15430, loss = 0.08216096\n",
      "Iteration 15431, loss = 0.08215483\n",
      "Iteration 15432, loss = 0.08214580\n",
      "Iteration 15433, loss = 0.08213423\n",
      "Iteration 15434, loss = 0.08212410\n",
      "Iteration 15435, loss = 0.08211531\n",
      "Iteration 15436, loss = 0.08210438\n",
      "Iteration 15437, loss = 0.08209615\n",
      "Iteration 15438, loss = 0.08209305\n",
      "Iteration 15439, loss = 0.08208145\n",
      "Iteration 15440, loss = 0.08207064\n",
      "Iteration 15441, loss = 0.08206440\n",
      "Iteration 15442, loss = 0.08205493\n",
      "Iteration 15443, loss = 0.08204938\n",
      "Iteration 15444, loss = 0.08204453\n",
      "Iteration 15445, loss = 0.08203641\n",
      "Iteration 15446, loss = 0.08202526\n",
      "Iteration 15447, loss = 0.08202093\n",
      "Iteration 15448, loss = 0.08201203\n",
      "Iteration 15449, loss = 0.08199910\n",
      "Iteration 15450, loss = 0.08198725\n",
      "Iteration 15451, loss = 0.08197689\n",
      "Iteration 15452, loss = 0.08197516\n",
      "Iteration 15453, loss = 0.08196837\n",
      "Iteration 15454, loss = 0.08195757\n",
      "Iteration 15455, loss = 0.08194624\n",
      "Iteration 15456, loss = 0.08193983\n",
      "Iteration 15457, loss = 0.08192955\n",
      "Iteration 15458, loss = 0.08192274\n",
      "Iteration 15459, loss = 0.08191309\n",
      "Iteration 15460, loss = 0.08190345\n",
      "Iteration 15461, loss = 0.08189532\n",
      "Iteration 15462, loss = 0.08188905\n",
      "Iteration 15463, loss = 0.08188175\n",
      "Iteration 15464, loss = 0.08187015\n",
      "Iteration 15465, loss = 0.08186070\n",
      "Iteration 15466, loss = 0.08185294\n",
      "Iteration 15467, loss = 0.08184545\n",
      "Iteration 15468, loss = 0.08184648\n",
      "Iteration 15469, loss = 0.08183686\n",
      "Iteration 15470, loss = 0.08182083\n",
      "Iteration 15471, loss = 0.08181499\n",
      "Iteration 15472, loss = 0.08180913\n",
      "Iteration 15473, loss = 0.08180138\n",
      "Iteration 15474, loss = 0.08179721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15475, loss = 0.08178628\n",
      "Iteration 15476, loss = 0.08177875\n",
      "Iteration 15477, loss = 0.08177164\n",
      "Iteration 15478, loss = 0.08176336\n",
      "Iteration 15479, loss = 0.08175399\n",
      "Iteration 15480, loss = 0.08174204\n",
      "Iteration 15481, loss = 0.08173198\n",
      "Iteration 15482, loss = 0.08172791\n",
      "Iteration 15483, loss = 0.08172215\n",
      "Iteration 15484, loss = 0.08171173\n",
      "Iteration 15485, loss = 0.08170068\n",
      "Iteration 15486, loss = 0.08169431\n",
      "Iteration 15487, loss = 0.08168842\n",
      "Iteration 15488, loss = 0.08168204\n",
      "Iteration 15489, loss = 0.08167566\n",
      "Iteration 15490, loss = 0.08167115\n",
      "Iteration 15491, loss = 0.08165772\n",
      "Iteration 15492, loss = 0.08164720\n",
      "Iteration 15493, loss = 0.08163628\n",
      "Iteration 15494, loss = 0.08162439\n",
      "Iteration 15495, loss = 0.08161495\n",
      "Iteration 15496, loss = 0.08160773\n",
      "Iteration 15497, loss = 0.08160380\n",
      "Iteration 15498, loss = 0.08159627\n",
      "Iteration 15499, loss = 0.08158715\n",
      "Iteration 15500, loss = 0.08157705\n",
      "Iteration 15501, loss = 0.08157034\n",
      "Iteration 15502, loss = 0.08156109\n",
      "Iteration 15503, loss = 0.08155266\n",
      "Iteration 15504, loss = 0.08154590\n",
      "Iteration 15505, loss = 0.08153778\n",
      "Iteration 15506, loss = 0.08153033\n",
      "Iteration 15507, loss = 0.08152293\n",
      "Iteration 15508, loss = 0.08151084\n",
      "Iteration 15509, loss = 0.08150309\n",
      "Iteration 15510, loss = 0.08149617\n",
      "Iteration 15511, loss = 0.08148809\n",
      "Iteration 15512, loss = 0.08148036\n",
      "Iteration 15513, loss = 0.08147589\n",
      "Iteration 15514, loss = 0.08146544\n",
      "Iteration 15515, loss = 0.08145598\n",
      "Iteration 15516, loss = 0.08145318\n",
      "Iteration 15517, loss = 0.08144491\n",
      "Iteration 15518, loss = 0.08143356\n",
      "Iteration 15519, loss = 0.08142469\n",
      "Iteration 15520, loss = 0.08141802\n",
      "Iteration 15521, loss = 0.08140828\n",
      "Iteration 15522, loss = 0.08139747\n",
      "Iteration 15523, loss = 0.08139436\n",
      "Iteration 15524, loss = 0.08138649\n",
      "Iteration 15525, loss = 0.08137691\n",
      "Iteration 15526, loss = 0.08136809\n",
      "Iteration 15527, loss = 0.08135908\n",
      "Iteration 15528, loss = 0.08134841\n",
      "Iteration 15529, loss = 0.08134290\n",
      "Iteration 15530, loss = 0.08133377\n",
      "Iteration 15531, loss = 0.08132087\n",
      "Iteration 15532, loss = 0.08131862\n",
      "Iteration 15533, loss = 0.08131468\n",
      "Iteration 15534, loss = 0.08130041\n",
      "Iteration 15535, loss = 0.08129014\n",
      "Iteration 15536, loss = 0.08128385\n",
      "Iteration 15537, loss = 0.08127952\n",
      "Iteration 15538, loss = 0.08127078\n",
      "Iteration 15539, loss = 0.08126360\n",
      "Iteration 15540, loss = 0.08125371\n",
      "Iteration 15541, loss = 0.08124838\n",
      "Iteration 15542, loss = 0.08124160\n",
      "Iteration 15543, loss = 0.08123130\n",
      "Iteration 15544, loss = 0.08122367\n",
      "Iteration 15545, loss = 0.08121189\n",
      "Iteration 15546, loss = 0.08120181\n",
      "Iteration 15547, loss = 0.08119699\n",
      "Iteration 15548, loss = 0.08118985\n",
      "Iteration 15549, loss = 0.08117592\n",
      "Iteration 15550, loss = 0.08117071\n",
      "Iteration 15551, loss = 0.08115941\n",
      "Iteration 15552, loss = 0.08115228\n",
      "Iteration 15553, loss = 0.08114590\n",
      "Iteration 15554, loss = 0.08113443\n",
      "Iteration 15555, loss = 0.08113432\n",
      "Iteration 15556, loss = 0.08112268\n",
      "Iteration 15557, loss = 0.08111010\n",
      "Iteration 15558, loss = 0.08110492\n",
      "Iteration 15559, loss = 0.08110056\n",
      "Iteration 15560, loss = 0.08109293\n",
      "Iteration 15561, loss = 0.08108354\n",
      "Iteration 15562, loss = 0.08107364\n",
      "Iteration 15563, loss = 0.08106694\n",
      "Iteration 15564, loss = 0.08106038\n",
      "Iteration 15565, loss = 0.08104934\n",
      "Iteration 15566, loss = 0.08104020\n",
      "Iteration 15567, loss = 0.08103841\n",
      "Iteration 15568, loss = 0.08102728\n",
      "Iteration 15569, loss = 0.08101453\n",
      "Iteration 15570, loss = 0.08101237\n",
      "Iteration 15571, loss = 0.08100066\n",
      "Iteration 15572, loss = 0.08099236\n",
      "Iteration 15573, loss = 0.08098757\n",
      "Iteration 15574, loss = 0.08097874\n",
      "Iteration 15575, loss = 0.08096852\n",
      "Iteration 15576, loss = 0.08096018\n",
      "Iteration 15577, loss = 0.08095360\n",
      "Iteration 15578, loss = 0.08094667\n",
      "Iteration 15579, loss = 0.08094118\n",
      "Iteration 15580, loss = 0.08093060\n",
      "Iteration 15581, loss = 0.08092275\n",
      "Iteration 15582, loss = 0.08091194\n",
      "Iteration 15583, loss = 0.08090436\n",
      "Iteration 15584, loss = 0.08089580\n",
      "Iteration 15585, loss = 0.08088993\n",
      "Iteration 15586, loss = 0.08088287\n",
      "Iteration 15587, loss = 0.08087413\n",
      "Iteration 15588, loss = 0.08086176\n",
      "Iteration 15589, loss = 0.08085552\n",
      "Iteration 15590, loss = 0.08084757\n",
      "Iteration 15591, loss = 0.08083807\n",
      "Iteration 15592, loss = 0.08083016\n",
      "Iteration 15593, loss = 0.08082354\n",
      "Iteration 15594, loss = 0.08081656\n",
      "Iteration 15595, loss = 0.08081058\n",
      "Iteration 15596, loss = 0.08080308\n",
      "Iteration 15597, loss = 0.08079613\n",
      "Iteration 15598, loss = 0.08078685\n",
      "Iteration 15599, loss = 0.08077543\n",
      "Iteration 15600, loss = 0.08076638\n",
      "Iteration 15601, loss = 0.08075772\n",
      "Iteration 15602, loss = 0.08075242\n",
      "Iteration 15603, loss = 0.08074888\n",
      "Iteration 15604, loss = 0.08073632\n",
      "Iteration 15605, loss = 0.08072562\n",
      "Iteration 15606, loss = 0.08072418\n",
      "Iteration 15607, loss = 0.08072040\n",
      "Iteration 15608, loss = 0.08070746\n",
      "Iteration 15609, loss = 0.08070016\n",
      "Iteration 15610, loss = 0.08069267\n",
      "Iteration 15611, loss = 0.08068577\n",
      "Iteration 15612, loss = 0.08067783\n",
      "Iteration 15613, loss = 0.08066714\n",
      "Iteration 15614, loss = 0.08065932\n",
      "Iteration 15615, loss = 0.08065100\n",
      "Iteration 15616, loss = 0.08064388\n",
      "Iteration 15617, loss = 0.08063008\n",
      "Iteration 15618, loss = 0.08062494\n",
      "Iteration 15619, loss = 0.08061890\n",
      "Iteration 15620, loss = 0.08061299\n",
      "Iteration 15621, loss = 0.08060023\n",
      "Iteration 15622, loss = 0.08059590\n",
      "Iteration 15623, loss = 0.08058839\n",
      "Iteration 15624, loss = 0.08057924\n",
      "Iteration 15625, loss = 0.08056881\n",
      "Iteration 15626, loss = 0.08056663\n",
      "Iteration 15627, loss = 0.08056033\n",
      "Iteration 15628, loss = 0.08054647\n",
      "Iteration 15629, loss = 0.08053927\n",
      "Iteration 15630, loss = 0.08053384\n",
      "Iteration 15631, loss = 0.08052653\n",
      "Iteration 15632, loss = 0.08051617\n",
      "Iteration 15633, loss = 0.08050536\n",
      "Iteration 15634, loss = 0.08050302\n",
      "Iteration 15635, loss = 0.08049710\n",
      "Iteration 15636, loss = 0.08048749\n",
      "Iteration 15637, loss = 0.08048454\n",
      "Iteration 15638, loss = 0.08047374\n",
      "Iteration 15639, loss = 0.08046634\n",
      "Iteration 15640, loss = 0.08046086\n",
      "Iteration 15641, loss = 0.08044914\n",
      "Iteration 15642, loss = 0.08043939\n",
      "Iteration 15643, loss = 0.08042862\n",
      "Iteration 15644, loss = 0.08042915\n",
      "Iteration 15645, loss = 0.08042297\n",
      "Iteration 15646, loss = 0.08040651\n",
      "Iteration 15647, loss = 0.08039948\n",
      "Iteration 15648, loss = 0.08039013\n",
      "Iteration 15649, loss = 0.08038363\n",
      "Iteration 15650, loss = 0.08037341\n",
      "Iteration 15651, loss = 0.08036530\n",
      "Iteration 15652, loss = 0.08035487\n",
      "Iteration 15653, loss = 0.08034948\n",
      "Iteration 15654, loss = 0.08034171\n",
      "Iteration 15655, loss = 0.08033505\n",
      "Iteration 15656, loss = 0.08032959\n",
      "Iteration 15657, loss = 0.08031868\n",
      "Iteration 15658, loss = 0.08031295\n",
      "Iteration 15659, loss = 0.08030438\n",
      "Iteration 15660, loss = 0.08029553\n",
      "Iteration 15661, loss = 0.08028830\n",
      "Iteration 15662, loss = 0.08028009\n",
      "Iteration 15663, loss = 0.08027391\n",
      "Iteration 15664, loss = 0.08026605\n",
      "Iteration 15665, loss = 0.08025865\n",
      "Iteration 15666, loss = 0.08025032\n",
      "Iteration 15667, loss = 0.08024159\n",
      "Iteration 15668, loss = 0.08023332\n",
      "Iteration 15669, loss = 0.08022660\n",
      "Iteration 15670, loss = 0.08021646\n",
      "Iteration 15671, loss = 0.08020791\n",
      "Iteration 15672, loss = 0.08020607\n",
      "Iteration 15673, loss = 0.08019580\n",
      "Iteration 15674, loss = 0.08018842\n",
      "Iteration 15675, loss = 0.08018034\n",
      "Iteration 15676, loss = 0.08017156\n",
      "Iteration 15677, loss = 0.08016109\n",
      "Iteration 15678, loss = 0.08015145\n",
      "Iteration 15679, loss = 0.08014097\n",
      "Iteration 15680, loss = 0.08013635\n",
      "Iteration 15681, loss = 0.08013276\n",
      "Iteration 15682, loss = 0.08012336\n",
      "Iteration 15683, loss = 0.08011371\n",
      "Iteration 15684, loss = 0.08010924\n",
      "Iteration 15685, loss = 0.08010236\n",
      "Iteration 15686, loss = 0.08009134\n",
      "Iteration 15687, loss = 0.08008657\n",
      "Iteration 15688, loss = 0.08007932\n",
      "Iteration 15689, loss = 0.08007098\n",
      "Iteration 15690, loss = 0.08006385\n",
      "Iteration 15691, loss = 0.08005921\n",
      "Iteration 15692, loss = 0.08004959\n",
      "Iteration 15693, loss = 0.08004017\n",
      "Iteration 15694, loss = 0.08003500\n",
      "Iteration 15695, loss = 0.08002089\n",
      "Iteration 15696, loss = 0.08001698\n",
      "Iteration 15697, loss = 0.08001211\n",
      "Iteration 15698, loss = 0.08000107\n",
      "Iteration 15699, loss = 0.07999206\n",
      "Iteration 15700, loss = 0.07998253\n",
      "Iteration 15701, loss = 0.07997730\n",
      "Iteration 15702, loss = 0.07996899\n",
      "Iteration 15703, loss = 0.07995968\n",
      "Iteration 15704, loss = 0.07995249\n",
      "Iteration 15705, loss = 0.07994442\n",
      "Iteration 15706, loss = 0.07993479\n",
      "Iteration 15707, loss = 0.07992609\n",
      "Iteration 15708, loss = 0.07991866\n",
      "Iteration 15709, loss = 0.07991332\n",
      "Iteration 15710, loss = 0.07990653\n",
      "Iteration 15711, loss = 0.07989640\n",
      "Iteration 15712, loss = 0.07988845\n",
      "Iteration 15713, loss = 0.07988566\n",
      "Iteration 15714, loss = 0.07987772\n",
      "Iteration 15715, loss = 0.07987377\n",
      "Iteration 15716, loss = 0.07986584\n",
      "Iteration 15717, loss = 0.07985664\n",
      "Iteration 15718, loss = 0.07985090\n",
      "Iteration 15719, loss = 0.07984474\n",
      "Iteration 15720, loss = 0.07983683\n",
      "Iteration 15721, loss = 0.07982370\n",
      "Iteration 15722, loss = 0.07981931\n",
      "Iteration 15723, loss = 0.07981082\n",
      "Iteration 15724, loss = 0.07980146\n",
      "Iteration 15725, loss = 0.07979009\n",
      "Iteration 15726, loss = 0.07978614\n",
      "Iteration 15727, loss = 0.07978304\n",
      "Iteration 15728, loss = 0.07977295\n",
      "Iteration 15729, loss = 0.07976254\n",
      "Iteration 15730, loss = 0.07975145\n",
      "Iteration 15731, loss = 0.07975100\n",
      "Iteration 15732, loss = 0.07974481\n",
      "Iteration 15733, loss = 0.07973219\n",
      "Iteration 15734, loss = 0.07972953\n",
      "Iteration 15735, loss = 0.07972210\n",
      "Iteration 15736, loss = 0.07971340\n",
      "Iteration 15737, loss = 0.07970797\n",
      "Iteration 15738, loss = 0.07969458\n",
      "Iteration 15739, loss = 0.07968476\n",
      "Iteration 15740, loss = 0.07968211\n",
      "Iteration 15741, loss = 0.07967557\n",
      "Iteration 15742, loss = 0.07966361\n",
      "Iteration 15743, loss = 0.07965377\n",
      "Iteration 15744, loss = 0.07964948\n",
      "Iteration 15745, loss = 0.07964253\n",
      "Iteration 15746, loss = 0.07963482\n",
      "Iteration 15747, loss = 0.07962818\n",
      "Iteration 15748, loss = 0.07961841\n",
      "Iteration 15749, loss = 0.07961162\n",
      "Iteration 15750, loss = 0.07960228\n",
      "Iteration 15751, loss = 0.07959531\n",
      "Iteration 15752, loss = 0.07958944\n",
      "Iteration 15753, loss = 0.07958289\n",
      "Iteration 15754, loss = 0.07957642\n",
      "Iteration 15755, loss = 0.07956572\n",
      "Iteration 15756, loss = 0.07955673\n",
      "Iteration 15757, loss = 0.07954631\n",
      "Iteration 15758, loss = 0.07953871\n",
      "Iteration 15759, loss = 0.07953252\n",
      "Iteration 15760, loss = 0.07952696\n",
      "Iteration 15761, loss = 0.07951421\n",
      "Iteration 15762, loss = 0.07950963\n",
      "Iteration 15763, loss = 0.07950108\n",
      "Iteration 15764, loss = 0.07949035\n",
      "Iteration 15765, loss = 0.07948212\n",
      "Iteration 15766, loss = 0.07947920\n",
      "Iteration 15767, loss = 0.07947354\n",
      "Iteration 15768, loss = 0.07946251\n",
      "Iteration 15769, loss = 0.07945478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15770, loss = 0.07944919\n",
      "Iteration 15771, loss = 0.07944237\n",
      "Iteration 15772, loss = 0.07943286\n",
      "Iteration 15773, loss = 0.07942868\n",
      "Iteration 15774, loss = 0.07942000\n",
      "Iteration 15775, loss = 0.07940995\n",
      "Iteration 15776, loss = 0.07940364\n",
      "Iteration 15777, loss = 0.07939507\n",
      "Iteration 15778, loss = 0.07938746\n",
      "Iteration 15779, loss = 0.07937686\n",
      "Iteration 15780, loss = 0.07937303\n",
      "Iteration 15781, loss = 0.07936509\n",
      "Iteration 15782, loss = 0.07935848\n",
      "Iteration 15783, loss = 0.07934734\n",
      "Iteration 15784, loss = 0.07933952\n",
      "Iteration 15785, loss = 0.07933151\n",
      "Iteration 15786, loss = 0.07932396\n",
      "Iteration 15787, loss = 0.07931544\n",
      "Iteration 15788, loss = 0.07930679\n",
      "Iteration 15789, loss = 0.07929979\n",
      "Iteration 15790, loss = 0.07929154\n",
      "Iteration 15791, loss = 0.07928533\n",
      "Iteration 15792, loss = 0.07927916\n",
      "Iteration 15793, loss = 0.07927049\n",
      "Iteration 15794, loss = 0.07926192\n",
      "Iteration 15795, loss = 0.07925603\n",
      "Iteration 15796, loss = 0.07925001\n",
      "Iteration 15797, loss = 0.07924246\n",
      "Iteration 15798, loss = 0.07923433\n",
      "Iteration 15799, loss = 0.07922478\n",
      "Iteration 15800, loss = 0.07922062\n",
      "Iteration 15801, loss = 0.07921225\n",
      "Iteration 15802, loss = 0.07920554\n",
      "Iteration 15803, loss = 0.07919618\n",
      "Iteration 15804, loss = 0.07919214\n",
      "Iteration 15805, loss = 0.07918720\n",
      "Iteration 15806, loss = 0.07917919\n",
      "Iteration 15807, loss = 0.07916936\n",
      "Iteration 15808, loss = 0.07916026\n",
      "Iteration 15809, loss = 0.07915672\n",
      "Iteration 15810, loss = 0.07914781\n",
      "Iteration 15811, loss = 0.07913973\n",
      "Iteration 15812, loss = 0.07913352\n",
      "Iteration 15813, loss = 0.07912639\n",
      "Iteration 15814, loss = 0.07912201\n",
      "Iteration 15815, loss = 0.07911680\n",
      "Iteration 15816, loss = 0.07910660\n",
      "Iteration 15817, loss = 0.07909772\n",
      "Iteration 15818, loss = 0.07909134\n",
      "Iteration 15819, loss = 0.07908262\n",
      "Iteration 15820, loss = 0.07907492\n",
      "Iteration 15821, loss = 0.07907175\n",
      "Iteration 15822, loss = 0.07906626\n",
      "Iteration 15823, loss = 0.07905647\n",
      "Iteration 15824, loss = 0.07904446\n",
      "Iteration 15825, loss = 0.07903739\n",
      "Iteration 15826, loss = 0.07903034\n",
      "Iteration 15827, loss = 0.07902334\n",
      "Iteration 15828, loss = 0.07901732\n",
      "Iteration 15829, loss = 0.07900853\n",
      "Iteration 15830, loss = 0.07900106\n",
      "Iteration 15831, loss = 0.07899408\n",
      "Iteration 15832, loss = 0.07898682\n",
      "Iteration 15833, loss = 0.07897980\n",
      "Iteration 15834, loss = 0.07897145\n",
      "Iteration 15835, loss = 0.07896162\n",
      "Iteration 15836, loss = 0.07895123\n",
      "Iteration 15837, loss = 0.07894587\n",
      "Iteration 15838, loss = 0.07894004\n",
      "Iteration 15839, loss = 0.07893294\n",
      "Iteration 15840, loss = 0.07891888\n",
      "Iteration 15841, loss = 0.07891593\n",
      "Iteration 15842, loss = 0.07891145\n",
      "Iteration 15843, loss = 0.07890390\n",
      "Iteration 15844, loss = 0.07889948\n",
      "Iteration 15845, loss = 0.07889273\n",
      "Iteration 15846, loss = 0.07888222\n",
      "Iteration 15847, loss = 0.07887576\n",
      "Iteration 15848, loss = 0.07886779\n",
      "Iteration 15849, loss = 0.07885581\n",
      "Iteration 15850, loss = 0.07885786\n",
      "Iteration 15851, loss = 0.07885132\n",
      "Iteration 15852, loss = 0.07883276\n",
      "Iteration 15853, loss = 0.07882704\n",
      "Iteration 15854, loss = 0.07882189\n",
      "Iteration 15855, loss = 0.07881752\n",
      "Iteration 15856, loss = 0.07881284\n",
      "Iteration 15857, loss = 0.07880428\n",
      "Iteration 15858, loss = 0.07879413\n",
      "Iteration 15859, loss = 0.07878918\n",
      "Iteration 15860, loss = 0.07878008\n",
      "Iteration 15861, loss = 0.07876999\n",
      "Iteration 15862, loss = 0.07875732\n",
      "Iteration 15863, loss = 0.07875761\n",
      "Iteration 15864, loss = 0.07874961\n",
      "Iteration 15865, loss = 0.07873759\n",
      "Iteration 15866, loss = 0.07873261\n",
      "Iteration 15867, loss = 0.07872535\n",
      "Iteration 15868, loss = 0.07871677\n",
      "Iteration 15869, loss = 0.07870872\n",
      "Iteration 15870, loss = 0.07870732\n",
      "Iteration 15871, loss = 0.07869917\n",
      "Iteration 15872, loss = 0.07869068\n",
      "Iteration 15873, loss = 0.07868578\n",
      "Iteration 15874, loss = 0.07867907\n",
      "Iteration 15875, loss = 0.07866757\n",
      "Iteration 15876, loss = 0.07866169\n",
      "Iteration 15877, loss = 0.07865819\n",
      "Iteration 15878, loss = 0.07864625\n",
      "Iteration 15879, loss = 0.07863495\n",
      "Iteration 15880, loss = 0.07862939\n",
      "Iteration 15881, loss = 0.07862061\n",
      "Iteration 15882, loss = 0.07861471\n",
      "Iteration 15883, loss = 0.07860786\n",
      "Iteration 15884, loss = 0.07859600\n",
      "Iteration 15885, loss = 0.07859124\n",
      "Iteration 15886, loss = 0.07858736\n",
      "Iteration 15887, loss = 0.07857869\n",
      "Iteration 15888, loss = 0.07856715\n",
      "Iteration 15889, loss = 0.07856250\n",
      "Iteration 15890, loss = 0.07855531\n",
      "Iteration 15891, loss = 0.07854761\n",
      "Iteration 15892, loss = 0.07853905\n",
      "Iteration 15893, loss = 0.07853042\n",
      "Iteration 15894, loss = 0.07852145\n",
      "Iteration 15895, loss = 0.07851778\n",
      "Iteration 15896, loss = 0.07850993\n",
      "Iteration 15897, loss = 0.07850378\n",
      "Iteration 15898, loss = 0.07849344\n",
      "Iteration 15899, loss = 0.07848704\n",
      "Iteration 15900, loss = 0.07848023\n",
      "Iteration 15901, loss = 0.07847455\n",
      "Iteration 15902, loss = 0.07846556\n",
      "Iteration 15903, loss = 0.07845409\n",
      "Iteration 15904, loss = 0.07845216\n",
      "Iteration 15905, loss = 0.07844346\n",
      "Iteration 15906, loss = 0.07843373\n",
      "Iteration 15907, loss = 0.07842491\n",
      "Iteration 15908, loss = 0.07841837\n",
      "Iteration 15909, loss = 0.07841176\n",
      "Iteration 15910, loss = 0.07840354\n",
      "Iteration 15911, loss = 0.07839664\n",
      "Iteration 15912, loss = 0.07839021\n",
      "Iteration 15913, loss = 0.07838035\n",
      "Iteration 15914, loss = 0.07837282\n",
      "Iteration 15915, loss = 0.07836486\n",
      "Iteration 15916, loss = 0.07835777\n",
      "Iteration 15917, loss = 0.07835508\n",
      "Iteration 15918, loss = 0.07834274\n",
      "Iteration 15919, loss = 0.07833668\n",
      "Iteration 15920, loss = 0.07832936\n",
      "Iteration 15921, loss = 0.07832122\n",
      "Iteration 15922, loss = 0.07831810\n",
      "Iteration 15923, loss = 0.07830661\n",
      "Iteration 15924, loss = 0.07830441\n",
      "Iteration 15925, loss = 0.07829722\n",
      "Iteration 15926, loss = 0.07829077\n",
      "Iteration 15927, loss = 0.07828669\n",
      "Iteration 15928, loss = 0.07827850\n",
      "Iteration 15929, loss = 0.07826930\n",
      "Iteration 15930, loss = 0.07826227\n",
      "Iteration 15931, loss = 0.07825540\n",
      "Iteration 15932, loss = 0.07824462\n",
      "Iteration 15933, loss = 0.07823812\n",
      "Iteration 15934, loss = 0.07823018\n",
      "Iteration 15935, loss = 0.07822725\n",
      "Iteration 15936, loss = 0.07821907\n",
      "Iteration 15937, loss = 0.07821176\n",
      "Iteration 15938, loss = 0.07820485\n",
      "Iteration 15939, loss = 0.07819609\n",
      "Iteration 15940, loss = 0.07818890\n",
      "Iteration 15941, loss = 0.07818195\n",
      "Iteration 15942, loss = 0.07817745\n",
      "Iteration 15943, loss = 0.07817121\n",
      "Iteration 15944, loss = 0.07816406\n",
      "Iteration 15945, loss = 0.07815939\n",
      "Iteration 15946, loss = 0.07815005\n",
      "Iteration 15947, loss = 0.07813975\n",
      "Iteration 15948, loss = 0.07812868\n",
      "Iteration 15949, loss = 0.07812735\n",
      "Iteration 15950, loss = 0.07811363\n",
      "Iteration 15951, loss = 0.07811117\n",
      "Iteration 15952, loss = 0.07810440\n",
      "Iteration 15953, loss = 0.07809705\n",
      "Iteration 15954, loss = 0.07809041\n",
      "Iteration 15955, loss = 0.07808392\n",
      "Iteration 15956, loss = 0.07807470\n",
      "Iteration 15957, loss = 0.07806418\n",
      "Iteration 15958, loss = 0.07806197\n",
      "Iteration 15959, loss = 0.07805582\n",
      "Iteration 15960, loss = 0.07804268\n",
      "Iteration 15961, loss = 0.07803802\n",
      "Iteration 15962, loss = 0.07803027\n",
      "Iteration 15963, loss = 0.07802203\n",
      "Iteration 15964, loss = 0.07801351\n",
      "Iteration 15965, loss = 0.07800793\n",
      "Iteration 15966, loss = 0.07800335\n",
      "Iteration 15967, loss = 0.07799305\n",
      "Iteration 15968, loss = 0.07798860\n",
      "Iteration 15969, loss = 0.07798394\n",
      "Iteration 15970, loss = 0.07797691\n",
      "Iteration 15971, loss = 0.07796727\n",
      "Iteration 15972, loss = 0.07795829\n",
      "Iteration 15973, loss = 0.07795189\n",
      "Iteration 15974, loss = 0.07794464\n",
      "Iteration 15975, loss = 0.07793608\n",
      "Iteration 15976, loss = 0.07792680\n",
      "Iteration 15977, loss = 0.07791979\n",
      "Iteration 15978, loss = 0.07791533\n",
      "Iteration 15979, loss = 0.07790650\n",
      "Iteration 15980, loss = 0.07789913\n",
      "Iteration 15981, loss = 0.07789391\n",
      "Iteration 15982, loss = 0.07788191\n",
      "Iteration 15983, loss = 0.07787388\n",
      "Iteration 15984, loss = 0.07787485\n",
      "Iteration 15985, loss = 0.07786669\n",
      "Iteration 15986, loss = 0.07785212\n",
      "Iteration 15987, loss = 0.07785089\n",
      "Iteration 15988, loss = 0.07784602\n",
      "Iteration 15989, loss = 0.07783538\n",
      "Iteration 15990, loss = 0.07782630\n",
      "Iteration 15991, loss = 0.07781729\n",
      "Iteration 15992, loss = 0.07781415\n",
      "Iteration 15993, loss = 0.07780541\n",
      "Iteration 15994, loss = 0.07779819\n",
      "Iteration 15995, loss = 0.07779113\n",
      "Iteration 15996, loss = 0.07778411\n",
      "Iteration 15997, loss = 0.07777923\n",
      "Iteration 15998, loss = 0.07777379\n",
      "Iteration 15999, loss = 0.07776473\n",
      "Iteration 16000, loss = 0.07775245\n",
      "Iteration 16001, loss = 0.07774286\n",
      "Iteration 16002, loss = 0.07773872\n",
      "Iteration 16003, loss = 0.07773647\n",
      "Iteration 16004, loss = 0.07772833\n",
      "Iteration 16005, loss = 0.07772328\n",
      "Iteration 16006, loss = 0.07771805\n",
      "Iteration 16007, loss = 0.07771142\n",
      "Iteration 16008, loss = 0.07769765\n",
      "Iteration 16009, loss = 0.07769059\n",
      "Iteration 16010, loss = 0.07768736\n",
      "Iteration 16011, loss = 0.07767975\n",
      "Iteration 16012, loss = 0.07767338\n",
      "Iteration 16013, loss = 0.07767316\n",
      "Iteration 16014, loss = 0.07766026\n",
      "Iteration 16015, loss = 0.07765062\n",
      "Iteration 16016, loss = 0.07764660\n",
      "Iteration 16017, loss = 0.07764158\n",
      "Iteration 16018, loss = 0.07763229\n",
      "Iteration 16019, loss = 0.07762466\n",
      "Iteration 16020, loss = 0.07761636\n",
      "Iteration 16021, loss = 0.07761128\n",
      "Iteration 16022, loss = 0.07759984\n",
      "Iteration 16023, loss = 0.07759687\n",
      "Iteration 16024, loss = 0.07758861\n",
      "Iteration 16025, loss = 0.07757978\n",
      "Iteration 16026, loss = 0.07757589\n",
      "Iteration 16027, loss = 0.07756617\n",
      "Iteration 16028, loss = 0.07756062\n",
      "Iteration 16029, loss = 0.07755295\n",
      "Iteration 16030, loss = 0.07754735\n",
      "Iteration 16031, loss = 0.07754404\n",
      "Iteration 16032, loss = 0.07753509\n",
      "Iteration 16033, loss = 0.07752721\n",
      "Iteration 16034, loss = 0.07751955\n",
      "Iteration 16035, loss = 0.07751347\n",
      "Iteration 16036, loss = 0.07750427\n",
      "Iteration 16037, loss = 0.07749761\n",
      "Iteration 16038, loss = 0.07748976\n",
      "Iteration 16039, loss = 0.07748192\n",
      "Iteration 16040, loss = 0.07748051\n",
      "Iteration 16041, loss = 0.07747409\n",
      "Iteration 16042, loss = 0.07746325\n",
      "Iteration 16043, loss = 0.07745372\n",
      "Iteration 16044, loss = 0.07744708\n",
      "Iteration 16045, loss = 0.07743778\n",
      "Iteration 16046, loss = 0.07743114\n",
      "Iteration 16047, loss = 0.07742717\n",
      "Iteration 16048, loss = 0.07742255\n",
      "Iteration 16049, loss = 0.07741193\n",
      "Iteration 16050, loss = 0.07740834\n",
      "Iteration 16051, loss = 0.07739938\n",
      "Iteration 16052, loss = 0.07738988\n",
      "Iteration 16053, loss = 0.07738929\n",
      "Iteration 16054, loss = 0.07738079\n",
      "Iteration 16055, loss = 0.07736919\n",
      "Iteration 16056, loss = 0.07736517\n",
      "Iteration 16057, loss = 0.07735697\n",
      "Iteration 16058, loss = 0.07734817\n",
      "Iteration 16059, loss = 0.07734378\n",
      "Iteration 16060, loss = 0.07733357\n",
      "Iteration 16061, loss = 0.07732663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16062, loss = 0.07732545\n",
      "Iteration 16063, loss = 0.07731620\n",
      "Iteration 16064, loss = 0.07730534\n",
      "Iteration 16065, loss = 0.07729898\n",
      "Iteration 16066, loss = 0.07729204\n",
      "Iteration 16067, loss = 0.07728537\n",
      "Iteration 16068, loss = 0.07728430\n",
      "Iteration 16069, loss = 0.07727420\n",
      "Iteration 16070, loss = 0.07726769\n",
      "Iteration 16071, loss = 0.07725612\n",
      "Iteration 16072, loss = 0.07724900\n",
      "Iteration 16073, loss = 0.07724192\n",
      "Iteration 16074, loss = 0.07724070\n",
      "Iteration 16075, loss = 0.07723747\n",
      "Iteration 16076, loss = 0.07722615\n",
      "Iteration 16077, loss = 0.07721812\n",
      "Iteration 16078, loss = 0.07721095\n",
      "Iteration 16079, loss = 0.07720281\n",
      "Iteration 16080, loss = 0.07720135\n",
      "Iteration 16081, loss = 0.07719045\n",
      "Iteration 16082, loss = 0.07717931\n",
      "Iteration 16083, loss = 0.07717622\n",
      "Iteration 16084, loss = 0.07717048\n",
      "Iteration 16085, loss = 0.07716241\n",
      "Iteration 16086, loss = 0.07715844\n",
      "Iteration 16087, loss = 0.07715341\n",
      "Iteration 16088, loss = 0.07714624\n",
      "Iteration 16089, loss = 0.07713921\n",
      "Iteration 16090, loss = 0.07713032\n",
      "Iteration 16091, loss = 0.07711700\n",
      "Iteration 16092, loss = 0.07710679\n",
      "Iteration 16093, loss = 0.07710131\n",
      "Iteration 16094, loss = 0.07709483\n",
      "Iteration 16095, loss = 0.07708623\n",
      "Iteration 16096, loss = 0.07708058\n",
      "Iteration 16097, loss = 0.07707747\n",
      "Iteration 16098, loss = 0.07707234\n",
      "Iteration 16099, loss = 0.07706524\n",
      "Iteration 16100, loss = 0.07705885\n",
      "Iteration 16101, loss = 0.07704377\n",
      "Iteration 16102, loss = 0.07704033\n",
      "Iteration 16103, loss = 0.07703987\n",
      "Iteration 16104, loss = 0.07702749\n",
      "Iteration 16105, loss = 0.07702151\n",
      "Iteration 16106, loss = 0.07701577\n",
      "Iteration 16107, loss = 0.07701335\n",
      "Iteration 16108, loss = 0.07700561\n",
      "Iteration 16109, loss = 0.07699602\n",
      "Iteration 16110, loss = 0.07699145\n",
      "Iteration 16111, loss = 0.07698237\n",
      "Iteration 16112, loss = 0.07697196\n",
      "Iteration 16113, loss = 0.07697264\n",
      "Iteration 16114, loss = 0.07696436\n",
      "Iteration 16115, loss = 0.07695388\n",
      "Iteration 16116, loss = 0.07695348\n",
      "Iteration 16117, loss = 0.07694725\n",
      "Iteration 16118, loss = 0.07694047\n",
      "Iteration 16119, loss = 0.07693079\n",
      "Iteration 16120, loss = 0.07691895\n",
      "Iteration 16121, loss = 0.07691220\n",
      "Iteration 16122, loss = 0.07690554\n",
      "Iteration 16123, loss = 0.07689614\n",
      "Iteration 16124, loss = 0.07689815\n",
      "Iteration 16125, loss = 0.07689029\n",
      "Iteration 16126, loss = 0.07687767\n",
      "Iteration 16127, loss = 0.07686790\n",
      "Iteration 16128, loss = 0.07686454\n",
      "Iteration 16129, loss = 0.07685730\n",
      "Iteration 16130, loss = 0.07684994\n",
      "Iteration 16131, loss = 0.07684432\n",
      "Iteration 16132, loss = 0.07683673\n",
      "Iteration 16133, loss = 0.07682613\n",
      "Iteration 16134, loss = 0.07681998\n",
      "Iteration 16135, loss = 0.07680668\n",
      "Iteration 16136, loss = 0.07681072\n",
      "Iteration 16137, loss = 0.07680593\n",
      "Iteration 16138, loss = 0.07679015\n",
      "Iteration 16139, loss = 0.07678595\n",
      "Iteration 16140, loss = 0.07678081\n",
      "Iteration 16141, loss = 0.07677601\n",
      "Iteration 16142, loss = 0.07676520\n",
      "Iteration 16143, loss = 0.07675852\n",
      "Iteration 16144, loss = 0.07675044\n",
      "Iteration 16145, loss = 0.07674605\n",
      "Iteration 16146, loss = 0.07673728\n",
      "Iteration 16147, loss = 0.07673278\n",
      "Iteration 16148, loss = 0.07672613\n",
      "Iteration 16149, loss = 0.07672115\n",
      "Iteration 16150, loss = 0.07671360\n",
      "Iteration 16151, loss = 0.07670450\n",
      "Iteration 16152, loss = 0.07670105\n",
      "Iteration 16153, loss = 0.07668964\n",
      "Iteration 16154, loss = 0.07668234\n",
      "Iteration 16155, loss = 0.07667701\n",
      "Iteration 16156, loss = 0.07666887\n",
      "Iteration 16157, loss = 0.07666368\n",
      "Iteration 16158, loss = 0.07665749\n",
      "Iteration 16159, loss = 0.07665170\n",
      "Iteration 16160, loss = 0.07664032\n",
      "Iteration 16161, loss = 0.07663039\n",
      "Iteration 16162, loss = 0.07663496\n",
      "Iteration 16163, loss = 0.07662920\n",
      "Iteration 16164, loss = 0.07661370\n",
      "Iteration 16165, loss = 0.07661032\n",
      "Iteration 16166, loss = 0.07660431\n",
      "Iteration 16167, loss = 0.07659627\n",
      "Iteration 16168, loss = 0.07659220\n",
      "Iteration 16169, loss = 0.07658819\n",
      "Iteration 16170, loss = 0.07657568\n",
      "Iteration 16171, loss = 0.07656879\n",
      "Iteration 16172, loss = 0.07656455\n",
      "Iteration 16173, loss = 0.07655504\n",
      "Iteration 16174, loss = 0.07654433\n",
      "Iteration 16175, loss = 0.07653725\n",
      "Iteration 16176, loss = 0.07653129\n",
      "Iteration 16177, loss = 0.07652168\n",
      "Iteration 16178, loss = 0.07651706\n",
      "Iteration 16179, loss = 0.07650814\n",
      "Iteration 16180, loss = 0.07650285\n",
      "Iteration 16181, loss = 0.07649687\n",
      "Iteration 16182, loss = 0.07649203\n",
      "Iteration 16183, loss = 0.07648294\n",
      "Iteration 16184, loss = 0.07647673\n",
      "Iteration 16185, loss = 0.07646819\n",
      "Iteration 16186, loss = 0.07646394\n",
      "Iteration 16187, loss = 0.07645798\n",
      "Iteration 16188, loss = 0.07644953\n",
      "Iteration 16189, loss = 0.07644322\n",
      "Iteration 16190, loss = 0.07643591\n",
      "Iteration 16191, loss = 0.07643028\n",
      "Iteration 16192, loss = 0.07643106\n",
      "Iteration 16193, loss = 0.07642253\n",
      "Iteration 16194, loss = 0.07640559\n",
      "Iteration 16195, loss = 0.07640080\n",
      "Iteration 16196, loss = 0.07640220\n",
      "Iteration 16197, loss = 0.07640072\n",
      "Iteration 16198, loss = 0.07639369\n",
      "Iteration 16199, loss = 0.07638148\n",
      "Iteration 16200, loss = 0.07637432\n",
      "Iteration 16201, loss = 0.07637019\n",
      "Iteration 16202, loss = 0.07636035\n",
      "Iteration 16203, loss = 0.07635643\n",
      "Iteration 16204, loss = 0.07634713\n",
      "Iteration 16205, loss = 0.07633672\n",
      "Iteration 16206, loss = 0.07633768\n",
      "Iteration 16207, loss = 0.07633724\n",
      "Iteration 16208, loss = 0.07632213\n",
      "Iteration 16209, loss = 0.07630944\n",
      "Iteration 16210, loss = 0.07630903\n",
      "Iteration 16211, loss = 0.07630491\n",
      "Iteration 16212, loss = 0.07629564\n",
      "Iteration 16213, loss = 0.07628841\n",
      "Iteration 16214, loss = 0.07627935\n",
      "Iteration 16215, loss = 0.07627197\n",
      "Iteration 16216, loss = 0.07626487\n",
      "Iteration 16217, loss = 0.07625726\n",
      "Iteration 16218, loss = 0.07625087\n",
      "Iteration 16219, loss = 0.07624649\n",
      "Iteration 16220, loss = 0.07622968\n",
      "Iteration 16221, loss = 0.07622985\n",
      "Iteration 16222, loss = 0.07622550\n",
      "Iteration 16223, loss = 0.07622000\n",
      "Iteration 16224, loss = 0.07621493\n",
      "Iteration 16225, loss = 0.07620935\n",
      "Iteration 16226, loss = 0.07620224\n",
      "Iteration 16227, loss = 0.07619423\n",
      "Iteration 16228, loss = 0.07618273\n",
      "Iteration 16229, loss = 0.07617684\n",
      "Iteration 16230, loss = 0.07616833\n",
      "Iteration 16231, loss = 0.07616038\n",
      "Iteration 16232, loss = 0.07615430\n",
      "Iteration 16233, loss = 0.07614674\n",
      "Iteration 16234, loss = 0.07614094\n",
      "Iteration 16235, loss = 0.07613231\n",
      "Iteration 16236, loss = 0.07612721\n",
      "Iteration 16237, loss = 0.07612273\n",
      "Iteration 16238, loss = 0.07611472\n",
      "Iteration 16239, loss = 0.07610503\n",
      "Iteration 16240, loss = 0.07610196\n",
      "Iteration 16241, loss = 0.07609610\n",
      "Iteration 16242, loss = 0.07608618\n",
      "Iteration 16243, loss = 0.07608501\n",
      "Iteration 16244, loss = 0.07607635\n",
      "Iteration 16245, loss = 0.07606903\n",
      "Iteration 16246, loss = 0.07605865\n",
      "Iteration 16247, loss = 0.07605000\n",
      "Iteration 16248, loss = 0.07604364\n",
      "Iteration 16249, loss = 0.07603857\n",
      "Iteration 16250, loss = 0.07603503\n",
      "Iteration 16251, loss = 0.07602688\n",
      "Iteration 16252, loss = 0.07602174\n",
      "Iteration 16253, loss = 0.07601600\n",
      "Iteration 16254, loss = 0.07600828\n",
      "Iteration 16255, loss = 0.07600214\n",
      "Iteration 16256, loss = 0.07599453\n",
      "Iteration 16257, loss = 0.07598533\n",
      "Iteration 16258, loss = 0.07597961\n",
      "Iteration 16259, loss = 0.07597562\n",
      "Iteration 16260, loss = 0.07596758\n",
      "Iteration 16261, loss = 0.07596089\n",
      "Iteration 16262, loss = 0.07595091\n",
      "Iteration 16263, loss = 0.07594702\n",
      "Iteration 16264, loss = 0.07594212\n",
      "Iteration 16265, loss = 0.07593534\n",
      "Iteration 16266, loss = 0.07592839\n",
      "Iteration 16267, loss = 0.07592398\n",
      "Iteration 16268, loss = 0.07592106\n",
      "Iteration 16269, loss = 0.07590947\n",
      "Iteration 16270, loss = 0.07590361\n",
      "Iteration 16271, loss = 0.07589899\n",
      "Iteration 16272, loss = 0.07588990\n",
      "Iteration 16273, loss = 0.07588538\n",
      "Iteration 16274, loss = 0.07587908\n",
      "Iteration 16275, loss = 0.07587280\n",
      "Iteration 16276, loss = 0.07586032\n",
      "Iteration 16277, loss = 0.07585409\n",
      "Iteration 16278, loss = 0.07584918\n",
      "Iteration 16279, loss = 0.07584268\n",
      "Iteration 16280, loss = 0.07582770\n",
      "Iteration 16281, loss = 0.07582715\n",
      "Iteration 16282, loss = 0.07582053\n",
      "Iteration 16283, loss = 0.07581281\n",
      "Iteration 16284, loss = 0.07580260\n",
      "Iteration 16285, loss = 0.07579854\n",
      "Iteration 16286, loss = 0.07579521\n",
      "Iteration 16287, loss = 0.07578703\n",
      "Iteration 16288, loss = 0.07577932\n",
      "Iteration 16289, loss = 0.07577551\n",
      "Iteration 16290, loss = 0.07576633\n",
      "Iteration 16291, loss = 0.07575930\n",
      "Iteration 16292, loss = 0.07575360\n",
      "Iteration 16293, loss = 0.07574807\n",
      "Iteration 16294, loss = 0.07573845\n",
      "Iteration 16295, loss = 0.07573715\n",
      "Iteration 16296, loss = 0.07573130\n",
      "Iteration 16297, loss = 0.07571909\n",
      "Iteration 16298, loss = 0.07571231\n",
      "Iteration 16299, loss = 0.07570662\n",
      "Iteration 16300, loss = 0.07570067\n",
      "Iteration 16301, loss = 0.07569277\n",
      "Iteration 16302, loss = 0.07568866\n",
      "Iteration 16303, loss = 0.07568368\n",
      "Iteration 16304, loss = 0.07567463\n",
      "Iteration 16305, loss = 0.07566882\n",
      "Iteration 16306, loss = 0.07566622\n",
      "Iteration 16307, loss = 0.07566169\n",
      "Iteration 16308, loss = 0.07565061\n",
      "Iteration 16309, loss = 0.07564522\n",
      "Iteration 16310, loss = 0.07563593\n",
      "Iteration 16311, loss = 0.07562681\n",
      "Iteration 16312, loss = 0.07562157\n",
      "Iteration 16313, loss = 0.07561448\n",
      "Iteration 16314, loss = 0.07560923\n",
      "Iteration 16315, loss = 0.07560548\n",
      "Iteration 16316, loss = 0.07559862\n",
      "Iteration 16317, loss = 0.07559085\n",
      "Iteration 16318, loss = 0.07558655\n",
      "Iteration 16319, loss = 0.07557981\n",
      "Iteration 16320, loss = 0.07556713\n",
      "Iteration 16321, loss = 0.07556722\n",
      "Iteration 16322, loss = 0.07555992\n",
      "Iteration 16323, loss = 0.07554901\n",
      "Iteration 16324, loss = 0.07554691\n",
      "Iteration 16325, loss = 0.07553912\n",
      "Iteration 16326, loss = 0.07553098\n",
      "Iteration 16327, loss = 0.07552346\n",
      "Iteration 16328, loss = 0.07552258\n",
      "Iteration 16329, loss = 0.07552173\n",
      "Iteration 16330, loss = 0.07551298\n",
      "Iteration 16331, loss = 0.07550019\n",
      "Iteration 16332, loss = 0.07550013\n",
      "Iteration 16333, loss = 0.07549665\n",
      "Iteration 16334, loss = 0.07548868\n",
      "Iteration 16335, loss = 0.07548141\n",
      "Iteration 16336, loss = 0.07546915\n",
      "Iteration 16337, loss = 0.07546264\n",
      "Iteration 16338, loss = 0.07545590\n",
      "Iteration 16339, loss = 0.07544622\n",
      "Iteration 16340, loss = 0.07544089\n",
      "Iteration 16341, loss = 0.07543468\n",
      "Iteration 16342, loss = 0.07543031\n",
      "Iteration 16343, loss = 0.07542413\n",
      "Iteration 16344, loss = 0.07541665\n",
      "Iteration 16345, loss = 0.07540717\n",
      "Iteration 16346, loss = 0.07539915\n",
      "Iteration 16347, loss = 0.07539139\n",
      "Iteration 16348, loss = 0.07538833\n",
      "Iteration 16349, loss = 0.07537782\n",
      "Iteration 16350, loss = 0.07537396\n",
      "Iteration 16351, loss = 0.07536806\n",
      "Iteration 16352, loss = 0.07536059\n",
      "Iteration 16353, loss = 0.07535400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16354, loss = 0.07534683\n",
      "Iteration 16355, loss = 0.07533984\n",
      "Iteration 16356, loss = 0.07533356\n",
      "Iteration 16357, loss = 0.07532661\n",
      "Iteration 16358, loss = 0.07532171\n",
      "Iteration 16359, loss = 0.07531526\n",
      "Iteration 16360, loss = 0.07530943\n",
      "Iteration 16361, loss = 0.07530024\n",
      "Iteration 16362, loss = 0.07529405\n",
      "Iteration 16363, loss = 0.07529005\n",
      "Iteration 16364, loss = 0.07528394\n",
      "Iteration 16365, loss = 0.07527515\n",
      "Iteration 16366, loss = 0.07526771\n",
      "Iteration 16367, loss = 0.07526527\n",
      "Iteration 16368, loss = 0.07525348\n",
      "Iteration 16369, loss = 0.07524673\n",
      "Iteration 16370, loss = 0.07524094\n",
      "Iteration 16371, loss = 0.07523595\n",
      "Iteration 16372, loss = 0.07523320\n",
      "Iteration 16373, loss = 0.07522717\n",
      "Iteration 16374, loss = 0.07521704\n",
      "Iteration 16375, loss = 0.07520691\n",
      "Iteration 16376, loss = 0.07520613\n",
      "Iteration 16377, loss = 0.07519763\n",
      "Iteration 16378, loss = 0.07518817\n",
      "Iteration 16379, loss = 0.07518239\n",
      "Iteration 16380, loss = 0.07517860\n",
      "Iteration 16381, loss = 0.07517526\n",
      "Iteration 16382, loss = 0.07516752\n",
      "Iteration 16383, loss = 0.07515952\n",
      "Iteration 16384, loss = 0.07515192\n",
      "Iteration 16385, loss = 0.07514542\n",
      "Iteration 16386, loss = 0.07513839\n",
      "Iteration 16387, loss = 0.07513387\n",
      "Iteration 16388, loss = 0.07512643\n",
      "Iteration 16389, loss = 0.07512320\n",
      "Iteration 16390, loss = 0.07512074\n",
      "Iteration 16391, loss = 0.07511216\n",
      "Iteration 16392, loss = 0.07510169\n",
      "Iteration 16393, loss = 0.07509543\n",
      "Iteration 16394, loss = 0.07508968\n",
      "Iteration 16395, loss = 0.07508625\n",
      "Iteration 16396, loss = 0.07508060\n",
      "Iteration 16397, loss = 0.07507300\n",
      "Iteration 16398, loss = 0.07506478\n",
      "Iteration 16399, loss = 0.07505788\n",
      "Iteration 16400, loss = 0.07505333\n",
      "Iteration 16401, loss = 0.07504538\n",
      "Iteration 16402, loss = 0.07503726\n",
      "Iteration 16403, loss = 0.07503640\n",
      "Iteration 16404, loss = 0.07502679\n",
      "Iteration 16405, loss = 0.07501774\n",
      "Iteration 16406, loss = 0.07500889\n",
      "Iteration 16407, loss = 0.07500341\n",
      "Iteration 16408, loss = 0.07499965\n",
      "Iteration 16409, loss = 0.07499336\n",
      "Iteration 16410, loss = 0.07498607\n",
      "Iteration 16411, loss = 0.07497713\n",
      "Iteration 16412, loss = 0.07496920\n",
      "Iteration 16413, loss = 0.07496650\n",
      "Iteration 16414, loss = 0.07496082\n",
      "Iteration 16415, loss = 0.07495011\n",
      "Iteration 16416, loss = 0.07494326\n",
      "Iteration 16417, loss = 0.07493983\n",
      "Iteration 16418, loss = 0.07493615\n",
      "Iteration 16419, loss = 0.07492891\n",
      "Iteration 16420, loss = 0.07492245\n",
      "Iteration 16421, loss = 0.07491545\n",
      "Iteration 16422, loss = 0.07490908\n",
      "Iteration 16423, loss = 0.07490238\n",
      "Iteration 16424, loss = 0.07489426\n",
      "Iteration 16425, loss = 0.07488654\n",
      "Iteration 16426, loss = 0.07488118\n",
      "Iteration 16427, loss = 0.07487733\n",
      "Iteration 16428, loss = 0.07487056\n",
      "Iteration 16429, loss = 0.07486596\n",
      "Iteration 16430, loss = 0.07485711\n",
      "Iteration 16431, loss = 0.07485065\n",
      "Iteration 16432, loss = 0.07484558\n",
      "Iteration 16433, loss = 0.07484011\n",
      "Iteration 16434, loss = 0.07483796\n",
      "Iteration 16435, loss = 0.07483242\n",
      "Iteration 16436, loss = 0.07482353\n",
      "Iteration 16437, loss = 0.07481651\n",
      "Iteration 16438, loss = 0.07481120\n",
      "Iteration 16439, loss = 0.07480223\n",
      "Iteration 16440, loss = 0.07479847\n",
      "Iteration 16441, loss = 0.07478580\n",
      "Iteration 16442, loss = 0.07478640\n",
      "Iteration 16443, loss = 0.07478506\n",
      "Iteration 16444, loss = 0.07477008\n",
      "Iteration 16445, loss = 0.07476575\n",
      "Iteration 16446, loss = 0.07476548\n",
      "Iteration 16447, loss = 0.07476195\n",
      "Iteration 16448, loss = 0.07475300\n",
      "Iteration 16449, loss = 0.07475006\n",
      "Iteration 16450, loss = 0.07473980\n",
      "Iteration 16451, loss = 0.07473039\n",
      "Iteration 16452, loss = 0.07472303\n",
      "Iteration 16453, loss = 0.07471584\n",
      "Iteration 16454, loss = 0.07470693\n",
      "Iteration 16455, loss = 0.07470108\n",
      "Iteration 16456, loss = 0.07469817\n",
      "Iteration 16457, loss = 0.07468591\n",
      "Iteration 16458, loss = 0.07468042\n",
      "Iteration 16459, loss = 0.07467454\n",
      "Iteration 16460, loss = 0.07466880\n",
      "Iteration 16461, loss = 0.07465915\n",
      "Iteration 16462, loss = 0.07466029\n",
      "Iteration 16463, loss = 0.07465043\n",
      "Iteration 16464, loss = 0.07464608\n",
      "Iteration 16465, loss = 0.07464303\n",
      "Iteration 16466, loss = 0.07463531\n",
      "Iteration 16467, loss = 0.07463122\n",
      "Iteration 16468, loss = 0.07462500\n",
      "Iteration 16469, loss = 0.07461778\n",
      "Iteration 16470, loss = 0.07460833\n",
      "Iteration 16471, loss = 0.07459631\n",
      "Iteration 16472, loss = 0.07459274\n",
      "Iteration 16473, loss = 0.07459375\n",
      "Iteration 16474, loss = 0.07458495\n",
      "Iteration 16475, loss = 0.07457875\n",
      "Iteration 16476, loss = 0.07457197\n",
      "Iteration 16477, loss = 0.07456411\n",
      "Iteration 16478, loss = 0.07455822\n",
      "Iteration 16479, loss = 0.07455182\n",
      "Iteration 16480, loss = 0.07454034\n",
      "Iteration 16481, loss = 0.07453621\n",
      "Iteration 16482, loss = 0.07453056\n",
      "Iteration 16483, loss = 0.07452330\n",
      "Iteration 16484, loss = 0.07452030\n",
      "Iteration 16485, loss = 0.07451662\n",
      "Iteration 16486, loss = 0.07450816\n",
      "Iteration 16487, loss = 0.07450325\n",
      "Iteration 16488, loss = 0.07449684\n",
      "Iteration 16489, loss = 0.07448843\n",
      "Iteration 16490, loss = 0.07448258\n",
      "Iteration 16491, loss = 0.07447528\n",
      "Iteration 16492, loss = 0.07446723\n",
      "Iteration 16493, loss = 0.07446123\n",
      "Iteration 16494, loss = 0.07445612\n",
      "Iteration 16495, loss = 0.07444744\n",
      "Iteration 16496, loss = 0.07444657\n",
      "Iteration 16497, loss = 0.07443803\n",
      "Iteration 16498, loss = 0.07442942\n",
      "Iteration 16499, loss = 0.07442795\n",
      "Iteration 16500, loss = 0.07442629\n",
      "Iteration 16501, loss = 0.07441682\n",
      "Iteration 16502, loss = 0.07441000\n",
      "Iteration 16503, loss = 0.07440441\n",
      "Iteration 16504, loss = 0.07439471\n",
      "Iteration 16505, loss = 0.07439122\n",
      "Iteration 16506, loss = 0.07438578\n",
      "Iteration 16507, loss = 0.07437473\n",
      "Iteration 16508, loss = 0.07436813\n",
      "Iteration 16509, loss = 0.07436412\n",
      "Iteration 16510, loss = 0.07435599\n",
      "Iteration 16511, loss = 0.07434746\n",
      "Iteration 16512, loss = 0.07434195\n",
      "Iteration 16513, loss = 0.07433655\n",
      "Iteration 16514, loss = 0.07433796\n",
      "Iteration 16515, loss = 0.07433111\n",
      "Iteration 16516, loss = 0.07431928\n",
      "Iteration 16517, loss = 0.07431434\n",
      "Iteration 16518, loss = 0.07430564\n",
      "Iteration 16519, loss = 0.07430234\n",
      "Iteration 16520, loss = 0.07429696\n",
      "Iteration 16521, loss = 0.07428653\n",
      "Iteration 16522, loss = 0.07428155\n",
      "Iteration 16523, loss = 0.07427932\n",
      "Iteration 16524, loss = 0.07427239\n",
      "Iteration 16525, loss = 0.07425920\n",
      "Iteration 16526, loss = 0.07425943\n",
      "Iteration 16527, loss = 0.07425191\n",
      "Iteration 16528, loss = 0.07424187\n",
      "Iteration 16529, loss = 0.07423806\n",
      "Iteration 16530, loss = 0.07423168\n",
      "Iteration 16531, loss = 0.07422869\n",
      "Iteration 16532, loss = 0.07422132\n",
      "Iteration 16533, loss = 0.07421608\n",
      "Iteration 16534, loss = 0.07421034\n",
      "Iteration 16535, loss = 0.07420337\n",
      "Iteration 16536, loss = 0.07419737\n",
      "Iteration 16537, loss = 0.07418818\n",
      "Iteration 16538, loss = 0.07418031\n",
      "Iteration 16539, loss = 0.07417393\n",
      "Iteration 16540, loss = 0.07417208\n",
      "Iteration 16541, loss = 0.07416668\n",
      "Iteration 16542, loss = 0.07415887\n",
      "Iteration 16543, loss = 0.07415306\n",
      "Iteration 16544, loss = 0.07414651\n",
      "Iteration 16545, loss = 0.07413916\n",
      "Iteration 16546, loss = 0.07413121\n",
      "Iteration 16547, loss = 0.07412436\n",
      "Iteration 16548, loss = 0.07411850\n",
      "Iteration 16549, loss = 0.07411096\n",
      "Iteration 16550, loss = 0.07410487\n",
      "Iteration 16551, loss = 0.07410302\n",
      "Iteration 16552, loss = 0.07409887\n",
      "Iteration 16553, loss = 0.07409200\n",
      "Iteration 16554, loss = 0.07408397\n",
      "Iteration 16555, loss = 0.07407594\n",
      "Iteration 16556, loss = 0.07406820\n",
      "Iteration 16557, loss = 0.07406472\n",
      "Iteration 16558, loss = 0.07405632\n",
      "Iteration 16559, loss = 0.07405036\n",
      "Iteration 16560, loss = 0.07404700\n",
      "Iteration 16561, loss = 0.07404093\n",
      "Iteration 16562, loss = 0.07403389\n",
      "Iteration 16563, loss = 0.07402990\n",
      "Iteration 16564, loss = 0.07402113\n",
      "Iteration 16565, loss = 0.07401291\n",
      "Iteration 16566, loss = 0.07400551\n",
      "Iteration 16567, loss = 0.07400337\n",
      "Iteration 16568, loss = 0.07399959\n",
      "Iteration 16569, loss = 0.07399230\n",
      "Iteration 16570, loss = 0.07398459\n",
      "Iteration 16571, loss = 0.07397976\n",
      "Iteration 16572, loss = 0.07397477\n",
      "Iteration 16573, loss = 0.07396689\n",
      "Iteration 16574, loss = 0.07396280\n",
      "Iteration 16575, loss = 0.07395398\n",
      "Iteration 16576, loss = 0.07394859\n",
      "Iteration 16577, loss = 0.07394488\n",
      "Iteration 16578, loss = 0.07393917\n",
      "Iteration 16579, loss = 0.07393086\n",
      "Iteration 16580, loss = 0.07392531\n",
      "Iteration 16581, loss = 0.07391914\n",
      "Iteration 16582, loss = 0.07391126\n",
      "Iteration 16583, loss = 0.07391156\n",
      "Iteration 16584, loss = 0.07390411\n",
      "Iteration 16585, loss = 0.07389234\n",
      "Iteration 16586, loss = 0.07388648\n",
      "Iteration 16587, loss = 0.07388139\n",
      "Iteration 16588, loss = 0.07387511\n",
      "Iteration 16589, loss = 0.07386747\n",
      "Iteration 16590, loss = 0.07385983\n",
      "Iteration 16591, loss = 0.07385845\n",
      "Iteration 16592, loss = 0.07385250\n",
      "Iteration 16593, loss = 0.07384360\n",
      "Iteration 16594, loss = 0.07383879\n",
      "Iteration 16595, loss = 0.07383101\n",
      "Iteration 16596, loss = 0.07382809\n",
      "Iteration 16597, loss = 0.07382169\n",
      "Iteration 16598, loss = 0.07381279\n",
      "Iteration 16599, loss = 0.07380730\n",
      "Iteration 16600, loss = 0.07380824\n",
      "Iteration 16601, loss = 0.07380288\n",
      "Iteration 16602, loss = 0.07379139\n",
      "Iteration 16603, loss = 0.07378551\n",
      "Iteration 16604, loss = 0.07377881\n",
      "Iteration 16605, loss = 0.07377288\n",
      "Iteration 16606, loss = 0.07376409\n",
      "Iteration 16607, loss = 0.07375812\n",
      "Iteration 16608, loss = 0.07375611\n",
      "Iteration 16609, loss = 0.07374922\n",
      "Iteration 16610, loss = 0.07374322\n",
      "Iteration 16611, loss = 0.07373873\n",
      "Iteration 16612, loss = 0.07373245\n",
      "Iteration 16613, loss = 0.07372753\n",
      "Iteration 16614, loss = 0.07371843\n",
      "Iteration 16615, loss = 0.07371066\n",
      "Iteration 16616, loss = 0.07370328\n",
      "Iteration 16617, loss = 0.07370086\n",
      "Iteration 16618, loss = 0.07369576\n",
      "Iteration 16619, loss = 0.07368492\n",
      "Iteration 16620, loss = 0.07368194\n",
      "Iteration 16621, loss = 0.07367800\n",
      "Iteration 16622, loss = 0.07367287\n",
      "Iteration 16623, loss = 0.07366351\n",
      "Iteration 16624, loss = 0.07365655\n",
      "Iteration 16625, loss = 0.07364870\n",
      "Iteration 16626, loss = 0.07363906\n",
      "Iteration 16627, loss = 0.07363652\n",
      "Iteration 16628, loss = 0.07363098\n",
      "Iteration 16629, loss = 0.07362548\n",
      "Iteration 16630, loss = 0.07362242\n",
      "Iteration 16631, loss = 0.07361786\n",
      "Iteration 16632, loss = 0.07360961\n",
      "Iteration 16633, loss = 0.07359975\n",
      "Iteration 16634, loss = 0.07359695\n",
      "Iteration 16635, loss = 0.07359326\n",
      "Iteration 16636, loss = 0.07358353\n",
      "Iteration 16637, loss = 0.07357975\n",
      "Iteration 16638, loss = 0.07357268\n",
      "Iteration 16639, loss = 0.07356671\n",
      "Iteration 16640, loss = 0.07355983\n",
      "Iteration 16641, loss = 0.07355318\n",
      "Iteration 16642, loss = 0.07354461\n",
      "Iteration 16643, loss = 0.07353961\n",
      "Iteration 16644, loss = 0.07353353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16645, loss = 0.07352542\n",
      "Iteration 16646, loss = 0.07351684\n",
      "Iteration 16647, loss = 0.07351367\n",
      "Iteration 16648, loss = 0.07351217\n",
      "Iteration 16649, loss = 0.07350286\n",
      "Iteration 16650, loss = 0.07349765\n",
      "Iteration 16651, loss = 0.07349147\n",
      "Iteration 16652, loss = 0.07348760\n",
      "Iteration 16653, loss = 0.07347981\n",
      "Iteration 16654, loss = 0.07347417\n",
      "Iteration 16655, loss = 0.07346697\n",
      "Iteration 16656, loss = 0.07346102\n",
      "Iteration 16657, loss = 0.07345193\n",
      "Iteration 16658, loss = 0.07344568\n",
      "Iteration 16659, loss = 0.07344388\n",
      "Iteration 16660, loss = 0.07343739\n",
      "Iteration 16661, loss = 0.07343099\n",
      "Iteration 16662, loss = 0.07342741\n",
      "Iteration 16663, loss = 0.07342357\n",
      "Iteration 16664, loss = 0.07341308\n",
      "Iteration 16665, loss = 0.07340777\n",
      "Iteration 16666, loss = 0.07340068\n",
      "Iteration 16667, loss = 0.07339794\n",
      "Iteration 16668, loss = 0.07339214\n",
      "Iteration 16669, loss = 0.07338492\n",
      "Iteration 16670, loss = 0.07337985\n",
      "Iteration 16671, loss = 0.07337260\n",
      "Iteration 16672, loss = 0.07336402\n",
      "Iteration 16673, loss = 0.07335749\n",
      "Iteration 16674, loss = 0.07335691\n",
      "Iteration 16675, loss = 0.07335349\n",
      "Iteration 16676, loss = 0.07334332\n",
      "Iteration 16677, loss = 0.07333816\n",
      "Iteration 16678, loss = 0.07333135\n",
      "Iteration 16679, loss = 0.07332493\n",
      "Iteration 16680, loss = 0.07331771\n",
      "Iteration 16681, loss = 0.07330995\n",
      "Iteration 16682, loss = 0.07331435\n",
      "Iteration 16683, loss = 0.07330792\n",
      "Iteration 16684, loss = 0.07329721\n",
      "Iteration 16685, loss = 0.07329340\n",
      "Iteration 16686, loss = 0.07329016\n",
      "Iteration 16687, loss = 0.07328388\n",
      "Iteration 16688, loss = 0.07328338\n",
      "Iteration 16689, loss = 0.07327380\n",
      "Iteration 16690, loss = 0.07326169\n",
      "Iteration 16691, loss = 0.07325896\n",
      "Iteration 16692, loss = 0.07325034\n",
      "Iteration 16693, loss = 0.07324325\n",
      "Iteration 16694, loss = 0.07323446\n",
      "Iteration 16695, loss = 0.07323502\n",
      "Iteration 16696, loss = 0.07322929\n",
      "Iteration 16697, loss = 0.07321579\n",
      "Iteration 16698, loss = 0.07320788\n",
      "Iteration 16699, loss = 0.07320473\n",
      "Iteration 16700, loss = 0.07319617\n",
      "Iteration 16701, loss = 0.07319017\n",
      "Iteration 16702, loss = 0.07318905\n",
      "Iteration 16703, loss = 0.07318058\n",
      "Iteration 16704, loss = 0.07317229\n",
      "Iteration 16705, loss = 0.07316931\n",
      "Iteration 16706, loss = 0.07316338\n",
      "Iteration 16707, loss = 0.07315942\n",
      "Iteration 16708, loss = 0.07315140\n",
      "Iteration 16709, loss = 0.07314196\n",
      "Iteration 16710, loss = 0.07314015\n",
      "Iteration 16711, loss = 0.07313099\n",
      "Iteration 16712, loss = 0.07312681\n",
      "Iteration 16713, loss = 0.07312032\n",
      "Iteration 16714, loss = 0.07311693\n",
      "Iteration 16715, loss = 0.07311136\n",
      "Iteration 16716, loss = 0.07310321\n",
      "Iteration 16717, loss = 0.07309791\n",
      "Iteration 16718, loss = 0.07309277\n",
      "Iteration 16719, loss = 0.07309140\n",
      "Iteration 16720, loss = 0.07308025\n",
      "Iteration 16721, loss = 0.07307390\n",
      "Iteration 16722, loss = 0.07306977\n",
      "Iteration 16723, loss = 0.07306762\n",
      "Iteration 16724, loss = 0.07306626\n",
      "Iteration 16725, loss = 0.07305838\n",
      "Iteration 16726, loss = 0.07304628\n",
      "Iteration 16727, loss = 0.07303616\n",
      "Iteration 16728, loss = 0.07303015\n",
      "Iteration 16729, loss = 0.07302865\n",
      "Iteration 16730, loss = 0.07302508\n",
      "Iteration 16731, loss = 0.07302045\n",
      "Iteration 16732, loss = 0.07300903\n",
      "Iteration 16733, loss = 0.07300472\n",
      "Iteration 16734, loss = 0.07300034\n",
      "Iteration 16735, loss = 0.07299357\n",
      "Iteration 16736, loss = 0.07298393\n",
      "Iteration 16737, loss = 0.07298274\n",
      "Iteration 16738, loss = 0.07297418\n",
      "Iteration 16739, loss = 0.07296200\n",
      "Iteration 16740, loss = 0.07295881\n",
      "Iteration 16741, loss = 0.07295325\n",
      "Iteration 16742, loss = 0.07294597\n",
      "Iteration 16743, loss = 0.07293930\n",
      "Iteration 16744, loss = 0.07293272\n",
      "Iteration 16745, loss = 0.07292741\n",
      "Iteration 16746, loss = 0.07292236\n",
      "Iteration 16747, loss = 0.07291656\n",
      "Iteration 16748, loss = 0.07291187\n",
      "Iteration 16749, loss = 0.07290432\n",
      "Iteration 16750, loss = 0.07289900\n",
      "Iteration 16751, loss = 0.07289210\n",
      "Iteration 16752, loss = 0.07288671\n",
      "Iteration 16753, loss = 0.07288146\n",
      "Iteration 16754, loss = 0.07287674\n",
      "Iteration 16755, loss = 0.07287097\n",
      "Iteration 16756, loss = 0.07286648\n",
      "Iteration 16757, loss = 0.07286316\n",
      "Iteration 16758, loss = 0.07285243\n",
      "Iteration 16759, loss = 0.07284789\n",
      "Iteration 16760, loss = 0.07284051\n",
      "Iteration 16761, loss = 0.07283625\n",
      "Iteration 16762, loss = 0.07283022\n",
      "Iteration 16763, loss = 0.07282538\n",
      "Iteration 16764, loss = 0.07281907\n",
      "Iteration 16765, loss = 0.07281103\n",
      "Iteration 16766, loss = 0.07280335\n",
      "Iteration 16767, loss = 0.07280077\n",
      "Iteration 16768, loss = 0.07279643\n",
      "Iteration 16769, loss = 0.07278382\n",
      "Iteration 16770, loss = 0.07278036\n",
      "Iteration 16771, loss = 0.07277468\n",
      "Iteration 16772, loss = 0.07276624\n",
      "Iteration 16773, loss = 0.07276371\n",
      "Iteration 16774, loss = 0.07275820\n",
      "Iteration 16775, loss = 0.07275080\n",
      "Iteration 16776, loss = 0.07274497\n",
      "Iteration 16777, loss = 0.07274093\n",
      "Iteration 16778, loss = 0.07273501\n",
      "Iteration 16779, loss = 0.07272728\n",
      "Iteration 16780, loss = 0.07272394\n",
      "Iteration 16781, loss = 0.07271563\n",
      "Iteration 16782, loss = 0.07270761\n",
      "Iteration 16783, loss = 0.07271045\n",
      "Iteration 16784, loss = 0.07269905\n",
      "Iteration 16785, loss = 0.07269136\n",
      "Iteration 16786, loss = 0.07268800\n",
      "Iteration 16787, loss = 0.07268286\n",
      "Iteration 16788, loss = 0.07267811\n",
      "Iteration 16789, loss = 0.07267225\n",
      "Iteration 16790, loss = 0.07266656\n",
      "Iteration 16791, loss = 0.07265708\n",
      "Iteration 16792, loss = 0.07265335\n",
      "Iteration 16793, loss = 0.07264724\n",
      "Iteration 16794, loss = 0.07263867\n",
      "Iteration 16795, loss = 0.07263386\n",
      "Iteration 16796, loss = 0.07262751\n",
      "Iteration 16797, loss = 0.07262154\n",
      "Iteration 16798, loss = 0.07261697\n",
      "Iteration 16799, loss = 0.07261101\n",
      "Iteration 16800, loss = 0.07260204\n",
      "Iteration 16801, loss = 0.07259457\n",
      "Iteration 16802, loss = 0.07258763\n",
      "Iteration 16803, loss = 0.07258521\n",
      "Iteration 16804, loss = 0.07257865\n",
      "Iteration 16805, loss = 0.07257068\n",
      "Iteration 16806, loss = 0.07256479\n",
      "Iteration 16807, loss = 0.07256417\n",
      "Iteration 16808, loss = 0.07256238\n",
      "Iteration 16809, loss = 0.07255097\n",
      "Iteration 16810, loss = 0.07254348\n",
      "Iteration 16811, loss = 0.07254003\n",
      "Iteration 16812, loss = 0.07253550\n",
      "Iteration 16813, loss = 0.07252643\n",
      "Iteration 16814, loss = 0.07252087\n",
      "Iteration 16815, loss = 0.07251769\n",
      "Iteration 16816, loss = 0.07251130\n",
      "Iteration 16817, loss = 0.07250214\n",
      "Iteration 16818, loss = 0.07249834\n",
      "Iteration 16819, loss = 0.07249002\n",
      "Iteration 16820, loss = 0.07247835\n",
      "Iteration 16821, loss = 0.07247771\n",
      "Iteration 16822, loss = 0.07247051\n",
      "Iteration 16823, loss = 0.07246419\n",
      "Iteration 16824, loss = 0.07245615\n",
      "Iteration 16825, loss = 0.07245049\n",
      "Iteration 16826, loss = 0.07244521\n",
      "Iteration 16827, loss = 0.07243877\n",
      "Iteration 16828, loss = 0.07243211\n",
      "Iteration 16829, loss = 0.07242705\n",
      "Iteration 16830, loss = 0.07242246\n",
      "Iteration 16831, loss = 0.07241723\n",
      "Iteration 16832, loss = 0.07241068\n",
      "Iteration 16833, loss = 0.07240397\n",
      "Iteration 16834, loss = 0.07239967\n",
      "Iteration 16835, loss = 0.07239121\n",
      "Iteration 16836, loss = 0.07238732\n",
      "Iteration 16837, loss = 0.07238457\n",
      "Iteration 16838, loss = 0.07237761\n",
      "Iteration 16839, loss = 0.07236798\n",
      "Iteration 16840, loss = 0.07236181\n",
      "Iteration 16841, loss = 0.07235518\n",
      "Iteration 16842, loss = 0.07234800\n",
      "Iteration 16843, loss = 0.07234366\n",
      "Iteration 16844, loss = 0.07233449\n",
      "Iteration 16845, loss = 0.07232812\n",
      "Iteration 16846, loss = 0.07232319\n",
      "Iteration 16847, loss = 0.07231815\n",
      "Iteration 16848, loss = 0.07230936\n",
      "Iteration 16849, loss = 0.07230577\n",
      "Iteration 16850, loss = 0.07229871\n",
      "Iteration 16851, loss = 0.07229392\n",
      "Iteration 16852, loss = 0.07228646\n",
      "Iteration 16853, loss = 0.07228271\n",
      "Iteration 16854, loss = 0.07227630\n",
      "Iteration 16855, loss = 0.07226995\n",
      "Iteration 16856, loss = 0.07226247\n",
      "Iteration 16857, loss = 0.07225581\n",
      "Iteration 16858, loss = 0.07225141\n",
      "Iteration 16859, loss = 0.07224262\n",
      "Iteration 16860, loss = 0.07224171\n",
      "Iteration 16861, loss = 0.07223401\n",
      "Iteration 16862, loss = 0.07222130\n",
      "Iteration 16863, loss = 0.07222320\n",
      "Iteration 16864, loss = 0.07221773\n",
      "Iteration 16865, loss = 0.07220959\n",
      "Iteration 16866, loss = 0.07219702\n",
      "Iteration 16867, loss = 0.07219166\n",
      "Iteration 16868, loss = 0.07219333\n",
      "Iteration 16869, loss = 0.07218328\n",
      "Iteration 16870, loss = 0.07217818\n",
      "Iteration 16871, loss = 0.07217178\n",
      "Iteration 16872, loss = 0.07216275\n",
      "Iteration 16873, loss = 0.07215377\n",
      "Iteration 16874, loss = 0.07214577\n",
      "Iteration 16875, loss = 0.07214164\n",
      "Iteration 16876, loss = 0.07213903\n",
      "Iteration 16877, loss = 0.07212885\n",
      "Iteration 16878, loss = 0.07211746\n",
      "Iteration 16879, loss = 0.07211268\n",
      "Iteration 16880, loss = 0.07210692\n",
      "Iteration 16881, loss = 0.07209883\n",
      "Iteration 16882, loss = 0.07209200\n",
      "Iteration 16883, loss = 0.07208563\n",
      "Iteration 16884, loss = 0.07208037\n",
      "Iteration 16885, loss = 0.07206895\n",
      "Iteration 16886, loss = 0.07206766\n",
      "Iteration 16887, loss = 0.07206390\n",
      "Iteration 16888, loss = 0.07205682\n",
      "Iteration 16889, loss = 0.07205132\n",
      "Iteration 16890, loss = 0.07204348\n",
      "Iteration 16891, loss = 0.07203732\n",
      "Iteration 16892, loss = 0.07202696\n",
      "Iteration 16893, loss = 0.07201763\n",
      "Iteration 16894, loss = 0.07202031\n",
      "Iteration 16895, loss = 0.07201228\n",
      "Iteration 16896, loss = 0.07200088\n",
      "Iteration 16897, loss = 0.07199572\n",
      "Iteration 16898, loss = 0.07199131\n",
      "Iteration 16899, loss = 0.07198525\n",
      "Iteration 16900, loss = 0.07197645\n",
      "Iteration 16901, loss = 0.07197100\n",
      "Iteration 16902, loss = 0.07196504\n",
      "Iteration 16903, loss = 0.07195815\n",
      "Iteration 16904, loss = 0.07195039\n",
      "Iteration 16905, loss = 0.07194213\n",
      "Iteration 16906, loss = 0.07193825\n",
      "Iteration 16907, loss = 0.07192950\n",
      "Iteration 16908, loss = 0.07191701\n",
      "Iteration 16909, loss = 0.07190841\n",
      "Iteration 16910, loss = 0.07189892\n",
      "Iteration 16911, loss = 0.07188722\n",
      "Iteration 16912, loss = 0.07188214\n",
      "Iteration 16913, loss = 0.07187714\n",
      "Iteration 16914, loss = 0.07186885\n",
      "Iteration 16915, loss = 0.07186012\n",
      "Iteration 16916, loss = 0.07185172\n",
      "Iteration 16917, loss = 0.07184081\n",
      "Iteration 16918, loss = 0.07183312\n",
      "Iteration 16919, loss = 0.07183007\n",
      "Iteration 16920, loss = 0.07182068\n",
      "Iteration 16921, loss = 0.07181398\n",
      "Iteration 16922, loss = 0.07180790\n",
      "Iteration 16923, loss = 0.07180389\n",
      "Iteration 16924, loss = 0.07179764\n",
      "Iteration 16925, loss = 0.07178724\n",
      "Iteration 16926, loss = 0.07177830\n",
      "Iteration 16927, loss = 0.07177207\n",
      "Iteration 16928, loss = 0.07176691\n",
      "Iteration 16929, loss = 0.07175635\n",
      "Iteration 16930, loss = 0.07174814\n",
      "Iteration 16931, loss = 0.07174250\n",
      "Iteration 16932, loss = 0.07173663\n",
      "Iteration 16933, loss = 0.07172720\n",
      "Iteration 16934, loss = 0.07171449\n",
      "Iteration 16935, loss = 0.07170931\n",
      "Iteration 16936, loss = 0.07170040\n",
      "Iteration 16937, loss = 0.07169238\n",
      "Iteration 16938, loss = 0.07168333\n",
      "Iteration 16939, loss = 0.07167959\n",
      "Iteration 16940, loss = 0.07167211\n",
      "Iteration 16941, loss = 0.07165995\n",
      "Iteration 16942, loss = 0.07165204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16943, loss = 0.07164386\n",
      "Iteration 16944, loss = 0.07163722\n",
      "Iteration 16945, loss = 0.07162962\n",
      "Iteration 16946, loss = 0.07162239\n",
      "Iteration 16947, loss = 0.07161554\n",
      "Iteration 16948, loss = 0.07161284\n",
      "Iteration 16949, loss = 0.07160219\n",
      "Iteration 16950, loss = 0.07159445\n",
      "Iteration 16951, loss = 0.07158292\n",
      "Iteration 16952, loss = 0.07157330\n",
      "Iteration 16953, loss = 0.07157205\n",
      "Iteration 16954, loss = 0.07156524\n",
      "Iteration 16955, loss = 0.07155461\n",
      "Iteration 16956, loss = 0.07154765\n",
      "Iteration 16957, loss = 0.07154441\n",
      "Iteration 16958, loss = 0.07153840\n",
      "Iteration 16959, loss = 0.07153012\n",
      "Iteration 16960, loss = 0.07152112\n",
      "Iteration 16961, loss = 0.07151399\n",
      "Iteration 16962, loss = 0.07150834\n",
      "Iteration 16963, loss = 0.07149692\n",
      "Iteration 16964, loss = 0.07148698\n",
      "Iteration 16965, loss = 0.07148503\n",
      "Iteration 16966, loss = 0.07147778\n",
      "Iteration 16967, loss = 0.07146816\n",
      "Iteration 16968, loss = 0.07145842\n",
      "Iteration 16969, loss = 0.07144871\n",
      "Iteration 16970, loss = 0.07144467\n",
      "Iteration 16971, loss = 0.07143715\n",
      "Iteration 16972, loss = 0.07142834\n",
      "Iteration 16973, loss = 0.07142305\n",
      "Iteration 16974, loss = 0.07141060\n",
      "Iteration 16975, loss = 0.07140255\n",
      "Iteration 16976, loss = 0.07140669\n",
      "Iteration 16977, loss = 0.07139661\n",
      "Iteration 16978, loss = 0.07138180\n",
      "Iteration 16979, loss = 0.07137713\n",
      "Iteration 16980, loss = 0.07137089\n",
      "Iteration 16981, loss = 0.07136327\n",
      "Iteration 16982, loss = 0.07135645\n",
      "Iteration 16983, loss = 0.07135283\n",
      "Iteration 16984, loss = 0.07134375\n",
      "Iteration 16985, loss = 0.07133665\n",
      "Iteration 16986, loss = 0.07133676\n",
      "Iteration 16987, loss = 0.07133118\n",
      "Iteration 16988, loss = 0.07132608\n",
      "Iteration 16989, loss = 0.07131609\n",
      "Iteration 16990, loss = 0.07130830\n",
      "Iteration 16991, loss = 0.07129826\n",
      "Iteration 16992, loss = 0.07129215\n",
      "Iteration 16993, loss = 0.07128460\n",
      "Iteration 16994, loss = 0.07128010\n",
      "Iteration 16995, loss = 0.07127059\n",
      "Iteration 16996, loss = 0.07126764\n",
      "Iteration 16997, loss = 0.07126038\n",
      "Iteration 16998, loss = 0.07125135\n",
      "Iteration 16999, loss = 0.07124397\n",
      "Iteration 17000, loss = 0.07124142\n",
      "Iteration 17001, loss = 0.07123390\n",
      "Iteration 17002, loss = 0.07122570\n",
      "Iteration 17003, loss = 0.07122117\n",
      "Iteration 17004, loss = 0.07121405\n",
      "Iteration 17005, loss = 0.07120657\n",
      "Iteration 17006, loss = 0.07120187\n",
      "Iteration 17007, loss = 0.07119333\n",
      "Iteration 17008, loss = 0.07119350\n",
      "Iteration 17009, loss = 0.07118405\n",
      "Iteration 17010, loss = 0.07117246\n",
      "Iteration 17011, loss = 0.07116903\n",
      "Iteration 17012, loss = 0.07116679\n",
      "Iteration 17013, loss = 0.07115523\n",
      "Iteration 17014, loss = 0.07114608\n",
      "Iteration 17015, loss = 0.07114040\n",
      "Iteration 17016, loss = 0.07113875\n",
      "Iteration 17017, loss = 0.07112672\n",
      "Iteration 17018, loss = 0.07111820\n",
      "Iteration 17019, loss = 0.07111400\n",
      "Iteration 17020, loss = 0.07111007\n",
      "Iteration 17021, loss = 0.07110145\n",
      "Iteration 17022, loss = 0.07109216\n",
      "Iteration 17023, loss = 0.07108277\n",
      "Iteration 17024, loss = 0.07108417\n",
      "Iteration 17025, loss = 0.07107545\n",
      "Iteration 17026, loss = 0.07106579\n",
      "Iteration 17027, loss = 0.07106257\n",
      "Iteration 17028, loss = 0.07105808\n",
      "Iteration 17029, loss = 0.07105227\n",
      "Iteration 17030, loss = 0.07104574\n",
      "Iteration 17031, loss = 0.07103468\n",
      "Iteration 17032, loss = 0.07102545\n",
      "Iteration 17033, loss = 0.07102967\n",
      "Iteration 17034, loss = 0.07102602\n",
      "Iteration 17035, loss = 0.07101229\n",
      "Iteration 17036, loss = 0.07100409\n",
      "Iteration 17037, loss = 0.07100150\n",
      "Iteration 17038, loss = 0.07100156\n",
      "Iteration 17039, loss = 0.07099596\n",
      "Iteration 17040, loss = 0.07099119\n",
      "Iteration 17041, loss = 0.07098260\n",
      "Iteration 17042, loss = 0.07097423\n",
      "Iteration 17043, loss = 0.07096426\n",
      "Iteration 17044, loss = 0.07095396\n",
      "Iteration 17045, loss = 0.07094661\n",
      "Iteration 17046, loss = 0.07094844\n",
      "Iteration 17047, loss = 0.07094270\n",
      "Iteration 17048, loss = 0.07093072\n",
      "Iteration 17049, loss = 0.07091985\n",
      "Iteration 17050, loss = 0.07091685\n",
      "Iteration 17051, loss = 0.07091489\n",
      "Iteration 17052, loss = 0.07090848\n",
      "Iteration 17053, loss = 0.07089916\n",
      "Iteration 17054, loss = 0.07089523\n",
      "Iteration 17055, loss = 0.07088556\n",
      "Iteration 17056, loss = 0.07087893\n",
      "Iteration 17057, loss = 0.07087231\n",
      "Iteration 17058, loss = 0.07086452\n",
      "Iteration 17059, loss = 0.07085855\n",
      "Iteration 17060, loss = 0.07085180\n",
      "Iteration 17061, loss = 0.07084674\n",
      "Iteration 17062, loss = 0.07084136\n",
      "Iteration 17063, loss = 0.07083910\n",
      "Iteration 17064, loss = 0.07083194\n",
      "Iteration 17065, loss = 0.07082769\n",
      "Iteration 17066, loss = 0.07081674\n",
      "Iteration 17067, loss = 0.07080627\n",
      "Iteration 17068, loss = 0.07080174\n",
      "Iteration 17069, loss = 0.07079689\n",
      "Iteration 17070, loss = 0.07078825\n",
      "Iteration 17071, loss = 0.07078173\n",
      "Iteration 17072, loss = 0.07077850\n",
      "Iteration 17073, loss = 0.07077740\n",
      "Iteration 17074, loss = 0.07076560\n",
      "Iteration 17075, loss = 0.07075904\n",
      "Iteration 17076, loss = 0.07075420\n",
      "Iteration 17077, loss = 0.07074692\n",
      "Iteration 17078, loss = 0.07074089\n",
      "Iteration 17079, loss = 0.07073472\n",
      "Iteration 17080, loss = 0.07072603\n",
      "Iteration 17081, loss = 0.07071967\n",
      "Iteration 17082, loss = 0.07071206\n",
      "Iteration 17083, loss = 0.07070958\n",
      "Iteration 17084, loss = 0.07070482\n",
      "Iteration 17085, loss = 0.07069604\n",
      "Iteration 17086, loss = 0.07068944\n",
      "Iteration 17087, loss = 0.07068478\n",
      "Iteration 17088, loss = 0.07067835\n",
      "Iteration 17089, loss = 0.07066748\n",
      "Iteration 17090, loss = 0.07066930\n",
      "Iteration 17091, loss = 0.07066211\n",
      "Iteration 17092, loss = 0.07064973\n",
      "Iteration 17093, loss = 0.07064664\n",
      "Iteration 17094, loss = 0.07063987\n",
      "Iteration 17095, loss = 0.07063159\n",
      "Iteration 17096, loss = 0.07062503\n",
      "Iteration 17097, loss = 0.07062448\n",
      "Iteration 17098, loss = 0.07061614\n",
      "Iteration 17099, loss = 0.07061055\n",
      "Iteration 17100, loss = 0.07060498\n",
      "Iteration 17101, loss = 0.07059944\n",
      "Iteration 17102, loss = 0.07059241\n",
      "Iteration 17103, loss = 0.07058739\n",
      "Iteration 17104, loss = 0.07058000\n",
      "Iteration 17105, loss = 0.07057234\n",
      "Iteration 17106, loss = 0.07056528\n",
      "Iteration 17107, loss = 0.07055608\n",
      "Iteration 17108, loss = 0.07055550\n",
      "Iteration 17109, loss = 0.07055332\n",
      "Iteration 17110, loss = 0.07054914\n",
      "Iteration 17111, loss = 0.07053981\n",
      "Iteration 17112, loss = 0.07052905\n",
      "Iteration 17113, loss = 0.07052316\n",
      "Iteration 17114, loss = 0.07052548\n",
      "Iteration 17115, loss = 0.07051835\n",
      "Iteration 17116, loss = 0.07051039\n",
      "Iteration 17117, loss = 0.07050080\n",
      "Iteration 17118, loss = 0.07049390\n",
      "Iteration 17119, loss = 0.07049200\n",
      "Iteration 17120, loss = 0.07048751\n",
      "Iteration 17121, loss = 0.07048217\n",
      "Iteration 17122, loss = 0.07047289\n",
      "Iteration 17123, loss = 0.07046072\n",
      "Iteration 17124, loss = 0.07045509\n",
      "Iteration 17125, loss = 0.07045724\n",
      "Iteration 17126, loss = 0.07045246\n",
      "Iteration 17127, loss = 0.07044727\n",
      "Iteration 17128, loss = 0.07043889\n",
      "Iteration 17129, loss = 0.07042652\n",
      "Iteration 17130, loss = 0.07041892\n",
      "Iteration 17131, loss = 0.07042066\n",
      "Iteration 17132, loss = 0.07041541\n",
      "Iteration 17133, loss = 0.07040284\n",
      "Iteration 17134, loss = 0.07039563\n",
      "Iteration 17135, loss = 0.07039490\n",
      "Iteration 17136, loss = 0.07039018\n",
      "Iteration 17137, loss = 0.07038057\n",
      "Iteration 17138, loss = 0.07037317\n",
      "Iteration 17139, loss = 0.07036620\n",
      "Iteration 17140, loss = 0.07036303\n",
      "Iteration 17141, loss = 0.07036131\n",
      "Iteration 17142, loss = 0.07035800\n",
      "Iteration 17143, loss = 0.07034703\n",
      "Iteration 17144, loss = 0.07033830\n",
      "Iteration 17145, loss = 0.07032965\n",
      "Iteration 17146, loss = 0.07032231\n",
      "Iteration 17147, loss = 0.07032022\n",
      "Iteration 17148, loss = 0.07031420\n",
      "Iteration 17149, loss = 0.07030463\n",
      "Iteration 17150, loss = 0.07029948\n",
      "Iteration 17151, loss = 0.07029688\n",
      "Iteration 17152, loss = 0.07029048\n",
      "Iteration 17153, loss = 0.07028671\n",
      "Iteration 17154, loss = 0.07027653\n",
      "Iteration 17155, loss = 0.07026861\n",
      "Iteration 17156, loss = 0.07026310\n",
      "Iteration 17157, loss = 0.07026025\n",
      "Iteration 17158, loss = 0.07025573\n",
      "Iteration 17159, loss = 0.07024978\n",
      "Iteration 17160, loss = 0.07024192\n",
      "Iteration 17161, loss = 0.07023442\n",
      "Iteration 17162, loss = 0.07022960\n",
      "Iteration 17163, loss = 0.07022375\n",
      "Iteration 17164, loss = 0.07021759\n",
      "Iteration 17165, loss = 0.07021300\n",
      "Iteration 17166, loss = 0.07020662\n",
      "Iteration 17167, loss = 0.07020004\n",
      "Iteration 17168, loss = 0.07019414\n",
      "Iteration 17169, loss = 0.07018922\n",
      "Iteration 17170, loss = 0.07018237\n",
      "Iteration 17171, loss = 0.07017412\n",
      "Iteration 17172, loss = 0.07017264\n",
      "Iteration 17173, loss = 0.07016618\n",
      "Iteration 17174, loss = 0.07015432\n",
      "Iteration 17175, loss = 0.07014448\n",
      "Iteration 17176, loss = 0.07014198\n",
      "Iteration 17177, loss = 0.07013681\n",
      "Iteration 17178, loss = 0.07013142\n",
      "Iteration 17179, loss = 0.07012693\n",
      "Iteration 17180, loss = 0.07011889\n",
      "Iteration 17181, loss = 0.07011664\n",
      "Iteration 17182, loss = 0.07011089\n",
      "Iteration 17183, loss = 0.07010548\n",
      "Iteration 17184, loss = 0.07010118\n",
      "Iteration 17185, loss = 0.07009442\n",
      "Iteration 17186, loss = 0.07008338\n",
      "Iteration 17187, loss = 0.07007654\n",
      "Iteration 17188, loss = 0.07006985\n",
      "Iteration 17189, loss = 0.07006528\n",
      "Iteration 17190, loss = 0.07005671\n",
      "Iteration 17191, loss = 0.07005537\n",
      "Iteration 17192, loss = 0.07005242\n",
      "Iteration 17193, loss = 0.07004487\n",
      "Iteration 17194, loss = 0.07003706\n",
      "Iteration 17195, loss = 0.07003098\n",
      "Iteration 17196, loss = 0.07002460\n",
      "Iteration 17197, loss = 0.07001659\n",
      "Iteration 17198, loss = 0.07001690\n",
      "Iteration 17199, loss = 0.07001013\n",
      "Iteration 17200, loss = 0.07000144\n",
      "Iteration 17201, loss = 0.06999894\n",
      "Iteration 17202, loss = 0.06999136\n",
      "Iteration 17203, loss = 0.06998073\n",
      "Iteration 17204, loss = 0.06997697\n",
      "Iteration 17205, loss = 0.06997947\n",
      "Iteration 17206, loss = 0.06997560\n",
      "Iteration 17207, loss = 0.06995649\n",
      "Iteration 17208, loss = 0.06995372\n",
      "Iteration 17209, loss = 0.06995283\n",
      "Iteration 17210, loss = 0.06995166\n",
      "Iteration 17211, loss = 0.06994732\n",
      "Iteration 17212, loss = 0.06993780\n",
      "Iteration 17213, loss = 0.06993327\n",
      "Iteration 17214, loss = 0.06992566\n",
      "Iteration 17215, loss = 0.06992021\n",
      "Iteration 17216, loss = 0.06991184\n",
      "Iteration 17217, loss = 0.06990069\n",
      "Iteration 17218, loss = 0.06989796\n",
      "Iteration 17219, loss = 0.06989018\n",
      "Iteration 17220, loss = 0.06988026\n",
      "Iteration 17221, loss = 0.06987994\n",
      "Iteration 17222, loss = 0.06987542\n",
      "Iteration 17223, loss = 0.06986857\n",
      "Iteration 17224, loss = 0.06986494\n",
      "Iteration 17225, loss = 0.06985884\n",
      "Iteration 17226, loss = 0.06985386\n",
      "Iteration 17227, loss = 0.06984478\n",
      "Iteration 17228, loss = 0.06983758\n",
      "Iteration 17229, loss = 0.06983210\n",
      "Iteration 17230, loss = 0.06983121\n",
      "Iteration 17231, loss = 0.06982568\n",
      "Iteration 17232, loss = 0.06981336\n",
      "Iteration 17233, loss = 0.06981118\n",
      "Iteration 17234, loss = 0.06981062\n",
      "Iteration 17235, loss = 0.06980461\n",
      "Iteration 17236, loss = 0.06979716\n",
      "Iteration 17237, loss = 0.06979081\n",
      "Iteration 17238, loss = 0.06978848\n",
      "Iteration 17239, loss = 0.06977886\n",
      "Iteration 17240, loss = 0.06977433\n",
      "Iteration 17241, loss = 0.06976706\n",
      "Iteration 17242, loss = 0.06975751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17243, loss = 0.06975129\n",
      "Iteration 17244, loss = 0.06974344\n",
      "Iteration 17245, loss = 0.06973973\n",
      "Iteration 17246, loss = 0.06973509\n",
      "Iteration 17247, loss = 0.06972449\n",
      "Iteration 17248, loss = 0.06972099\n",
      "Iteration 17249, loss = 0.06972107\n",
      "Iteration 17250, loss = 0.06971583\n",
      "Iteration 17251, loss = 0.06970763\n",
      "Iteration 17252, loss = 0.06969933\n",
      "Iteration 17253, loss = 0.06969919\n",
      "Iteration 17254, loss = 0.06969426\n",
      "Iteration 17255, loss = 0.06968751\n",
      "Iteration 17256, loss = 0.06967888\n",
      "Iteration 17257, loss = 0.06967364\n",
      "Iteration 17258, loss = 0.06966807\n",
      "Iteration 17259, loss = 0.06966161\n",
      "Iteration 17260, loss = 0.06965285\n",
      "Iteration 17261, loss = 0.06964712\n",
      "Iteration 17262, loss = 0.06963872\n",
      "Iteration 17263, loss = 0.06963464\n",
      "Iteration 17264, loss = 0.06962479\n",
      "Iteration 17265, loss = 0.06962102\n",
      "Iteration 17266, loss = 0.06961623\n",
      "Iteration 17267, loss = 0.06961246\n",
      "Iteration 17268, loss = 0.06960653\n",
      "Iteration 17269, loss = 0.06960222\n",
      "Iteration 17270, loss = 0.06959517\n",
      "Iteration 17271, loss = 0.06959524\n",
      "Iteration 17272, loss = 0.06959008\n",
      "Iteration 17273, loss = 0.06958386\n",
      "Iteration 17274, loss = 0.06957836\n",
      "Iteration 17275, loss = 0.06957104\n",
      "Iteration 17276, loss = 0.06956525\n",
      "Iteration 17277, loss = 0.06955944\n",
      "Iteration 17278, loss = 0.06954860\n",
      "Iteration 17279, loss = 0.06954592\n",
      "Iteration 17280, loss = 0.06954324\n",
      "Iteration 17281, loss = 0.06953299\n",
      "Iteration 17282, loss = 0.06953000\n",
      "Iteration 17283, loss = 0.06952272\n",
      "Iteration 17284, loss = 0.06952256\n",
      "Iteration 17285, loss = 0.06951846\n",
      "Iteration 17286, loss = 0.06950844\n",
      "Iteration 17287, loss = 0.06950166\n",
      "Iteration 17288, loss = 0.06949546\n",
      "Iteration 17289, loss = 0.06949056\n",
      "Iteration 17290, loss = 0.06948118\n",
      "Iteration 17291, loss = 0.06947048\n",
      "Iteration 17292, loss = 0.06947473\n",
      "Iteration 17293, loss = 0.06946918\n",
      "Iteration 17294, loss = 0.06946134\n",
      "Iteration 17295, loss = 0.06945115\n",
      "Iteration 17296, loss = 0.06944733\n",
      "Iteration 17297, loss = 0.06944404\n",
      "Iteration 17298, loss = 0.06943874\n",
      "Iteration 17299, loss = 0.06943312\n",
      "Iteration 17300, loss = 0.06942806\n",
      "Iteration 17301, loss = 0.06941837\n",
      "Iteration 17302, loss = 0.06941150\n",
      "Iteration 17303, loss = 0.06940597\n",
      "Iteration 17304, loss = 0.06940867\n",
      "Iteration 17305, loss = 0.06940317\n",
      "Iteration 17306, loss = 0.06939003\n",
      "Iteration 17307, loss = 0.06937900\n",
      "Iteration 17308, loss = 0.06937941\n",
      "Iteration 17309, loss = 0.06937611\n",
      "Iteration 17310, loss = 0.06936971\n",
      "Iteration 17311, loss = 0.06936439\n",
      "Iteration 17312, loss = 0.06935660\n",
      "Iteration 17313, loss = 0.06935346\n",
      "Iteration 17314, loss = 0.06934642\n",
      "Iteration 17315, loss = 0.06933822\n",
      "Iteration 17316, loss = 0.06933415\n",
      "Iteration 17317, loss = 0.06932912\n",
      "Iteration 17318, loss = 0.06931819\n",
      "Iteration 17319, loss = 0.06931374\n",
      "Iteration 17320, loss = 0.06930941\n",
      "Iteration 17321, loss = 0.06930338\n",
      "Iteration 17322, loss = 0.06929876\n",
      "Iteration 17323, loss = 0.06928931\n",
      "Iteration 17324, loss = 0.06928224\n",
      "Iteration 17325, loss = 0.06927777\n",
      "Iteration 17326, loss = 0.06926968\n",
      "Iteration 17327, loss = 0.06926632\n",
      "Iteration 17328, loss = 0.06926193\n",
      "Iteration 17329, loss = 0.06925527\n",
      "Iteration 17330, loss = 0.06924881\n",
      "Iteration 17331, loss = 0.06924842\n",
      "Iteration 17332, loss = 0.06924135\n",
      "Iteration 17333, loss = 0.06923499\n",
      "Iteration 17334, loss = 0.06923068\n",
      "Iteration 17335, loss = 0.06922918\n",
      "Iteration 17336, loss = 0.06922334\n",
      "Iteration 17337, loss = 0.06921502\n",
      "Iteration 17338, loss = 0.06920730\n",
      "Iteration 17339, loss = 0.06920239\n",
      "Iteration 17340, loss = 0.06919750\n",
      "Iteration 17341, loss = 0.06919022\n",
      "Iteration 17342, loss = 0.06918411\n",
      "Iteration 17343, loss = 0.06917958\n",
      "Iteration 17344, loss = 0.06917556\n",
      "Iteration 17345, loss = 0.06916796\n",
      "Iteration 17346, loss = 0.06916160\n",
      "Iteration 17347, loss = 0.06915667\n",
      "Iteration 17348, loss = 0.06915118\n",
      "Iteration 17349, loss = 0.06914346\n",
      "Iteration 17350, loss = 0.06913597\n",
      "Iteration 17351, loss = 0.06913447\n",
      "Iteration 17352, loss = 0.06913254\n",
      "Iteration 17353, loss = 0.06912542\n",
      "Iteration 17354, loss = 0.06911550\n",
      "Iteration 17355, loss = 0.06910972\n",
      "Iteration 17356, loss = 0.06910610\n",
      "Iteration 17357, loss = 0.06910171\n",
      "Iteration 17358, loss = 0.06909565\n",
      "Iteration 17359, loss = 0.06909094\n",
      "Iteration 17360, loss = 0.06908361\n",
      "Iteration 17361, loss = 0.06907704\n",
      "Iteration 17362, loss = 0.06907073\n",
      "Iteration 17363, loss = 0.06906175\n",
      "Iteration 17364, loss = 0.06905907\n",
      "Iteration 17365, loss = 0.06905397\n",
      "Iteration 17366, loss = 0.06904983\n",
      "Iteration 17367, loss = 0.06904367\n",
      "Iteration 17368, loss = 0.06903874\n",
      "Iteration 17369, loss = 0.06903617\n",
      "Iteration 17370, loss = 0.06903342\n",
      "Iteration 17371, loss = 0.06902629\n",
      "Iteration 17372, loss = 0.06901491\n",
      "Iteration 17373, loss = 0.06901966\n",
      "Iteration 17374, loss = 0.06901655\n",
      "Iteration 17375, loss = 0.06900391\n",
      "Iteration 17376, loss = 0.06899600\n",
      "Iteration 17377, loss = 0.06899219\n",
      "Iteration 17378, loss = 0.06898605\n",
      "Iteration 17379, loss = 0.06897690\n",
      "Iteration 17380, loss = 0.06897304\n",
      "Iteration 17381, loss = 0.06897761\n",
      "Iteration 17382, loss = 0.06896888\n",
      "Iteration 17383, loss = 0.06895732\n",
      "Iteration 17384, loss = 0.06895025\n",
      "Iteration 17385, loss = 0.06894598\n",
      "Iteration 17386, loss = 0.06894008\n",
      "Iteration 17387, loss = 0.06893307\n",
      "Iteration 17388, loss = 0.06892244\n",
      "Iteration 17389, loss = 0.06892246\n",
      "Iteration 17390, loss = 0.06891609\n",
      "Iteration 17391, loss = 0.06891065\n",
      "Iteration 17392, loss = 0.06890475\n",
      "Iteration 17393, loss = 0.06890090\n",
      "Iteration 17394, loss = 0.06889475\n",
      "Iteration 17395, loss = 0.06888851\n",
      "Iteration 17396, loss = 0.06888193\n",
      "Iteration 17397, loss = 0.06887306\n",
      "Iteration 17398, loss = 0.06887360\n",
      "Iteration 17399, loss = 0.06886837\n",
      "Iteration 17400, loss = 0.06885331\n",
      "Iteration 17401, loss = 0.06885450\n",
      "Iteration 17402, loss = 0.06884783\n",
      "Iteration 17403, loss = 0.06884122\n",
      "Iteration 17404, loss = 0.06883380\n",
      "Iteration 17405, loss = 0.06883271\n",
      "Iteration 17406, loss = 0.06882522\n",
      "Iteration 17407, loss = 0.06882092\n",
      "Iteration 17408, loss = 0.06881815\n",
      "Iteration 17409, loss = 0.06881295\n",
      "Iteration 17410, loss = 0.06880686\n",
      "Iteration 17411, loss = 0.06880109\n",
      "Iteration 17412, loss = 0.06879289\n",
      "Iteration 17413, loss = 0.06878397\n",
      "Iteration 17414, loss = 0.06877802\n",
      "Iteration 17415, loss = 0.06877038\n",
      "Iteration 17416, loss = 0.06876524\n",
      "Iteration 17417, loss = 0.06876242\n",
      "Iteration 17418, loss = 0.06876068\n",
      "Iteration 17419, loss = 0.06875567\n",
      "Iteration 17420, loss = 0.06874820\n",
      "Iteration 17421, loss = 0.06874029\n",
      "Iteration 17422, loss = 0.06873758\n",
      "Iteration 17423, loss = 0.06873179\n",
      "Iteration 17424, loss = 0.06872466\n",
      "Iteration 17425, loss = 0.06871884\n",
      "Iteration 17426, loss = 0.06871493\n",
      "Iteration 17427, loss = 0.06871360\n",
      "Iteration 17428, loss = 0.06870386\n",
      "Iteration 17429, loss = 0.06869725\n",
      "Iteration 17430, loss = 0.06869235\n",
      "Iteration 17431, loss = 0.06868503\n",
      "Iteration 17432, loss = 0.06867965\n",
      "Iteration 17433, loss = 0.06867740\n",
      "Iteration 17434, loss = 0.06867586\n",
      "Iteration 17435, loss = 0.06866715\n",
      "Iteration 17436, loss = 0.06865749\n",
      "Iteration 17437, loss = 0.06865350\n",
      "Iteration 17438, loss = 0.06865006\n",
      "Iteration 17439, loss = 0.06864152\n",
      "Iteration 17440, loss = 0.06863849\n",
      "Iteration 17441, loss = 0.06863256\n",
      "Iteration 17442, loss = 0.06862687\n",
      "Iteration 17443, loss = 0.06862547\n",
      "Iteration 17444, loss = 0.06862091\n",
      "Iteration 17445, loss = 0.06861515\n",
      "Iteration 17446, loss = 0.06860706\n",
      "Iteration 17447, loss = 0.06859891\n",
      "Iteration 17448, loss = 0.06859527\n",
      "Iteration 17449, loss = 0.06858823\n",
      "Iteration 17450, loss = 0.06858404\n",
      "Iteration 17451, loss = 0.06857892\n",
      "Iteration 17452, loss = 0.06857110\n",
      "Iteration 17453, loss = 0.06856413\n",
      "Iteration 17454, loss = 0.06855506\n",
      "Iteration 17455, loss = 0.06855524\n",
      "Iteration 17456, loss = 0.06855141\n",
      "Iteration 17457, loss = 0.06854216\n",
      "Iteration 17458, loss = 0.06853853\n",
      "Iteration 17459, loss = 0.06853525\n",
      "Iteration 17460, loss = 0.06852996\n",
      "Iteration 17461, loss = 0.06852215\n",
      "Iteration 17462, loss = 0.06851319\n",
      "Iteration 17463, loss = 0.06851215\n",
      "Iteration 17464, loss = 0.06850558\n",
      "Iteration 17465, loss = 0.06849440\n",
      "Iteration 17466, loss = 0.06849628\n",
      "Iteration 17467, loss = 0.06849024\n",
      "Iteration 17468, loss = 0.06848395\n",
      "Iteration 17469, loss = 0.06847800\n",
      "Iteration 17470, loss = 0.06847130\n",
      "Iteration 17471, loss = 0.06846621\n",
      "Iteration 17472, loss = 0.06845859\n",
      "Iteration 17473, loss = 0.06845801\n",
      "Iteration 17474, loss = 0.06845035\n",
      "Iteration 17475, loss = 0.06844730\n",
      "Iteration 17476, loss = 0.06844028\n",
      "Iteration 17477, loss = 0.06843079\n",
      "Iteration 17478, loss = 0.06843037\n",
      "Iteration 17479, loss = 0.06842260\n",
      "Iteration 17480, loss = 0.06842036\n",
      "Iteration 17481, loss = 0.06841410\n",
      "Iteration 17482, loss = 0.06840425\n",
      "Iteration 17483, loss = 0.06840094\n",
      "Iteration 17484, loss = 0.06839389\n",
      "Iteration 17485, loss = 0.06839124\n",
      "Iteration 17486, loss = 0.06838593\n",
      "Iteration 17487, loss = 0.06838133\n",
      "Iteration 17488, loss = 0.06837201\n",
      "Iteration 17489, loss = 0.06836577\n",
      "Iteration 17490, loss = 0.06836250\n",
      "Iteration 17491, loss = 0.06835811\n",
      "Iteration 17492, loss = 0.06835223\n",
      "Iteration 17493, loss = 0.06834552\n",
      "Iteration 17494, loss = 0.06834131\n",
      "Iteration 17495, loss = 0.06833811\n",
      "Iteration 17496, loss = 0.06832837\n",
      "Iteration 17497, loss = 0.06832657\n",
      "Iteration 17498, loss = 0.06832157\n",
      "Iteration 17499, loss = 0.06831505\n",
      "Iteration 17500, loss = 0.06831239\n",
      "Iteration 17501, loss = 0.06830620\n",
      "Iteration 17502, loss = 0.06829767\n",
      "Iteration 17503, loss = 0.06829297\n",
      "Iteration 17504, loss = 0.06829109\n",
      "Iteration 17505, loss = 0.06828315\n",
      "Iteration 17506, loss = 0.06827371\n",
      "Iteration 17507, loss = 0.06827097\n",
      "Iteration 17508, loss = 0.06826356\n",
      "Iteration 17509, loss = 0.06826083\n",
      "Iteration 17510, loss = 0.06825521\n",
      "Iteration 17511, loss = 0.06824940\n",
      "Iteration 17512, loss = 0.06824258\n",
      "Iteration 17513, loss = 0.06823669\n",
      "Iteration 17514, loss = 0.06823281\n",
      "Iteration 17515, loss = 0.06822652\n",
      "Iteration 17516, loss = 0.06821854\n",
      "Iteration 17517, loss = 0.06821495\n",
      "Iteration 17518, loss = 0.06821132\n",
      "Iteration 17519, loss = 0.06820245\n",
      "Iteration 17520, loss = 0.06819883\n",
      "Iteration 17521, loss = 0.06819400\n",
      "Iteration 17522, loss = 0.06819084\n",
      "Iteration 17523, loss = 0.06818296\n",
      "Iteration 17524, loss = 0.06817946\n",
      "Iteration 17525, loss = 0.06817294\n",
      "Iteration 17526, loss = 0.06816789\n",
      "Iteration 17527, loss = 0.06816455\n",
      "Iteration 17528, loss = 0.06816298\n",
      "Iteration 17529, loss = 0.06815314\n",
      "Iteration 17530, loss = 0.06814762\n",
      "Iteration 17531, loss = 0.06814240\n",
      "Iteration 17532, loss = 0.06813544\n",
      "Iteration 17533, loss = 0.06813039\n",
      "Iteration 17534, loss = 0.06812249\n",
      "Iteration 17535, loss = 0.06812167\n",
      "Iteration 17536, loss = 0.06811982\n",
      "Iteration 17537, loss = 0.06811419\n",
      "Iteration 17538, loss = 0.06810418\n",
      "Iteration 17539, loss = 0.06809732\n",
      "Iteration 17540, loss = 0.06809182\n",
      "Iteration 17541, loss = 0.06809177\n",
      "Iteration 17542, loss = 0.06808530\n",
      "Iteration 17543, loss = 0.06807962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17544, loss = 0.06807537\n",
      "Iteration 17545, loss = 0.06807300\n",
      "Iteration 17546, loss = 0.06806629\n",
      "Iteration 17547, loss = 0.06805838\n",
      "Iteration 17548, loss = 0.06805022\n",
      "Iteration 17549, loss = 0.06804382\n",
      "Iteration 17550, loss = 0.06804087\n",
      "Iteration 17551, loss = 0.06803771\n",
      "Iteration 17552, loss = 0.06803380\n",
      "Iteration 17553, loss = 0.06802541\n",
      "Iteration 17554, loss = 0.06802165\n",
      "Iteration 17555, loss = 0.06801522\n",
      "Iteration 17556, loss = 0.06800864\n",
      "Iteration 17557, loss = 0.06800233\n",
      "Iteration 17558, loss = 0.06800378\n",
      "Iteration 17559, loss = 0.06799695\n",
      "Iteration 17560, loss = 0.06798116\n",
      "Iteration 17561, loss = 0.06798247\n",
      "Iteration 17562, loss = 0.06798058\n",
      "Iteration 17563, loss = 0.06797532\n",
      "Iteration 17564, loss = 0.06797156\n",
      "Iteration 17565, loss = 0.06796434\n",
      "Iteration 17566, loss = 0.06795685\n",
      "Iteration 17567, loss = 0.06795549\n",
      "Iteration 17568, loss = 0.06794909\n",
      "Iteration 17569, loss = 0.06794245\n",
      "Iteration 17570, loss = 0.06792999\n",
      "Iteration 17571, loss = 0.06793191\n",
      "Iteration 17572, loss = 0.06792998\n",
      "Iteration 17573, loss = 0.06791928\n",
      "Iteration 17574, loss = 0.06791763\n",
      "Iteration 17575, loss = 0.06791412\n",
      "Iteration 17576, loss = 0.06790676\n",
      "Iteration 17577, loss = 0.06790364\n",
      "Iteration 17578, loss = 0.06789915\n",
      "Iteration 17579, loss = 0.06789118\n",
      "Iteration 17580, loss = 0.06788248\n",
      "Iteration 17581, loss = 0.06787530\n",
      "Iteration 17582, loss = 0.06786608\n",
      "Iteration 17583, loss = 0.06786615\n",
      "Iteration 17584, loss = 0.06786330\n",
      "Iteration 17585, loss = 0.06785281\n",
      "Iteration 17586, loss = 0.06784804\n",
      "Iteration 17587, loss = 0.06784680\n",
      "Iteration 17588, loss = 0.06784293\n",
      "Iteration 17589, loss = 0.06783830\n",
      "Iteration 17590, loss = 0.06783173\n",
      "Iteration 17591, loss = 0.06782677\n",
      "Iteration 17592, loss = 0.06782064\n",
      "Iteration 17593, loss = 0.06781433\n",
      "Iteration 17594, loss = 0.06780224\n",
      "Iteration 17595, loss = 0.06779835\n",
      "Iteration 17596, loss = 0.06779593\n",
      "Iteration 17597, loss = 0.06778886\n",
      "Iteration 17598, loss = 0.06778371\n",
      "Iteration 17599, loss = 0.06777841\n",
      "Iteration 17600, loss = 0.06777022\n",
      "Iteration 17601, loss = 0.06776821\n",
      "Iteration 17602, loss = 0.06776267\n",
      "Iteration 17603, loss = 0.06776105\n",
      "Iteration 17604, loss = 0.06775674\n",
      "Iteration 17605, loss = 0.06775107\n",
      "Iteration 17606, loss = 0.06773886\n",
      "Iteration 17607, loss = 0.06773521\n",
      "Iteration 17608, loss = 0.06773325\n",
      "Iteration 17609, loss = 0.06772704\n",
      "Iteration 17610, loss = 0.06771600\n",
      "Iteration 17611, loss = 0.06771440\n",
      "Iteration 17612, loss = 0.06771180\n",
      "Iteration 17613, loss = 0.06770943\n",
      "Iteration 17614, loss = 0.06769959\n",
      "Iteration 17615, loss = 0.06769279\n",
      "Iteration 17616, loss = 0.06768771\n",
      "Iteration 17617, loss = 0.06768319\n",
      "Iteration 17618, loss = 0.06768462\n",
      "Iteration 17619, loss = 0.06767350\n",
      "Iteration 17620, loss = 0.06766664\n",
      "Iteration 17621, loss = 0.06766436\n",
      "Iteration 17622, loss = 0.06766153\n",
      "Iteration 17623, loss = 0.06765811\n",
      "Iteration 17624, loss = 0.06765144\n",
      "Iteration 17625, loss = 0.06764408\n",
      "Iteration 17626, loss = 0.06763528\n",
      "Iteration 17627, loss = 0.06763151\n",
      "Iteration 17628, loss = 0.06762366\n",
      "Iteration 17629, loss = 0.06761551\n",
      "Iteration 17630, loss = 0.06761444\n",
      "Iteration 17631, loss = 0.06761208\n",
      "Iteration 17632, loss = 0.06760161\n",
      "Iteration 17633, loss = 0.06759666\n",
      "Iteration 17634, loss = 0.06759590\n",
      "Iteration 17635, loss = 0.06759372\n",
      "Iteration 17636, loss = 0.06758879\n",
      "Iteration 17637, loss = 0.06757714\n",
      "Iteration 17638, loss = 0.06757252\n",
      "Iteration 17639, loss = 0.06757077\n",
      "Iteration 17640, loss = 0.06756540\n",
      "Iteration 17641, loss = 0.06755652\n",
      "Iteration 17642, loss = 0.06754895\n",
      "Iteration 17643, loss = 0.06754958\n",
      "Iteration 17644, loss = 0.06754368\n",
      "Iteration 17645, loss = 0.06753299\n",
      "Iteration 17646, loss = 0.06752920\n",
      "Iteration 17647, loss = 0.06752709\n",
      "Iteration 17648, loss = 0.06752000\n",
      "Iteration 17649, loss = 0.06751466\n",
      "Iteration 17650, loss = 0.06750913\n",
      "Iteration 17651, loss = 0.06750643\n",
      "Iteration 17652, loss = 0.06749891\n",
      "Iteration 17653, loss = 0.06749414\n",
      "Iteration 17654, loss = 0.06749094\n",
      "Iteration 17655, loss = 0.06748421\n",
      "Iteration 17656, loss = 0.06747734\n",
      "Iteration 17657, loss = 0.06747600\n",
      "Iteration 17658, loss = 0.06746763\n",
      "Iteration 17659, loss = 0.06746243\n",
      "Iteration 17660, loss = 0.06745583\n",
      "Iteration 17661, loss = 0.06745012\n",
      "Iteration 17662, loss = 0.06744680\n",
      "Iteration 17663, loss = 0.06744277\n",
      "Iteration 17664, loss = 0.06744043\n",
      "Iteration 17665, loss = 0.06743362\n",
      "Iteration 17666, loss = 0.06742543\n",
      "Iteration 17667, loss = 0.06742217\n",
      "Iteration 17668, loss = 0.06741277\n",
      "Iteration 17669, loss = 0.06741209\n",
      "Iteration 17670, loss = 0.06740699\n",
      "Iteration 17671, loss = 0.06740313\n",
      "Iteration 17672, loss = 0.06739581\n",
      "Iteration 17673, loss = 0.06738892\n",
      "Iteration 17674, loss = 0.06738425\n",
      "Iteration 17675, loss = 0.06737656\n",
      "Iteration 17676, loss = 0.06738163\n",
      "Iteration 17677, loss = 0.06737410\n",
      "Iteration 17678, loss = 0.06736195\n",
      "Iteration 17679, loss = 0.06735639\n",
      "Iteration 17680, loss = 0.06735850\n",
      "Iteration 17681, loss = 0.06735329\n",
      "Iteration 17682, loss = 0.06734382\n",
      "Iteration 17683, loss = 0.06733569\n",
      "Iteration 17684, loss = 0.06733013\n",
      "Iteration 17685, loss = 0.06733110\n",
      "Iteration 17686, loss = 0.06732564\n",
      "Iteration 17687, loss = 0.06732150\n",
      "Iteration 17688, loss = 0.06731422\n",
      "Iteration 17689, loss = 0.06731167\n",
      "Iteration 17690, loss = 0.06730605\n",
      "Iteration 17691, loss = 0.06729884\n",
      "Iteration 17692, loss = 0.06729479\n",
      "Iteration 17693, loss = 0.06728769\n",
      "Iteration 17694, loss = 0.06727775\n",
      "Iteration 17695, loss = 0.06727503\n",
      "Iteration 17696, loss = 0.06727364\n",
      "Iteration 17697, loss = 0.06726634\n",
      "Iteration 17698, loss = 0.06725719\n",
      "Iteration 17699, loss = 0.06725110\n",
      "Iteration 17700, loss = 0.06725001\n",
      "Iteration 17701, loss = 0.06724567\n",
      "Iteration 17702, loss = 0.06724052\n",
      "Iteration 17703, loss = 0.06723307\n",
      "Iteration 17704, loss = 0.06722683\n",
      "Iteration 17705, loss = 0.06722411\n",
      "Iteration 17706, loss = 0.06721963\n",
      "Iteration 17707, loss = 0.06721187\n",
      "Iteration 17708, loss = 0.06720628\n",
      "Iteration 17709, loss = 0.06720293\n",
      "Iteration 17710, loss = 0.06719928\n",
      "Iteration 17711, loss = 0.06719287\n",
      "Iteration 17712, loss = 0.06718746\n",
      "Iteration 17713, loss = 0.06718457\n",
      "Iteration 17714, loss = 0.06717721\n",
      "Iteration 17715, loss = 0.06716936\n",
      "Iteration 17716, loss = 0.06716447\n",
      "Iteration 17717, loss = 0.06716455\n",
      "Iteration 17718, loss = 0.06715911\n",
      "Iteration 17719, loss = 0.06715084\n",
      "Iteration 17720, loss = 0.06714936\n",
      "Iteration 17721, loss = 0.06714422\n",
      "Iteration 17722, loss = 0.06713471\n",
      "Iteration 17723, loss = 0.06713058\n",
      "Iteration 17724, loss = 0.06712839\n",
      "Iteration 17725, loss = 0.06712384\n",
      "Iteration 17726, loss = 0.06711405\n",
      "Iteration 17727, loss = 0.06711254\n",
      "Iteration 17728, loss = 0.06710898\n",
      "Iteration 17729, loss = 0.06710650\n",
      "Iteration 17730, loss = 0.06709867\n",
      "Iteration 17731, loss = 0.06709713\n",
      "Iteration 17732, loss = 0.06709337\n",
      "Iteration 17733, loss = 0.06708263\n",
      "Iteration 17734, loss = 0.06707497\n",
      "Iteration 17735, loss = 0.06706989\n",
      "Iteration 17736, loss = 0.06706632\n",
      "Iteration 17737, loss = 0.06706147\n",
      "Iteration 17738, loss = 0.06705304\n",
      "Iteration 17739, loss = 0.06704467\n",
      "Iteration 17740, loss = 0.06704212\n",
      "Iteration 17741, loss = 0.06703991\n",
      "Iteration 17742, loss = 0.06703828\n",
      "Iteration 17743, loss = 0.06703458\n",
      "Iteration 17744, loss = 0.06702785\n",
      "Iteration 17745, loss = 0.06702157\n",
      "Iteration 17746, loss = 0.06701524\n",
      "Iteration 17747, loss = 0.06701015\n",
      "Iteration 17748, loss = 0.06700193\n",
      "Iteration 17749, loss = 0.06699565\n",
      "Iteration 17750, loss = 0.06699306\n",
      "Iteration 17751, loss = 0.06698833\n",
      "Iteration 17752, loss = 0.06698057\n",
      "Iteration 17753, loss = 0.06697986\n",
      "Iteration 17754, loss = 0.06697686\n",
      "Iteration 17755, loss = 0.06697094\n",
      "Iteration 17756, loss = 0.06696689\n",
      "Iteration 17757, loss = 0.06695818\n",
      "Iteration 17758, loss = 0.06695319\n",
      "Iteration 17759, loss = 0.06695100\n",
      "Iteration 17760, loss = 0.06694539\n",
      "Iteration 17761, loss = 0.06693934\n",
      "Iteration 17762, loss = 0.06693208\n",
      "Iteration 17763, loss = 0.06692463\n",
      "Iteration 17764, loss = 0.06691991\n",
      "Iteration 17765, loss = 0.06691661\n",
      "Iteration 17766, loss = 0.06691335\n",
      "Iteration 17767, loss = 0.06690745\n",
      "Iteration 17768, loss = 0.06690676\n",
      "Iteration 17769, loss = 0.06690238\n",
      "Iteration 17770, loss = 0.06689601\n",
      "Iteration 17771, loss = 0.06688707\n",
      "Iteration 17772, loss = 0.06687582\n",
      "Iteration 17773, loss = 0.06687973\n",
      "Iteration 17774, loss = 0.06687868\n",
      "Iteration 17775, loss = 0.06686876\n",
      "Iteration 17776, loss = 0.06686504\n",
      "Iteration 17777, loss = 0.06685957\n",
      "Iteration 17778, loss = 0.06685690\n",
      "Iteration 17779, loss = 0.06685666\n",
      "Iteration 17780, loss = 0.06685093\n",
      "Iteration 17781, loss = 0.06684090\n",
      "Iteration 17782, loss = 0.06683653\n",
      "Iteration 17783, loss = 0.06683174\n",
      "Iteration 17784, loss = 0.06682283\n",
      "Iteration 17785, loss = 0.06681177\n",
      "Iteration 17786, loss = 0.06681761\n",
      "Iteration 17787, loss = 0.06681509\n",
      "Iteration 17788, loss = 0.06680324\n",
      "Iteration 17789, loss = 0.06679791\n",
      "Iteration 17790, loss = 0.06679108\n",
      "Iteration 17791, loss = 0.06678761\n",
      "Iteration 17792, loss = 0.06678325\n",
      "Iteration 17793, loss = 0.06677975\n",
      "Iteration 17794, loss = 0.06677355\n",
      "Iteration 17795, loss = 0.06676623\n",
      "Iteration 17796, loss = 0.06675804\n",
      "Iteration 17797, loss = 0.06675564\n",
      "Iteration 17798, loss = 0.06674859\n",
      "Iteration 17799, loss = 0.06673915\n",
      "Iteration 17800, loss = 0.06673662\n",
      "Iteration 17801, loss = 0.06673334\n",
      "Iteration 17802, loss = 0.06672845\n",
      "Iteration 17803, loss = 0.06672297\n",
      "Iteration 17804, loss = 0.06672158\n",
      "Iteration 17805, loss = 0.06671166\n",
      "Iteration 17806, loss = 0.06670788\n",
      "Iteration 17807, loss = 0.06670778\n",
      "Iteration 17808, loss = 0.06670099\n",
      "Iteration 17809, loss = 0.06669336\n",
      "Iteration 17810, loss = 0.06668720\n",
      "Iteration 17811, loss = 0.06668129\n",
      "Iteration 17812, loss = 0.06667420\n",
      "Iteration 17813, loss = 0.06667641\n",
      "Iteration 17814, loss = 0.06666926\n",
      "Iteration 17815, loss = 0.06666014\n",
      "Iteration 17816, loss = 0.06665690\n",
      "Iteration 17817, loss = 0.06665269\n",
      "Iteration 17818, loss = 0.06664544\n",
      "Iteration 17819, loss = 0.06664258\n",
      "Iteration 17820, loss = 0.06663835\n",
      "Iteration 17821, loss = 0.06663260\n",
      "Iteration 17822, loss = 0.06663027\n",
      "Iteration 17823, loss = 0.06662853\n",
      "Iteration 17824, loss = 0.06662322\n",
      "Iteration 17825, loss = 0.06661947\n",
      "Iteration 17826, loss = 0.06661497\n",
      "Iteration 17827, loss = 0.06660579\n",
      "Iteration 17828, loss = 0.06659798\n",
      "Iteration 17829, loss = 0.06659767\n",
      "Iteration 17830, loss = 0.06659020\n",
      "Iteration 17831, loss = 0.06657862\n",
      "Iteration 17832, loss = 0.06657596\n",
      "Iteration 17833, loss = 0.06657348\n",
      "Iteration 17834, loss = 0.06656915\n",
      "Iteration 17835, loss = 0.06656403\n",
      "Iteration 17836, loss = 0.06655644\n",
      "Iteration 17837, loss = 0.06655138\n",
      "Iteration 17838, loss = 0.06654776\n",
      "Iteration 17839, loss = 0.06654335\n",
      "Iteration 17840, loss = 0.06653777\n",
      "Iteration 17841, loss = 0.06653275\n",
      "Iteration 17842, loss = 0.06652577\n",
      "Iteration 17843, loss = 0.06652173\n",
      "Iteration 17844, loss = 0.06651732\n",
      "Iteration 17845, loss = 0.06651211\n",
      "Iteration 17846, loss = 0.06650791\n",
      "Iteration 17847, loss = 0.06650671\n",
      "Iteration 17848, loss = 0.06650038\n",
      "Iteration 17849, loss = 0.06649512\n",
      "Iteration 17850, loss = 0.06648896\n",
      "Iteration 17851, loss = 0.06648618\n",
      "Iteration 17852, loss = 0.06647824\n",
      "Iteration 17853, loss = 0.06647111\n",
      "Iteration 17854, loss = 0.06646473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17855, loss = 0.06646414\n",
      "Iteration 17856, loss = 0.06646228\n",
      "Iteration 17857, loss = 0.06645515\n",
      "Iteration 17858, loss = 0.06644720\n",
      "Iteration 17859, loss = 0.06643866\n",
      "Iteration 17860, loss = 0.06643474\n",
      "Iteration 17861, loss = 0.06643771\n",
      "Iteration 17862, loss = 0.06643259\n",
      "Iteration 17863, loss = 0.06641806\n",
      "Iteration 17864, loss = 0.06641981\n",
      "Iteration 17865, loss = 0.06641943\n",
      "Iteration 17866, loss = 0.06641255\n",
      "Iteration 17867, loss = 0.06640155\n",
      "Iteration 17868, loss = 0.06639748\n",
      "Iteration 17869, loss = 0.06639906\n",
      "Iteration 17870, loss = 0.06639391\n",
      "Iteration 17871, loss = 0.06638264\n",
      "Iteration 17872, loss = 0.06637761\n",
      "Iteration 17873, loss = 0.06637266\n",
      "Iteration 17874, loss = 0.06636704\n",
      "Iteration 17875, loss = 0.06636459\n",
      "Iteration 17876, loss = 0.06635694\n",
      "Iteration 17877, loss = 0.06635549\n",
      "Iteration 17878, loss = 0.06634995\n",
      "Iteration 17879, loss = 0.06634284\n",
      "Iteration 17880, loss = 0.06634004\n",
      "Iteration 17881, loss = 0.06633542\n",
      "Iteration 17882, loss = 0.06633071\n",
      "Iteration 17883, loss = 0.06632287\n",
      "Iteration 17884, loss = 0.06632075\n",
      "Iteration 17885, loss = 0.06632108\n",
      "Iteration 17886, loss = 0.06631500\n",
      "Iteration 17887, loss = 0.06630661\n",
      "Iteration 17888, loss = 0.06630125\n",
      "Iteration 17889, loss = 0.06629425\n",
      "Iteration 17890, loss = 0.06628814\n",
      "Iteration 17891, loss = 0.06628814\n",
      "Iteration 17892, loss = 0.06628225\n",
      "Iteration 17893, loss = 0.06627808\n",
      "Iteration 17894, loss = 0.06626741\n",
      "Iteration 17895, loss = 0.06625869\n",
      "Iteration 17896, loss = 0.06625952\n",
      "Iteration 17897, loss = 0.06625701\n",
      "Iteration 17898, loss = 0.06624922\n",
      "Iteration 17899, loss = 0.06624605\n",
      "Iteration 17900, loss = 0.06623892\n",
      "Iteration 17901, loss = 0.06623407\n",
      "Iteration 17902, loss = 0.06622890\n",
      "Iteration 17903, loss = 0.06622452\n",
      "Iteration 17904, loss = 0.06622148\n",
      "Iteration 17905, loss = 0.06621840\n",
      "Iteration 17906, loss = 0.06621435\n",
      "Iteration 17907, loss = 0.06620549\n",
      "Iteration 17908, loss = 0.06619813\n",
      "Iteration 17909, loss = 0.06619661\n",
      "Iteration 17910, loss = 0.06619057\n",
      "Iteration 17911, loss = 0.06618767\n",
      "Iteration 17912, loss = 0.06618285\n",
      "Iteration 17913, loss = 0.06617279\n",
      "Iteration 17914, loss = 0.06617312\n",
      "Iteration 17915, loss = 0.06616827\n",
      "Iteration 17916, loss = 0.06616169\n",
      "Iteration 17917, loss = 0.06616018\n",
      "Iteration 17918, loss = 0.06615162\n",
      "Iteration 17919, loss = 0.06614502\n",
      "Iteration 17920, loss = 0.06614159\n",
      "Iteration 17921, loss = 0.06614083\n",
      "Iteration 17922, loss = 0.06613434\n",
      "Iteration 17923, loss = 0.06612715\n",
      "Iteration 17924, loss = 0.06611938\n",
      "Iteration 17925, loss = 0.06611808\n",
      "Iteration 17926, loss = 0.06610988\n",
      "Iteration 17927, loss = 0.06610479\n",
      "Iteration 17928, loss = 0.06610424\n",
      "Iteration 17929, loss = 0.06609875\n",
      "Iteration 17930, loss = 0.06609162\n",
      "Iteration 17931, loss = 0.06608321\n",
      "Iteration 17932, loss = 0.06607784\n",
      "Iteration 17933, loss = 0.06607805\n",
      "Iteration 17934, loss = 0.06607264\n",
      "Iteration 17935, loss = 0.06606717\n",
      "Iteration 17936, loss = 0.06606359\n",
      "Iteration 17937, loss = 0.06605889\n",
      "Iteration 17938, loss = 0.06605473\n",
      "Iteration 17939, loss = 0.06605017\n",
      "Iteration 17940, loss = 0.06604656\n",
      "Iteration 17941, loss = 0.06604177\n",
      "Iteration 17942, loss = 0.06603298\n",
      "Iteration 17943, loss = 0.06602811\n",
      "Iteration 17944, loss = 0.06602327\n",
      "Iteration 17945, loss = 0.06601813\n",
      "Iteration 17946, loss = 0.06601631\n",
      "Iteration 17947, loss = 0.06600858\n",
      "Iteration 17948, loss = 0.06600365\n",
      "Iteration 17949, loss = 0.06599938\n",
      "Iteration 17950, loss = 0.06599364\n",
      "Iteration 17951, loss = 0.06599114\n",
      "Iteration 17952, loss = 0.06598800\n",
      "Iteration 17953, loss = 0.06598778\n",
      "Iteration 17954, loss = 0.06598001\n",
      "Iteration 17955, loss = 0.06596883\n",
      "Iteration 17956, loss = 0.06596496\n",
      "Iteration 17957, loss = 0.06596003\n",
      "Iteration 17958, loss = 0.06595738\n",
      "Iteration 17959, loss = 0.06595795\n",
      "Iteration 17960, loss = 0.06594900\n",
      "Iteration 17961, loss = 0.06593965\n",
      "Iteration 17962, loss = 0.06593814\n",
      "Iteration 17963, loss = 0.06593520\n",
      "Iteration 17964, loss = 0.06592862\n",
      "Iteration 17965, loss = 0.06592142\n",
      "Iteration 17966, loss = 0.06591635\n",
      "Iteration 17967, loss = 0.06590913\n",
      "Iteration 17968, loss = 0.06590806\n",
      "Iteration 17969, loss = 0.06590459\n",
      "Iteration 17970, loss = 0.06589875\n",
      "Iteration 17971, loss = 0.06588967\n",
      "Iteration 17972, loss = 0.06588911\n",
      "Iteration 17973, loss = 0.06588148\n",
      "Iteration 17974, loss = 0.06587803\n",
      "Iteration 17975, loss = 0.06587558\n",
      "Iteration 17976, loss = 0.06587285\n",
      "Iteration 17977, loss = 0.06586911\n",
      "Iteration 17978, loss = 0.06585919\n",
      "Iteration 17979, loss = 0.06585262\n",
      "Iteration 17980, loss = 0.06584640\n",
      "Iteration 17981, loss = 0.06584389\n",
      "Iteration 17982, loss = 0.06583685\n",
      "Iteration 17983, loss = 0.06583311\n",
      "Iteration 17984, loss = 0.06582882\n",
      "Iteration 17985, loss = 0.06582834\n",
      "Iteration 17986, loss = 0.06581794\n",
      "Iteration 17987, loss = 0.06581372\n",
      "Iteration 17988, loss = 0.06580757\n",
      "Iteration 17989, loss = 0.06580707\n",
      "Iteration 17990, loss = 0.06580100\n",
      "Iteration 17991, loss = 0.06579684\n",
      "Iteration 17992, loss = 0.06579009\n",
      "Iteration 17993, loss = 0.06578651\n",
      "Iteration 17994, loss = 0.06578166\n",
      "Iteration 17995, loss = 0.06577703\n",
      "Iteration 17996, loss = 0.06576671\n",
      "Iteration 17997, loss = 0.06575904\n",
      "Iteration 17998, loss = 0.06575370\n",
      "Iteration 17999, loss = 0.06575155\n",
      "Iteration 18000, loss = 0.06574406\n",
      "Iteration 18001, loss = 0.06573912\n",
      "Iteration 18002, loss = 0.06573594\n",
      "Iteration 18003, loss = 0.06573140\n",
      "Iteration 18004, loss = 0.06572449\n",
      "Iteration 18005, loss = 0.06572383\n",
      "Iteration 18006, loss = 0.06571986\n",
      "Iteration 18007, loss = 0.06571635\n",
      "Iteration 18008, loss = 0.06571165\n",
      "Iteration 18009, loss = 0.06570530\n",
      "Iteration 18010, loss = 0.06569884\n",
      "Iteration 18011, loss = 0.06569369\n",
      "Iteration 18012, loss = 0.06568614\n",
      "Iteration 18013, loss = 0.06568338\n",
      "Iteration 18014, loss = 0.06567920\n",
      "Iteration 18015, loss = 0.06567303\n",
      "Iteration 18016, loss = 0.06566542\n",
      "Iteration 18017, loss = 0.06566078\n",
      "Iteration 18018, loss = 0.06565527\n",
      "Iteration 18019, loss = 0.06565126\n",
      "Iteration 18020, loss = 0.06564495\n",
      "Iteration 18021, loss = 0.06564340\n",
      "Iteration 18022, loss = 0.06563608\n",
      "Iteration 18023, loss = 0.06563034\n",
      "Iteration 18024, loss = 0.06562768\n",
      "Iteration 18025, loss = 0.06562311\n",
      "Iteration 18026, loss = 0.06561534\n",
      "Iteration 18027, loss = 0.06561197\n",
      "Iteration 18028, loss = 0.06560731\n",
      "Iteration 18029, loss = 0.06560481\n",
      "Iteration 18030, loss = 0.06560031\n",
      "Iteration 18031, loss = 0.06559275\n",
      "Iteration 18032, loss = 0.06558864\n",
      "Iteration 18033, loss = 0.06558749\n",
      "Iteration 18034, loss = 0.06558236\n",
      "Iteration 18035, loss = 0.06557371\n",
      "Iteration 18036, loss = 0.06556566\n",
      "Iteration 18037, loss = 0.06556439\n",
      "Iteration 18038, loss = 0.06555944\n",
      "Iteration 18039, loss = 0.06555172\n",
      "Iteration 18040, loss = 0.06555104\n",
      "Iteration 18041, loss = 0.06554703\n",
      "Iteration 18042, loss = 0.06554095\n",
      "Iteration 18043, loss = 0.06553333\n",
      "Iteration 18044, loss = 0.06553022\n",
      "Iteration 18045, loss = 0.06552813\n",
      "Iteration 18046, loss = 0.06552141\n",
      "Iteration 18047, loss = 0.06551430\n",
      "Iteration 18048, loss = 0.06551127\n",
      "Iteration 18049, loss = 0.06550642\n",
      "Iteration 18050, loss = 0.06550211\n",
      "Iteration 18051, loss = 0.06549576\n",
      "Iteration 18052, loss = 0.06549001\n",
      "Iteration 18053, loss = 0.06548922\n",
      "Iteration 18054, loss = 0.06548349\n",
      "Iteration 18055, loss = 0.06547417\n",
      "Iteration 18056, loss = 0.06547013\n",
      "Iteration 18057, loss = 0.06546888\n",
      "Iteration 18058, loss = 0.06546433\n",
      "Iteration 18059, loss = 0.06545823\n",
      "Iteration 18060, loss = 0.06545279\n",
      "Iteration 18061, loss = 0.06545369\n",
      "Iteration 18062, loss = 0.06544994\n",
      "Iteration 18063, loss = 0.06544050\n",
      "Iteration 18064, loss = 0.06543797\n",
      "Iteration 18065, loss = 0.06543440\n",
      "Iteration 18066, loss = 0.06542736\n",
      "Iteration 18067, loss = 0.06541873\n",
      "Iteration 18068, loss = 0.06541537\n",
      "Iteration 18069, loss = 0.06541711\n",
      "Iteration 18070, loss = 0.06540973\n",
      "Iteration 18071, loss = 0.06540081\n",
      "Iteration 18072, loss = 0.06539497\n",
      "Iteration 18073, loss = 0.06539065\n",
      "Iteration 18074, loss = 0.06538591\n",
      "Iteration 18075, loss = 0.06537937\n",
      "Iteration 18076, loss = 0.06537268\n",
      "Iteration 18077, loss = 0.06536540\n",
      "Iteration 18078, loss = 0.06536231\n",
      "Iteration 18079, loss = 0.06535775\n",
      "Iteration 18080, loss = 0.06535055\n",
      "Iteration 18081, loss = 0.06534759\n",
      "Iteration 18082, loss = 0.06534449\n",
      "Iteration 18083, loss = 0.06534153\n",
      "Iteration 18084, loss = 0.06533544\n",
      "Iteration 18085, loss = 0.06532402\n",
      "Iteration 18086, loss = 0.06532613\n",
      "Iteration 18087, loss = 0.06532470\n",
      "Iteration 18088, loss = 0.06531917\n",
      "Iteration 18089, loss = 0.06531237\n",
      "Iteration 18090, loss = 0.06530662\n",
      "Iteration 18091, loss = 0.06530133\n",
      "Iteration 18092, loss = 0.06529733\n",
      "Iteration 18093, loss = 0.06529413\n",
      "Iteration 18094, loss = 0.06528493\n",
      "Iteration 18095, loss = 0.06527528\n",
      "Iteration 18096, loss = 0.06527973\n",
      "Iteration 18097, loss = 0.06527494\n",
      "Iteration 18098, loss = 0.06527056\n",
      "Iteration 18099, loss = 0.06526154\n",
      "Iteration 18100, loss = 0.06525607\n",
      "Iteration 18101, loss = 0.06525184\n",
      "Iteration 18102, loss = 0.06524755\n",
      "Iteration 18103, loss = 0.06524366\n",
      "Iteration 18104, loss = 0.06524019\n",
      "Iteration 18105, loss = 0.06523441\n",
      "Iteration 18106, loss = 0.06522846\n",
      "Iteration 18107, loss = 0.06522020\n",
      "Iteration 18108, loss = 0.06521197\n",
      "Iteration 18109, loss = 0.06521447\n",
      "Iteration 18110, loss = 0.06520837\n",
      "Iteration 18111, loss = 0.06519926\n",
      "Iteration 18112, loss = 0.06519483\n",
      "Iteration 18113, loss = 0.06518986\n",
      "Iteration 18114, loss = 0.06518383\n",
      "Iteration 18115, loss = 0.06517887\n",
      "Iteration 18116, loss = 0.06517657\n",
      "Iteration 18117, loss = 0.06517270\n",
      "Iteration 18118, loss = 0.06516555\n",
      "Iteration 18119, loss = 0.06516163\n",
      "Iteration 18120, loss = 0.06515621\n",
      "Iteration 18121, loss = 0.06515028\n",
      "Iteration 18122, loss = 0.06514349\n",
      "Iteration 18123, loss = 0.06514144\n",
      "Iteration 18124, loss = 0.06513642\n",
      "Iteration 18125, loss = 0.06513147\n",
      "Iteration 18126, loss = 0.06512302\n",
      "Iteration 18127, loss = 0.06512003\n",
      "Iteration 18128, loss = 0.06511648\n",
      "Iteration 18129, loss = 0.06511122\n",
      "Iteration 18130, loss = 0.06510688\n",
      "Iteration 18131, loss = 0.06510004\n",
      "Iteration 18132, loss = 0.06510034\n",
      "Iteration 18133, loss = 0.06509449\n",
      "Iteration 18134, loss = 0.06508779\n",
      "Iteration 18135, loss = 0.06508711\n",
      "Iteration 18136, loss = 0.06508517\n",
      "Iteration 18137, loss = 0.06507645\n",
      "Iteration 18138, loss = 0.06506812\n",
      "Iteration 18139, loss = 0.06506106\n",
      "Iteration 18140, loss = 0.06506358\n",
      "Iteration 18141, loss = 0.06506097\n",
      "Iteration 18142, loss = 0.06505383\n",
      "Iteration 18143, loss = 0.06504796\n",
      "Iteration 18144, loss = 0.06504156\n",
      "Iteration 18145, loss = 0.06503652\n",
      "Iteration 18146, loss = 0.06503103\n",
      "Iteration 18147, loss = 0.06502476\n",
      "Iteration 18148, loss = 0.06502017\n",
      "Iteration 18149, loss = 0.06501554\n",
      "Iteration 18150, loss = 0.06501518\n",
      "Iteration 18151, loss = 0.06500938\n",
      "Iteration 18152, loss = 0.06500107\n",
      "Iteration 18153, loss = 0.06499737\n",
      "Iteration 18154, loss = 0.06499503\n",
      "Iteration 18155, loss = 0.06498945\n",
      "Iteration 18156, loss = 0.06498768\n",
      "Iteration 18157, loss = 0.06497815\n",
      "Iteration 18158, loss = 0.06496743\n",
      "Iteration 18159, loss = 0.06496788\n",
      "Iteration 18160, loss = 0.06496250\n",
      "Iteration 18161, loss = 0.06495429\n",
      "Iteration 18162, loss = 0.06494960\n",
      "Iteration 18163, loss = 0.06494710\n",
      "Iteration 18164, loss = 0.06493961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18165, loss = 0.06493614\n",
      "Iteration 18166, loss = 0.06493322\n",
      "Iteration 18167, loss = 0.06492895\n",
      "Iteration 18168, loss = 0.06492279\n",
      "Iteration 18169, loss = 0.06491958\n",
      "Iteration 18170, loss = 0.06491306\n",
      "Iteration 18171, loss = 0.06491193\n",
      "Iteration 18172, loss = 0.06490542\n",
      "Iteration 18173, loss = 0.06490179\n",
      "Iteration 18174, loss = 0.06489576\n",
      "Iteration 18175, loss = 0.06488992\n",
      "Iteration 18176, loss = 0.06488486\n",
      "Iteration 18177, loss = 0.06488567\n",
      "Iteration 18178, loss = 0.06487638\n",
      "Iteration 18179, loss = 0.06486536\n",
      "Iteration 18180, loss = 0.06486403\n",
      "Iteration 18181, loss = 0.06486031\n",
      "Iteration 18182, loss = 0.06485358\n",
      "Iteration 18183, loss = 0.06484928\n",
      "Iteration 18184, loss = 0.06484549\n",
      "Iteration 18185, loss = 0.06483671\n",
      "Iteration 18186, loss = 0.06483333\n",
      "Iteration 18187, loss = 0.06482958\n",
      "Iteration 18188, loss = 0.06482131\n",
      "Iteration 18189, loss = 0.06481867\n",
      "Iteration 18190, loss = 0.06480937\n",
      "Iteration 18191, loss = 0.06480754\n",
      "Iteration 18192, loss = 0.06480330\n",
      "Iteration 18193, loss = 0.06479762\n",
      "Iteration 18194, loss = 0.06479336\n",
      "Iteration 18195, loss = 0.06478844\n",
      "Iteration 18196, loss = 0.06478661\n",
      "Iteration 18197, loss = 0.06478255\n",
      "Iteration 18198, loss = 0.06477366\n",
      "Iteration 18199, loss = 0.06477032\n",
      "Iteration 18200, loss = 0.06476176\n",
      "Iteration 18201, loss = 0.06475724\n",
      "Iteration 18202, loss = 0.06475381\n",
      "Iteration 18203, loss = 0.06475204\n",
      "Iteration 18204, loss = 0.06474812\n",
      "Iteration 18205, loss = 0.06474094\n",
      "Iteration 18206, loss = 0.06473274\n",
      "Iteration 18207, loss = 0.06473267\n",
      "Iteration 18208, loss = 0.06472592\n",
      "Iteration 18209, loss = 0.06472117\n",
      "Iteration 18210, loss = 0.06471339\n",
      "Iteration 18211, loss = 0.06470421\n",
      "Iteration 18212, loss = 0.06470187\n",
      "Iteration 18213, loss = 0.06469968\n",
      "Iteration 18214, loss = 0.06469612\n",
      "Iteration 18215, loss = 0.06469207\n",
      "Iteration 18216, loss = 0.06468524\n",
      "Iteration 18217, loss = 0.06467698\n",
      "Iteration 18218, loss = 0.06467323\n",
      "Iteration 18219, loss = 0.06467161\n",
      "Iteration 18220, loss = 0.06466595\n",
      "Iteration 18221, loss = 0.06465850\n",
      "Iteration 18222, loss = 0.06465736\n",
      "Iteration 18223, loss = 0.06465367\n",
      "Iteration 18224, loss = 0.06464738\n",
      "Iteration 18225, loss = 0.06464198\n",
      "Iteration 18226, loss = 0.06463087\n",
      "Iteration 18227, loss = 0.06463046\n",
      "Iteration 18228, loss = 0.06462635\n",
      "Iteration 18229, loss = 0.06462331\n",
      "Iteration 18230, loss = 0.06461875\n",
      "Iteration 18231, loss = 0.06461370\n",
      "Iteration 18232, loss = 0.06461029\n",
      "Iteration 18233, loss = 0.06460258\n",
      "Iteration 18234, loss = 0.06459481\n",
      "Iteration 18235, loss = 0.06459062\n",
      "Iteration 18236, loss = 0.06458790\n",
      "Iteration 18237, loss = 0.06458240\n",
      "Iteration 18238, loss = 0.06457889\n",
      "Iteration 18239, loss = 0.06457296\n",
      "Iteration 18240, loss = 0.06456587\n",
      "Iteration 18241, loss = 0.06456310\n",
      "Iteration 18242, loss = 0.06455802\n",
      "Iteration 18243, loss = 0.06455580\n",
      "Iteration 18244, loss = 0.06454748\n",
      "Iteration 18245, loss = 0.06454330\n",
      "Iteration 18246, loss = 0.06454218\n",
      "Iteration 18247, loss = 0.06453588\n",
      "Iteration 18248, loss = 0.06452863\n",
      "Iteration 18249, loss = 0.06452295\n",
      "Iteration 18250, loss = 0.06451950\n",
      "Iteration 18251, loss = 0.06451843\n",
      "Iteration 18252, loss = 0.06451010\n",
      "Iteration 18253, loss = 0.06450031\n",
      "Iteration 18254, loss = 0.06449683\n",
      "Iteration 18255, loss = 0.06449503\n",
      "Iteration 18256, loss = 0.06448832\n",
      "Iteration 18257, loss = 0.06447957\n",
      "Iteration 18258, loss = 0.06447468\n",
      "Iteration 18259, loss = 0.06446894\n",
      "Iteration 18260, loss = 0.06446458\n",
      "Iteration 18261, loss = 0.06446001\n",
      "Iteration 18262, loss = 0.06445776\n",
      "Iteration 18263, loss = 0.06445354\n",
      "Iteration 18264, loss = 0.06444572\n",
      "Iteration 18265, loss = 0.06444018\n",
      "Iteration 18266, loss = 0.06443736\n",
      "Iteration 18267, loss = 0.06443089\n",
      "Iteration 18268, loss = 0.06443069\n",
      "Iteration 18269, loss = 0.06442609\n",
      "Iteration 18270, loss = 0.06442092\n",
      "Iteration 18271, loss = 0.06441863\n",
      "Iteration 18272, loss = 0.06441522\n",
      "Iteration 18273, loss = 0.06440977\n",
      "Iteration 18274, loss = 0.06440278\n",
      "Iteration 18275, loss = 0.06439381\n",
      "Iteration 18276, loss = 0.06439199\n",
      "Iteration 18277, loss = 0.06438723\n",
      "Iteration 18278, loss = 0.06438440\n",
      "Iteration 18279, loss = 0.06437849\n",
      "Iteration 18280, loss = 0.06437182\n",
      "Iteration 18281, loss = 0.06436506\n",
      "Iteration 18282, loss = 0.06436064\n",
      "Iteration 18283, loss = 0.06435243\n",
      "Iteration 18284, loss = 0.06434722\n",
      "Iteration 18285, loss = 0.06434885\n",
      "Iteration 18286, loss = 0.06434688\n",
      "Iteration 18287, loss = 0.06434036\n",
      "Iteration 18288, loss = 0.06433398\n",
      "Iteration 18289, loss = 0.06432918\n",
      "Iteration 18290, loss = 0.06432293\n",
      "Iteration 18291, loss = 0.06431526\n",
      "Iteration 18292, loss = 0.06430961\n",
      "Iteration 18293, loss = 0.06430928\n",
      "Iteration 18294, loss = 0.06430520\n",
      "Iteration 18295, loss = 0.06429927\n",
      "Iteration 18296, loss = 0.06429505\n",
      "Iteration 18297, loss = 0.06428942\n",
      "Iteration 18298, loss = 0.06428440\n",
      "Iteration 18299, loss = 0.06427979\n",
      "Iteration 18300, loss = 0.06427677\n",
      "Iteration 18301, loss = 0.06427397\n",
      "Iteration 18302, loss = 0.06426899\n",
      "Iteration 18303, loss = 0.06426484\n",
      "Iteration 18304, loss = 0.06425663\n",
      "Iteration 18305, loss = 0.06425234\n",
      "Iteration 18306, loss = 0.06424469\n",
      "Iteration 18307, loss = 0.06424686\n",
      "Iteration 18308, loss = 0.06424372\n",
      "Iteration 18309, loss = 0.06423311\n",
      "Iteration 18310, loss = 0.06422727\n",
      "Iteration 18311, loss = 0.06422501\n",
      "Iteration 18312, loss = 0.06422189\n",
      "Iteration 18313, loss = 0.06421741\n",
      "Iteration 18314, loss = 0.06421086\n",
      "Iteration 18315, loss = 0.06420785\n",
      "Iteration 18316, loss = 0.06419929\n",
      "Iteration 18317, loss = 0.06419347\n",
      "Iteration 18318, loss = 0.06418536\n",
      "Iteration 18319, loss = 0.06418177\n",
      "Iteration 18320, loss = 0.06417789\n",
      "Iteration 18321, loss = 0.06416888\n",
      "Iteration 18322, loss = 0.06416393\n",
      "Iteration 18323, loss = 0.06415862\n",
      "Iteration 18324, loss = 0.06415442\n",
      "Iteration 18325, loss = 0.06415172\n",
      "Iteration 18326, loss = 0.06414398\n",
      "Iteration 18327, loss = 0.06414186\n",
      "Iteration 18328, loss = 0.06413430\n",
      "Iteration 18329, loss = 0.06413120\n",
      "Iteration 18330, loss = 0.06412884\n",
      "Iteration 18331, loss = 0.06412231\n",
      "Iteration 18332, loss = 0.06411697\n",
      "Iteration 18333, loss = 0.06411243\n",
      "Iteration 18334, loss = 0.06410578\n",
      "Iteration 18335, loss = 0.06410218\n",
      "Iteration 18336, loss = 0.06409993\n",
      "Iteration 18337, loss = 0.06409709\n",
      "Iteration 18338, loss = 0.06408960\n",
      "Iteration 18339, loss = 0.06408339\n",
      "Iteration 18340, loss = 0.06408081\n",
      "Iteration 18341, loss = 0.06407367\n",
      "Iteration 18342, loss = 0.06407211\n",
      "Iteration 18343, loss = 0.06406839\n",
      "Iteration 18344, loss = 0.06406075\n",
      "Iteration 18345, loss = 0.06405689\n",
      "Iteration 18346, loss = 0.06405465\n",
      "Iteration 18347, loss = 0.06404978\n",
      "Iteration 18348, loss = 0.06404513\n",
      "Iteration 18349, loss = 0.06403961\n",
      "Iteration 18350, loss = 0.06403677\n",
      "Iteration 18351, loss = 0.06403472\n",
      "Iteration 18352, loss = 0.06402839\n",
      "Iteration 18353, loss = 0.06402569\n",
      "Iteration 18354, loss = 0.06402366\n",
      "Iteration 18355, loss = 0.06401705\n",
      "Iteration 18356, loss = 0.06400877\n",
      "Iteration 18357, loss = 0.06400155\n",
      "Iteration 18358, loss = 0.06399734\n",
      "Iteration 18359, loss = 0.06399465\n",
      "Iteration 18360, loss = 0.06398509\n",
      "Iteration 18361, loss = 0.06398489\n",
      "Iteration 18362, loss = 0.06398379\n",
      "Iteration 18363, loss = 0.06397498\n",
      "Iteration 18364, loss = 0.06396469\n",
      "Iteration 18365, loss = 0.06396570\n",
      "Iteration 18366, loss = 0.06396070\n",
      "Iteration 18367, loss = 0.06395501\n",
      "Iteration 18368, loss = 0.06395115\n",
      "Iteration 18369, loss = 0.06394852\n",
      "Iteration 18370, loss = 0.06394247\n",
      "Iteration 18371, loss = 0.06393536\n",
      "Iteration 18372, loss = 0.06392909\n",
      "Iteration 18373, loss = 0.06392073\n",
      "Iteration 18374, loss = 0.06392286\n",
      "Iteration 18375, loss = 0.06391570\n",
      "Iteration 18376, loss = 0.06390717\n",
      "Iteration 18377, loss = 0.06390371\n",
      "Iteration 18378, loss = 0.06389992\n",
      "Iteration 18379, loss = 0.06390153\n",
      "Iteration 18380, loss = 0.06389940\n",
      "Iteration 18381, loss = 0.06389199\n",
      "Iteration 18382, loss = 0.06388485\n",
      "Iteration 18383, loss = 0.06387871\n",
      "Iteration 18384, loss = 0.06387193\n",
      "Iteration 18385, loss = 0.06386000\n",
      "Iteration 18386, loss = 0.06385955\n",
      "Iteration 18387, loss = 0.06385777\n",
      "Iteration 18388, loss = 0.06384855\n",
      "Iteration 18389, loss = 0.06384051\n",
      "Iteration 18390, loss = 0.06384182\n",
      "Iteration 18391, loss = 0.06383972\n",
      "Iteration 18392, loss = 0.06383471\n",
      "Iteration 18393, loss = 0.06382479\n",
      "Iteration 18394, loss = 0.06381888\n",
      "Iteration 18395, loss = 0.06381523\n",
      "Iteration 18396, loss = 0.06381018\n",
      "Iteration 18397, loss = 0.06380358\n",
      "Iteration 18398, loss = 0.06379754\n",
      "Iteration 18399, loss = 0.06379690\n",
      "Iteration 18400, loss = 0.06379133\n",
      "Iteration 18401, loss = 0.06378618\n",
      "Iteration 18402, loss = 0.06378139\n",
      "Iteration 18403, loss = 0.06378190\n",
      "Iteration 18404, loss = 0.06377887\n",
      "Iteration 18405, loss = 0.06377482\n",
      "Iteration 18406, loss = 0.06376955\n",
      "Iteration 18407, loss = 0.06376129\n",
      "Iteration 18408, loss = 0.06375392\n",
      "Iteration 18409, loss = 0.06374746\n",
      "Iteration 18410, loss = 0.06374442\n",
      "Iteration 18411, loss = 0.06373781\n",
      "Iteration 18412, loss = 0.06373452\n",
      "Iteration 18413, loss = 0.06373063\n",
      "Iteration 18414, loss = 0.06372735\n",
      "Iteration 18415, loss = 0.06372320\n",
      "Iteration 18416, loss = 0.06371717\n",
      "Iteration 18417, loss = 0.06371118\n",
      "Iteration 18418, loss = 0.06370818\n",
      "Iteration 18419, loss = 0.06370600\n",
      "Iteration 18420, loss = 0.06369792\n",
      "Iteration 18421, loss = 0.06368995\n",
      "Iteration 18422, loss = 0.06368752\n",
      "Iteration 18423, loss = 0.06368163\n",
      "Iteration 18424, loss = 0.06367844\n",
      "Iteration 18425, loss = 0.06366989\n",
      "Iteration 18426, loss = 0.06366558\n",
      "Iteration 18427, loss = 0.06366514\n",
      "Iteration 18428, loss = 0.06366102\n",
      "Iteration 18429, loss = 0.06365915\n",
      "Iteration 18430, loss = 0.06365451\n",
      "Iteration 18431, loss = 0.06364643\n",
      "Iteration 18432, loss = 0.06363492\n",
      "Iteration 18433, loss = 0.06363554\n",
      "Iteration 18434, loss = 0.06363517\n",
      "Iteration 18435, loss = 0.06362719\n",
      "Iteration 18436, loss = 0.06362374\n",
      "Iteration 18437, loss = 0.06361953\n",
      "Iteration 18438, loss = 0.06361407\n",
      "Iteration 18439, loss = 0.06361013\n",
      "Iteration 18440, loss = 0.06360569\n",
      "Iteration 18441, loss = 0.06360056\n",
      "Iteration 18442, loss = 0.06359323\n",
      "Iteration 18443, loss = 0.06358703\n",
      "Iteration 18444, loss = 0.06358052\n",
      "Iteration 18445, loss = 0.06357603\n",
      "Iteration 18446, loss = 0.06357401\n",
      "Iteration 18447, loss = 0.06356343\n",
      "Iteration 18448, loss = 0.06356006\n",
      "Iteration 18449, loss = 0.06355296\n",
      "Iteration 18450, loss = 0.06354964\n",
      "Iteration 18451, loss = 0.06354998\n",
      "Iteration 18452, loss = 0.06354197\n",
      "Iteration 18453, loss = 0.06354039\n",
      "Iteration 18454, loss = 0.06353722\n",
      "Iteration 18455, loss = 0.06353180\n",
      "Iteration 18456, loss = 0.06352716\n",
      "Iteration 18457, loss = 0.06352198\n",
      "Iteration 18458, loss = 0.06351465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18459, loss = 0.06350678\n",
      "Iteration 18460, loss = 0.06350642\n",
      "Iteration 18461, loss = 0.06350290\n",
      "Iteration 18462, loss = 0.06349522\n",
      "Iteration 18463, loss = 0.06349094\n",
      "Iteration 18464, loss = 0.06348552\n",
      "Iteration 18465, loss = 0.06348177\n",
      "Iteration 18466, loss = 0.06347927\n",
      "Iteration 18467, loss = 0.06347309\n",
      "Iteration 18468, loss = 0.06346269\n",
      "Iteration 18469, loss = 0.06346212\n",
      "Iteration 18470, loss = 0.06345930\n",
      "Iteration 18471, loss = 0.06345544\n",
      "Iteration 18472, loss = 0.06345075\n",
      "Iteration 18473, loss = 0.06344557\n",
      "Iteration 18474, loss = 0.06343723\n",
      "Iteration 18475, loss = 0.06342749\n",
      "Iteration 18476, loss = 0.06343155\n",
      "Iteration 18477, loss = 0.06342938\n",
      "Iteration 18478, loss = 0.06341935\n",
      "Iteration 18479, loss = 0.06341783\n",
      "Iteration 18480, loss = 0.06341397\n",
      "Iteration 18481, loss = 0.06341280\n",
      "Iteration 18482, loss = 0.06340936\n",
      "Iteration 18483, loss = 0.06340318\n",
      "Iteration 18484, loss = 0.06339629\n",
      "Iteration 18485, loss = 0.06339398\n",
      "Iteration 18486, loss = 0.06338646\n",
      "Iteration 18487, loss = 0.06337918\n",
      "Iteration 18488, loss = 0.06337328\n",
      "Iteration 18489, loss = 0.06336953\n",
      "Iteration 18490, loss = 0.06336546\n",
      "Iteration 18491, loss = 0.06336186\n",
      "Iteration 18492, loss = 0.06335640\n",
      "Iteration 18493, loss = 0.06335037\n",
      "Iteration 18494, loss = 0.06334522\n",
      "Iteration 18495, loss = 0.06334102\n",
      "Iteration 18496, loss = 0.06333858\n",
      "Iteration 18497, loss = 0.06333728\n",
      "Iteration 18498, loss = 0.06333152\n",
      "Iteration 18499, loss = 0.06332523\n",
      "Iteration 18500, loss = 0.06332343\n",
      "Iteration 18501, loss = 0.06331731\n",
      "Iteration 18502, loss = 0.06330905\n",
      "Iteration 18503, loss = 0.06330671\n",
      "Iteration 18504, loss = 0.06330056\n",
      "Iteration 18505, loss = 0.06329032\n",
      "Iteration 18506, loss = 0.06329319\n",
      "Iteration 18507, loss = 0.06329158\n",
      "Iteration 18508, loss = 0.06328419\n",
      "Iteration 18509, loss = 0.06327508\n",
      "Iteration 18510, loss = 0.06327290\n",
      "Iteration 18511, loss = 0.06326463\n",
      "Iteration 18512, loss = 0.06326732\n",
      "Iteration 18513, loss = 0.06326456\n",
      "Iteration 18514, loss = 0.06325877\n",
      "Iteration 18515, loss = 0.06325151\n",
      "Iteration 18516, loss = 0.06324372\n",
      "Iteration 18517, loss = 0.06324064\n",
      "Iteration 18518, loss = 0.06323717\n",
      "Iteration 18519, loss = 0.06323011\n",
      "Iteration 18520, loss = 0.06322221\n",
      "Iteration 18521, loss = 0.06321908\n",
      "Iteration 18522, loss = 0.06321456\n",
      "Iteration 18523, loss = 0.06321156\n",
      "Iteration 18524, loss = 0.06320213\n",
      "Iteration 18525, loss = 0.06319365\n",
      "Iteration 18526, loss = 0.06319742\n",
      "Iteration 18527, loss = 0.06319130\n",
      "Iteration 18528, loss = 0.06318185\n",
      "Iteration 18529, loss = 0.06317793\n",
      "Iteration 18530, loss = 0.06317558\n",
      "Iteration 18531, loss = 0.06317170\n",
      "Iteration 18532, loss = 0.06316777\n",
      "Iteration 18533, loss = 0.06315997\n",
      "Iteration 18534, loss = 0.06316054\n",
      "Iteration 18535, loss = 0.06315458\n",
      "Iteration 18536, loss = 0.06314452\n",
      "Iteration 18537, loss = 0.06313933\n",
      "Iteration 18538, loss = 0.06313598\n",
      "Iteration 18539, loss = 0.06313155\n",
      "Iteration 18540, loss = 0.06312549\n",
      "Iteration 18541, loss = 0.06312355\n",
      "Iteration 18542, loss = 0.06311803\n",
      "Iteration 18543, loss = 0.06311502\n",
      "Iteration 18544, loss = 0.06311224\n",
      "Iteration 18545, loss = 0.06310821\n",
      "Iteration 18546, loss = 0.06310116\n",
      "Iteration 18547, loss = 0.06309634\n",
      "Iteration 18548, loss = 0.06309270\n",
      "Iteration 18549, loss = 0.06308418\n",
      "Iteration 18550, loss = 0.06308100\n",
      "Iteration 18551, loss = 0.06307866\n",
      "Iteration 18552, loss = 0.06306838\n",
      "Iteration 18553, loss = 0.06306595\n",
      "Iteration 18554, loss = 0.06306217\n",
      "Iteration 18555, loss = 0.06305771\n",
      "Iteration 18556, loss = 0.06305403\n",
      "Iteration 18557, loss = 0.06304830\n",
      "Iteration 18558, loss = 0.06304360\n",
      "Iteration 18559, loss = 0.06303706\n",
      "Iteration 18560, loss = 0.06303594\n",
      "Iteration 18561, loss = 0.06302556\n",
      "Iteration 18562, loss = 0.06302739\n",
      "Iteration 18563, loss = 0.06302501\n",
      "Iteration 18564, loss = 0.06302181\n",
      "Iteration 18565, loss = 0.06301836\n",
      "Iteration 18566, loss = 0.06301546\n",
      "Iteration 18567, loss = 0.06300659\n",
      "Iteration 18568, loss = 0.06299542\n",
      "Iteration 18569, loss = 0.06299141\n",
      "Iteration 18570, loss = 0.06298661\n",
      "Iteration 18571, loss = 0.06298475\n",
      "Iteration 18572, loss = 0.06298132\n",
      "Iteration 18573, loss = 0.06297949\n",
      "Iteration 18574, loss = 0.06297346\n",
      "Iteration 18575, loss = 0.06296695\n",
      "Iteration 18576, loss = 0.06295964\n",
      "Iteration 18577, loss = 0.06295258\n",
      "Iteration 18578, loss = 0.06294642\n",
      "Iteration 18579, loss = 0.06294672\n",
      "Iteration 18580, loss = 0.06294013\n",
      "Iteration 18581, loss = 0.06293279\n",
      "Iteration 18582, loss = 0.06292961\n",
      "Iteration 18583, loss = 0.06292893\n",
      "Iteration 18584, loss = 0.06292278\n",
      "Iteration 18585, loss = 0.06291742\n",
      "Iteration 18586, loss = 0.06291193\n",
      "Iteration 18587, loss = 0.06291205\n",
      "Iteration 18588, loss = 0.06290482\n",
      "Iteration 18589, loss = 0.06290344\n",
      "Iteration 18590, loss = 0.06289962\n",
      "Iteration 18591, loss = 0.06289327\n",
      "Iteration 18592, loss = 0.06289373\n",
      "Iteration 18593, loss = 0.06288505\n",
      "Iteration 18594, loss = 0.06287697\n",
      "Iteration 18595, loss = 0.06287302\n",
      "Iteration 18596, loss = 0.06286495\n",
      "Iteration 18597, loss = 0.06286830\n",
      "Iteration 18598, loss = 0.06286818\n",
      "Iteration 18599, loss = 0.06285774\n",
      "Iteration 18600, loss = 0.06285224\n",
      "Iteration 18601, loss = 0.06284977\n",
      "Iteration 18602, loss = 0.06284687\n",
      "Iteration 18603, loss = 0.06284223\n",
      "Iteration 18604, loss = 0.06283607\n",
      "Iteration 18605, loss = 0.06283163\n",
      "Iteration 18606, loss = 0.06282625\n",
      "Iteration 18607, loss = 0.06281953\n",
      "Iteration 18608, loss = 0.06281681\n",
      "Iteration 18609, loss = 0.06281044\n",
      "Iteration 18610, loss = 0.06280312\n",
      "Iteration 18611, loss = 0.06279931\n",
      "Iteration 18612, loss = 0.06279250\n",
      "Iteration 18613, loss = 0.06278680\n",
      "Iteration 18614, loss = 0.06278171\n",
      "Iteration 18615, loss = 0.06277793\n",
      "Iteration 18616, loss = 0.06277082\n",
      "Iteration 18617, loss = 0.06276553\n",
      "Iteration 18618, loss = 0.06276294\n",
      "Iteration 18619, loss = 0.06275198\n",
      "Iteration 18620, loss = 0.06274400\n",
      "Iteration 18621, loss = 0.06274329\n",
      "Iteration 18622, loss = 0.06274061\n",
      "Iteration 18623, loss = 0.06272994\n",
      "Iteration 18624, loss = 0.06272347\n",
      "Iteration 18625, loss = 0.06271484\n",
      "Iteration 18626, loss = 0.06271013\n",
      "Iteration 18627, loss = 0.06270886\n",
      "Iteration 18628, loss = 0.06270166\n",
      "Iteration 18629, loss = 0.06269236\n",
      "Iteration 18630, loss = 0.06268738\n",
      "Iteration 18631, loss = 0.06268330\n",
      "Iteration 18632, loss = 0.06267758\n",
      "Iteration 18633, loss = 0.06267077\n",
      "Iteration 18634, loss = 0.06266965\n",
      "Iteration 18635, loss = 0.06266318\n",
      "Iteration 18636, loss = 0.06265781\n",
      "Iteration 18637, loss = 0.06265592\n",
      "Iteration 18638, loss = 0.06265235\n",
      "Iteration 18639, loss = 0.06264648\n",
      "Iteration 18640, loss = 0.06264254\n",
      "Iteration 18641, loss = 0.06263435\n",
      "Iteration 18642, loss = 0.06262808\n",
      "Iteration 18643, loss = 0.06262014\n",
      "Iteration 18644, loss = 0.06261559\n",
      "Iteration 18645, loss = 0.06261305\n",
      "Iteration 18646, loss = 0.06260752\n",
      "Iteration 18647, loss = 0.06260006\n",
      "Iteration 18648, loss = 0.06259590\n",
      "Iteration 18649, loss = 0.06258810\n",
      "Iteration 18650, loss = 0.06258465\n",
      "Iteration 18651, loss = 0.06258036\n",
      "Iteration 18652, loss = 0.06257493\n",
      "Iteration 18653, loss = 0.06257094\n",
      "Iteration 18654, loss = 0.06256465\n",
      "Iteration 18655, loss = 0.06255912\n",
      "Iteration 18656, loss = 0.06255223\n",
      "Iteration 18657, loss = 0.06254678\n",
      "Iteration 18658, loss = 0.06254145\n",
      "Iteration 18659, loss = 0.06253656\n",
      "Iteration 18660, loss = 0.06253417\n",
      "Iteration 18661, loss = 0.06252861\n",
      "Iteration 18662, loss = 0.06252390\n",
      "Iteration 18663, loss = 0.06251703\n",
      "Iteration 18664, loss = 0.06251393\n",
      "Iteration 18665, loss = 0.06250591\n",
      "Iteration 18666, loss = 0.06249897\n",
      "Iteration 18667, loss = 0.06250009\n",
      "Iteration 18668, loss = 0.06249623\n",
      "Iteration 18669, loss = 0.06248965\n",
      "Iteration 18670, loss = 0.06247993\n",
      "Iteration 18671, loss = 0.06247615\n",
      "Iteration 18672, loss = 0.06246987\n",
      "Iteration 18673, loss = 0.06246491\n",
      "Iteration 18674, loss = 0.06245505\n",
      "Iteration 18675, loss = 0.06245122\n",
      "Iteration 18676, loss = 0.06244479\n",
      "Iteration 18677, loss = 0.06243694\n",
      "Iteration 18678, loss = 0.06243341\n",
      "Iteration 18679, loss = 0.06242837\n",
      "Iteration 18680, loss = 0.06242560\n",
      "Iteration 18681, loss = 0.06241956\n",
      "Iteration 18682, loss = 0.06241628\n",
      "Iteration 18683, loss = 0.06240942\n",
      "Iteration 18684, loss = 0.06240645\n",
      "Iteration 18685, loss = 0.06240311\n",
      "Iteration 18686, loss = 0.06239684\n",
      "Iteration 18687, loss = 0.06238701\n",
      "Iteration 18688, loss = 0.06238208\n",
      "Iteration 18689, loss = 0.06238586\n",
      "Iteration 18690, loss = 0.06238017\n",
      "Iteration 18691, loss = 0.06236691\n",
      "Iteration 18692, loss = 0.06236354\n",
      "Iteration 18693, loss = 0.06236067\n",
      "Iteration 18694, loss = 0.06235910\n",
      "Iteration 18695, loss = 0.06235592\n",
      "Iteration 18696, loss = 0.06234865\n",
      "Iteration 18697, loss = 0.06234050\n",
      "Iteration 18698, loss = 0.06233402\n",
      "Iteration 18699, loss = 0.06232878\n",
      "Iteration 18700, loss = 0.06232031\n",
      "Iteration 18701, loss = 0.06231871\n",
      "Iteration 18702, loss = 0.06231658\n",
      "Iteration 18703, loss = 0.06230715\n",
      "Iteration 18704, loss = 0.06230046\n",
      "Iteration 18705, loss = 0.06229816\n",
      "Iteration 18706, loss = 0.06229324\n",
      "Iteration 18707, loss = 0.06228846\n",
      "Iteration 18708, loss = 0.06228110\n",
      "Iteration 18709, loss = 0.06227755\n",
      "Iteration 18710, loss = 0.06227388\n",
      "Iteration 18711, loss = 0.06226390\n",
      "Iteration 18712, loss = 0.06226114\n",
      "Iteration 18713, loss = 0.06225866\n",
      "Iteration 18714, loss = 0.06225124\n",
      "Iteration 18715, loss = 0.06224425\n",
      "Iteration 18716, loss = 0.06223681\n",
      "Iteration 18717, loss = 0.06223035\n",
      "Iteration 18718, loss = 0.06222679\n",
      "Iteration 18719, loss = 0.06221828\n",
      "Iteration 18720, loss = 0.06221574\n",
      "Iteration 18721, loss = 0.06221305\n",
      "Iteration 18722, loss = 0.06220873\n",
      "Iteration 18723, loss = 0.06220439\n",
      "Iteration 18724, loss = 0.06219746\n",
      "Iteration 18725, loss = 0.06219108\n",
      "Iteration 18726, loss = 0.06218583\n",
      "Iteration 18727, loss = 0.06218024\n",
      "Iteration 18728, loss = 0.06217741\n",
      "Iteration 18729, loss = 0.06216653\n",
      "Iteration 18730, loss = 0.06216242\n",
      "Iteration 18731, loss = 0.06215872\n",
      "Iteration 18732, loss = 0.06215461\n",
      "Iteration 18733, loss = 0.06214527\n",
      "Iteration 18734, loss = 0.06214558\n",
      "Iteration 18735, loss = 0.06214324\n",
      "Iteration 18736, loss = 0.06213388\n",
      "Iteration 18737, loss = 0.06212849\n",
      "Iteration 18738, loss = 0.06212171\n",
      "Iteration 18739, loss = 0.06212037\n",
      "Iteration 18740, loss = 0.06211445\n",
      "Iteration 18741, loss = 0.06210814\n",
      "Iteration 18742, loss = 0.06210861\n",
      "Iteration 18743, loss = 0.06209762\n",
      "Iteration 18744, loss = 0.06209033\n",
      "Iteration 18745, loss = 0.06208782\n",
      "Iteration 18746, loss = 0.06208384\n",
      "Iteration 18747, loss = 0.06207555\n",
      "Iteration 18748, loss = 0.06207007\n",
      "Iteration 18749, loss = 0.06206231\n",
      "Iteration 18750, loss = 0.06205924\n",
      "Iteration 18751, loss = 0.06205881\n",
      "Iteration 18752, loss = 0.06205225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18753, loss = 0.06204619\n",
      "Iteration 18754, loss = 0.06204022\n",
      "Iteration 18755, loss = 0.06203093\n",
      "Iteration 18756, loss = 0.06202310\n",
      "Iteration 18757, loss = 0.06202037\n",
      "Iteration 18758, loss = 0.06201238\n",
      "Iteration 18759, loss = 0.06200813\n",
      "Iteration 18760, loss = 0.06200361\n",
      "Iteration 18761, loss = 0.06199780\n",
      "Iteration 18762, loss = 0.06199214\n",
      "Iteration 18763, loss = 0.06198566\n",
      "Iteration 18764, loss = 0.06198127\n",
      "Iteration 18765, loss = 0.06197187\n",
      "Iteration 18766, loss = 0.06196789\n",
      "Iteration 18767, loss = 0.06196449\n",
      "Iteration 18768, loss = 0.06196012\n",
      "Iteration 18769, loss = 0.06195244\n",
      "Iteration 18770, loss = 0.06194986\n",
      "Iteration 18771, loss = 0.06194304\n",
      "Iteration 18772, loss = 0.06193513\n",
      "Iteration 18773, loss = 0.06192664\n",
      "Iteration 18774, loss = 0.06192273\n",
      "Iteration 18775, loss = 0.06191699\n",
      "Iteration 18776, loss = 0.06191565\n",
      "Iteration 18777, loss = 0.06190973\n",
      "Iteration 18778, loss = 0.06190423\n",
      "Iteration 18779, loss = 0.06189648\n",
      "Iteration 18780, loss = 0.06189280\n",
      "Iteration 18781, loss = 0.06189001\n",
      "Iteration 18782, loss = 0.06188007\n",
      "Iteration 18783, loss = 0.06187434\n",
      "Iteration 18784, loss = 0.06187015\n",
      "Iteration 18785, loss = 0.06186546\n",
      "Iteration 18786, loss = 0.06185706\n",
      "Iteration 18787, loss = 0.06185265\n",
      "Iteration 18788, loss = 0.06184594\n",
      "Iteration 18789, loss = 0.06184079\n",
      "Iteration 18790, loss = 0.06183582\n",
      "Iteration 18791, loss = 0.06182954\n",
      "Iteration 18792, loss = 0.06182571\n",
      "Iteration 18793, loss = 0.06181612\n",
      "Iteration 18794, loss = 0.06181880\n",
      "Iteration 18795, loss = 0.06181186\n",
      "Iteration 18796, loss = 0.06180639\n",
      "Iteration 18797, loss = 0.06180055\n",
      "Iteration 18798, loss = 0.06179908\n",
      "Iteration 18799, loss = 0.06179132\n",
      "Iteration 18800, loss = 0.06178808\n",
      "Iteration 18801, loss = 0.06178679\n",
      "Iteration 18802, loss = 0.06177947\n",
      "Iteration 18803, loss = 0.06177282\n",
      "Iteration 18804, loss = 0.06177252\n",
      "Iteration 18805, loss = 0.06176658\n",
      "Iteration 18806, loss = 0.06176185\n",
      "Iteration 18807, loss = 0.06175205\n",
      "Iteration 18808, loss = 0.06174838\n",
      "Iteration 18809, loss = 0.06174514\n",
      "Iteration 18810, loss = 0.06174135\n",
      "Iteration 18811, loss = 0.06173246\n",
      "Iteration 18812, loss = 0.06172611\n",
      "Iteration 18813, loss = 0.06171998\n",
      "Iteration 18814, loss = 0.06171003\n",
      "Iteration 18815, loss = 0.06171260\n",
      "Iteration 18816, loss = 0.06170784\n",
      "Iteration 18817, loss = 0.06169651\n",
      "Iteration 18818, loss = 0.06169729\n",
      "Iteration 18819, loss = 0.06169534\n",
      "Iteration 18820, loss = 0.06168929\n",
      "Iteration 18821, loss = 0.06168881\n",
      "Iteration 18822, loss = 0.06168538\n",
      "Iteration 18823, loss = 0.06167636\n",
      "Iteration 18824, loss = 0.06166842\n",
      "Iteration 18825, loss = 0.06166270\n",
      "Iteration 18826, loss = 0.06165244\n",
      "Iteration 18827, loss = 0.06165348\n",
      "Iteration 18828, loss = 0.06165070\n",
      "Iteration 18829, loss = 0.06164414\n",
      "Iteration 18830, loss = 0.06163541\n",
      "Iteration 18831, loss = 0.06163205\n",
      "Iteration 18832, loss = 0.06162700\n",
      "Iteration 18833, loss = 0.06162397\n",
      "Iteration 18834, loss = 0.06162070\n",
      "Iteration 18835, loss = 0.06161431\n",
      "Iteration 18836, loss = 0.06160482\n",
      "Iteration 18837, loss = 0.06159820\n",
      "Iteration 18838, loss = 0.06159414\n",
      "Iteration 18839, loss = 0.06158761\n",
      "Iteration 18840, loss = 0.06158328\n",
      "Iteration 18841, loss = 0.06157902\n",
      "Iteration 18842, loss = 0.06157491\n",
      "Iteration 18843, loss = 0.06157056\n",
      "Iteration 18844, loss = 0.06156156\n",
      "Iteration 18845, loss = 0.06155561\n",
      "Iteration 18846, loss = 0.06155155\n",
      "Iteration 18847, loss = 0.06154852\n",
      "Iteration 18848, loss = 0.06154164\n",
      "Iteration 18849, loss = 0.06153775\n",
      "Iteration 18850, loss = 0.06153351\n",
      "Iteration 18851, loss = 0.06152680\n",
      "Iteration 18852, loss = 0.06152585\n",
      "Iteration 18853, loss = 0.06152163\n",
      "Iteration 18854, loss = 0.06151022\n",
      "Iteration 18855, loss = 0.06150640\n",
      "Iteration 18856, loss = 0.06149980\n",
      "Iteration 18857, loss = 0.06149339\n",
      "Iteration 18858, loss = 0.06149077\n",
      "Iteration 18859, loss = 0.06148488\n",
      "Iteration 18860, loss = 0.06147781\n",
      "Iteration 18861, loss = 0.06147544\n",
      "Iteration 18862, loss = 0.06147020\n",
      "Iteration 18863, loss = 0.06146810\n",
      "Iteration 18864, loss = 0.06146109\n",
      "Iteration 18865, loss = 0.06145904\n",
      "Iteration 18866, loss = 0.06145043\n",
      "Iteration 18867, loss = 0.06144817\n",
      "Iteration 18868, loss = 0.06144479\n",
      "Iteration 18869, loss = 0.06144014\n",
      "Iteration 18870, loss = 0.06143412\n",
      "Iteration 18871, loss = 0.06142717\n",
      "Iteration 18872, loss = 0.06142148\n",
      "Iteration 18873, loss = 0.06141772\n",
      "Iteration 18874, loss = 0.06141203\n",
      "Iteration 18875, loss = 0.06141036\n",
      "Iteration 18876, loss = 0.06140432\n",
      "Iteration 18877, loss = 0.06139533\n",
      "Iteration 18878, loss = 0.06139081\n",
      "Iteration 18879, loss = 0.06138637\n",
      "Iteration 18880, loss = 0.06137885\n",
      "Iteration 18881, loss = 0.06137499\n",
      "Iteration 18882, loss = 0.06137726\n",
      "Iteration 18883, loss = 0.06136914\n",
      "Iteration 18884, loss = 0.06135981\n",
      "Iteration 18885, loss = 0.06135815\n",
      "Iteration 18886, loss = 0.06135475\n",
      "Iteration 18887, loss = 0.06134948\n",
      "Iteration 18888, loss = 0.06134312\n",
      "Iteration 18889, loss = 0.06133436\n",
      "Iteration 18890, loss = 0.06133402\n",
      "Iteration 18891, loss = 0.06133266\n",
      "Iteration 18892, loss = 0.06132210\n",
      "Iteration 18893, loss = 0.06131547\n",
      "Iteration 18894, loss = 0.06131331\n",
      "Iteration 18895, loss = 0.06130857\n",
      "Iteration 18896, loss = 0.06130044\n",
      "Iteration 18897, loss = 0.06129571\n",
      "Iteration 18898, loss = 0.06128823\n",
      "Iteration 18899, loss = 0.06128690\n",
      "Iteration 18900, loss = 0.06128408\n",
      "Iteration 18901, loss = 0.06128007\n",
      "Iteration 18902, loss = 0.06127567\n",
      "Iteration 18903, loss = 0.06126753\n",
      "Iteration 18904, loss = 0.06126022\n",
      "Iteration 18905, loss = 0.06125296\n",
      "Iteration 18906, loss = 0.06125333\n",
      "Iteration 18907, loss = 0.06124714\n",
      "Iteration 18908, loss = 0.06124127\n",
      "Iteration 18909, loss = 0.06123764\n",
      "Iteration 18910, loss = 0.06123632\n",
      "Iteration 18911, loss = 0.06122932\n",
      "Iteration 18912, loss = 0.06121734\n",
      "Iteration 18913, loss = 0.06122143\n",
      "Iteration 18914, loss = 0.06121874\n",
      "Iteration 18915, loss = 0.06120614\n",
      "Iteration 18916, loss = 0.06120373\n",
      "Iteration 18917, loss = 0.06119682\n",
      "Iteration 18918, loss = 0.06119620\n",
      "Iteration 18919, loss = 0.06119309\n",
      "Iteration 18920, loss = 0.06118877\n",
      "Iteration 18921, loss = 0.06118240\n",
      "Iteration 18922, loss = 0.06117699\n",
      "Iteration 18923, loss = 0.06116780\n",
      "Iteration 18924, loss = 0.06115918\n",
      "Iteration 18925, loss = 0.06115912\n",
      "Iteration 18926, loss = 0.06115488\n",
      "Iteration 18927, loss = 0.06114603\n",
      "Iteration 18928, loss = 0.06114339\n",
      "Iteration 18929, loss = 0.06114133\n",
      "Iteration 18930, loss = 0.06113967\n",
      "Iteration 18931, loss = 0.06113282\n",
      "Iteration 18932, loss = 0.06112556\n",
      "Iteration 18933, loss = 0.06112116\n",
      "Iteration 18934, loss = 0.06111459\n",
      "Iteration 18935, loss = 0.06110533\n",
      "Iteration 18936, loss = 0.06110564\n",
      "Iteration 18937, loss = 0.06110345\n",
      "Iteration 18938, loss = 0.06109448\n",
      "Iteration 18939, loss = 0.06109269\n",
      "Iteration 18940, loss = 0.06108900\n",
      "Iteration 18941, loss = 0.06108263\n",
      "Iteration 18942, loss = 0.06107898\n",
      "Iteration 18943, loss = 0.06107456\n",
      "Iteration 18944, loss = 0.06106823\n",
      "Iteration 18945, loss = 0.06106631\n",
      "Iteration 18946, loss = 0.06105660\n",
      "Iteration 18947, loss = 0.06104831\n",
      "Iteration 18948, loss = 0.06104789\n",
      "Iteration 18949, loss = 0.06104103\n",
      "Iteration 18950, loss = 0.06103771\n",
      "Iteration 18951, loss = 0.06103245\n",
      "Iteration 18952, loss = 0.06103032\n",
      "Iteration 18953, loss = 0.06102589\n",
      "Iteration 18954, loss = 0.06102027\n",
      "Iteration 18955, loss = 0.06101185\n",
      "Iteration 18956, loss = 0.06100975\n",
      "Iteration 18957, loss = 0.06100663\n",
      "Iteration 18958, loss = 0.06100298\n",
      "Iteration 18959, loss = 0.06099454\n",
      "Iteration 18960, loss = 0.06098890\n",
      "Iteration 18961, loss = 0.06099069\n",
      "Iteration 18962, loss = 0.06098693\n",
      "Iteration 18963, loss = 0.06098174\n",
      "Iteration 18964, loss = 0.06097487\n",
      "Iteration 18965, loss = 0.06096930\n",
      "Iteration 18966, loss = 0.06096343\n",
      "Iteration 18967, loss = 0.06095768\n",
      "Iteration 18968, loss = 0.06095169\n",
      "Iteration 18969, loss = 0.06094482\n",
      "Iteration 18970, loss = 0.06093884\n",
      "Iteration 18971, loss = 0.06093548\n",
      "Iteration 18972, loss = 0.06093051\n",
      "Iteration 18973, loss = 0.06092482\n",
      "Iteration 18974, loss = 0.06091975\n",
      "Iteration 18975, loss = 0.06091292\n",
      "Iteration 18976, loss = 0.06090884\n",
      "Iteration 18977, loss = 0.06090232\n",
      "Iteration 18978, loss = 0.06089793\n",
      "Iteration 18979, loss = 0.06089375\n",
      "Iteration 18980, loss = 0.06089031\n",
      "Iteration 18981, loss = 0.06088669\n",
      "Iteration 18982, loss = 0.06088227\n",
      "Iteration 18983, loss = 0.06087268\n",
      "Iteration 18984, loss = 0.06087361\n",
      "Iteration 18985, loss = 0.06086894\n",
      "Iteration 18986, loss = 0.06086074\n",
      "Iteration 18987, loss = 0.06085504\n",
      "Iteration 18988, loss = 0.06085323\n",
      "Iteration 18989, loss = 0.06085015\n",
      "Iteration 18990, loss = 0.06084459\n",
      "Iteration 18991, loss = 0.06083796\n",
      "Iteration 18992, loss = 0.06083240\n",
      "Iteration 18993, loss = 0.06082760\n",
      "Iteration 18994, loss = 0.06082281\n",
      "Iteration 18995, loss = 0.06081734\n",
      "Iteration 18996, loss = 0.06081509\n",
      "Iteration 18997, loss = 0.06080911\n",
      "Iteration 18998, loss = 0.06080388\n",
      "Iteration 18999, loss = 0.06080193\n",
      "Iteration 19000, loss = 0.06079886\n",
      "Iteration 19001, loss = 0.06078939\n",
      "Iteration 19002, loss = 0.06078687\n",
      "Iteration 19003, loss = 0.06078191\n",
      "Iteration 19004, loss = 0.06077488\n",
      "Iteration 19005, loss = 0.06076735\n",
      "Iteration 19006, loss = 0.06076518\n",
      "Iteration 19007, loss = 0.06075655\n",
      "Iteration 19008, loss = 0.06075192\n",
      "Iteration 19009, loss = 0.06075131\n",
      "Iteration 19010, loss = 0.06074987\n",
      "Iteration 19011, loss = 0.06074353\n",
      "Iteration 19012, loss = 0.06073473\n",
      "Iteration 19013, loss = 0.06072860\n",
      "Iteration 19014, loss = 0.06072258\n",
      "Iteration 19015, loss = 0.06071668\n",
      "Iteration 19016, loss = 0.06071400\n",
      "Iteration 19017, loss = 0.06071111\n",
      "Iteration 19018, loss = 0.06070428\n",
      "Iteration 19019, loss = 0.06070368\n",
      "Iteration 19020, loss = 0.06069716\n",
      "Iteration 19021, loss = 0.06069041\n",
      "Iteration 19022, loss = 0.06068351\n",
      "Iteration 19023, loss = 0.06067991\n",
      "Iteration 19024, loss = 0.06067785\n",
      "Iteration 19025, loss = 0.06067383\n",
      "Iteration 19026, loss = 0.06066843\n",
      "Iteration 19027, loss = 0.06066226\n",
      "Iteration 19028, loss = 0.06065852\n",
      "Iteration 19029, loss = 0.06065605\n",
      "Iteration 19030, loss = 0.06064708\n",
      "Iteration 19031, loss = 0.06064307\n",
      "Iteration 19032, loss = 0.06063971\n",
      "Iteration 19033, loss = 0.06063334\n",
      "Iteration 19034, loss = 0.06062824\n",
      "Iteration 19035, loss = 0.06062343\n",
      "Iteration 19036, loss = 0.06062156\n",
      "Iteration 19037, loss = 0.06061505\n",
      "Iteration 19038, loss = 0.06061208\n",
      "Iteration 19039, loss = 0.06060600\n",
      "Iteration 19040, loss = 0.06060300\n",
      "Iteration 19041, loss = 0.06059868\n",
      "Iteration 19042, loss = 0.06059399\n",
      "Iteration 19043, loss = 0.06058756\n",
      "Iteration 19044, loss = 0.06058017\n",
      "Iteration 19045, loss = 0.06057427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19046, loss = 0.06057348\n",
      "Iteration 19047, loss = 0.06056938\n",
      "Iteration 19048, loss = 0.06056485\n",
      "Iteration 19049, loss = 0.06055788\n",
      "Iteration 19050, loss = 0.06055242\n",
      "Iteration 19051, loss = 0.06054581\n",
      "Iteration 19052, loss = 0.06055028\n",
      "Iteration 19053, loss = 0.06054576\n",
      "Iteration 19054, loss = 0.06053572\n",
      "Iteration 19055, loss = 0.06053241\n",
      "Iteration 19056, loss = 0.06052790\n",
      "Iteration 19057, loss = 0.06052508\n",
      "Iteration 19058, loss = 0.06052088\n",
      "Iteration 19059, loss = 0.06051477\n",
      "Iteration 19060, loss = 0.06051068\n",
      "Iteration 19061, loss = 0.06050491\n",
      "Iteration 19062, loss = 0.06049996\n",
      "Iteration 19063, loss = 0.06048967\n",
      "Iteration 19064, loss = 0.06048864\n",
      "Iteration 19065, loss = 0.06048497\n",
      "Iteration 19066, loss = 0.06047957\n",
      "Iteration 19067, loss = 0.06047535\n",
      "Iteration 19068, loss = 0.06047093\n",
      "Iteration 19069, loss = 0.06046756\n",
      "Iteration 19070, loss = 0.06046640\n",
      "Iteration 19071, loss = 0.06046442\n",
      "Iteration 19072, loss = 0.06045639\n",
      "Iteration 19073, loss = 0.06044897\n",
      "Iteration 19074, loss = 0.06044338\n",
      "Iteration 19075, loss = 0.06044239\n",
      "Iteration 19076, loss = 0.06044002\n",
      "Iteration 19077, loss = 0.06042967\n",
      "Iteration 19078, loss = 0.06042185\n",
      "Iteration 19079, loss = 0.06041949\n",
      "Iteration 19080, loss = 0.06042063\n",
      "Iteration 19081, loss = 0.06041213\n",
      "Iteration 19082, loss = 0.06040804\n",
      "Iteration 19083, loss = 0.06040390\n",
      "Iteration 19084, loss = 0.06039788\n",
      "Iteration 19085, loss = 0.06039048\n",
      "Iteration 19086, loss = 0.06038808\n",
      "Iteration 19087, loss = 0.06038538\n",
      "Iteration 19088, loss = 0.06037287\n",
      "Iteration 19089, loss = 0.06036856\n",
      "Iteration 19090, loss = 0.06036915\n",
      "Iteration 19091, loss = 0.06036884\n",
      "Iteration 19092, loss = 0.06036476\n",
      "Iteration 19093, loss = 0.06035856\n",
      "Iteration 19094, loss = 0.06035254\n",
      "Iteration 19095, loss = 0.06034471\n",
      "Iteration 19096, loss = 0.06034169\n",
      "Iteration 19097, loss = 0.06033496\n",
      "Iteration 19098, loss = 0.06032649\n",
      "Iteration 19099, loss = 0.06032428\n",
      "Iteration 19100, loss = 0.06032139\n",
      "Iteration 19101, loss = 0.06031489\n",
      "Iteration 19102, loss = 0.06030846\n",
      "Iteration 19103, loss = 0.06030453\n",
      "Iteration 19104, loss = 0.06029696\n",
      "Iteration 19105, loss = 0.06029486\n",
      "Iteration 19106, loss = 0.06029140\n",
      "Iteration 19107, loss = 0.06028612\n",
      "Iteration 19108, loss = 0.06028520\n",
      "Iteration 19109, loss = 0.06027943\n",
      "Iteration 19110, loss = 0.06027167\n",
      "Iteration 19111, loss = 0.06027158\n",
      "Iteration 19112, loss = 0.06026448\n",
      "Iteration 19113, loss = 0.06025819\n",
      "Iteration 19114, loss = 0.06026152\n",
      "Iteration 19115, loss = 0.06025643\n",
      "Iteration 19116, loss = 0.06025394\n",
      "Iteration 19117, loss = 0.06024533\n",
      "Iteration 19118, loss = 0.06023501\n",
      "Iteration 19119, loss = 0.06023607\n",
      "Iteration 19120, loss = 0.06023014\n",
      "Iteration 19121, loss = 0.06022306\n",
      "Iteration 19122, loss = 0.06022238\n",
      "Iteration 19123, loss = 0.06021672\n",
      "Iteration 19124, loss = 0.06021101\n",
      "Iteration 19125, loss = 0.06020069\n",
      "Iteration 19126, loss = 0.06020049\n",
      "Iteration 19127, loss = 0.06019791\n",
      "Iteration 19128, loss = 0.06019075\n",
      "Iteration 19129, loss = 0.06018478\n",
      "Iteration 19130, loss = 0.06018089\n",
      "Iteration 19131, loss = 0.06018099\n",
      "Iteration 19132, loss = 0.06017482\n",
      "Iteration 19133, loss = 0.06016654\n",
      "Iteration 19134, loss = 0.06016819\n",
      "Iteration 19135, loss = 0.06016395\n",
      "Iteration 19136, loss = 0.06014713\n",
      "Iteration 19137, loss = 0.06014536\n",
      "Iteration 19138, loss = 0.06014285\n",
      "Iteration 19139, loss = 0.06013638\n",
      "Iteration 19140, loss = 0.06013126\n",
      "Iteration 19141, loss = 0.06013283\n",
      "Iteration 19142, loss = 0.06012799\n",
      "Iteration 19143, loss = 0.06011649\n",
      "Iteration 19144, loss = 0.06011534\n",
      "Iteration 19145, loss = 0.06011311\n",
      "Iteration 19146, loss = 0.06010762\n",
      "Iteration 19147, loss = 0.06009780\n",
      "Iteration 19148, loss = 0.06009304\n",
      "Iteration 19149, loss = 0.06009941\n",
      "Iteration 19150, loss = 0.06009246\n",
      "Iteration 19151, loss = 0.06008213\n",
      "Iteration 19152, loss = 0.06007714\n",
      "Iteration 19153, loss = 0.06007305\n",
      "Iteration 19154, loss = 0.06007005\n",
      "Iteration 19155, loss = 0.06006115\n",
      "Iteration 19156, loss = 0.06005709\n",
      "Iteration 19157, loss = 0.06005175\n",
      "Iteration 19158, loss = 0.06004923\n",
      "Iteration 19159, loss = 0.06004727\n",
      "Iteration 19160, loss = 0.06003958\n",
      "Iteration 19161, loss = 0.06003788\n",
      "Iteration 19162, loss = 0.06003702\n",
      "Iteration 19163, loss = 0.06003277\n",
      "Iteration 19164, loss = 0.06002805\n",
      "Iteration 19165, loss = 0.06002269\n",
      "Iteration 19166, loss = 0.06001649\n",
      "Iteration 19167, loss = 0.06000943\n",
      "Iteration 19168, loss = 0.06000718\n",
      "Iteration 19169, loss = 0.06000176\n",
      "Iteration 19170, loss = 0.05999232\n",
      "Iteration 19171, loss = 0.05998924\n",
      "Iteration 19172, loss = 0.05998383\n",
      "Iteration 19173, loss = 0.05997813\n",
      "Iteration 19174, loss = 0.05997558\n",
      "Iteration 19175, loss = 0.05997126\n",
      "Iteration 19176, loss = 0.05996499\n",
      "Iteration 19177, loss = 0.05996345\n",
      "Iteration 19178, loss = 0.05996024\n",
      "Iteration 19179, loss = 0.05995460\n",
      "Iteration 19180, loss = 0.05995031\n",
      "Iteration 19181, loss = 0.05994207\n",
      "Iteration 19182, loss = 0.05994039\n",
      "Iteration 19183, loss = 0.05994067\n",
      "Iteration 19184, loss = 0.05993403\n",
      "Iteration 19185, loss = 0.05992922\n",
      "Iteration 19186, loss = 0.05992864\n",
      "Iteration 19187, loss = 0.05992653\n",
      "Iteration 19188, loss = 0.05991907\n",
      "Iteration 19189, loss = 0.05991774\n",
      "Iteration 19190, loss = 0.05991778\n",
      "Iteration 19191, loss = 0.05990952\n",
      "Iteration 19192, loss = 0.05989985\n",
      "Iteration 19193, loss = 0.05989499\n",
      "Iteration 19194, loss = 0.05989250\n",
      "Iteration 19195, loss = 0.05988863\n",
      "Iteration 19196, loss = 0.05988116\n",
      "Iteration 19197, loss = 0.05988019\n",
      "Iteration 19198, loss = 0.05987527\n",
      "Iteration 19199, loss = 0.05986532\n",
      "Iteration 19200, loss = 0.05985583\n",
      "Iteration 19201, loss = 0.05985579\n",
      "Iteration 19202, loss = 0.05985494\n",
      "Iteration 19203, loss = 0.05984563\n",
      "Iteration 19204, loss = 0.05983868\n",
      "Iteration 19205, loss = 0.05983522\n",
      "Iteration 19206, loss = 0.05983399\n",
      "Iteration 19207, loss = 0.05982762\n",
      "Iteration 19208, loss = 0.05982129\n",
      "Iteration 19209, loss = 0.05981637\n",
      "Iteration 19210, loss = 0.05981762\n",
      "Iteration 19211, loss = 0.05981798\n",
      "Iteration 19212, loss = 0.05980771\n",
      "Iteration 19213, loss = 0.05980225\n",
      "Iteration 19214, loss = 0.05979657\n",
      "Iteration 19215, loss = 0.05979071\n",
      "Iteration 19216, loss = 0.05979451\n",
      "Iteration 19217, loss = 0.05978938\n",
      "Iteration 19218, loss = 0.05977322\n",
      "Iteration 19219, loss = 0.05977703\n",
      "Iteration 19220, loss = 0.05977571\n",
      "Iteration 19221, loss = 0.05977033\n",
      "Iteration 19222, loss = 0.05976763\n",
      "Iteration 19223, loss = 0.05976305\n",
      "Iteration 19224, loss = 0.05975556\n",
      "Iteration 19225, loss = 0.05975123\n",
      "Iteration 19226, loss = 0.05974744\n",
      "Iteration 19227, loss = 0.05974116\n",
      "Iteration 19228, loss = 0.05973875\n",
      "Iteration 19229, loss = 0.05973602\n",
      "Iteration 19230, loss = 0.05972422\n",
      "Iteration 19231, loss = 0.05971943\n",
      "Iteration 19232, loss = 0.05971796\n",
      "Iteration 19233, loss = 0.05971007\n",
      "Iteration 19234, loss = 0.05970936\n",
      "Iteration 19235, loss = 0.05970686\n",
      "Iteration 19236, loss = 0.05969977\n",
      "Iteration 19237, loss = 0.05969284\n",
      "Iteration 19238, loss = 0.05968765\n",
      "Iteration 19239, loss = 0.05968083\n",
      "Iteration 19240, loss = 0.05967481\n",
      "Iteration 19241, loss = 0.05966758\n",
      "Iteration 19242, loss = 0.05966472\n",
      "Iteration 19243, loss = 0.05966054\n",
      "Iteration 19244, loss = 0.05965289\n",
      "Iteration 19245, loss = 0.05964951\n",
      "Iteration 19246, loss = 0.05964664\n",
      "Iteration 19247, loss = 0.05964158\n",
      "Iteration 19248, loss = 0.05963676\n",
      "Iteration 19249, loss = 0.05962953\n",
      "Iteration 19250, loss = 0.05962771\n",
      "Iteration 19251, loss = 0.05962189\n",
      "Iteration 19252, loss = 0.05961760\n",
      "Iteration 19253, loss = 0.05961483\n",
      "Iteration 19254, loss = 0.05961213\n",
      "Iteration 19255, loss = 0.05960069\n",
      "Iteration 19256, loss = 0.05959963\n",
      "Iteration 19257, loss = 0.05959614\n",
      "Iteration 19258, loss = 0.05959501\n",
      "Iteration 19259, loss = 0.05959027\n",
      "Iteration 19260, loss = 0.05958807\n",
      "Iteration 19261, loss = 0.05958416\n",
      "Iteration 19262, loss = 0.05957621\n",
      "Iteration 19263, loss = 0.05956595\n",
      "Iteration 19264, loss = 0.05955887\n",
      "Iteration 19265, loss = 0.05955855\n",
      "Iteration 19266, loss = 0.05955299\n",
      "Iteration 19267, loss = 0.05954582\n",
      "Iteration 19268, loss = 0.05953959\n",
      "Iteration 19269, loss = 0.05953656\n",
      "Iteration 19270, loss = 0.05953490\n",
      "Iteration 19271, loss = 0.05952883\n",
      "Iteration 19272, loss = 0.05952311\n",
      "Iteration 19273, loss = 0.05951885\n",
      "Iteration 19274, loss = 0.05951279\n",
      "Iteration 19275, loss = 0.05950456\n",
      "Iteration 19276, loss = 0.05949806\n",
      "Iteration 19277, loss = 0.05949510\n",
      "Iteration 19278, loss = 0.05949229\n",
      "Iteration 19279, loss = 0.05948792\n",
      "Iteration 19280, loss = 0.05948187\n",
      "Iteration 19281, loss = 0.05947812\n",
      "Iteration 19282, loss = 0.05947570\n",
      "Iteration 19283, loss = 0.05946739\n",
      "Iteration 19284, loss = 0.05946484\n",
      "Iteration 19285, loss = 0.05945969\n",
      "Iteration 19286, loss = 0.05945527\n",
      "Iteration 19287, loss = 0.05944887\n",
      "Iteration 19288, loss = 0.05944325\n",
      "Iteration 19289, loss = 0.05943580\n",
      "Iteration 19290, loss = 0.05943816\n",
      "Iteration 19291, loss = 0.05943485\n",
      "Iteration 19292, loss = 0.05942092\n",
      "Iteration 19293, loss = 0.05942089\n",
      "Iteration 19294, loss = 0.05942036\n",
      "Iteration 19295, loss = 0.05941749\n",
      "Iteration 19296, loss = 0.05941202\n",
      "Iteration 19297, loss = 0.05940692\n",
      "Iteration 19298, loss = 0.05940386\n",
      "Iteration 19299, loss = 0.05939624\n",
      "Iteration 19300, loss = 0.05939005\n",
      "Iteration 19301, loss = 0.05938017\n",
      "Iteration 19302, loss = 0.05938075\n",
      "Iteration 19303, loss = 0.05937958\n",
      "Iteration 19304, loss = 0.05937252\n",
      "Iteration 19305, loss = 0.05936484\n",
      "Iteration 19306, loss = 0.05935904\n",
      "Iteration 19307, loss = 0.05935910\n",
      "Iteration 19308, loss = 0.05935381\n",
      "Iteration 19309, loss = 0.05934641\n",
      "Iteration 19310, loss = 0.05934348\n",
      "Iteration 19311, loss = 0.05933645\n",
      "Iteration 19312, loss = 0.05933198\n",
      "Iteration 19313, loss = 0.05932388\n",
      "Iteration 19314, loss = 0.05931907\n",
      "Iteration 19315, loss = 0.05931567\n",
      "Iteration 19316, loss = 0.05930999\n",
      "Iteration 19317, loss = 0.05930496\n",
      "Iteration 19318, loss = 0.05930068\n",
      "Iteration 19319, loss = 0.05930134\n",
      "Iteration 19320, loss = 0.05929255\n",
      "Iteration 19321, loss = 0.05928552\n",
      "Iteration 19322, loss = 0.05928078\n",
      "Iteration 19323, loss = 0.05927929\n",
      "Iteration 19324, loss = 0.05927471\n",
      "Iteration 19325, loss = 0.05926576\n",
      "Iteration 19326, loss = 0.05926005\n",
      "Iteration 19327, loss = 0.05925376\n",
      "Iteration 19328, loss = 0.05925081\n",
      "Iteration 19329, loss = 0.05925138\n",
      "Iteration 19330, loss = 0.05924913\n",
      "Iteration 19331, loss = 0.05924255\n",
      "Iteration 19332, loss = 0.05923694\n",
      "Iteration 19333, loss = 0.05922474\n",
      "Iteration 19334, loss = 0.05921867\n",
      "Iteration 19335, loss = 0.05921676\n",
      "Iteration 19336, loss = 0.05921341\n",
      "Iteration 19337, loss = 0.05921031\n",
      "Iteration 19338, loss = 0.05920282\n",
      "Iteration 19339, loss = 0.05919818\n",
      "Iteration 19340, loss = 0.05919259\n",
      "Iteration 19341, loss = 0.05918992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19342, loss = 0.05918435\n",
      "Iteration 19343, loss = 0.05917931\n",
      "Iteration 19344, loss = 0.05917391\n",
      "Iteration 19345, loss = 0.05917062\n",
      "Iteration 19346, loss = 0.05916398\n",
      "Iteration 19347, loss = 0.05915670\n",
      "Iteration 19348, loss = 0.05915198\n",
      "Iteration 19349, loss = 0.05915663\n",
      "Iteration 19350, loss = 0.05915137\n",
      "Iteration 19351, loss = 0.05914078\n",
      "Iteration 19352, loss = 0.05913659\n",
      "Iteration 19353, loss = 0.05913337\n",
      "Iteration 19354, loss = 0.05912617\n",
      "Iteration 19355, loss = 0.05911847\n",
      "Iteration 19356, loss = 0.05911449\n",
      "Iteration 19357, loss = 0.05911066\n",
      "Iteration 19358, loss = 0.05910341\n",
      "Iteration 19359, loss = 0.05910175\n",
      "Iteration 19360, loss = 0.05909759\n",
      "Iteration 19361, loss = 0.05909312\n",
      "Iteration 19362, loss = 0.05908807\n",
      "Iteration 19363, loss = 0.05908625\n",
      "Iteration 19364, loss = 0.05908103\n",
      "Iteration 19365, loss = 0.05907725\n",
      "Iteration 19366, loss = 0.05907087\n",
      "Iteration 19367, loss = 0.05906393\n",
      "Iteration 19368, loss = 0.05906194\n",
      "Iteration 19369, loss = 0.05905528\n",
      "Iteration 19370, loss = 0.05904559\n",
      "Iteration 19371, loss = 0.05904663\n",
      "Iteration 19372, loss = 0.05904188\n",
      "Iteration 19373, loss = 0.05903348\n",
      "Iteration 19374, loss = 0.05903220\n",
      "Iteration 19375, loss = 0.05902338\n",
      "Iteration 19376, loss = 0.05901592\n",
      "Iteration 19377, loss = 0.05901284\n",
      "Iteration 19378, loss = 0.05900926\n",
      "Iteration 19379, loss = 0.05900420\n",
      "Iteration 19380, loss = 0.05899964\n",
      "Iteration 19381, loss = 0.05899784\n",
      "Iteration 19382, loss = 0.05899054\n",
      "Iteration 19383, loss = 0.05898684\n",
      "Iteration 19384, loss = 0.05898311\n",
      "Iteration 19385, loss = 0.05897675\n",
      "Iteration 19386, loss = 0.05897338\n",
      "Iteration 19387, loss = 0.05896748\n",
      "Iteration 19388, loss = 0.05895942\n",
      "Iteration 19389, loss = 0.05895291\n",
      "Iteration 19390, loss = 0.05895592\n",
      "Iteration 19391, loss = 0.05895402\n",
      "Iteration 19392, loss = 0.05894363\n",
      "Iteration 19393, loss = 0.05893324\n",
      "Iteration 19394, loss = 0.05893311\n",
      "Iteration 19395, loss = 0.05893128\n",
      "Iteration 19396, loss = 0.05892638\n",
      "Iteration 19397, loss = 0.05892297\n",
      "Iteration 19398, loss = 0.05891668\n",
      "Iteration 19399, loss = 0.05890745\n",
      "Iteration 19400, loss = 0.05889861\n",
      "Iteration 19401, loss = 0.05889871\n",
      "Iteration 19402, loss = 0.05889612\n",
      "Iteration 19403, loss = 0.05888398\n",
      "Iteration 19404, loss = 0.05888382\n",
      "Iteration 19405, loss = 0.05888149\n",
      "Iteration 19406, loss = 0.05887653\n",
      "Iteration 19407, loss = 0.05887254\n",
      "Iteration 19408, loss = 0.05886291\n",
      "Iteration 19409, loss = 0.05885756\n",
      "Iteration 19410, loss = 0.05884974\n",
      "Iteration 19411, loss = 0.05884489\n",
      "Iteration 19412, loss = 0.05884138\n",
      "Iteration 19413, loss = 0.05883672\n",
      "Iteration 19414, loss = 0.05882658\n",
      "Iteration 19415, loss = 0.05882506\n",
      "Iteration 19416, loss = 0.05882296\n",
      "Iteration 19417, loss = 0.05881950\n",
      "Iteration 19418, loss = 0.05881566\n",
      "Iteration 19419, loss = 0.05881210\n",
      "Iteration 19420, loss = 0.05880547\n",
      "Iteration 19421, loss = 0.05879791\n",
      "Iteration 19422, loss = 0.05878884\n",
      "Iteration 19423, loss = 0.05878714\n",
      "Iteration 19424, loss = 0.05878595\n",
      "Iteration 19425, loss = 0.05878065\n",
      "Iteration 19426, loss = 0.05877240\n",
      "Iteration 19427, loss = 0.05877028\n",
      "Iteration 19428, loss = 0.05876831\n",
      "Iteration 19429, loss = 0.05876628\n",
      "Iteration 19430, loss = 0.05875907\n",
      "Iteration 19431, loss = 0.05875282\n",
      "Iteration 19432, loss = 0.05874281\n",
      "Iteration 19433, loss = 0.05873753\n",
      "Iteration 19434, loss = 0.05873984\n",
      "Iteration 19435, loss = 0.05873534\n",
      "Iteration 19436, loss = 0.05872633\n",
      "Iteration 19437, loss = 0.05871966\n",
      "Iteration 19438, loss = 0.05871835\n",
      "Iteration 19439, loss = 0.05871393\n",
      "Iteration 19440, loss = 0.05870710\n",
      "Iteration 19441, loss = 0.05870112\n",
      "Iteration 19442, loss = 0.05869638\n",
      "Iteration 19443, loss = 0.05868982\n",
      "Iteration 19444, loss = 0.05868577\n",
      "Iteration 19445, loss = 0.05868759\n",
      "Iteration 19446, loss = 0.05868219\n",
      "Iteration 19447, loss = 0.05867331\n",
      "Iteration 19448, loss = 0.05866743\n",
      "Iteration 19449, loss = 0.05866165\n",
      "Iteration 19450, loss = 0.05866024\n",
      "Iteration 19451, loss = 0.05865688\n",
      "Iteration 19452, loss = 0.05864925\n",
      "Iteration 19453, loss = 0.05864593\n",
      "Iteration 19454, loss = 0.05864054\n",
      "Iteration 19455, loss = 0.05862818\n",
      "Iteration 19456, loss = 0.05863368\n",
      "Iteration 19457, loss = 0.05863012\n",
      "Iteration 19458, loss = 0.05862683\n",
      "Iteration 19459, loss = 0.05861621\n",
      "Iteration 19460, loss = 0.05861400\n",
      "Iteration 19461, loss = 0.05861067\n",
      "Iteration 19462, loss = 0.05860935\n",
      "Iteration 19463, loss = 0.05860266\n",
      "Iteration 19464, loss = 0.05859485\n",
      "Iteration 19465, loss = 0.05858696\n",
      "Iteration 19466, loss = 0.05857925\n",
      "Iteration 19467, loss = 0.05857438\n",
      "Iteration 19468, loss = 0.05857213\n",
      "Iteration 19469, loss = 0.05857015\n",
      "Iteration 19470, loss = 0.05856446\n",
      "Iteration 19471, loss = 0.05856191\n",
      "Iteration 19472, loss = 0.05855581\n",
      "Iteration 19473, loss = 0.05855413\n",
      "Iteration 19474, loss = 0.05854839\n",
      "Iteration 19475, loss = 0.05853814\n",
      "Iteration 19476, loss = 0.05853104\n",
      "Iteration 19477, loss = 0.05853010\n",
      "Iteration 19478, loss = 0.05852638\n",
      "Iteration 19479, loss = 0.05851551\n",
      "Iteration 19480, loss = 0.05851074\n",
      "Iteration 19481, loss = 0.05850898\n",
      "Iteration 19482, loss = 0.05850468\n",
      "Iteration 19483, loss = 0.05850255\n",
      "Iteration 19484, loss = 0.05849814\n",
      "Iteration 19485, loss = 0.05849084\n",
      "Iteration 19486, loss = 0.05848673\n",
      "Iteration 19487, loss = 0.05848173\n",
      "Iteration 19488, loss = 0.05848034\n",
      "Iteration 19489, loss = 0.05847277\n",
      "Iteration 19490, loss = 0.05846587\n",
      "Iteration 19491, loss = 0.05846332\n",
      "Iteration 19492, loss = 0.05845812\n",
      "Iteration 19493, loss = 0.05845246\n",
      "Iteration 19494, loss = 0.05844858\n",
      "Iteration 19495, loss = 0.05844123\n",
      "Iteration 19496, loss = 0.05843670\n",
      "Iteration 19497, loss = 0.05843498\n",
      "Iteration 19498, loss = 0.05842944\n",
      "Iteration 19499, loss = 0.05842668\n",
      "Iteration 19500, loss = 0.05842298\n",
      "Iteration 19501, loss = 0.05841257\n",
      "Iteration 19502, loss = 0.05841003\n",
      "Iteration 19503, loss = 0.05840830\n",
      "Iteration 19504, loss = 0.05840028\n",
      "Iteration 19505, loss = 0.05839573\n",
      "Iteration 19506, loss = 0.05839138\n",
      "Iteration 19507, loss = 0.05838933\n",
      "Iteration 19508, loss = 0.05838835\n",
      "Iteration 19509, loss = 0.05837981\n",
      "Iteration 19510, loss = 0.05837356\n",
      "Iteration 19511, loss = 0.05836905\n",
      "Iteration 19512, loss = 0.05836718\n",
      "Iteration 19513, loss = 0.05835917\n",
      "Iteration 19514, loss = 0.05835540\n",
      "Iteration 19515, loss = 0.05835153\n",
      "Iteration 19516, loss = 0.05835113\n",
      "Iteration 19517, loss = 0.05834145\n",
      "Iteration 19518, loss = 0.05833549\n",
      "Iteration 19519, loss = 0.05833310\n",
      "Iteration 19520, loss = 0.05832875\n",
      "Iteration 19521, loss = 0.05832438\n",
      "Iteration 19522, loss = 0.05831761\n",
      "Iteration 19523, loss = 0.05831181\n",
      "Iteration 19524, loss = 0.05830721\n",
      "Iteration 19525, loss = 0.05830802\n",
      "Iteration 19526, loss = 0.05830381\n",
      "Iteration 19527, loss = 0.05829372\n",
      "Iteration 19528, loss = 0.05828663\n",
      "Iteration 19529, loss = 0.05828453\n",
      "Iteration 19530, loss = 0.05828483\n",
      "Iteration 19531, loss = 0.05828123\n",
      "Iteration 19532, loss = 0.05827714\n",
      "Iteration 19533, loss = 0.05827463\n",
      "Iteration 19534, loss = 0.05826997\n",
      "Iteration 19535, loss = 0.05825954\n",
      "Iteration 19536, loss = 0.05825654\n",
      "Iteration 19537, loss = 0.05825741\n",
      "Iteration 19538, loss = 0.05824458\n",
      "Iteration 19539, loss = 0.05824139\n",
      "Iteration 19540, loss = 0.05824076\n",
      "Iteration 19541, loss = 0.05823442\n",
      "Iteration 19542, loss = 0.05823014\n",
      "Iteration 19543, loss = 0.05822696\n",
      "Iteration 19544, loss = 0.05821927\n",
      "Iteration 19545, loss = 0.05821444\n",
      "Iteration 19546, loss = 0.05820703\n",
      "Iteration 19547, loss = 0.05820324\n",
      "Iteration 19548, loss = 0.05819844\n",
      "Iteration 19549, loss = 0.05819149\n",
      "Iteration 19550, loss = 0.05819222\n",
      "Iteration 19551, loss = 0.05818838\n",
      "Iteration 19552, loss = 0.05818478\n",
      "Iteration 19553, loss = 0.05818192\n",
      "Iteration 19554, loss = 0.05817518\n",
      "Iteration 19555, loss = 0.05816707\n",
      "Iteration 19556, loss = 0.05816147\n",
      "Iteration 19557, loss = 0.05815284\n",
      "Iteration 19558, loss = 0.05815088\n",
      "Iteration 19559, loss = 0.05815259\n",
      "Iteration 19560, loss = 0.05814787\n",
      "Iteration 19561, loss = 0.05813524\n",
      "Iteration 19562, loss = 0.05813200\n",
      "Iteration 19563, loss = 0.05812731\n",
      "Iteration 19564, loss = 0.05812671\n",
      "Iteration 19565, loss = 0.05811931\n",
      "Iteration 19566, loss = 0.05811043\n",
      "Iteration 19567, loss = 0.05810805\n",
      "Iteration 19568, loss = 0.05810402\n",
      "Iteration 19569, loss = 0.05809746\n",
      "Iteration 19570, loss = 0.05809230\n",
      "Iteration 19571, loss = 0.05809240\n",
      "Iteration 19572, loss = 0.05808614\n",
      "Iteration 19573, loss = 0.05807946\n",
      "Iteration 19574, loss = 0.05807478\n",
      "Iteration 19575, loss = 0.05807623\n",
      "Iteration 19576, loss = 0.05807067\n",
      "Iteration 19577, loss = 0.05806305\n",
      "Iteration 19578, loss = 0.05805281\n",
      "Iteration 19579, loss = 0.05805519\n",
      "Iteration 19580, loss = 0.05805461\n",
      "Iteration 19581, loss = 0.05804980\n",
      "Iteration 19582, loss = 0.05804475\n",
      "Iteration 19583, loss = 0.05803508\n",
      "Iteration 19584, loss = 0.05802946\n",
      "Iteration 19585, loss = 0.05802694\n",
      "Iteration 19586, loss = 0.05802280\n",
      "Iteration 19587, loss = 0.05801923\n",
      "Iteration 19588, loss = 0.05801201\n",
      "Iteration 19589, loss = 0.05801146\n",
      "Iteration 19590, loss = 0.05800610\n",
      "Iteration 19591, loss = 0.05799972\n",
      "Iteration 19592, loss = 0.05799266\n",
      "Iteration 19593, loss = 0.05798919\n",
      "Iteration 19594, loss = 0.05798701\n",
      "Iteration 19595, loss = 0.05798271\n",
      "Iteration 19596, loss = 0.05797704\n",
      "Iteration 19597, loss = 0.05796955\n",
      "Iteration 19598, loss = 0.05796652\n",
      "Iteration 19599, loss = 0.05796504\n",
      "Iteration 19600, loss = 0.05796035\n",
      "Iteration 19601, loss = 0.05795457\n",
      "Iteration 19602, loss = 0.05795037\n",
      "Iteration 19603, loss = 0.05794545\n",
      "Iteration 19604, loss = 0.05794350\n",
      "Iteration 19605, loss = 0.05793506\n",
      "Iteration 19606, loss = 0.05793306\n",
      "Iteration 19607, loss = 0.05792762\n",
      "Iteration 19608, loss = 0.05792019\n",
      "Iteration 19609, loss = 0.05791536\n",
      "Iteration 19610, loss = 0.05791380\n",
      "Iteration 19611, loss = 0.05791672\n",
      "Iteration 19612, loss = 0.05790921\n",
      "Iteration 19613, loss = 0.05790013\n",
      "Iteration 19614, loss = 0.05789824\n",
      "Iteration 19615, loss = 0.05789329\n",
      "Iteration 19616, loss = 0.05788596\n",
      "Iteration 19617, loss = 0.05787792\n",
      "Iteration 19618, loss = 0.05787317\n",
      "Iteration 19619, loss = 0.05787307\n",
      "Iteration 19620, loss = 0.05787182\n",
      "Iteration 19621, loss = 0.05786060\n",
      "Iteration 19622, loss = 0.05785744\n",
      "Iteration 19623, loss = 0.05785164\n",
      "Iteration 19624, loss = 0.05784794\n",
      "Iteration 19625, loss = 0.05784498\n",
      "Iteration 19626, loss = 0.05784188\n",
      "Iteration 19627, loss = 0.05783426\n",
      "Iteration 19628, loss = 0.05783320\n",
      "Iteration 19629, loss = 0.05783057\n",
      "Iteration 19630, loss = 0.05782576\n",
      "Iteration 19631, loss = 0.05781862\n",
      "Iteration 19632, loss = 0.05781759\n",
      "Iteration 19633, loss = 0.05781511\n",
      "Iteration 19634, loss = 0.05780882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19635, loss = 0.05780384\n",
      "Iteration 19636, loss = 0.05780358\n",
      "Iteration 19637, loss = 0.05779822\n",
      "Iteration 19638, loss = 0.05779449\n",
      "Iteration 19639, loss = 0.05778573\n",
      "Iteration 19640, loss = 0.05777583\n",
      "Iteration 19641, loss = 0.05777270\n",
      "Iteration 19642, loss = 0.05777148\n",
      "Iteration 19643, loss = 0.05776899\n",
      "Iteration 19644, loss = 0.05777020\n",
      "Iteration 19645, loss = 0.05776410\n",
      "Iteration 19646, loss = 0.05775558\n",
      "Iteration 19647, loss = 0.05774762\n",
      "Iteration 19648, loss = 0.05774511\n",
      "Iteration 19649, loss = 0.05774621\n",
      "Iteration 19650, loss = 0.05773800\n",
      "Iteration 19651, loss = 0.05773541\n",
      "Iteration 19652, loss = 0.05772904\n",
      "Iteration 19653, loss = 0.05773195\n",
      "Iteration 19654, loss = 0.05772840\n",
      "Iteration 19655, loss = 0.05772264\n",
      "Iteration 19656, loss = 0.05771871\n",
      "Iteration 19657, loss = 0.05771142\n",
      "Iteration 19658, loss = 0.05770236\n",
      "Iteration 19659, loss = 0.05769042\n",
      "Iteration 19660, loss = 0.05769505\n",
      "Iteration 19661, loss = 0.05769493\n",
      "Iteration 19662, loss = 0.05768469\n",
      "Iteration 19663, loss = 0.05767344\n",
      "Iteration 19664, loss = 0.05767124\n",
      "Iteration 19665, loss = 0.05766988\n",
      "Iteration 19666, loss = 0.05766689\n",
      "Iteration 19667, loss = 0.05766097\n",
      "Iteration 19668, loss = 0.05765364\n",
      "Iteration 19669, loss = 0.05764624\n",
      "Iteration 19670, loss = 0.05764867\n",
      "Iteration 19671, loss = 0.05764677\n",
      "Iteration 19672, loss = 0.05763525\n",
      "Iteration 19673, loss = 0.05763144\n",
      "Iteration 19674, loss = 0.05762912\n",
      "Iteration 19675, loss = 0.05762736\n",
      "Iteration 19676, loss = 0.05762260\n",
      "Iteration 19677, loss = 0.05761855\n",
      "Iteration 19678, loss = 0.05761427\n",
      "Iteration 19679, loss = 0.05760368\n",
      "Iteration 19680, loss = 0.05760002\n",
      "Iteration 19681, loss = 0.05759391\n",
      "Iteration 19682, loss = 0.05759511\n",
      "Iteration 19683, loss = 0.05758510\n",
      "Iteration 19684, loss = 0.05758142\n",
      "Iteration 19685, loss = 0.05757942\n",
      "Iteration 19686, loss = 0.05757526\n",
      "Iteration 19687, loss = 0.05757452\n",
      "Iteration 19688, loss = 0.05756989\n",
      "Iteration 19689, loss = 0.05756179\n",
      "Iteration 19690, loss = 0.05755779\n",
      "Iteration 19691, loss = 0.05755835\n",
      "Iteration 19692, loss = 0.05755849\n",
      "Iteration 19693, loss = 0.05754938\n",
      "Iteration 19694, loss = 0.05753839\n",
      "Iteration 19695, loss = 0.05753897\n",
      "Iteration 19696, loss = 0.05754210\n",
      "Iteration 19697, loss = 0.05753175\n",
      "Iteration 19698, loss = 0.05752490\n",
      "Iteration 19699, loss = 0.05751872\n",
      "Iteration 19700, loss = 0.05751271\n",
      "Iteration 19701, loss = 0.05751166\n",
      "Iteration 19702, loss = 0.05750167\n",
      "Iteration 19703, loss = 0.05750021\n",
      "Iteration 19704, loss = 0.05749844\n",
      "Iteration 19705, loss = 0.05749335\n",
      "Iteration 19706, loss = 0.05748829\n",
      "Iteration 19707, loss = 0.05747959\n",
      "Iteration 19708, loss = 0.05747607\n",
      "Iteration 19709, loss = 0.05747129\n",
      "Iteration 19710, loss = 0.05746886\n",
      "Iteration 19711, loss = 0.05746414\n",
      "Iteration 19712, loss = 0.05745956\n",
      "Iteration 19713, loss = 0.05745588\n",
      "Iteration 19714, loss = 0.05745005\n",
      "Iteration 19715, loss = 0.05744308\n",
      "Iteration 19716, loss = 0.05744608\n",
      "Iteration 19717, loss = 0.05743917\n",
      "Iteration 19718, loss = 0.05743522\n",
      "Iteration 19719, loss = 0.05743392\n",
      "Iteration 19720, loss = 0.05743439\n",
      "Iteration 19721, loss = 0.05742922\n",
      "Iteration 19722, loss = 0.05742026\n",
      "Iteration 19723, loss = 0.05740982\n",
      "Iteration 19724, loss = 0.05741333\n",
      "Iteration 19725, loss = 0.05741110\n",
      "Iteration 19726, loss = 0.05740293\n",
      "Iteration 19727, loss = 0.05739612\n",
      "Iteration 19728, loss = 0.05739071\n",
      "Iteration 19729, loss = 0.05738906\n",
      "Iteration 19730, loss = 0.05738457\n",
      "Iteration 19731, loss = 0.05737850\n",
      "Iteration 19732, loss = 0.05737230\n",
      "Iteration 19733, loss = 0.05736881\n",
      "Iteration 19734, loss = 0.05736263\n",
      "Iteration 19735, loss = 0.05736216\n",
      "Iteration 19736, loss = 0.05736300\n",
      "Iteration 19737, loss = 0.05735775\n",
      "Iteration 19738, loss = 0.05734869\n",
      "Iteration 19739, loss = 0.05734277\n",
      "Iteration 19740, loss = 0.05733607\n",
      "Iteration 19741, loss = 0.05733486\n",
      "Iteration 19742, loss = 0.05732903\n",
      "Iteration 19743, loss = 0.05732440\n",
      "Iteration 19744, loss = 0.05732040\n",
      "Iteration 19745, loss = 0.05731886\n",
      "Iteration 19746, loss = 0.05731260\n",
      "Iteration 19747, loss = 0.05731062\n",
      "Iteration 19748, loss = 0.05731223\n",
      "Iteration 19749, loss = 0.05730476\n",
      "Iteration 19750, loss = 0.05729757\n",
      "Iteration 19751, loss = 0.05729479\n",
      "Iteration 19752, loss = 0.05728985\n",
      "Iteration 19753, loss = 0.05728991\n",
      "Iteration 19754, loss = 0.05728199\n",
      "Iteration 19755, loss = 0.05727351\n",
      "Iteration 19756, loss = 0.05727362\n",
      "Iteration 19757, loss = 0.05727085\n",
      "Iteration 19758, loss = 0.05726529\n",
      "Iteration 19759, loss = 0.05726364\n",
      "Iteration 19760, loss = 0.05726040\n",
      "Iteration 19761, loss = 0.05725377\n",
      "Iteration 19762, loss = 0.05724797\n",
      "Iteration 19763, loss = 0.05723901\n",
      "Iteration 19764, loss = 0.05723125\n",
      "Iteration 19765, loss = 0.05723877\n",
      "Iteration 19766, loss = 0.05723560\n",
      "Iteration 19767, loss = 0.05723008\n",
      "Iteration 19768, loss = 0.05721464\n",
      "Iteration 19769, loss = 0.05721175\n",
      "Iteration 19770, loss = 0.05721079\n",
      "Iteration 19771, loss = 0.05720754\n",
      "Iteration 19772, loss = 0.05720158\n",
      "Iteration 19773, loss = 0.05719480\n",
      "Iteration 19774, loss = 0.05719135\n",
      "Iteration 19775, loss = 0.05718873\n",
      "Iteration 19776, loss = 0.05718605\n",
      "Iteration 19777, loss = 0.05717780\n",
      "Iteration 19778, loss = 0.05717574\n",
      "Iteration 19779, loss = 0.05717370\n",
      "Iteration 19780, loss = 0.05717040\n",
      "Iteration 19781, loss = 0.05716520\n",
      "Iteration 19782, loss = 0.05715897\n",
      "Iteration 19783, loss = 0.05715657\n",
      "Iteration 19784, loss = 0.05715188\n",
      "Iteration 19785, loss = 0.05714881\n",
      "Iteration 19786, loss = 0.05714234\n",
      "Iteration 19787, loss = 0.05714012\n",
      "Iteration 19788, loss = 0.05713262\n",
      "Iteration 19789, loss = 0.05713222\n",
      "Iteration 19790, loss = 0.05712556\n",
      "Iteration 19791, loss = 0.05711849\n",
      "Iteration 19792, loss = 0.05711952\n",
      "Iteration 19793, loss = 0.05711596\n",
      "Iteration 19794, loss = 0.05710732\n",
      "Iteration 19795, loss = 0.05710368\n",
      "Iteration 19796, loss = 0.05709831\n",
      "Iteration 19797, loss = 0.05709659\n",
      "Iteration 19798, loss = 0.05709246\n",
      "Iteration 19799, loss = 0.05709149\n",
      "Iteration 19800, loss = 0.05708340\n",
      "Iteration 19801, loss = 0.05708046\n",
      "Iteration 19802, loss = 0.05707628\n",
      "Iteration 19803, loss = 0.05707327\n",
      "Iteration 19804, loss = 0.05706693\n",
      "Iteration 19805, loss = 0.05706420\n",
      "Iteration 19806, loss = 0.05706243\n",
      "Iteration 19807, loss = 0.05705203\n",
      "Iteration 19808, loss = 0.05704902\n",
      "Iteration 19809, loss = 0.05704575\n",
      "Iteration 19810, loss = 0.05704376\n",
      "Iteration 19811, loss = 0.05703913\n",
      "Iteration 19812, loss = 0.05703251\n",
      "Iteration 19813, loss = 0.05703157\n",
      "Iteration 19814, loss = 0.05702357\n",
      "Iteration 19815, loss = 0.05702007\n",
      "Iteration 19816, loss = 0.05701712\n",
      "Iteration 19817, loss = 0.05701511\n",
      "Iteration 19818, loss = 0.05701245\n",
      "Iteration 19819, loss = 0.05700554\n",
      "Iteration 19820, loss = 0.05699668\n",
      "Iteration 19821, loss = 0.05699925\n",
      "Iteration 19822, loss = 0.05699417\n",
      "Iteration 19823, loss = 0.05698070\n",
      "Iteration 19824, loss = 0.05698324\n",
      "Iteration 19825, loss = 0.05697878\n",
      "Iteration 19826, loss = 0.05697492\n",
      "Iteration 19827, loss = 0.05697157\n",
      "Iteration 19828, loss = 0.05696860\n",
      "Iteration 19829, loss = 0.05696374\n",
      "Iteration 19830, loss = 0.05696236\n",
      "Iteration 19831, loss = 0.05695385\n",
      "Iteration 19832, loss = 0.05694638\n",
      "Iteration 19833, loss = 0.05695278\n",
      "Iteration 19834, loss = 0.05694895\n",
      "Iteration 19835, loss = 0.05694053\n",
      "Iteration 19836, loss = 0.05693454\n",
      "Iteration 19837, loss = 0.05692953\n",
      "Iteration 19838, loss = 0.05692947\n",
      "Iteration 19839, loss = 0.05692815\n",
      "Iteration 19840, loss = 0.05692431\n",
      "Iteration 19841, loss = 0.05691993\n",
      "Iteration 19842, loss = 0.05690721\n",
      "Iteration 19843, loss = 0.05690115\n",
      "Iteration 19844, loss = 0.05690923\n",
      "Iteration 19845, loss = 0.05690947\n",
      "Iteration 19846, loss = 0.05690041\n",
      "Iteration 19847, loss = 0.05688899\n",
      "Iteration 19848, loss = 0.05688436\n",
      "Iteration 19849, loss = 0.05688068\n",
      "Iteration 19850, loss = 0.05688038\n",
      "Iteration 19851, loss = 0.05687907\n",
      "Iteration 19852, loss = 0.05687430\n",
      "Iteration 19853, loss = 0.05686553\n",
      "Iteration 19854, loss = 0.05685492\n",
      "Iteration 19855, loss = 0.05685298\n",
      "Iteration 19856, loss = 0.05684948\n",
      "Iteration 19857, loss = 0.05684582\n",
      "Iteration 19858, loss = 0.05684117\n",
      "Iteration 19859, loss = 0.05683447\n",
      "Iteration 19860, loss = 0.05682925\n",
      "Iteration 19861, loss = 0.05682853\n",
      "Iteration 19862, loss = 0.05682098\n",
      "Iteration 19863, loss = 0.05681808\n",
      "Iteration 19864, loss = 0.05680919\n",
      "Iteration 19865, loss = 0.05681012\n",
      "Iteration 19866, loss = 0.05680703\n",
      "Iteration 19867, loss = 0.05680376\n",
      "Iteration 19868, loss = 0.05679335\n",
      "Iteration 19869, loss = 0.05678940\n",
      "Iteration 19870, loss = 0.05678818\n",
      "Iteration 19871, loss = 0.05678606\n",
      "Iteration 19872, loss = 0.05678083\n",
      "Iteration 19873, loss = 0.05677264\n",
      "Iteration 19874, loss = 0.05677143\n",
      "Iteration 19875, loss = 0.05676992\n",
      "Iteration 19876, loss = 0.05676335\n",
      "Iteration 19877, loss = 0.05676010\n",
      "Iteration 19878, loss = 0.05675828\n",
      "Iteration 19879, loss = 0.05675335\n",
      "Iteration 19880, loss = 0.05675145\n",
      "Iteration 19881, loss = 0.05674689\n",
      "Iteration 19882, loss = 0.05674418\n",
      "Iteration 19883, loss = 0.05674564\n",
      "Iteration 19884, loss = 0.05673919\n",
      "Iteration 19885, loss = 0.05673311\n",
      "Iteration 19886, loss = 0.05672415\n",
      "Iteration 19887, loss = 0.05672682\n",
      "Iteration 19888, loss = 0.05672117\n",
      "Iteration 19889, loss = 0.05671127\n",
      "Iteration 19890, loss = 0.05670663\n",
      "Iteration 19891, loss = 0.05670491\n",
      "Iteration 19892, loss = 0.05670371\n",
      "Iteration 19893, loss = 0.05669795\n",
      "Iteration 19894, loss = 0.05669088\n",
      "Iteration 19895, loss = 0.05668837\n",
      "Iteration 19896, loss = 0.05668366\n",
      "Iteration 19897, loss = 0.05667885\n",
      "Iteration 19898, loss = 0.05667898\n",
      "Iteration 19899, loss = 0.05667469\n",
      "Iteration 19900, loss = 0.05666950\n",
      "Iteration 19901, loss = 0.05666081\n",
      "Iteration 19902, loss = 0.05665595\n",
      "Iteration 19903, loss = 0.05665609\n",
      "Iteration 19904, loss = 0.05665148\n",
      "Iteration 19905, loss = 0.05664320\n",
      "Iteration 19906, loss = 0.05663960\n",
      "Iteration 19907, loss = 0.05663887\n",
      "Iteration 19908, loss = 0.05663168\n",
      "Iteration 19909, loss = 0.05662847\n",
      "Iteration 19910, loss = 0.05662341\n",
      "Iteration 19911, loss = 0.05661988\n",
      "Iteration 19912, loss = 0.05661480\n",
      "Iteration 19913, loss = 0.05661236\n",
      "Iteration 19914, loss = 0.05660773\n",
      "Iteration 19915, loss = 0.05660234\n",
      "Iteration 19916, loss = 0.05659725\n",
      "Iteration 19917, loss = 0.05659241\n",
      "Iteration 19918, loss = 0.05658933\n",
      "Iteration 19919, loss = 0.05658509\n",
      "Iteration 19920, loss = 0.05658168\n",
      "Iteration 19921, loss = 0.05657841\n",
      "Iteration 19922, loss = 0.05657416\n",
      "Iteration 19923, loss = 0.05656742\n",
      "Iteration 19924, loss = 0.05656318\n",
      "Iteration 19925, loss = 0.05656249\n",
      "Iteration 19926, loss = 0.05655822\n",
      "Iteration 19927, loss = 0.05655449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19928, loss = 0.05655206\n",
      "Iteration 19929, loss = 0.05654848\n",
      "Iteration 19930, loss = 0.05654291\n",
      "Iteration 19931, loss = 0.05654008\n",
      "Iteration 19932, loss = 0.05653757\n",
      "Iteration 19933, loss = 0.05653174\n",
      "Iteration 19934, loss = 0.05652595\n",
      "Iteration 19935, loss = 0.05651586\n",
      "Iteration 19936, loss = 0.05650901\n",
      "Iteration 19937, loss = 0.05651543\n",
      "Iteration 19938, loss = 0.05651369\n",
      "Iteration 19939, loss = 0.05650145\n",
      "Iteration 19940, loss = 0.05649848\n",
      "Iteration 19941, loss = 0.05650180\n",
      "Iteration 19942, loss = 0.05649999\n",
      "Iteration 19943, loss = 0.05649260\n",
      "Iteration 19944, loss = 0.05649088\n",
      "Iteration 19945, loss = 0.05648832\n",
      "Iteration 19946, loss = 0.05648057\n",
      "Iteration 19947, loss = 0.05647318\n",
      "Iteration 19948, loss = 0.05647095\n",
      "Iteration 19949, loss = 0.05646678\n",
      "Iteration 19950, loss = 0.05646249\n",
      "Iteration 19951, loss = 0.05645854\n",
      "Iteration 19952, loss = 0.05644661\n",
      "Iteration 19953, loss = 0.05644738\n",
      "Iteration 19954, loss = 0.05644876\n",
      "Iteration 19955, loss = 0.05644300\n",
      "Iteration 19956, loss = 0.05643593\n",
      "Iteration 19957, loss = 0.05643507\n",
      "Iteration 19958, loss = 0.05643110\n",
      "Iteration 19959, loss = 0.05642755\n",
      "Iteration 19960, loss = 0.05642394\n",
      "Iteration 19961, loss = 0.05641848\n",
      "Iteration 19962, loss = 0.05641037\n",
      "Iteration 19963, loss = 0.05640385\n",
      "Iteration 19964, loss = 0.05640595\n",
      "Iteration 19965, loss = 0.05640226\n",
      "Iteration 19966, loss = 0.05639433\n",
      "Iteration 19967, loss = 0.05638638\n",
      "Iteration 19968, loss = 0.05638726\n",
      "Iteration 19969, loss = 0.05638179\n",
      "Iteration 19970, loss = 0.05637790\n",
      "Iteration 19971, loss = 0.05638068\n",
      "Iteration 19972, loss = 0.05637460\n",
      "Iteration 19973, loss = 0.05637033\n",
      "Iteration 19974, loss = 0.05636824\n",
      "Iteration 19975, loss = 0.05636345\n",
      "Iteration 19976, loss = 0.05635609\n",
      "Iteration 19977, loss = 0.05635137\n",
      "Iteration 19978, loss = 0.05635133\n",
      "Iteration 19979, loss = 0.05634534\n",
      "Iteration 19980, loss = 0.05634243\n",
      "Iteration 19981, loss = 0.05633671\n",
      "Iteration 19982, loss = 0.05633181\n",
      "Iteration 19983, loss = 0.05633088\n",
      "Iteration 19984, loss = 0.05632192\n",
      "Iteration 19985, loss = 0.05631658\n",
      "Iteration 19986, loss = 0.05631480\n",
      "Iteration 19987, loss = 0.05631639\n",
      "Iteration 19988, loss = 0.05631064\n",
      "Iteration 19989, loss = 0.05630419\n",
      "Iteration 19990, loss = 0.05629960\n",
      "Iteration 19991, loss = 0.05629185\n",
      "Iteration 19992, loss = 0.05629012\n",
      "Iteration 19993, loss = 0.05629087\n",
      "Iteration 19994, loss = 0.05628410\n",
      "Iteration 19995, loss = 0.05627633\n",
      "Iteration 19996, loss = 0.05627354\n",
      "Iteration 19997, loss = 0.05626880\n",
      "Iteration 19998, loss = 0.05626290\n",
      "Iteration 19999, loss = 0.05626051\n",
      "Iteration 20000, loss = 0.05625579\n",
      "Iteration 20001, loss = 0.05625569\n",
      "Iteration 20002, loss = 0.05625020\n",
      "Iteration 20003, loss = 0.05624423\n",
      "Iteration 20004, loss = 0.05623959\n",
      "Iteration 20005, loss = 0.05623981\n",
      "Iteration 20006, loss = 0.05623603\n",
      "Iteration 20007, loss = 0.05622649\n",
      "Iteration 20008, loss = 0.05622237\n",
      "Iteration 20009, loss = 0.05621889\n",
      "Iteration 20010, loss = 0.05621646\n",
      "Iteration 20011, loss = 0.05621395\n",
      "Iteration 20012, loss = 0.05620713\n",
      "Iteration 20013, loss = 0.05620812\n",
      "Iteration 20014, loss = 0.05620670\n",
      "Iteration 20015, loss = 0.05619947\n",
      "Iteration 20016, loss = 0.05619710\n",
      "Iteration 20017, loss = 0.05619308\n",
      "Iteration 20018, loss = 0.05618778\n",
      "Iteration 20019, loss = 0.05618099\n",
      "Iteration 20020, loss = 0.05617737\n",
      "Iteration 20021, loss = 0.05617720\n",
      "Iteration 20022, loss = 0.05617362\n",
      "Iteration 20023, loss = 0.05616671\n",
      "Iteration 20024, loss = 0.05616075\n",
      "Iteration 20025, loss = 0.05615515\n",
      "Iteration 20026, loss = 0.05615907\n",
      "Iteration 20027, loss = 0.05615244\n",
      "Iteration 20028, loss = 0.05614405\n",
      "Iteration 20029, loss = 0.05614152\n",
      "Iteration 20030, loss = 0.05613984\n",
      "Iteration 20031, loss = 0.05613525\n",
      "Iteration 20032, loss = 0.05613272\n",
      "Iteration 20033, loss = 0.05612677\n",
      "Iteration 20034, loss = 0.05612154\n",
      "Iteration 20035, loss = 0.05611920\n",
      "Iteration 20036, loss = 0.05612147\n",
      "Iteration 20037, loss = 0.05611930\n",
      "Iteration 20038, loss = 0.05611008\n",
      "Iteration 20039, loss = 0.05610283\n",
      "Iteration 20040, loss = 0.05609746\n",
      "Iteration 20041, loss = 0.05609638\n",
      "Iteration 20042, loss = 0.05609949\n",
      "Iteration 20043, loss = 0.05609038\n",
      "Iteration 20044, loss = 0.05608234\n",
      "Iteration 20045, loss = 0.05608043\n",
      "Iteration 20046, loss = 0.05607978\n",
      "Iteration 20047, loss = 0.05607703\n",
      "Iteration 20048, loss = 0.05606962\n",
      "Iteration 20049, loss = 0.05606351\n",
      "Iteration 20050, loss = 0.05605732\n",
      "Iteration 20051, loss = 0.05604757\n",
      "Iteration 20052, loss = 0.05604050\n",
      "Iteration 20053, loss = 0.05604447\n",
      "Iteration 20054, loss = 0.05603594\n",
      "Iteration 20055, loss = 0.05602918\n",
      "Iteration 20056, loss = 0.05602516\n",
      "Iteration 20057, loss = 0.05602508\n",
      "Iteration 20058, loss = 0.05602040\n",
      "Iteration 20059, loss = 0.05601274\n",
      "Iteration 20060, loss = 0.05600628\n",
      "Iteration 20061, loss = 0.05600661\n",
      "Iteration 20062, loss = 0.05599703\n",
      "Iteration 20063, loss = 0.05599794\n",
      "Iteration 20064, loss = 0.05599926\n",
      "Iteration 20065, loss = 0.05599480\n",
      "Iteration 20066, loss = 0.05598567\n",
      "Iteration 20067, loss = 0.05598412\n",
      "Iteration 20068, loss = 0.05597842\n",
      "Iteration 20069, loss = 0.05597324\n",
      "Iteration 20070, loss = 0.05597401\n",
      "Iteration 20071, loss = 0.05596744\n",
      "Iteration 20072, loss = 0.05595434\n",
      "Iteration 20073, loss = 0.05595514\n",
      "Iteration 20074, loss = 0.05595356\n",
      "Iteration 20075, loss = 0.05594793\n",
      "Iteration 20076, loss = 0.05594538\n",
      "Iteration 20077, loss = 0.05594343\n",
      "Iteration 20078, loss = 0.05593819\n",
      "Iteration 20079, loss = 0.05592847\n",
      "Iteration 20080, loss = 0.05591815\n",
      "Iteration 20081, loss = 0.05591992\n",
      "Iteration 20082, loss = 0.05591800\n",
      "Iteration 20083, loss = 0.05590995\n",
      "Iteration 20084, loss = 0.05590768\n",
      "Iteration 20085, loss = 0.05589970\n",
      "Iteration 20086, loss = 0.05589865\n",
      "Iteration 20087, loss = 0.05589716\n",
      "Iteration 20088, loss = 0.05589214\n",
      "Iteration 20089, loss = 0.05588568\n",
      "Iteration 20090, loss = 0.05587736\n",
      "Iteration 20091, loss = 0.05587286\n",
      "Iteration 20092, loss = 0.05587128\n",
      "Iteration 20093, loss = 0.05586666\n",
      "Iteration 20094, loss = 0.05586404\n",
      "Iteration 20095, loss = 0.05585828\n",
      "Iteration 20096, loss = 0.05585184\n",
      "Iteration 20097, loss = 0.05584913\n",
      "Iteration 20098, loss = 0.05584627\n",
      "Iteration 20099, loss = 0.05584225\n",
      "Iteration 20100, loss = 0.05583954\n",
      "Iteration 20101, loss = 0.05583306\n",
      "Iteration 20102, loss = 0.05582510\n",
      "Iteration 20103, loss = 0.05582110\n",
      "Iteration 20104, loss = 0.05581542\n",
      "Iteration 20105, loss = 0.05581263\n",
      "Iteration 20106, loss = 0.05580665\n",
      "Iteration 20107, loss = 0.05580059\n",
      "Iteration 20108, loss = 0.05580042\n",
      "Iteration 20109, loss = 0.05579417\n",
      "Iteration 20110, loss = 0.05578888\n",
      "Iteration 20111, loss = 0.05578475\n",
      "Iteration 20112, loss = 0.05578543\n",
      "Iteration 20113, loss = 0.05578006\n",
      "Iteration 20114, loss = 0.05577243\n",
      "Iteration 20115, loss = 0.05576714\n",
      "Iteration 20116, loss = 0.05577076\n",
      "Iteration 20117, loss = 0.05576490\n",
      "Iteration 20118, loss = 0.05575580\n",
      "Iteration 20119, loss = 0.05575093\n",
      "Iteration 20120, loss = 0.05574779\n",
      "Iteration 20121, loss = 0.05574203\n",
      "Iteration 20122, loss = 0.05573738\n",
      "Iteration 20123, loss = 0.05573710\n",
      "Iteration 20124, loss = 0.05573122\n",
      "Iteration 20125, loss = 0.05572664\n",
      "Iteration 20126, loss = 0.05572406\n",
      "Iteration 20127, loss = 0.05572110\n",
      "Iteration 20128, loss = 0.05571705\n",
      "Iteration 20129, loss = 0.05570894\n",
      "Iteration 20130, loss = 0.05570802\n",
      "Iteration 20131, loss = 0.05570471\n",
      "Iteration 20132, loss = 0.05570025\n",
      "Iteration 20133, loss = 0.05569588\n",
      "Iteration 20134, loss = 0.05569042\n",
      "Iteration 20135, loss = 0.05568888\n",
      "Iteration 20136, loss = 0.05568741\n",
      "Iteration 20137, loss = 0.05567911\n",
      "Iteration 20138, loss = 0.05567777\n",
      "Iteration 20139, loss = 0.05567191\n",
      "Iteration 20140, loss = 0.05566416\n",
      "Iteration 20141, loss = 0.05566221\n",
      "Iteration 20142, loss = 0.05566222\n",
      "Iteration 20143, loss = 0.05565872\n",
      "Iteration 20144, loss = 0.05565726\n",
      "Iteration 20145, loss = 0.05565219\n",
      "Iteration 20146, loss = 0.05564593\n",
      "Iteration 20147, loss = 0.05564133\n",
      "Iteration 20148, loss = 0.05563745\n",
      "Iteration 20149, loss = 0.05563118\n",
      "Iteration 20150, loss = 0.05562402\n",
      "Iteration 20151, loss = 0.05561974\n",
      "Iteration 20152, loss = 0.05561670\n",
      "Iteration 20153, loss = 0.05561498\n",
      "Iteration 20154, loss = 0.05560726\n",
      "Iteration 20155, loss = 0.05560917\n",
      "Iteration 20156, loss = 0.05560630\n",
      "Iteration 20157, loss = 0.05560183\n",
      "Iteration 20158, loss = 0.05560000\n",
      "Iteration 20159, loss = 0.05559393\n",
      "Iteration 20160, loss = 0.05558342\n",
      "Iteration 20161, loss = 0.05558269\n",
      "Iteration 20162, loss = 0.05557872\n",
      "Iteration 20163, loss = 0.05557390\n",
      "Iteration 20164, loss = 0.05556762\n",
      "Iteration 20165, loss = 0.05556453\n",
      "Iteration 20166, loss = 0.05555926\n",
      "Iteration 20167, loss = 0.05556138\n",
      "Iteration 20168, loss = 0.05556286\n",
      "Iteration 20169, loss = 0.05555707\n",
      "Iteration 20170, loss = 0.05555322\n",
      "Iteration 20171, loss = 0.05554436\n",
      "Iteration 20172, loss = 0.05553450\n",
      "Iteration 20173, loss = 0.05553366\n",
      "Iteration 20174, loss = 0.05553235\n",
      "Iteration 20175, loss = 0.05552482\n",
      "Iteration 20176, loss = 0.05552028\n",
      "Iteration 20177, loss = 0.05551567\n",
      "Iteration 20178, loss = 0.05551209\n",
      "Iteration 20179, loss = 0.05551368\n",
      "Iteration 20180, loss = 0.05551122\n",
      "Iteration 20181, loss = 0.05550417\n",
      "Iteration 20182, loss = 0.05549939\n",
      "Iteration 20183, loss = 0.05549346\n",
      "Iteration 20184, loss = 0.05548917\n",
      "Iteration 20185, loss = 0.05549157\n",
      "Iteration 20186, loss = 0.05548409\n",
      "Iteration 20187, loss = 0.05547978\n",
      "Iteration 20188, loss = 0.05547609\n",
      "Iteration 20189, loss = 0.05547178\n",
      "Iteration 20190, loss = 0.05546716\n",
      "Iteration 20191, loss = 0.05546560\n",
      "Iteration 20192, loss = 0.05546239\n",
      "Iteration 20193, loss = 0.05545447\n",
      "Iteration 20194, loss = 0.05544825\n",
      "Iteration 20195, loss = 0.05544482\n",
      "Iteration 20196, loss = 0.05544457\n",
      "Iteration 20197, loss = 0.05543563\n",
      "Iteration 20198, loss = 0.05542964\n",
      "Iteration 20199, loss = 0.05543124\n",
      "Iteration 20200, loss = 0.05542639\n",
      "Iteration 20201, loss = 0.05542210\n",
      "Iteration 20202, loss = 0.05541644\n",
      "Iteration 20203, loss = 0.05541588\n",
      "Iteration 20204, loss = 0.05541191\n",
      "Iteration 20205, loss = 0.05540693\n",
      "Iteration 20206, loss = 0.05540823\n",
      "Iteration 20207, loss = 0.05540429\n",
      "Iteration 20208, loss = 0.05539548\n",
      "Iteration 20209, loss = 0.05539464\n",
      "Iteration 20210, loss = 0.05539204\n",
      "Iteration 20211, loss = 0.05538425\n",
      "Iteration 20212, loss = 0.05538266\n",
      "Iteration 20213, loss = 0.05537859\n",
      "Iteration 20214, loss = 0.05537440\n",
      "Iteration 20215, loss = 0.05536780\n",
      "Iteration 20216, loss = 0.05536543\n",
      "Iteration 20217, loss = 0.05536437\n",
      "Iteration 20218, loss = 0.05535922\n",
      "Iteration 20219, loss = 0.05535172\n",
      "Iteration 20220, loss = 0.05535084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20221, loss = 0.05534969\n",
      "Iteration 20222, loss = 0.05534478\n",
      "Iteration 20223, loss = 0.05533904\n",
      "Iteration 20224, loss = 0.05533519\n",
      "Iteration 20225, loss = 0.05533639\n",
      "Iteration 20226, loss = 0.05533245\n",
      "Iteration 20227, loss = 0.05532565\n",
      "Iteration 20228, loss = 0.05531826\n",
      "Iteration 20229, loss = 0.05531929\n",
      "Iteration 20230, loss = 0.05531652\n",
      "Iteration 20231, loss = 0.05531240\n",
      "Iteration 20232, loss = 0.05530675\n",
      "Iteration 20233, loss = 0.05529947\n",
      "Iteration 20234, loss = 0.05529588\n",
      "Iteration 20235, loss = 0.05529214\n",
      "Iteration 20236, loss = 0.05528467\n",
      "Iteration 20237, loss = 0.05528182\n",
      "Iteration 20238, loss = 0.05527814\n",
      "Iteration 20239, loss = 0.05527450\n",
      "Iteration 20240, loss = 0.05527368\n",
      "Iteration 20241, loss = 0.05527000\n",
      "Iteration 20242, loss = 0.05526784\n",
      "Iteration 20243, loss = 0.05526452\n",
      "Iteration 20244, loss = 0.05526268\n",
      "Iteration 20245, loss = 0.05525536\n",
      "Iteration 20246, loss = 0.05525052\n",
      "Iteration 20247, loss = 0.05524595\n",
      "Iteration 20248, loss = 0.05524158\n",
      "Iteration 20249, loss = 0.05523784\n",
      "Iteration 20250, loss = 0.05523494\n",
      "Iteration 20251, loss = 0.05523401\n",
      "Iteration 20252, loss = 0.05522490\n",
      "Iteration 20253, loss = 0.05522049\n",
      "Iteration 20254, loss = 0.05521947\n",
      "Iteration 20255, loss = 0.05522127\n",
      "Iteration 20256, loss = 0.05521411\n",
      "Iteration 20257, loss = 0.05520487\n",
      "Iteration 20258, loss = 0.05520897\n",
      "Iteration 20259, loss = 0.05520528\n",
      "Iteration 20260, loss = 0.05519850\n",
      "Iteration 20261, loss = 0.05519154\n",
      "Iteration 20262, loss = 0.05518478\n",
      "Iteration 20263, loss = 0.05518683\n",
      "Iteration 20264, loss = 0.05518440\n",
      "Iteration 20265, loss = 0.05517649\n",
      "Iteration 20266, loss = 0.05517507\n",
      "Iteration 20267, loss = 0.05517511\n",
      "Iteration 20268, loss = 0.05517435\n",
      "Iteration 20269, loss = 0.05517043\n",
      "Iteration 20270, loss = 0.05516531\n",
      "Iteration 20271, loss = 0.05515761\n",
      "Iteration 20272, loss = 0.05515461\n",
      "Iteration 20273, loss = 0.05514941\n",
      "Iteration 20274, loss = 0.05514617\n",
      "Iteration 20275, loss = 0.05513972\n",
      "Iteration 20276, loss = 0.05514056\n",
      "Iteration 20277, loss = 0.05513757\n",
      "Iteration 20278, loss = 0.05513262\n",
      "Iteration 20279, loss = 0.05513054\n",
      "Iteration 20280, loss = 0.05512680\n",
      "Iteration 20281, loss = 0.05512034\n",
      "Iteration 20282, loss = 0.05511605\n",
      "Iteration 20283, loss = 0.05511403\n",
      "Iteration 20284, loss = 0.05510570\n",
      "Iteration 20285, loss = 0.05510331\n",
      "Iteration 20286, loss = 0.05510349\n",
      "Iteration 20287, loss = 0.05509558\n",
      "Iteration 20288, loss = 0.05509753\n",
      "Iteration 20289, loss = 0.05509377\n",
      "Iteration 20290, loss = 0.05508911\n",
      "Iteration 20291, loss = 0.05509057\n",
      "Iteration 20292, loss = 0.05508284\n",
      "Iteration 20293, loss = 0.05507304\n",
      "Iteration 20294, loss = 0.05507552\n",
      "Iteration 20295, loss = 0.05507515\n",
      "Iteration 20296, loss = 0.05507254\n",
      "Iteration 20297, loss = 0.05506658\n",
      "Iteration 20298, loss = 0.05506169\n",
      "Iteration 20299, loss = 0.05506107\n",
      "Iteration 20300, loss = 0.05505201\n",
      "Iteration 20301, loss = 0.05504712\n",
      "Iteration 20302, loss = 0.05504945\n",
      "Iteration 20303, loss = 0.05504488\n",
      "Iteration 20304, loss = 0.05503538\n",
      "Iteration 20305, loss = 0.05503432\n",
      "Iteration 20306, loss = 0.05503398\n",
      "Iteration 20307, loss = 0.05502966\n",
      "Iteration 20308, loss = 0.05502180\n",
      "Iteration 20309, loss = 0.05502367\n",
      "Iteration 20310, loss = 0.05501888\n",
      "Iteration 20311, loss = 0.05501275\n",
      "Iteration 20312, loss = 0.05500451\n",
      "Iteration 20313, loss = 0.05500181\n",
      "Iteration 20314, loss = 0.05499870\n",
      "Iteration 20315, loss = 0.05499397\n",
      "Iteration 20316, loss = 0.05498982\n",
      "Iteration 20317, loss = 0.05498367\n",
      "Iteration 20318, loss = 0.05498405\n",
      "Iteration 20319, loss = 0.05498121\n",
      "Iteration 20320, loss = 0.05497863\n",
      "Iteration 20321, loss = 0.05497301\n",
      "Iteration 20322, loss = 0.05496834\n",
      "Iteration 20323, loss = 0.05496392\n",
      "Iteration 20324, loss = 0.05496076\n",
      "Iteration 20325, loss = 0.05495532\n",
      "Iteration 20326, loss = 0.05495283\n",
      "Iteration 20327, loss = 0.05495435\n",
      "Iteration 20328, loss = 0.05494474\n",
      "Iteration 20329, loss = 0.05494106\n",
      "Iteration 20330, loss = 0.05493789\n",
      "Iteration 20331, loss = 0.05492923\n",
      "Iteration 20332, loss = 0.05492626\n",
      "Iteration 20333, loss = 0.05492082\n",
      "Iteration 20334, loss = 0.05492353\n",
      "Iteration 20335, loss = 0.05492156\n",
      "Iteration 20336, loss = 0.05491562\n",
      "Iteration 20337, loss = 0.05490935\n",
      "Iteration 20338, loss = 0.05490838\n",
      "Iteration 20339, loss = 0.05490528\n",
      "Iteration 20340, loss = 0.05490089\n",
      "Iteration 20341, loss = 0.05489209\n",
      "Iteration 20342, loss = 0.05488912\n",
      "Iteration 20343, loss = 0.05489333\n",
      "Iteration 20344, loss = 0.05488951\n",
      "Iteration 20345, loss = 0.05488077\n",
      "Iteration 20346, loss = 0.05487483\n",
      "Iteration 20347, loss = 0.05487796\n",
      "Iteration 20348, loss = 0.05487471\n",
      "Iteration 20349, loss = 0.05486366\n",
      "Iteration 20350, loss = 0.05486348\n",
      "Iteration 20351, loss = 0.05485947\n",
      "Iteration 20352, loss = 0.05485583\n",
      "Iteration 20353, loss = 0.05485287\n",
      "Iteration 20354, loss = 0.05485010\n",
      "Iteration 20355, loss = 0.05484715\n",
      "Iteration 20356, loss = 0.05484089\n",
      "Iteration 20357, loss = 0.05483659\n",
      "Iteration 20358, loss = 0.05483444\n",
      "Iteration 20359, loss = 0.05483030\n",
      "Iteration 20360, loss = 0.05482677\n",
      "Iteration 20361, loss = 0.05482094\n",
      "Iteration 20362, loss = 0.05481931\n",
      "Iteration 20363, loss = 0.05481728\n",
      "Iteration 20364, loss = 0.05481363\n",
      "Iteration 20365, loss = 0.05480639\n",
      "Iteration 20366, loss = 0.05480028\n",
      "Iteration 20367, loss = 0.05479884\n",
      "Iteration 20368, loss = 0.05479567\n",
      "Iteration 20369, loss = 0.05478995\n",
      "Iteration 20370, loss = 0.05478895\n",
      "Iteration 20371, loss = 0.05478445\n",
      "Iteration 20372, loss = 0.05478036\n",
      "Iteration 20373, loss = 0.05477553\n",
      "Iteration 20374, loss = 0.05476934\n",
      "Iteration 20375, loss = 0.05477019\n",
      "Iteration 20376, loss = 0.05476528\n",
      "Iteration 20377, loss = 0.05476081\n",
      "Iteration 20378, loss = 0.05475894\n",
      "Iteration 20379, loss = 0.05475337\n",
      "Iteration 20380, loss = 0.05475084\n",
      "Iteration 20381, loss = 0.05475223\n",
      "Iteration 20382, loss = 0.05474840\n",
      "Iteration 20383, loss = 0.05474197\n",
      "Iteration 20384, loss = 0.05473828\n",
      "Iteration 20385, loss = 0.05473311\n",
      "Iteration 20386, loss = 0.05472865\n",
      "Iteration 20387, loss = 0.05472776\n",
      "Iteration 20388, loss = 0.05472058\n",
      "Iteration 20389, loss = 0.05472333\n",
      "Iteration 20390, loss = 0.05471704\n",
      "Iteration 20391, loss = 0.05471108\n",
      "Iteration 20392, loss = 0.05470957\n",
      "Iteration 20393, loss = 0.05471096\n",
      "Iteration 20394, loss = 0.05470770\n",
      "Iteration 20395, loss = 0.05469462\n",
      "Iteration 20396, loss = 0.05469336\n",
      "Iteration 20397, loss = 0.05469452\n",
      "Iteration 20398, loss = 0.05468746\n",
      "Iteration 20399, loss = 0.05468096\n",
      "Iteration 20400, loss = 0.05468312\n",
      "Iteration 20401, loss = 0.05468191\n",
      "Iteration 20402, loss = 0.05467734\n",
      "Iteration 20403, loss = 0.05467280\n",
      "Iteration 20404, loss = 0.05466667\n",
      "Iteration 20405, loss = 0.05466328\n",
      "Iteration 20406, loss = 0.05465800\n",
      "Iteration 20407, loss = 0.05465434\n",
      "Iteration 20408, loss = 0.05464942\n",
      "Iteration 20409, loss = 0.05464648\n",
      "Iteration 20410, loss = 0.05464489\n",
      "Iteration 20411, loss = 0.05463726\n",
      "Iteration 20412, loss = 0.05463357\n",
      "Iteration 20413, loss = 0.05463113\n",
      "Iteration 20414, loss = 0.05462906\n",
      "Iteration 20415, loss = 0.05462085\n",
      "Iteration 20416, loss = 0.05462108\n",
      "Iteration 20417, loss = 0.05462167\n",
      "Iteration 20418, loss = 0.05461638\n",
      "Iteration 20419, loss = 0.05461263\n",
      "Iteration 20420, loss = 0.05460707\n",
      "Iteration 20421, loss = 0.05460240\n",
      "Iteration 20422, loss = 0.05459682\n",
      "Iteration 20423, loss = 0.05459498\n",
      "Iteration 20424, loss = 0.05459135\n",
      "Iteration 20425, loss = 0.05458524\n",
      "Iteration 20426, loss = 0.05458795\n",
      "Iteration 20427, loss = 0.05458982\n",
      "Iteration 20428, loss = 0.05458322\n",
      "Iteration 20429, loss = 0.05457633\n",
      "Iteration 20430, loss = 0.05457489\n",
      "Iteration 20431, loss = 0.05456755\n",
      "Iteration 20432, loss = 0.05456548\n",
      "Iteration 20433, loss = 0.05456285\n",
      "Iteration 20434, loss = 0.05455744\n",
      "Iteration 20435, loss = 0.05455514\n",
      "Iteration 20436, loss = 0.05455272\n",
      "Iteration 20437, loss = 0.05455057\n",
      "Iteration 20438, loss = 0.05454121\n",
      "Iteration 20439, loss = 0.05453459\n",
      "Iteration 20440, loss = 0.05453980\n",
      "Iteration 20441, loss = 0.05453815\n",
      "Iteration 20442, loss = 0.05452829\n",
      "Iteration 20443, loss = 0.05452092\n",
      "Iteration 20444, loss = 0.05451947\n",
      "Iteration 20445, loss = 0.05452088\n",
      "Iteration 20446, loss = 0.05451344\n",
      "Iteration 20447, loss = 0.05451153\n",
      "Iteration 20448, loss = 0.05451052\n",
      "Iteration 20449, loss = 0.05450742\n",
      "Iteration 20450, loss = 0.05450215\n",
      "Iteration 20451, loss = 0.05449688\n",
      "Iteration 20452, loss = 0.05449212\n",
      "Iteration 20453, loss = 0.05448394\n",
      "Iteration 20454, loss = 0.05448216\n",
      "Iteration 20455, loss = 0.05448212\n",
      "Iteration 20456, loss = 0.05448152\n",
      "Iteration 20457, loss = 0.05447644\n",
      "Iteration 20458, loss = 0.05447040\n",
      "Iteration 20459, loss = 0.05446433\n",
      "Iteration 20460, loss = 0.05446379\n",
      "Iteration 20461, loss = 0.05445942\n",
      "Iteration 20462, loss = 0.05445054\n",
      "Iteration 20463, loss = 0.05445049\n",
      "Iteration 20464, loss = 0.05445029\n",
      "Iteration 20465, loss = 0.05444491\n",
      "Iteration 20466, loss = 0.05443837\n",
      "Iteration 20467, loss = 0.05443661\n",
      "Iteration 20468, loss = 0.05443228\n",
      "Iteration 20469, loss = 0.05442521\n",
      "Iteration 20470, loss = 0.05442614\n",
      "Iteration 20471, loss = 0.05442100\n",
      "Iteration 20472, loss = 0.05441453\n",
      "Iteration 20473, loss = 0.05441049\n",
      "Iteration 20474, loss = 0.05441368\n",
      "Iteration 20475, loss = 0.05440828\n",
      "Iteration 20476, loss = 0.05440124\n",
      "Iteration 20477, loss = 0.05439780\n",
      "Iteration 20478, loss = 0.05439531\n",
      "Iteration 20479, loss = 0.05439139\n",
      "Iteration 20480, loss = 0.05438407\n",
      "Iteration 20481, loss = 0.05438169\n",
      "Iteration 20482, loss = 0.05437759\n",
      "Iteration 20483, loss = 0.05437457\n",
      "Iteration 20484, loss = 0.05437433\n",
      "Iteration 20485, loss = 0.05436652\n",
      "Iteration 20486, loss = 0.05436377\n",
      "Iteration 20487, loss = 0.05436066\n",
      "Iteration 20488, loss = 0.05435471\n",
      "Iteration 20489, loss = 0.05434697\n",
      "Iteration 20490, loss = 0.05434574\n",
      "Iteration 20491, loss = 0.05434441\n",
      "Iteration 20492, loss = 0.05434147\n",
      "Iteration 20493, loss = 0.05433735\n",
      "Iteration 20494, loss = 0.05433446\n",
      "Iteration 20495, loss = 0.05432467\n",
      "Iteration 20496, loss = 0.05432293\n",
      "Iteration 20497, loss = 0.05432423\n",
      "Iteration 20498, loss = 0.05431593\n",
      "Iteration 20499, loss = 0.05431072\n",
      "Iteration 20500, loss = 0.05430700\n",
      "Iteration 20501, loss = 0.05430137\n",
      "Iteration 20502, loss = 0.05430068\n",
      "Iteration 20503, loss = 0.05429453\n",
      "Iteration 20504, loss = 0.05428996\n",
      "Iteration 20505, loss = 0.05429191\n",
      "Iteration 20506, loss = 0.05428712\n",
      "Iteration 20507, loss = 0.05427975\n",
      "Iteration 20508, loss = 0.05427572\n",
      "Iteration 20509, loss = 0.05426931\n",
      "Iteration 20510, loss = 0.05427175\n",
      "Iteration 20511, loss = 0.05426742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20512, loss = 0.05425935\n",
      "Iteration 20513, loss = 0.05425756\n",
      "Iteration 20514, loss = 0.05425708\n",
      "Iteration 20515, loss = 0.05425628\n",
      "Iteration 20516, loss = 0.05425392\n",
      "Iteration 20517, loss = 0.05424579\n",
      "Iteration 20518, loss = 0.05424236\n",
      "Iteration 20519, loss = 0.05424008\n",
      "Iteration 20520, loss = 0.05423449\n",
      "Iteration 20521, loss = 0.05422666\n",
      "Iteration 20522, loss = 0.05422646\n",
      "Iteration 20523, loss = 0.05421810\n",
      "Iteration 20524, loss = 0.05422195\n",
      "Iteration 20525, loss = 0.05421259\n",
      "Iteration 20526, loss = 0.05420616\n",
      "Iteration 20527, loss = 0.05420796\n",
      "Iteration 20528, loss = 0.05420711\n",
      "Iteration 20529, loss = 0.05419914\n",
      "Iteration 20530, loss = 0.05419573\n",
      "Iteration 20531, loss = 0.05419043\n",
      "Iteration 20532, loss = 0.05418753\n",
      "Iteration 20533, loss = 0.05418242\n",
      "Iteration 20534, loss = 0.05418411\n",
      "Iteration 20535, loss = 0.05417936\n",
      "Iteration 20536, loss = 0.05417556\n",
      "Iteration 20537, loss = 0.05417234\n",
      "Iteration 20538, loss = 0.05416676\n",
      "Iteration 20539, loss = 0.05416446\n",
      "Iteration 20540, loss = 0.05415648\n",
      "Iteration 20541, loss = 0.05414965\n",
      "Iteration 20542, loss = 0.05414855\n",
      "Iteration 20543, loss = 0.05414619\n",
      "Iteration 20544, loss = 0.05414217\n",
      "Iteration 20545, loss = 0.05414108\n",
      "Iteration 20546, loss = 0.05413515\n",
      "Iteration 20547, loss = 0.05413077\n",
      "Iteration 20548, loss = 0.05412582\n",
      "Iteration 20549, loss = 0.05412639\n",
      "Iteration 20550, loss = 0.05412166\n",
      "Iteration 20551, loss = 0.05411534\n",
      "Iteration 20552, loss = 0.05411173\n",
      "Iteration 20553, loss = 0.05410997\n",
      "Iteration 20554, loss = 0.05410617\n",
      "Iteration 20555, loss = 0.05410475\n",
      "Iteration 20556, loss = 0.05410140\n",
      "Iteration 20557, loss = 0.05409691\n",
      "Iteration 20558, loss = 0.05409213\n",
      "Iteration 20559, loss = 0.05408258\n",
      "Iteration 20560, loss = 0.05408725\n",
      "Iteration 20561, loss = 0.05408493\n",
      "Iteration 20562, loss = 0.05407675\n",
      "Iteration 20563, loss = 0.05407477\n",
      "Iteration 20564, loss = 0.05407072\n",
      "Iteration 20565, loss = 0.05406290\n",
      "Iteration 20566, loss = 0.05406504\n",
      "Iteration 20567, loss = 0.05406049\n",
      "Iteration 20568, loss = 0.05405149\n",
      "Iteration 20569, loss = 0.05404542\n",
      "Iteration 20570, loss = 0.05404454\n",
      "Iteration 20571, loss = 0.05404192\n",
      "Iteration 20572, loss = 0.05403699\n",
      "Iteration 20573, loss = 0.05403254\n",
      "Iteration 20574, loss = 0.05403014\n",
      "Iteration 20575, loss = 0.05402942\n",
      "Iteration 20576, loss = 0.05402525\n",
      "Iteration 20577, loss = 0.05401797\n",
      "Iteration 20578, loss = 0.05401650\n",
      "Iteration 20579, loss = 0.05401575\n",
      "Iteration 20580, loss = 0.05401224\n",
      "Iteration 20581, loss = 0.05400662\n",
      "Iteration 20582, loss = 0.05400029\n",
      "Iteration 20583, loss = 0.05399499\n",
      "Iteration 20584, loss = 0.05399198\n",
      "Iteration 20585, loss = 0.05399130\n",
      "Iteration 20586, loss = 0.05398274\n",
      "Iteration 20587, loss = 0.05398314\n",
      "Iteration 20588, loss = 0.05398233\n",
      "Iteration 20589, loss = 0.05398226\n",
      "Iteration 20590, loss = 0.05397742\n",
      "Iteration 20591, loss = 0.05397168\n",
      "Iteration 20592, loss = 0.05396597\n",
      "Iteration 20593, loss = 0.05395961\n",
      "Iteration 20594, loss = 0.05395119\n",
      "Iteration 20595, loss = 0.05395152\n",
      "Iteration 20596, loss = 0.05395091\n",
      "Iteration 20597, loss = 0.05394477\n",
      "Iteration 20598, loss = 0.05394155\n",
      "Iteration 20599, loss = 0.05393830\n",
      "Iteration 20600, loss = 0.05393917\n",
      "Iteration 20601, loss = 0.05393997\n",
      "Iteration 20602, loss = 0.05393061\n",
      "Iteration 20603, loss = 0.05392447\n",
      "Iteration 20604, loss = 0.05391748\n",
      "Iteration 20605, loss = 0.05391342\n",
      "Iteration 20606, loss = 0.05391418\n",
      "Iteration 20607, loss = 0.05390675\n",
      "Iteration 20608, loss = 0.05390328\n",
      "Iteration 20609, loss = 0.05389675\n",
      "Iteration 20610, loss = 0.05389420\n",
      "Iteration 20611, loss = 0.05389206\n",
      "Iteration 20612, loss = 0.05388380\n",
      "Iteration 20613, loss = 0.05388456\n",
      "Iteration 20614, loss = 0.05388736\n",
      "Iteration 20615, loss = 0.05388340\n",
      "Iteration 20616, loss = 0.05387698\n",
      "Iteration 20617, loss = 0.05386905\n",
      "Iteration 20618, loss = 0.05386874\n",
      "Iteration 20619, loss = 0.05387040\n",
      "Iteration 20620, loss = 0.05386854\n",
      "Iteration 20621, loss = 0.05385774\n",
      "Iteration 20622, loss = 0.05385121\n",
      "Iteration 20623, loss = 0.05384726\n",
      "Iteration 20624, loss = 0.05384940\n",
      "Iteration 20625, loss = 0.05384597\n",
      "Iteration 20626, loss = 0.05383705\n",
      "Iteration 20627, loss = 0.05383426\n",
      "Iteration 20628, loss = 0.05383127\n",
      "Iteration 20629, loss = 0.05383312\n",
      "Iteration 20630, loss = 0.05383173\n",
      "Iteration 20631, loss = 0.05382584\n",
      "Iteration 20632, loss = 0.05381842\n",
      "Iteration 20633, loss = 0.05381904\n",
      "Iteration 20634, loss = 0.05381553\n",
      "Iteration 20635, loss = 0.05381023\n",
      "Iteration 20636, loss = 0.05380898\n",
      "Iteration 20637, loss = 0.05379883\n",
      "Iteration 20638, loss = 0.05379527\n",
      "Iteration 20639, loss = 0.05379469\n",
      "Iteration 20640, loss = 0.05379160\n",
      "Iteration 20641, loss = 0.05378696\n",
      "Iteration 20642, loss = 0.05378348\n",
      "Iteration 20643, loss = 0.05377921\n",
      "Iteration 20644, loss = 0.05377437\n",
      "Iteration 20645, loss = 0.05377238\n",
      "Iteration 20646, loss = 0.05376760\n",
      "Iteration 20647, loss = 0.05376288\n",
      "Iteration 20648, loss = 0.05376225\n",
      "Iteration 20649, loss = 0.05375679\n",
      "Iteration 20650, loss = 0.05375287\n",
      "Iteration 20651, loss = 0.05374874\n",
      "Iteration 20652, loss = 0.05374811\n",
      "Iteration 20653, loss = 0.05374670\n",
      "Iteration 20654, loss = 0.05374330\n",
      "Iteration 20655, loss = 0.05373660\n",
      "Iteration 20656, loss = 0.05373649\n",
      "Iteration 20657, loss = 0.05373280\n",
      "Iteration 20658, loss = 0.05372805\n",
      "Iteration 20659, loss = 0.05372370\n",
      "Iteration 20660, loss = 0.05372065\n",
      "Iteration 20661, loss = 0.05371851\n",
      "Iteration 20662, loss = 0.05371318\n",
      "Iteration 20663, loss = 0.05370487\n",
      "Iteration 20664, loss = 0.05370605\n",
      "Iteration 20665, loss = 0.05370563\n",
      "Iteration 20666, loss = 0.05370158\n",
      "Iteration 20667, loss = 0.05369680\n",
      "Iteration 20668, loss = 0.05369418\n",
      "Iteration 20669, loss = 0.05368690\n",
      "Iteration 20670, loss = 0.05368698\n",
      "Iteration 20671, loss = 0.05368546\n",
      "Iteration 20672, loss = 0.05368371\n",
      "Iteration 20673, loss = 0.05367797\n",
      "Iteration 20674, loss = 0.05367078\n",
      "Iteration 20675, loss = 0.05366925\n",
      "Iteration 20676, loss = 0.05366861\n",
      "Iteration 20677, loss = 0.05366471\n",
      "Iteration 20678, loss = 0.05366407\n",
      "Iteration 20679, loss = 0.05365972\n",
      "Iteration 20680, loss = 0.05365338\n",
      "Iteration 20681, loss = 0.05365094\n",
      "Iteration 20682, loss = 0.05365040\n",
      "Iteration 20683, loss = 0.05364298\n",
      "Iteration 20684, loss = 0.05363642\n",
      "Iteration 20685, loss = 0.05363933\n",
      "Iteration 20686, loss = 0.05363605\n",
      "Iteration 20687, loss = 0.05362784\n",
      "Iteration 20688, loss = 0.05362406\n",
      "Iteration 20689, loss = 0.05361936\n",
      "Iteration 20690, loss = 0.05361766\n",
      "Iteration 20691, loss = 0.05361027\n",
      "Iteration 20692, loss = 0.05360828\n",
      "Iteration 20693, loss = 0.05360531\n",
      "Iteration 20694, loss = 0.05360168\n",
      "Iteration 20695, loss = 0.05359884\n",
      "Iteration 20696, loss = 0.05359585\n",
      "Iteration 20697, loss = 0.05359049\n",
      "Iteration 20698, loss = 0.05358838\n",
      "Iteration 20699, loss = 0.05358127\n",
      "Iteration 20700, loss = 0.05358119\n",
      "Iteration 20701, loss = 0.05358059\n",
      "Iteration 20702, loss = 0.05357778\n",
      "Iteration 20703, loss = 0.05357176\n",
      "Iteration 20704, loss = 0.05356462\n",
      "Iteration 20705, loss = 0.05356592\n",
      "Iteration 20706, loss = 0.05356467\n",
      "Iteration 20707, loss = 0.05356121\n",
      "Iteration 20708, loss = 0.05355627\n",
      "Iteration 20709, loss = 0.05354898\n",
      "Iteration 20710, loss = 0.05354741\n",
      "Iteration 20711, loss = 0.05354292\n",
      "Iteration 20712, loss = 0.05353742\n",
      "Iteration 20713, loss = 0.05353627\n",
      "Iteration 20714, loss = 0.05353009\n",
      "Iteration 20715, loss = 0.05353031\n",
      "Iteration 20716, loss = 0.05352748\n",
      "Iteration 20717, loss = 0.05352678\n",
      "Iteration 20718, loss = 0.05352529\n",
      "Iteration 20719, loss = 0.05352495\n",
      "Iteration 20720, loss = 0.05352076\n",
      "Iteration 20721, loss = 0.05351076\n",
      "Iteration 20722, loss = 0.05350397\n",
      "Iteration 20723, loss = 0.05350847\n",
      "Iteration 20724, loss = 0.05350708\n",
      "Iteration 20725, loss = 0.05349730\n",
      "Iteration 20726, loss = 0.05349526\n",
      "Iteration 20727, loss = 0.05349253\n",
      "Iteration 20728, loss = 0.05348752\n",
      "Iteration 20729, loss = 0.05348605\n",
      "Iteration 20730, loss = 0.05348498\n",
      "Iteration 20731, loss = 0.05347973\n",
      "Iteration 20732, loss = 0.05347409\n",
      "Iteration 20733, loss = 0.05346943\n",
      "Iteration 20734, loss = 0.05346589\n",
      "Iteration 20735, loss = 0.05345908\n",
      "Iteration 20736, loss = 0.05345597\n",
      "Iteration 20737, loss = 0.05345693\n",
      "Iteration 20738, loss = 0.05345439\n",
      "Iteration 20739, loss = 0.05344905\n",
      "Iteration 20740, loss = 0.05344720\n",
      "Iteration 20741, loss = 0.05344469\n",
      "Iteration 20742, loss = 0.05343696\n",
      "Iteration 20743, loss = 0.05343778\n",
      "Iteration 20744, loss = 0.05343625\n",
      "Iteration 20745, loss = 0.05342869\n",
      "Iteration 20746, loss = 0.05342433\n",
      "Iteration 20747, loss = 0.05342334\n",
      "Iteration 20748, loss = 0.05341965\n",
      "Iteration 20749, loss = 0.05341378\n",
      "Iteration 20750, loss = 0.05340999\n",
      "Iteration 20751, loss = 0.05340970\n",
      "Iteration 20752, loss = 0.05340867\n",
      "Iteration 20753, loss = 0.05340251\n",
      "Iteration 20754, loss = 0.05339858\n",
      "Iteration 20755, loss = 0.05339646\n",
      "Iteration 20756, loss = 0.05338961\n",
      "Iteration 20757, loss = 0.05338530\n",
      "Iteration 20758, loss = 0.05338600\n",
      "Iteration 20759, loss = 0.05338041\n",
      "Iteration 20760, loss = 0.05337697\n",
      "Iteration 20761, loss = 0.05337616\n",
      "Iteration 20762, loss = 0.05337060\n",
      "Iteration 20763, loss = 0.05336610\n",
      "Iteration 20764, loss = 0.05336317\n",
      "Iteration 20765, loss = 0.05336422\n",
      "Iteration 20766, loss = 0.05336016\n",
      "Iteration 20767, loss = 0.05335237\n",
      "Iteration 20768, loss = 0.05335144\n",
      "Iteration 20769, loss = 0.05335394\n",
      "Iteration 20770, loss = 0.05334781\n",
      "Iteration 20771, loss = 0.05334069\n",
      "Iteration 20772, loss = 0.05334054\n",
      "Iteration 20773, loss = 0.05333789\n",
      "Iteration 20774, loss = 0.05333456\n",
      "Iteration 20775, loss = 0.05332998\n",
      "Iteration 20776, loss = 0.05332766\n",
      "Iteration 20777, loss = 0.05332092\n",
      "Iteration 20778, loss = 0.05331971\n",
      "Iteration 20779, loss = 0.05331715\n",
      "Iteration 20780, loss = 0.05331147\n",
      "Iteration 20781, loss = 0.05330964\n",
      "Iteration 20782, loss = 0.05330575\n",
      "Iteration 20783, loss = 0.05330616\n",
      "Iteration 20784, loss = 0.05330155\n",
      "Iteration 20785, loss = 0.05330019\n",
      "Iteration 20786, loss = 0.05330012\n",
      "Iteration 20787, loss = 0.05329378\n",
      "Iteration 20788, loss = 0.05328492\n",
      "Iteration 20789, loss = 0.05328099\n",
      "Iteration 20790, loss = 0.05328155\n",
      "Iteration 20791, loss = 0.05327410\n",
      "Iteration 20792, loss = 0.05327299\n",
      "Iteration 20793, loss = 0.05327352\n",
      "Iteration 20794, loss = 0.05326920\n",
      "Iteration 20795, loss = 0.05326086\n",
      "Iteration 20796, loss = 0.05325973\n",
      "Iteration 20797, loss = 0.05326075\n",
      "Iteration 20798, loss = 0.05325590\n",
      "Iteration 20799, loss = 0.05325038\n",
      "Iteration 20800, loss = 0.05324488\n",
      "Iteration 20801, loss = 0.05323893\n",
      "Iteration 20802, loss = 0.05323493\n",
      "Iteration 20803, loss = 0.05323060\n",
      "Iteration 20804, loss = 0.05323390\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20805, loss = 0.05323090\n",
      "Iteration 20806, loss = 0.05322420\n",
      "Iteration 20807, loss = 0.05322084\n",
      "Iteration 20808, loss = 0.05321724\n",
      "Iteration 20809, loss = 0.05321568\n",
      "Iteration 20810, loss = 0.05320941\n",
      "Iteration 20811, loss = 0.05321191\n",
      "Iteration 20812, loss = 0.05320931\n",
      "Iteration 20813, loss = 0.05320223\n",
      "Iteration 20814, loss = 0.05319959\n",
      "Iteration 20815, loss = 0.05319801\n",
      "Iteration 20816, loss = 0.05319260\n",
      "Iteration 20817, loss = 0.05318540\n",
      "Iteration 20818, loss = 0.05318490\n",
      "Iteration 20819, loss = 0.05318124\n",
      "Iteration 20820, loss = 0.05317523\n",
      "Iteration 20821, loss = 0.05317552\n",
      "Iteration 20822, loss = 0.05317508\n",
      "Iteration 20823, loss = 0.05317045\n",
      "Iteration 20824, loss = 0.05316373\n",
      "Iteration 20825, loss = 0.05316462\n",
      "Iteration 20826, loss = 0.05316114\n",
      "Iteration 20827, loss = 0.05315369\n",
      "Iteration 20828, loss = 0.05315034\n",
      "Iteration 20829, loss = 0.05315060\n",
      "Iteration 20830, loss = 0.05314663\n",
      "Iteration 20831, loss = 0.05313967\n",
      "Iteration 20832, loss = 0.05314098\n",
      "Iteration 20833, loss = 0.05313869\n",
      "Iteration 20834, loss = 0.05313194\n",
      "Iteration 20835, loss = 0.05312595\n",
      "Iteration 20836, loss = 0.05312536\n",
      "Iteration 20837, loss = 0.05312190\n",
      "Iteration 20838, loss = 0.05311822\n",
      "Iteration 20839, loss = 0.05311733\n",
      "Iteration 20840, loss = 0.05311168\n",
      "Iteration 20841, loss = 0.05310719\n",
      "Iteration 20842, loss = 0.05310180\n",
      "Iteration 20843, loss = 0.05310257\n",
      "Iteration 20844, loss = 0.05309933\n",
      "Iteration 20845, loss = 0.05309506\n",
      "Iteration 20846, loss = 0.05309407\n",
      "Iteration 20847, loss = 0.05309177\n",
      "Iteration 20848, loss = 0.05308344\n",
      "Iteration 20849, loss = 0.05308273\n",
      "Iteration 20850, loss = 0.05307911\n",
      "Iteration 20851, loss = 0.05307769\n",
      "Iteration 20852, loss = 0.05307052\n",
      "Iteration 20853, loss = 0.05307442\n",
      "Iteration 20854, loss = 0.05307128\n",
      "Iteration 20855, loss = 0.05306647\n",
      "Iteration 20856, loss = 0.05306225\n",
      "Iteration 20857, loss = 0.05306030\n",
      "Iteration 20858, loss = 0.05305287\n",
      "Iteration 20859, loss = 0.05305318\n",
      "Iteration 20860, loss = 0.05305319\n",
      "Iteration 20861, loss = 0.05304853\n",
      "Iteration 20862, loss = 0.05303866\n",
      "Iteration 20863, loss = 0.05303869\n",
      "Iteration 20864, loss = 0.05303713\n",
      "Iteration 20865, loss = 0.05303067\n",
      "Iteration 20866, loss = 0.05302451\n",
      "Iteration 20867, loss = 0.05302530\n",
      "Iteration 20868, loss = 0.05302641\n",
      "Iteration 20869, loss = 0.05301899\n",
      "Iteration 20870, loss = 0.05301039\n",
      "Iteration 20871, loss = 0.05300975\n",
      "Iteration 20872, loss = 0.05300747\n",
      "Iteration 20873, loss = 0.05300293\n",
      "Iteration 20874, loss = 0.05300238\n",
      "Iteration 20875, loss = 0.05299362\n",
      "Iteration 20876, loss = 0.05299217\n",
      "Iteration 20877, loss = 0.05299149\n",
      "Iteration 20878, loss = 0.05298493\n",
      "Iteration 20879, loss = 0.05298241\n",
      "Iteration 20880, loss = 0.05298268\n",
      "Iteration 20881, loss = 0.05297648\n",
      "Iteration 20882, loss = 0.05297049\n",
      "Iteration 20883, loss = 0.05297060\n",
      "Iteration 20884, loss = 0.05296817\n",
      "Iteration 20885, loss = 0.05296704\n",
      "Iteration 20886, loss = 0.05296032\n",
      "Iteration 20887, loss = 0.05295704\n",
      "Iteration 20888, loss = 0.05295024\n",
      "Iteration 20889, loss = 0.05294902\n",
      "Iteration 20890, loss = 0.05295147\n",
      "Iteration 20891, loss = 0.05294244\n",
      "Iteration 20892, loss = 0.05293894\n",
      "Iteration 20893, loss = 0.05293569\n",
      "Iteration 20894, loss = 0.05293516\n",
      "Iteration 20895, loss = 0.05293476\n",
      "Iteration 20896, loss = 0.05293006\n",
      "Iteration 20897, loss = 0.05292797\n",
      "Iteration 20898, loss = 0.05292603\n",
      "Iteration 20899, loss = 0.05292457\n",
      "Iteration 20900, loss = 0.05291756\n",
      "Iteration 20901, loss = 0.05291237\n",
      "Iteration 20902, loss = 0.05290749\n",
      "Iteration 20903, loss = 0.05290655\n",
      "Iteration 20904, loss = 0.05290491\n",
      "Iteration 20905, loss = 0.05289997\n",
      "Iteration 20906, loss = 0.05289592\n",
      "Iteration 20907, loss = 0.05288647\n",
      "Iteration 20908, loss = 0.05288556\n",
      "Iteration 20909, loss = 0.05288941\n",
      "Iteration 20910, loss = 0.05288546\n",
      "Iteration 20911, loss = 0.05287911\n",
      "Iteration 20912, loss = 0.05287629\n",
      "Iteration 20913, loss = 0.05287458\n",
      "Iteration 20914, loss = 0.05287008\n",
      "Iteration 20915, loss = 0.05286449\n",
      "Iteration 20916, loss = 0.05285834\n",
      "Iteration 20917, loss = 0.05285903\n",
      "Iteration 20918, loss = 0.05285743\n",
      "Iteration 20919, loss = 0.05285042\n",
      "Iteration 20920, loss = 0.05284922\n",
      "Iteration 20921, loss = 0.05284992\n",
      "Iteration 20922, loss = 0.05284341\n",
      "Iteration 20923, loss = 0.05284482\n",
      "Iteration 20924, loss = 0.05283895\n",
      "Iteration 20925, loss = 0.05283260\n",
      "Iteration 20926, loss = 0.05283061\n",
      "Iteration 20927, loss = 0.05282692\n",
      "Iteration 20928, loss = 0.05282223\n",
      "Iteration 20929, loss = 0.05282045\n",
      "Iteration 20930, loss = 0.05281587\n",
      "Iteration 20931, loss = 0.05281350\n",
      "Iteration 20932, loss = 0.05281365\n",
      "Iteration 20933, loss = 0.05280705\n",
      "Iteration 20934, loss = 0.05280978\n",
      "Iteration 20935, loss = 0.05280551\n",
      "Iteration 20936, loss = 0.05279854\n",
      "Iteration 20937, loss = 0.05279345\n",
      "Iteration 20938, loss = 0.05279312\n",
      "Iteration 20939, loss = 0.05279258\n",
      "Iteration 20940, loss = 0.05279250\n",
      "Iteration 20941, loss = 0.05278713\n",
      "Iteration 20942, loss = 0.05277844\n",
      "Iteration 20943, loss = 0.05277699\n",
      "Iteration 20944, loss = 0.05277367\n",
      "Iteration 20945, loss = 0.05277370\n",
      "Iteration 20946, loss = 0.05277242\n",
      "Iteration 20947, loss = 0.05276340\n",
      "Iteration 20948, loss = 0.05276234\n",
      "Iteration 20949, loss = 0.05276315\n",
      "Iteration 20950, loss = 0.05275860\n",
      "Iteration 20951, loss = 0.05275233\n",
      "Iteration 20952, loss = 0.05274739\n",
      "Iteration 20953, loss = 0.05274680\n",
      "Iteration 20954, loss = 0.05274088\n",
      "Iteration 20955, loss = 0.05273478\n",
      "Iteration 20956, loss = 0.05273303\n",
      "Iteration 20957, loss = 0.05273219\n",
      "Iteration 20958, loss = 0.05273166\n",
      "Iteration 20959, loss = 0.05272716\n",
      "Iteration 20960, loss = 0.05272011\n",
      "Iteration 20961, loss = 0.05271909\n",
      "Iteration 20962, loss = 0.05271785\n",
      "Iteration 20963, loss = 0.05271468\n",
      "Iteration 20964, loss = 0.05271258\n",
      "Iteration 20965, loss = 0.05270588\n",
      "Iteration 20966, loss = 0.05269874\n",
      "Iteration 20967, loss = 0.05269817\n",
      "Iteration 20968, loss = 0.05270338\n",
      "Iteration 20969, loss = 0.05269687\n",
      "Iteration 20970, loss = 0.05269055\n",
      "Iteration 20971, loss = 0.05268747\n",
      "Iteration 20972, loss = 0.05269221\n",
      "Iteration 20973, loss = 0.05268772\n",
      "Iteration 20974, loss = 0.05268030\n",
      "Iteration 20975, loss = 0.05268415\n",
      "Iteration 20976, loss = 0.05267948\n",
      "Iteration 20977, loss = 0.05267299\n",
      "Iteration 20978, loss = 0.05267238\n",
      "Iteration 20979, loss = 0.05266768\n",
      "Iteration 20980, loss = 0.05266142\n",
      "Iteration 20981, loss = 0.05265426\n",
      "Iteration 20982, loss = 0.05265543\n",
      "Iteration 20983, loss = 0.05265571\n",
      "Iteration 20984, loss = 0.05264972\n",
      "Iteration 20985, loss = 0.05264643\n",
      "Iteration 20986, loss = 0.05263996\n",
      "Iteration 20987, loss = 0.05263306\n",
      "Iteration 20988, loss = 0.05263043\n",
      "Iteration 20989, loss = 0.05263407\n",
      "Iteration 20990, loss = 0.05263069\n",
      "Iteration 20991, loss = 0.05262115\n",
      "Iteration 20992, loss = 0.05262003\n",
      "Iteration 20993, loss = 0.05261845\n",
      "Iteration 20994, loss = 0.05261376\n",
      "Iteration 20995, loss = 0.05261151\n",
      "Iteration 20996, loss = 0.05260907\n",
      "Iteration 20997, loss = 0.05260203\n",
      "Iteration 20998, loss = 0.05259974\n",
      "Iteration 20999, loss = 0.05259628\n",
      "Iteration 21000, loss = 0.05259217\n",
      "Iteration 21001, loss = 0.05258710\n",
      "Iteration 21002, loss = 0.05258687\n",
      "Iteration 21003, loss = 0.05258441\n",
      "Iteration 21004, loss = 0.05258417\n",
      "Iteration 21005, loss = 0.05257900\n",
      "Iteration 21006, loss = 0.05257835\n",
      "Iteration 21007, loss = 0.05257603\n",
      "Iteration 21008, loss = 0.05257418\n",
      "Iteration 21009, loss = 0.05256771\n",
      "Iteration 21010, loss = 0.05256078\n",
      "Iteration 21011, loss = 0.05256127\n",
      "Iteration 21012, loss = 0.05255834\n",
      "Iteration 21013, loss = 0.05255848\n",
      "Iteration 21014, loss = 0.05255869\n",
      "Iteration 21015, loss = 0.05254965\n",
      "Iteration 21016, loss = 0.05253965\n",
      "Iteration 21017, loss = 0.05253610\n",
      "Iteration 21018, loss = 0.05253747\n",
      "Iteration 21019, loss = 0.05253553\n",
      "Iteration 21020, loss = 0.05252780\n",
      "Iteration 21021, loss = 0.05252619\n",
      "Iteration 21022, loss = 0.05252416\n",
      "Iteration 21023, loss = 0.05252357\n",
      "Iteration 21024, loss = 0.05252083\n",
      "Iteration 21025, loss = 0.05251477\n",
      "Iteration 21026, loss = 0.05250947\n",
      "Iteration 21027, loss = 0.05251076\n",
      "Iteration 21028, loss = 0.05251001\n",
      "Iteration 21029, loss = 0.05250392\n",
      "Iteration 21030, loss = 0.05250195\n",
      "Iteration 21031, loss = 0.05250119\n",
      "Iteration 21032, loss = 0.05250062\n",
      "Iteration 21033, loss = 0.05249608\n",
      "Iteration 21034, loss = 0.05249237\n",
      "Iteration 21035, loss = 0.05248777\n",
      "Iteration 21036, loss = 0.05248594\n",
      "Iteration 21037, loss = 0.05248301\n",
      "Iteration 21038, loss = 0.05247508\n",
      "Iteration 21039, loss = 0.05247489\n",
      "Iteration 21040, loss = 0.05247005\n",
      "Iteration 21041, loss = 0.05247062\n",
      "Iteration 21042, loss = 0.05246814\n",
      "Iteration 21043, loss = 0.05246541\n",
      "Iteration 21044, loss = 0.05245916\n",
      "Iteration 21045, loss = 0.05245545\n",
      "Iteration 21046, loss = 0.05244920\n",
      "Iteration 21047, loss = 0.05244339\n",
      "Iteration 21048, loss = 0.05244610\n",
      "Iteration 21049, loss = 0.05244300\n",
      "Iteration 21050, loss = 0.05243527\n",
      "Iteration 21051, loss = 0.05243228\n",
      "Iteration 21052, loss = 0.05243009\n",
      "Iteration 21053, loss = 0.05242744\n",
      "Iteration 21054, loss = 0.05242431\n",
      "Iteration 21055, loss = 0.05242533\n",
      "Iteration 21056, loss = 0.05241727\n",
      "Iteration 21057, loss = 0.05241194\n",
      "Iteration 21058, loss = 0.05241292\n",
      "Iteration 21059, loss = 0.05240747\n",
      "Iteration 21060, loss = 0.05240370\n",
      "Iteration 21061, loss = 0.05240077\n",
      "Iteration 21062, loss = 0.05239831\n",
      "Iteration 21063, loss = 0.05239462\n",
      "Iteration 21064, loss = 0.05239179\n",
      "Iteration 21065, loss = 0.05239119\n",
      "Iteration 21066, loss = 0.05238530\n",
      "Iteration 21067, loss = 0.05238193\n",
      "Iteration 21068, loss = 0.05237721\n",
      "Iteration 21069, loss = 0.05237702\n",
      "Iteration 21070, loss = 0.05237697\n",
      "Iteration 21071, loss = 0.05237033\n",
      "Iteration 21072, loss = 0.05236893\n",
      "Iteration 21073, loss = 0.05236273\n",
      "Iteration 21074, loss = 0.05236040\n",
      "Iteration 21075, loss = 0.05235497\n",
      "Iteration 21076, loss = 0.05235910\n",
      "Iteration 21077, loss = 0.05235090\n",
      "Iteration 21078, loss = 0.05234603\n",
      "Iteration 21079, loss = 0.05234565\n",
      "Iteration 21080, loss = 0.05234237\n",
      "Iteration 21081, loss = 0.05233702\n",
      "Iteration 21082, loss = 0.05233626\n",
      "Iteration 21083, loss = 0.05233312\n",
      "Iteration 21084, loss = 0.05232800\n",
      "Iteration 21085, loss = 0.05232332\n",
      "Iteration 21086, loss = 0.05232050\n",
      "Iteration 21087, loss = 0.05231785\n",
      "Iteration 21088, loss = 0.05231429\n",
      "Iteration 21089, loss = 0.05231363\n",
      "Iteration 21090, loss = 0.05230840\n",
      "Iteration 21091, loss = 0.05230773\n",
      "Iteration 21092, loss = 0.05230764\n",
      "Iteration 21093, loss = 0.05230178\n",
      "Iteration 21094, loss = 0.05229873\n",
      "Iteration 21095, loss = 0.05229108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21096, loss = 0.05229263\n",
      "Iteration 21097, loss = 0.05229053\n",
      "Iteration 21098, loss = 0.05228861\n",
      "Iteration 21099, loss = 0.05228469\n",
      "Iteration 21100, loss = 0.05228333\n",
      "Iteration 21101, loss = 0.05227979\n",
      "Iteration 21102, loss = 0.05227786\n",
      "Iteration 21103, loss = 0.05227377\n",
      "Iteration 21104, loss = 0.05226947\n",
      "Iteration 21105, loss = 0.05226462\n",
      "Iteration 21106, loss = 0.05226055\n",
      "Iteration 21107, loss = 0.05225927\n",
      "Iteration 21108, loss = 0.05225929\n",
      "Iteration 21109, loss = 0.05225339\n",
      "Iteration 21110, loss = 0.05224552\n",
      "Iteration 21111, loss = 0.05224722\n",
      "Iteration 21112, loss = 0.05224748\n",
      "Iteration 21113, loss = 0.05224339\n",
      "Iteration 21114, loss = 0.05223854\n",
      "Iteration 21115, loss = 0.05223728\n",
      "Iteration 21116, loss = 0.05223247\n",
      "Iteration 21117, loss = 0.05223151\n",
      "Iteration 21118, loss = 0.05222627\n",
      "Iteration 21119, loss = 0.05222704\n",
      "Iteration 21120, loss = 0.05222085\n",
      "Iteration 21121, loss = 0.05221864\n",
      "Iteration 21122, loss = 0.05221934\n",
      "Iteration 21123, loss = 0.05221530\n",
      "Iteration 21124, loss = 0.05221270\n",
      "Iteration 21125, loss = 0.05220527\n",
      "Iteration 21126, loss = 0.05220067\n",
      "Iteration 21127, loss = 0.05220238\n",
      "Iteration 21128, loss = 0.05220107\n",
      "Iteration 21129, loss = 0.05219556\n",
      "Iteration 21130, loss = 0.05219070\n",
      "Iteration 21131, loss = 0.05218832\n",
      "Iteration 21132, loss = 0.05218888\n",
      "Iteration 21133, loss = 0.05218424\n",
      "Iteration 21134, loss = 0.05217726\n",
      "Iteration 21135, loss = 0.05217374\n",
      "Iteration 21136, loss = 0.05216990\n",
      "Iteration 21137, loss = 0.05217502\n",
      "Iteration 21138, loss = 0.05217523\n",
      "Iteration 21139, loss = 0.05216997\n",
      "Iteration 21140, loss = 0.05216093\n",
      "Iteration 21141, loss = 0.05215956\n",
      "Iteration 21142, loss = 0.05215737\n",
      "Iteration 21143, loss = 0.05215685\n",
      "Iteration 21144, loss = 0.05215310\n",
      "Iteration 21145, loss = 0.05214598\n",
      "Iteration 21146, loss = 0.05214291\n",
      "Iteration 21147, loss = 0.05213909\n",
      "Iteration 21148, loss = 0.05213649\n",
      "Iteration 21149, loss = 0.05213551\n",
      "Iteration 21150, loss = 0.05212853\n",
      "Iteration 21151, loss = 0.05212677\n",
      "Iteration 21152, loss = 0.05212307\n",
      "Iteration 21153, loss = 0.05212525\n",
      "Iteration 21154, loss = 0.05212030\n",
      "Iteration 21155, loss = 0.05211030\n",
      "Iteration 21156, loss = 0.05210958\n",
      "Iteration 21157, loss = 0.05211303\n",
      "Iteration 21158, loss = 0.05210857\n",
      "Iteration 21159, loss = 0.05210409\n",
      "Iteration 21160, loss = 0.05210071\n",
      "Iteration 21161, loss = 0.05209536\n",
      "Iteration 21162, loss = 0.05208708\n",
      "Iteration 21163, loss = 0.05208379\n",
      "Iteration 21164, loss = 0.05208424\n",
      "Iteration 21165, loss = 0.05208304\n",
      "Iteration 21166, loss = 0.05207927\n",
      "Iteration 21167, loss = 0.05207604\n",
      "Iteration 21168, loss = 0.05207238\n",
      "Iteration 21169, loss = 0.05206942\n",
      "Iteration 21170, loss = 0.05207030\n",
      "Iteration 21171, loss = 0.05206853\n",
      "Iteration 21172, loss = 0.05205658\n",
      "Iteration 21173, loss = 0.05205136\n",
      "Iteration 21174, loss = 0.05205962\n",
      "Iteration 21175, loss = 0.05205462\n",
      "Iteration 21176, loss = 0.05204814\n",
      "Iteration 21177, loss = 0.05204363\n",
      "Iteration 21178, loss = 0.05203862\n",
      "Iteration 21179, loss = 0.05204269\n",
      "Iteration 21180, loss = 0.05204054\n",
      "Iteration 21181, loss = 0.05203353\n",
      "Iteration 21182, loss = 0.05202668\n",
      "Iteration 21183, loss = 0.05202806\n",
      "Iteration 21184, loss = 0.05202359\n",
      "Iteration 21185, loss = 0.05201962\n",
      "Iteration 21186, loss = 0.05201768\n",
      "Iteration 21187, loss = 0.05201489\n",
      "Iteration 21188, loss = 0.05201406\n",
      "Iteration 21189, loss = 0.05200706\n",
      "Iteration 21190, loss = 0.05200590\n",
      "Iteration 21191, loss = 0.05200722\n",
      "Iteration 21192, loss = 0.05200022\n",
      "Iteration 21193, loss = 0.05199669\n",
      "Iteration 21194, loss = 0.05199423\n",
      "Iteration 21195, loss = 0.05199532\n",
      "Iteration 21196, loss = 0.05199014\n",
      "Iteration 21197, loss = 0.05198235\n",
      "Iteration 21198, loss = 0.05198262\n",
      "Iteration 21199, loss = 0.05198099\n",
      "Iteration 21200, loss = 0.05197628\n",
      "Iteration 21201, loss = 0.05197445\n",
      "Iteration 21202, loss = 0.05197111\n",
      "Iteration 21203, loss = 0.05196717\n",
      "Iteration 21204, loss = 0.05196634\n",
      "Iteration 21205, loss = 0.05196289\n",
      "Iteration 21206, loss = 0.05195757\n",
      "Iteration 21207, loss = 0.05194969\n",
      "Iteration 21208, loss = 0.05194663\n",
      "Iteration 21209, loss = 0.05195073\n",
      "Iteration 21210, loss = 0.05194794\n",
      "Iteration 21211, loss = 0.05194073\n",
      "Iteration 21212, loss = 0.05193489\n",
      "Iteration 21213, loss = 0.05193552\n",
      "Iteration 21214, loss = 0.05193458\n",
      "Iteration 21215, loss = 0.05193086\n",
      "Iteration 21216, loss = 0.05192733\n",
      "Iteration 21217, loss = 0.05192394\n",
      "Iteration 21218, loss = 0.05191930\n",
      "Iteration 21219, loss = 0.05191921\n",
      "Iteration 21220, loss = 0.05191790\n",
      "Iteration 21221, loss = 0.05191205\n",
      "Iteration 21222, loss = 0.05190948\n",
      "Iteration 21223, loss = 0.05190756\n",
      "Iteration 21224, loss = 0.05190238\n",
      "Iteration 21225, loss = 0.05189996\n",
      "Iteration 21226, loss = 0.05189986\n",
      "Iteration 21227, loss = 0.05189256\n",
      "Iteration 21228, loss = 0.05188458\n",
      "Iteration 21229, loss = 0.05188372\n",
      "Iteration 21230, loss = 0.05188181\n",
      "Iteration 21231, loss = 0.05188185\n",
      "Iteration 21232, loss = 0.05187587\n",
      "Iteration 21233, loss = 0.05187210\n",
      "Iteration 21234, loss = 0.05186993\n",
      "Iteration 21235, loss = 0.05187286\n",
      "Iteration 21236, loss = 0.05186814\n",
      "Iteration 21237, loss = 0.05186458\n",
      "Iteration 21238, loss = 0.05185945\n",
      "Iteration 21239, loss = 0.05186120\n",
      "Iteration 21240, loss = 0.05185954\n",
      "Iteration 21241, loss = 0.05185155\n",
      "Iteration 21242, loss = 0.05184507\n",
      "Iteration 21243, loss = 0.05184816\n",
      "Iteration 21244, loss = 0.05184626\n",
      "Iteration 21245, loss = 0.05183762\n",
      "Iteration 21246, loss = 0.05183500\n",
      "Iteration 21247, loss = 0.05183504\n",
      "Iteration 21248, loss = 0.05183162\n",
      "Iteration 21249, loss = 0.05182964\n",
      "Iteration 21250, loss = 0.05182645\n",
      "Iteration 21251, loss = 0.05182182\n",
      "Iteration 21252, loss = 0.05181731\n",
      "Iteration 21253, loss = 0.05181437\n",
      "Iteration 21254, loss = 0.05181616\n",
      "Iteration 21255, loss = 0.05181086\n",
      "Iteration 21256, loss = 0.05180618\n",
      "Iteration 21257, loss = 0.05180183\n",
      "Iteration 21258, loss = 0.05179956\n",
      "Iteration 21259, loss = 0.05179930\n",
      "Iteration 21260, loss = 0.05179478\n",
      "Iteration 21261, loss = 0.05178997\n",
      "Iteration 21262, loss = 0.05178784\n",
      "Iteration 21263, loss = 0.05178434\n",
      "Iteration 21264, loss = 0.05178737\n",
      "Iteration 21265, loss = 0.05178264\n",
      "Iteration 21266, loss = 0.05177574\n",
      "Iteration 21267, loss = 0.05177659\n",
      "Iteration 21268, loss = 0.05177410\n",
      "Iteration 21269, loss = 0.05176994\n",
      "Iteration 21270, loss = 0.05176447\n",
      "Iteration 21271, loss = 0.05176158\n",
      "Iteration 21272, loss = 0.05175866\n",
      "Iteration 21273, loss = 0.05175364\n",
      "Iteration 21274, loss = 0.05174991\n",
      "Iteration 21275, loss = 0.05174621\n",
      "Iteration 21276, loss = 0.05174577\n",
      "Iteration 21277, loss = 0.05174161\n",
      "Iteration 21278, loss = 0.05173981\n",
      "Iteration 21279, loss = 0.05173579\n",
      "Iteration 21280, loss = 0.05173648\n",
      "Iteration 21281, loss = 0.05173205\n",
      "Iteration 21282, loss = 0.05173057\n",
      "Iteration 21283, loss = 0.05172502\n",
      "Iteration 21284, loss = 0.05172155\n",
      "Iteration 21285, loss = 0.05172249\n",
      "Iteration 21286, loss = 0.05171979\n",
      "Iteration 21287, loss = 0.05170867\n",
      "Iteration 21288, loss = 0.05171166\n",
      "Iteration 21289, loss = 0.05171314\n",
      "Iteration 21290, loss = 0.05170482\n",
      "Iteration 21291, loss = 0.05170415\n",
      "Iteration 21292, loss = 0.05170244\n",
      "Iteration 21293, loss = 0.05169649\n",
      "Iteration 21294, loss = 0.05169610\n",
      "Iteration 21295, loss = 0.05169006\n",
      "Iteration 21296, loss = 0.05168613\n",
      "Iteration 21297, loss = 0.05168398\n",
      "Iteration 21298, loss = 0.05167977\n",
      "Iteration 21299, loss = 0.05168021\n",
      "Iteration 21300, loss = 0.05167853\n",
      "Iteration 21301, loss = 0.05167123\n",
      "Iteration 21302, loss = 0.05166947\n",
      "Iteration 21303, loss = 0.05166639\n",
      "Iteration 21304, loss = 0.05166242\n",
      "Iteration 21305, loss = 0.05166022\n",
      "Iteration 21306, loss = 0.05165945\n",
      "Iteration 21307, loss = 0.05165378\n",
      "Iteration 21308, loss = 0.05164526\n",
      "Iteration 21309, loss = 0.05164965\n",
      "Iteration 21310, loss = 0.05165117\n",
      "Iteration 21311, loss = 0.05164126\n",
      "Iteration 21312, loss = 0.05163820\n",
      "Iteration 21313, loss = 0.05163473\n",
      "Iteration 21314, loss = 0.05163774\n",
      "Iteration 21315, loss = 0.05163426\n",
      "Iteration 21316, loss = 0.05162879\n",
      "Iteration 21317, loss = 0.05162561\n",
      "Iteration 21318, loss = 0.05162497\n",
      "Iteration 21319, loss = 0.05162094\n",
      "Iteration 21320, loss = 0.05161621\n",
      "Iteration 21321, loss = 0.05161124\n",
      "Iteration 21322, loss = 0.05161271\n",
      "Iteration 21323, loss = 0.05160865\n",
      "Iteration 21324, loss = 0.05160313\n",
      "Iteration 21325, loss = 0.05160278\n",
      "Iteration 21326, loss = 0.05160000\n",
      "Iteration 21327, loss = 0.05160052\n",
      "Iteration 21328, loss = 0.05159665\n",
      "Iteration 21329, loss = 0.05159350\n",
      "Iteration 21330, loss = 0.05158892\n",
      "Iteration 21331, loss = 0.05157763\n",
      "Iteration 21332, loss = 0.05157684\n",
      "Iteration 21333, loss = 0.05158079\n",
      "Iteration 21334, loss = 0.05157735\n",
      "Iteration 21335, loss = 0.05157008\n",
      "Iteration 21336, loss = 0.05156434\n",
      "Iteration 21337, loss = 0.05156592\n",
      "Iteration 21338, loss = 0.05156598\n",
      "Iteration 21339, loss = 0.05156294\n",
      "Iteration 21340, loss = 0.05155693\n",
      "Iteration 21341, loss = 0.05155472\n",
      "Iteration 21342, loss = 0.05155166\n",
      "Iteration 21343, loss = 0.05154721\n",
      "Iteration 21344, loss = 0.05155054\n",
      "Iteration 21345, loss = 0.05154975\n",
      "Iteration 21346, loss = 0.05154251\n",
      "Iteration 21347, loss = 0.05153851\n",
      "Iteration 21348, loss = 0.05154137\n",
      "Iteration 21349, loss = 0.05153765\n",
      "Iteration 21350, loss = 0.05153180\n",
      "Iteration 21351, loss = 0.05152907\n",
      "Iteration 21352, loss = 0.05152286\n",
      "Iteration 21353, loss = 0.05152280\n",
      "Iteration 21354, loss = 0.05151852\n",
      "Iteration 21355, loss = 0.05151230\n",
      "Iteration 21356, loss = 0.05151125\n",
      "Iteration 21357, loss = 0.05151006\n",
      "Iteration 21358, loss = 0.05150828\n",
      "Iteration 21359, loss = 0.05150491\n",
      "Iteration 21360, loss = 0.05150120\n",
      "Iteration 21361, loss = 0.05149548\n",
      "Iteration 21362, loss = 0.05149159\n",
      "Iteration 21363, loss = 0.05148942\n",
      "Iteration 21364, loss = 0.05148615\n",
      "Iteration 21365, loss = 0.05148032\n",
      "Iteration 21366, loss = 0.05147613\n",
      "Iteration 21367, loss = 0.05147826\n",
      "Iteration 21368, loss = 0.05147332\n",
      "Iteration 21369, loss = 0.05147024\n",
      "Iteration 21370, loss = 0.05146732\n",
      "Iteration 21371, loss = 0.05146288\n",
      "Iteration 21372, loss = 0.05145810\n",
      "Iteration 21373, loss = 0.05145630\n",
      "Iteration 21374, loss = 0.05145310\n",
      "Iteration 21375, loss = 0.05144795\n",
      "Iteration 21376, loss = 0.05144853\n",
      "Iteration 21377, loss = 0.05144866\n",
      "Iteration 21378, loss = 0.05144415\n",
      "Iteration 21379, loss = 0.05144133\n",
      "Iteration 21380, loss = 0.05143756\n",
      "Iteration 21381, loss = 0.05143657\n",
      "Iteration 21382, loss = 0.05143162\n",
      "Iteration 21383, loss = 0.05142483\n",
      "Iteration 21384, loss = 0.05142751\n",
      "Iteration 21385, loss = 0.05142678\n",
      "Iteration 21386, loss = 0.05141820\n",
      "Iteration 21387, loss = 0.05142178\n",
      "Iteration 21388, loss = 0.05141337\n",
      "Iteration 21389, loss = 0.05140942\n",
      "Iteration 21390, loss = 0.05140810\n",
      "Iteration 21391, loss = 0.05140580\n",
      "Iteration 21392, loss = 0.05139882\n",
      "Iteration 21393, loss = 0.05139464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21394, loss = 0.05139104\n",
      "Iteration 21395, loss = 0.05138986\n",
      "Iteration 21396, loss = 0.05138881\n",
      "Iteration 21397, loss = 0.05137949\n",
      "Iteration 21398, loss = 0.05138372\n",
      "Iteration 21399, loss = 0.05138238\n",
      "Iteration 21400, loss = 0.05137909\n",
      "Iteration 21401, loss = 0.05137842\n",
      "Iteration 21402, loss = 0.05137620\n",
      "Iteration 21403, loss = 0.05136983\n",
      "Iteration 21404, loss = 0.05136310\n",
      "Iteration 21405, loss = 0.05136168\n",
      "Iteration 21406, loss = 0.05135959\n",
      "Iteration 21407, loss = 0.05135817\n",
      "Iteration 21408, loss = 0.05134975\n",
      "Iteration 21409, loss = 0.05134724\n",
      "Iteration 21410, loss = 0.05134755\n",
      "Iteration 21411, loss = 0.05134314\n",
      "Iteration 21412, loss = 0.05134213\n",
      "Iteration 21413, loss = 0.05133545\n",
      "Iteration 21414, loss = 0.05133345\n",
      "Iteration 21415, loss = 0.05133363\n",
      "Iteration 21416, loss = 0.05133310\n",
      "Iteration 21417, loss = 0.05133166\n",
      "Iteration 21418, loss = 0.05132079\n",
      "Iteration 21419, loss = 0.05131758\n",
      "Iteration 21420, loss = 0.05131812\n",
      "Iteration 21421, loss = 0.05131564\n",
      "Iteration 21422, loss = 0.05131315\n",
      "Iteration 21423, loss = 0.05131244\n",
      "Iteration 21424, loss = 0.05130742\n",
      "Iteration 21425, loss = 0.05130224\n",
      "Iteration 21426, loss = 0.05129694\n",
      "Iteration 21427, loss = 0.05130181\n",
      "Iteration 21428, loss = 0.05129795\n",
      "Iteration 21429, loss = 0.05128855\n",
      "Iteration 21430, loss = 0.05128744\n",
      "Iteration 21431, loss = 0.05128941\n",
      "Iteration 21432, loss = 0.05127968\n",
      "Iteration 21433, loss = 0.05127465\n",
      "Iteration 21434, loss = 0.05127095\n",
      "Iteration 21435, loss = 0.05126764\n",
      "Iteration 21436, loss = 0.05126085\n",
      "Iteration 21437, loss = 0.05126073\n",
      "Iteration 21438, loss = 0.05126017\n",
      "Iteration 21439, loss = 0.05125632\n",
      "Iteration 21440, loss = 0.05125382\n",
      "Iteration 21441, loss = 0.05124974\n",
      "Iteration 21442, loss = 0.05124956\n",
      "Iteration 21443, loss = 0.05124840\n",
      "Iteration 21444, loss = 0.05123999\n",
      "Iteration 21445, loss = 0.05123477\n",
      "Iteration 21446, loss = 0.05123280\n",
      "Iteration 21447, loss = 0.05122978\n",
      "Iteration 21448, loss = 0.05123087\n",
      "Iteration 21449, loss = 0.05122767\n",
      "Iteration 21450, loss = 0.05122572\n",
      "Iteration 21451, loss = 0.05122155\n",
      "Iteration 21452, loss = 0.05121819\n",
      "Iteration 21453, loss = 0.05121510\n",
      "Iteration 21454, loss = 0.05121165\n",
      "Iteration 21455, loss = 0.05120587\n",
      "Iteration 21456, loss = 0.05120562\n",
      "Iteration 21457, loss = 0.05120369\n",
      "Iteration 21458, loss = 0.05119834\n",
      "Iteration 21459, loss = 0.05119680\n",
      "Iteration 21460, loss = 0.05119415\n",
      "Iteration 21461, loss = 0.05119136\n",
      "Iteration 21462, loss = 0.05118848\n",
      "Iteration 21463, loss = 0.05119037\n",
      "Iteration 21464, loss = 0.05118224\n",
      "Iteration 21465, loss = 0.05117840\n",
      "Iteration 21466, loss = 0.05117957\n",
      "Iteration 21467, loss = 0.05117590\n",
      "Iteration 21468, loss = 0.05117163\n",
      "Iteration 21469, loss = 0.05116755\n",
      "Iteration 21470, loss = 0.05116686\n",
      "Iteration 21471, loss = 0.05116448\n",
      "Iteration 21472, loss = 0.05116418\n",
      "Iteration 21473, loss = 0.05115720\n",
      "Iteration 21474, loss = 0.05115302\n",
      "Iteration 21475, loss = 0.05114892\n",
      "Iteration 21476, loss = 0.05114478\n",
      "Iteration 21477, loss = 0.05114744\n",
      "Iteration 21478, loss = 0.05114273\n",
      "Iteration 21479, loss = 0.05113884\n",
      "Iteration 21480, loss = 0.05114053\n",
      "Iteration 21481, loss = 0.05113774\n",
      "Iteration 21482, loss = 0.05113787\n",
      "Iteration 21483, loss = 0.05113194\n",
      "Iteration 21484, loss = 0.05112744\n",
      "Iteration 21485, loss = 0.05112523\n",
      "Iteration 21486, loss = 0.05112139\n",
      "Iteration 21487, loss = 0.05111746\n",
      "Iteration 21488, loss = 0.05112303\n",
      "Iteration 21489, loss = 0.05112048\n",
      "Iteration 21490, loss = 0.05111852\n",
      "Iteration 21491, loss = 0.05111416\n",
      "Iteration 21492, loss = 0.05110817\n",
      "Iteration 21493, loss = 0.05110724\n",
      "Iteration 21494, loss = 0.05110098\n",
      "Iteration 21495, loss = 0.05109630\n",
      "Iteration 21496, loss = 0.05109871\n",
      "Iteration 21497, loss = 0.05109232\n",
      "Iteration 21498, loss = 0.05108433\n",
      "Iteration 21499, loss = 0.05107839\n",
      "Iteration 21500, loss = 0.05108152\n",
      "Iteration 21501, loss = 0.05108000\n",
      "Iteration 21502, loss = 0.05107958\n",
      "Iteration 21503, loss = 0.05107701\n",
      "Iteration 21504, loss = 0.05106554\n",
      "Iteration 21505, loss = 0.05105781\n",
      "Iteration 21506, loss = 0.05107278\n",
      "Iteration 21507, loss = 0.05106894\n",
      "Iteration 21508, loss = 0.05105518\n",
      "Iteration 21509, loss = 0.05105227\n",
      "Iteration 21510, loss = 0.05105202\n",
      "Iteration 21511, loss = 0.05105083\n",
      "Iteration 21512, loss = 0.05104809\n",
      "Iteration 21513, loss = 0.05104149\n",
      "Iteration 21514, loss = 0.05103640\n",
      "Iteration 21515, loss = 0.05103025\n",
      "Iteration 21516, loss = 0.05103114\n",
      "Iteration 21517, loss = 0.05102883\n",
      "Iteration 21518, loss = 0.05102149\n",
      "Iteration 21519, loss = 0.05101908\n",
      "Iteration 21520, loss = 0.05101634\n",
      "Iteration 21521, loss = 0.05101301\n",
      "Iteration 21522, loss = 0.05101114\n",
      "Iteration 21523, loss = 0.05100541\n",
      "Iteration 21524, loss = 0.05099965\n",
      "Iteration 21525, loss = 0.05100199\n",
      "Iteration 21526, loss = 0.05099690\n",
      "Iteration 21527, loss = 0.05099819\n",
      "Iteration 21528, loss = 0.05099287\n",
      "Iteration 21529, loss = 0.05099222\n",
      "Iteration 21530, loss = 0.05099059\n",
      "Iteration 21531, loss = 0.05098725\n",
      "Iteration 21532, loss = 0.05098260\n",
      "Iteration 21533, loss = 0.05098216\n",
      "Iteration 21534, loss = 0.05097654\n",
      "Iteration 21535, loss = 0.05097181\n",
      "Iteration 21536, loss = 0.05097101\n",
      "Iteration 21537, loss = 0.05096590\n",
      "Iteration 21538, loss = 0.05096293\n",
      "Iteration 21539, loss = 0.05096294\n",
      "Iteration 21540, loss = 0.05096056\n",
      "Iteration 21541, loss = 0.05095492\n",
      "Iteration 21542, loss = 0.05095132\n",
      "Iteration 21543, loss = 0.05094806\n",
      "Iteration 21544, loss = 0.05094733\n",
      "Iteration 21545, loss = 0.05094143\n",
      "Iteration 21546, loss = 0.05093806\n",
      "Iteration 21547, loss = 0.05093596\n",
      "Iteration 21548, loss = 0.05093276\n",
      "Iteration 21549, loss = 0.05092907\n",
      "Iteration 21550, loss = 0.05093011\n",
      "Iteration 21551, loss = 0.05092863\n",
      "Iteration 21552, loss = 0.05092176\n",
      "Iteration 21553, loss = 0.05092052\n",
      "Iteration 21554, loss = 0.05091670\n",
      "Iteration 21555, loss = 0.05091497\n",
      "Iteration 21556, loss = 0.05091124\n",
      "Iteration 21557, loss = 0.05091385\n",
      "Iteration 21558, loss = 0.05091179\n",
      "Iteration 21559, loss = 0.05090312\n",
      "Iteration 21560, loss = 0.05089997\n",
      "Iteration 21561, loss = 0.05089973\n",
      "Iteration 21562, loss = 0.05089437\n",
      "Iteration 21563, loss = 0.05089180\n",
      "Iteration 21564, loss = 0.05089067\n",
      "Iteration 21565, loss = 0.05088852\n",
      "Iteration 21566, loss = 0.05088193\n",
      "Iteration 21567, loss = 0.05088121\n",
      "Iteration 21568, loss = 0.05087952\n",
      "Iteration 21569, loss = 0.05087584\n",
      "Iteration 21570, loss = 0.05087226\n",
      "Iteration 21571, loss = 0.05086755\n",
      "Iteration 21572, loss = 0.05086543\n",
      "Iteration 21573, loss = 0.05086069\n",
      "Iteration 21574, loss = 0.05086069\n",
      "Iteration 21575, loss = 0.05086385\n",
      "Iteration 21576, loss = 0.05085226\n",
      "Iteration 21577, loss = 0.05085204\n",
      "Iteration 21578, loss = 0.05084668\n",
      "Iteration 21579, loss = 0.05085187\n",
      "Iteration 21580, loss = 0.05084979\n",
      "Iteration 21581, loss = 0.05084183\n",
      "Iteration 21582, loss = 0.05083931\n",
      "Iteration 21583, loss = 0.05083829\n",
      "Iteration 21584, loss = 0.05083543\n",
      "Iteration 21585, loss = 0.05083318\n",
      "Iteration 21586, loss = 0.05082765\n",
      "Iteration 21587, loss = 0.05082185\n",
      "Iteration 21588, loss = 0.05082354\n",
      "Iteration 21589, loss = 0.05082174\n",
      "Iteration 21590, loss = 0.05081505\n",
      "Iteration 21591, loss = 0.05081137\n",
      "Iteration 21592, loss = 0.05081143\n",
      "Iteration 21593, loss = 0.05080598\n",
      "Iteration 21594, loss = 0.05080414\n",
      "Iteration 21595, loss = 0.05080109\n",
      "Iteration 21596, loss = 0.05079817\n",
      "Iteration 21597, loss = 0.05079887\n",
      "Iteration 21598, loss = 0.05079474\n",
      "Iteration 21599, loss = 0.05079515\n",
      "Iteration 21600, loss = 0.05079198\n",
      "Iteration 21601, loss = 0.05078416\n",
      "Iteration 21602, loss = 0.05078248\n",
      "Iteration 21603, loss = 0.05078256\n",
      "Iteration 21604, loss = 0.05077601\n",
      "Iteration 21605, loss = 0.05077227\n",
      "Iteration 21606, loss = 0.05077334\n",
      "Iteration 21607, loss = 0.05077283\n",
      "Iteration 21608, loss = 0.05076759\n",
      "Iteration 21609, loss = 0.05076708\n",
      "Iteration 21610, loss = 0.05076109\n",
      "Iteration 21611, loss = 0.05075874\n",
      "Iteration 21612, loss = 0.05075370\n",
      "Iteration 21613, loss = 0.05075813\n",
      "Iteration 21614, loss = 0.05075690\n",
      "Iteration 21615, loss = 0.05075197\n",
      "Iteration 21616, loss = 0.05074449\n",
      "Iteration 21617, loss = 0.05074305\n",
      "Iteration 21618, loss = 0.05074119\n",
      "Iteration 21619, loss = 0.05074008\n",
      "Iteration 21620, loss = 0.05073913\n",
      "Iteration 21621, loss = 0.05073251\n",
      "Iteration 21622, loss = 0.05073006\n",
      "Iteration 21623, loss = 0.05073137\n",
      "Iteration 21624, loss = 0.05072323\n",
      "Iteration 21625, loss = 0.05071902\n",
      "Iteration 21626, loss = 0.05072111\n",
      "Iteration 21627, loss = 0.05071860\n",
      "Iteration 21628, loss = 0.05071523\n",
      "Iteration 21629, loss = 0.05070732\n",
      "Iteration 21630, loss = 0.05070387\n",
      "Iteration 21631, loss = 0.05070551\n",
      "Iteration 21632, loss = 0.05070195\n",
      "Iteration 21633, loss = 0.05069615\n",
      "Iteration 21634, loss = 0.05069388\n",
      "Iteration 21635, loss = 0.05069031\n",
      "Iteration 21636, loss = 0.05069039\n",
      "Iteration 21637, loss = 0.05068918\n",
      "Iteration 21638, loss = 0.05068528\n",
      "Iteration 21639, loss = 0.05067879\n",
      "Iteration 21640, loss = 0.05067556\n",
      "Iteration 21641, loss = 0.05067580\n",
      "Iteration 21642, loss = 0.05067466\n",
      "Iteration 21643, loss = 0.05067325\n",
      "Iteration 21644, loss = 0.05067293\n",
      "Iteration 21645, loss = 0.05066912\n",
      "Iteration 21646, loss = 0.05066801\n",
      "Iteration 21647, loss = 0.05066239\n",
      "Iteration 21648, loss = 0.05065798\n",
      "Iteration 21649, loss = 0.05065665\n",
      "Iteration 21650, loss = 0.05065734\n",
      "Iteration 21651, loss = 0.05064990\n",
      "Iteration 21652, loss = 0.05064711\n",
      "Iteration 21653, loss = 0.05064742\n",
      "Iteration 21654, loss = 0.05064478\n",
      "Iteration 21655, loss = 0.05064106\n",
      "Iteration 21656, loss = 0.05063707\n",
      "Iteration 21657, loss = 0.05063760\n",
      "Iteration 21658, loss = 0.05063359\n",
      "Iteration 21659, loss = 0.05062911\n",
      "Iteration 21660, loss = 0.05062557\n",
      "Iteration 21661, loss = 0.05062076\n",
      "Iteration 21662, loss = 0.05061464\n",
      "Iteration 21663, loss = 0.05061666\n",
      "Iteration 21664, loss = 0.05061338\n",
      "Iteration 21665, loss = 0.05060722\n",
      "Iteration 21666, loss = 0.05060846\n",
      "Iteration 21667, loss = 0.05060674\n",
      "Iteration 21668, loss = 0.05060301\n",
      "Iteration 21669, loss = 0.05060052\n",
      "Iteration 21670, loss = 0.05059593\n",
      "Iteration 21671, loss = 0.05059191\n",
      "Iteration 21672, loss = 0.05058844\n",
      "Iteration 21673, loss = 0.05059100\n",
      "Iteration 21674, loss = 0.05058919\n",
      "Iteration 21675, loss = 0.05058462\n",
      "Iteration 21676, loss = 0.05058676\n",
      "Iteration 21677, loss = 0.05058142\n",
      "Iteration 21678, loss = 0.05057340\n",
      "Iteration 21679, loss = 0.05057198\n",
      "Iteration 21680, loss = 0.05057227\n",
      "Iteration 21681, loss = 0.05057395\n",
      "Iteration 21682, loss = 0.05056580\n",
      "Iteration 21683, loss = 0.05056268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21684, loss = 0.05056027\n",
      "Iteration 21685, loss = 0.05055588\n",
      "Iteration 21686, loss = 0.05054920\n",
      "Iteration 21687, loss = 0.05055485\n",
      "Iteration 21688, loss = 0.05055178\n",
      "Iteration 21689, loss = 0.05054174\n",
      "Iteration 21690, loss = 0.05054143\n",
      "Iteration 21691, loss = 0.05054494\n",
      "Iteration 21692, loss = 0.05054336\n",
      "Iteration 21693, loss = 0.05054261\n",
      "Iteration 21694, loss = 0.05054173\n",
      "Iteration 21695, loss = 0.05053332\n",
      "Iteration 21696, loss = 0.05052716\n",
      "Iteration 21697, loss = 0.05052417\n",
      "Iteration 21698, loss = 0.05052542\n",
      "Iteration 21699, loss = 0.05052279\n",
      "Iteration 21700, loss = 0.05051523\n",
      "Iteration 21701, loss = 0.05050921\n",
      "Iteration 21702, loss = 0.05050990\n",
      "Iteration 21703, loss = 0.05051052\n",
      "Iteration 21704, loss = 0.05050733\n",
      "Iteration 21705, loss = 0.05050656\n",
      "Iteration 21706, loss = 0.05050283\n",
      "Iteration 21707, loss = 0.05049788\n",
      "Iteration 21708, loss = 0.05049490\n",
      "Iteration 21709, loss = 0.05049320\n",
      "Iteration 21710, loss = 0.05049050\n",
      "Iteration 21711, loss = 0.05048793\n",
      "Iteration 21712, loss = 0.05048295\n",
      "Iteration 21713, loss = 0.05048241\n",
      "Iteration 21714, loss = 0.05047996\n",
      "Iteration 21715, loss = 0.05047527\n",
      "Iteration 21716, loss = 0.05046994\n",
      "Iteration 21717, loss = 0.05047170\n",
      "Iteration 21718, loss = 0.05046574\n",
      "Iteration 21719, loss = 0.05046327\n",
      "Iteration 21720, loss = 0.05046179\n",
      "Iteration 21721, loss = 0.05046448\n",
      "Iteration 21722, loss = 0.05045935\n",
      "Iteration 21723, loss = 0.05045407\n",
      "Iteration 21724, loss = 0.05045189\n",
      "Iteration 21725, loss = 0.05045334\n",
      "Iteration 21726, loss = 0.05045007\n",
      "Iteration 21727, loss = 0.05045137\n",
      "Iteration 21728, loss = 0.05044922\n",
      "Iteration 21729, loss = 0.05044245\n",
      "Iteration 21730, loss = 0.05044176\n",
      "Iteration 21731, loss = 0.05043749\n",
      "Iteration 21732, loss = 0.05043206\n",
      "Iteration 21733, loss = 0.05042876\n",
      "Iteration 21734, loss = 0.05042638\n",
      "Iteration 21735, loss = 0.05042005\n",
      "Iteration 21736, loss = 0.05042187\n",
      "Iteration 21737, loss = 0.05042094\n",
      "Iteration 21738, loss = 0.05041856\n",
      "Iteration 21739, loss = 0.05041112\n",
      "Iteration 21740, loss = 0.05040774\n",
      "Iteration 21741, loss = 0.05040803\n",
      "Iteration 21742, loss = 0.05040699\n",
      "Iteration 21743, loss = 0.05040320\n",
      "Iteration 21744, loss = 0.05039784\n",
      "Iteration 21745, loss = 0.05039241\n",
      "Iteration 21746, loss = 0.05039301\n",
      "Iteration 21747, loss = 0.05039188\n",
      "Iteration 21748, loss = 0.05038679\n",
      "Iteration 21749, loss = 0.05038297\n",
      "Iteration 21750, loss = 0.05038482\n",
      "Iteration 21751, loss = 0.05038186\n",
      "Iteration 21752, loss = 0.05037731\n",
      "Iteration 21753, loss = 0.05037834\n",
      "Iteration 21754, loss = 0.05037426\n",
      "Iteration 21755, loss = 0.05037040\n",
      "Iteration 21756, loss = 0.05036610\n",
      "Iteration 21757, loss = 0.05036310\n",
      "Iteration 21758, loss = 0.05036139\n",
      "Iteration 21759, loss = 0.05035438\n",
      "Iteration 21760, loss = 0.05035136\n",
      "Iteration 21761, loss = 0.05035290\n",
      "Iteration 21762, loss = 0.05035535\n",
      "Iteration 21763, loss = 0.05035241\n",
      "Iteration 21764, loss = 0.05034452\n",
      "Iteration 21765, loss = 0.05033850\n",
      "Iteration 21766, loss = 0.05033643\n",
      "Iteration 21767, loss = 0.05033409\n",
      "Iteration 21768, loss = 0.05033265\n",
      "Iteration 21769, loss = 0.05032876\n",
      "Iteration 21770, loss = 0.05032609\n",
      "Iteration 21771, loss = 0.05032319\n",
      "Iteration 21772, loss = 0.05031679\n",
      "Iteration 21773, loss = 0.05031563\n",
      "Iteration 21774, loss = 0.05031820\n",
      "Iteration 21775, loss = 0.05031487\n",
      "Iteration 21776, loss = 0.05031075\n",
      "Iteration 21777, loss = 0.05030858\n",
      "Iteration 21778, loss = 0.05030637\n",
      "Iteration 21779, loss = 0.05030462\n",
      "Iteration 21780, loss = 0.05029955\n",
      "Iteration 21781, loss = 0.05029810\n",
      "Iteration 21782, loss = 0.05029579\n",
      "Iteration 21783, loss = 0.05029538\n",
      "Iteration 21784, loss = 0.05029307\n",
      "Iteration 21785, loss = 0.05028661\n",
      "Iteration 21786, loss = 0.05028246\n",
      "Iteration 21787, loss = 0.05028192\n",
      "Iteration 21788, loss = 0.05028172\n",
      "Iteration 21789, loss = 0.05027460\n",
      "Iteration 21790, loss = 0.05027237\n",
      "Iteration 21791, loss = 0.05026989\n",
      "Iteration 21792, loss = 0.05027361\n",
      "Iteration 21793, loss = 0.05027156\n",
      "Iteration 21794, loss = 0.05027014\n",
      "Iteration 21795, loss = 0.05026572\n",
      "Iteration 21796, loss = 0.05025957\n",
      "Iteration 21797, loss = 0.05025660\n",
      "Iteration 21798, loss = 0.05025563\n",
      "Iteration 21799, loss = 0.05024614\n",
      "Iteration 21800, loss = 0.05024845\n",
      "Iteration 21801, loss = 0.05024971\n",
      "Iteration 21802, loss = 0.05024200\n",
      "Iteration 21803, loss = 0.05024021\n",
      "Iteration 21804, loss = 0.05023852\n",
      "Iteration 21805, loss = 0.05023746\n",
      "Iteration 21806, loss = 0.05023689\n",
      "Iteration 21807, loss = 0.05023229\n",
      "Iteration 21808, loss = 0.05023012\n",
      "Iteration 21809, loss = 0.05022681\n",
      "Iteration 21810, loss = 0.05022264\n",
      "Iteration 21811, loss = 0.05022775\n",
      "Iteration 21812, loss = 0.05022819\n",
      "Iteration 21813, loss = 0.05022004\n",
      "Iteration 21814, loss = 0.05021595\n",
      "Iteration 21815, loss = 0.05021418\n",
      "Iteration 21816, loss = 0.05020799\n",
      "Iteration 21817, loss = 0.05020643\n",
      "Iteration 21818, loss = 0.05020353\n",
      "Iteration 21819, loss = 0.05020245\n",
      "Iteration 21820, loss = 0.05020141\n",
      "Iteration 21821, loss = 0.05019326\n",
      "Iteration 21822, loss = 0.05018580\n",
      "Iteration 21823, loss = 0.05019164\n",
      "Iteration 21824, loss = 0.05019217\n",
      "Iteration 21825, loss = 0.05018397\n",
      "Iteration 21826, loss = 0.05017641\n",
      "Iteration 21827, loss = 0.05017163\n",
      "Iteration 21828, loss = 0.05017627\n",
      "Iteration 21829, loss = 0.05017313\n",
      "Iteration 21830, loss = 0.05016439\n",
      "Iteration 21831, loss = 0.05016371\n",
      "Iteration 21832, loss = 0.05016629\n",
      "Iteration 21833, loss = 0.05016166\n",
      "Iteration 21834, loss = 0.05015872\n",
      "Iteration 21835, loss = 0.05016284\n",
      "Iteration 21836, loss = 0.05015627\n",
      "Iteration 21837, loss = 0.05015252\n",
      "Iteration 21838, loss = 0.05014795\n",
      "Iteration 21839, loss = 0.05014537\n",
      "Iteration 21840, loss = 0.05014529\n",
      "Iteration 21841, loss = 0.05014001\n",
      "Iteration 21842, loss = 0.05013275\n",
      "Iteration 21843, loss = 0.05013760\n",
      "Iteration 21844, loss = 0.05013721\n",
      "Iteration 21845, loss = 0.05012774\n",
      "Iteration 21846, loss = 0.05012775\n",
      "Iteration 21847, loss = 0.05012565\n",
      "Iteration 21848, loss = 0.05012247\n",
      "Iteration 21849, loss = 0.05012360\n",
      "Iteration 21850, loss = 0.05012105\n",
      "Iteration 21851, loss = 0.05011385\n",
      "Iteration 21852, loss = 0.05010911\n",
      "Iteration 21853, loss = 0.05011110\n",
      "Iteration 21854, loss = 0.05010998\n",
      "Iteration 21855, loss = 0.05010341\n",
      "Iteration 21856, loss = 0.05009744\n",
      "Iteration 21857, loss = 0.05010332\n",
      "Iteration 21858, loss = 0.05010319\n",
      "Iteration 21859, loss = 0.05009666\n",
      "Iteration 21860, loss = 0.05009200\n",
      "Iteration 21861, loss = 0.05008605\n",
      "Iteration 21862, loss = 0.05008735\n",
      "Iteration 21863, loss = 0.05008636\n",
      "Iteration 21864, loss = 0.05007450\n",
      "Iteration 21865, loss = 0.05008014\n",
      "Iteration 21866, loss = 0.05008160\n",
      "Iteration 21867, loss = 0.05007383\n",
      "Iteration 21868, loss = 0.05007558\n",
      "Iteration 21869, loss = 0.05007445\n",
      "Iteration 21870, loss = 0.05007026\n",
      "Iteration 21871, loss = 0.05006641\n",
      "Iteration 21872, loss = 0.05006172\n",
      "Iteration 21873, loss = 0.05005967\n",
      "Iteration 21874, loss = 0.05006257\n",
      "Iteration 21875, loss = 0.05006481\n",
      "Iteration 21876, loss = 0.05005422\n",
      "Iteration 21877, loss = 0.05005018\n",
      "Iteration 21878, loss = 0.05004719\n",
      "Iteration 21879, loss = 0.05004859\n",
      "Iteration 21880, loss = 0.05004760\n",
      "Iteration 21881, loss = 0.05004504\n",
      "Iteration 21882, loss = 0.05003913\n",
      "Iteration 21883, loss = 0.05003549\n",
      "Iteration 21884, loss = 0.05003340\n",
      "Iteration 21885, loss = 0.05003116\n",
      "Iteration 21886, loss = 0.05002905\n",
      "Iteration 21887, loss = 0.05002021\n",
      "Iteration 21888, loss = 0.05001741\n",
      "Iteration 21889, loss = 0.05001551\n",
      "Iteration 21890, loss = 0.05001362\n",
      "Iteration 21891, loss = 0.05001275\n",
      "Iteration 21892, loss = 0.05000705\n",
      "Iteration 21893, loss = 0.05000630\n",
      "Iteration 21894, loss = 0.05000349\n",
      "Iteration 21895, loss = 0.05000067\n",
      "Iteration 21896, loss = 0.05000186\n",
      "Iteration 21897, loss = 0.04999541\n",
      "Iteration 21898, loss = 0.04999342\n",
      "Iteration 21899, loss = 0.04998884\n",
      "Iteration 21900, loss = 0.04998976\n",
      "Iteration 21901, loss = 0.04998468\n",
      "Iteration 21902, loss = 0.04998064\n",
      "Iteration 21903, loss = 0.04997850\n",
      "Iteration 21904, loss = 0.04997560\n",
      "Iteration 21905, loss = 0.04997278\n",
      "Iteration 21906, loss = 0.04997323\n",
      "Iteration 21907, loss = 0.04997041\n",
      "Iteration 21908, loss = 0.04996593\n",
      "Iteration 21909, loss = 0.04996471\n",
      "Iteration 21910, loss = 0.04996278\n",
      "Iteration 21911, loss = 0.04995914\n",
      "Iteration 21912, loss = 0.04995832\n",
      "Iteration 21913, loss = 0.04995744\n",
      "Iteration 21914, loss = 0.04995465\n",
      "Iteration 21915, loss = 0.04994782\n",
      "Iteration 21916, loss = 0.04994637\n",
      "Iteration 21917, loss = 0.04994473\n",
      "Iteration 21918, loss = 0.04994376\n",
      "Iteration 21919, loss = 0.04993902\n",
      "Iteration 21920, loss = 0.04993585\n",
      "Iteration 21921, loss = 0.04993638\n",
      "Iteration 21922, loss = 0.04993206\n",
      "Iteration 21923, loss = 0.04993233\n",
      "Iteration 21924, loss = 0.04993018\n",
      "Iteration 21925, loss = 0.04992106\n",
      "Iteration 21926, loss = 0.04992203\n",
      "Iteration 21927, loss = 0.04992494\n",
      "Iteration 21928, loss = 0.04992043\n",
      "Iteration 21929, loss = 0.04991991\n",
      "Iteration 21930, loss = 0.04992233\n",
      "Iteration 21931, loss = 0.04991933\n",
      "Iteration 21932, loss = 0.04991048\n",
      "Iteration 21933, loss = 0.04990535\n",
      "Iteration 21934, loss = 0.04990186\n",
      "Iteration 21935, loss = 0.04989861\n",
      "Iteration 21936, loss = 0.04989013\n",
      "Iteration 21937, loss = 0.04989281\n",
      "Iteration 21938, loss = 0.04988990\n",
      "Iteration 21939, loss = 0.04989053\n",
      "Iteration 21940, loss = 0.04988792\n",
      "Iteration 21941, loss = 0.04988021\n",
      "Iteration 21942, loss = 0.04988333\n",
      "Iteration 21943, loss = 0.04988299\n",
      "Iteration 21944, loss = 0.04987534\n",
      "Iteration 21945, loss = 0.04987288\n",
      "Iteration 21946, loss = 0.04987010\n",
      "Iteration 21947, loss = 0.04986632\n",
      "Iteration 21948, loss = 0.04986390\n",
      "Iteration 21949, loss = 0.04986283\n",
      "Iteration 21950, loss = 0.04986082\n",
      "Iteration 21951, loss = 0.04985582\n",
      "Iteration 21952, loss = 0.04985841\n",
      "Iteration 21953, loss = 0.04985797\n",
      "Iteration 21954, loss = 0.04984842\n",
      "Iteration 21955, loss = 0.04984116\n",
      "Iteration 21956, loss = 0.04984805\n",
      "Iteration 21957, loss = 0.04984588\n",
      "Iteration 21958, loss = 0.04983709\n",
      "Iteration 21959, loss = 0.04983503\n",
      "Iteration 21960, loss = 0.04983545\n",
      "Iteration 21961, loss = 0.04983232\n",
      "Iteration 21962, loss = 0.04982972\n",
      "Iteration 21963, loss = 0.04982366\n",
      "Iteration 21964, loss = 0.04982340\n",
      "Iteration 21965, loss = 0.04982390\n",
      "Iteration 21966, loss = 0.04982019\n",
      "Iteration 21967, loss = 0.04981624\n",
      "Iteration 21968, loss = 0.04981394\n",
      "Iteration 21969, loss = 0.04981043\n",
      "Iteration 21970, loss = 0.04981322\n",
      "Iteration 21971, loss = 0.04980746\n",
      "Iteration 21972, loss = 0.04980297\n",
      "Iteration 21973, loss = 0.04980221\n",
      "Iteration 21974, loss = 0.04980084\n",
      "Iteration 21975, loss = 0.04979980\n",
      "Iteration 21976, loss = 0.04979755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21977, loss = 0.04979408\n",
      "Iteration 21978, loss = 0.04979136\n",
      "Iteration 21979, loss = 0.04978187\n",
      "Iteration 21980, loss = 0.04978287\n",
      "Iteration 21981, loss = 0.04978198\n",
      "Iteration 21982, loss = 0.04977715\n",
      "Iteration 21983, loss = 0.04977520\n",
      "Iteration 21984, loss = 0.04977146\n",
      "Iteration 21985, loss = 0.04976839\n",
      "Iteration 21986, loss = 0.04976759\n",
      "Iteration 21987, loss = 0.04976454\n",
      "Iteration 21988, loss = 0.04976082\n",
      "Iteration 21989, loss = 0.04975706\n",
      "Iteration 21990, loss = 0.04975856\n",
      "Iteration 21991, loss = 0.04975629\n",
      "Iteration 21992, loss = 0.04975172\n",
      "Iteration 21993, loss = 0.04975241\n",
      "Iteration 21994, loss = 0.04974853\n",
      "Iteration 21995, loss = 0.04974690\n",
      "Iteration 21996, loss = 0.04974588\n",
      "Iteration 21997, loss = 0.04974132\n",
      "Iteration 21998, loss = 0.04973723\n",
      "Iteration 21999, loss = 0.04973545\n",
      "Iteration 22000, loss = 0.04973918\n",
      "Iteration 22001, loss = 0.04973744\n",
      "Iteration 22002, loss = 0.04972923\n",
      "Iteration 22003, loss = 0.04972588\n",
      "Iteration 22004, loss = 0.04972775\n",
      "Iteration 22005, loss = 0.04972277\n",
      "Iteration 22006, loss = 0.04971854\n",
      "Iteration 22007, loss = 0.04971178\n",
      "Iteration 22008, loss = 0.04971118\n",
      "Iteration 22009, loss = 0.04970824\n",
      "Iteration 22010, loss = 0.04970550\n",
      "Iteration 22011, loss = 0.04970674\n",
      "Iteration 22012, loss = 0.04970462\n",
      "Iteration 22013, loss = 0.04969715\n",
      "Iteration 22014, loss = 0.04969785\n",
      "Iteration 22015, loss = 0.04969801\n",
      "Iteration 22016, loss = 0.04968872\n",
      "Iteration 22017, loss = 0.04969046\n",
      "Iteration 22018, loss = 0.04969032\n",
      "Iteration 22019, loss = 0.04968774\n",
      "Iteration 22020, loss = 0.04968745\n",
      "Iteration 22021, loss = 0.04968242\n",
      "Iteration 22022, loss = 0.04967987\n",
      "Iteration 22023, loss = 0.04967529\n",
      "Iteration 22024, loss = 0.04967075\n",
      "Iteration 22025, loss = 0.04967311\n",
      "Iteration 22026, loss = 0.04967209\n",
      "Iteration 22027, loss = 0.04966708\n",
      "Iteration 22028, loss = 0.04966130\n",
      "Iteration 22029, loss = 0.04966154\n",
      "Iteration 22030, loss = 0.04966051\n",
      "Iteration 22031, loss = 0.04965607\n",
      "Iteration 22032, loss = 0.04965146\n",
      "Iteration 22033, loss = 0.04964658\n",
      "Iteration 22034, loss = 0.04965197\n",
      "Iteration 22035, loss = 0.04964933\n",
      "Iteration 22036, loss = 0.04963613\n",
      "Iteration 22037, loss = 0.04963530\n",
      "Iteration 22038, loss = 0.04963294\n",
      "Iteration 22039, loss = 0.04962891\n",
      "Iteration 22040, loss = 0.04963319\n",
      "Iteration 22041, loss = 0.04962740\n",
      "Iteration 22042, loss = 0.04962591\n",
      "Iteration 22043, loss = 0.04962698\n",
      "Iteration 22044, loss = 0.04962407\n",
      "Iteration 22045, loss = 0.04962154\n",
      "Iteration 22046, loss = 0.04961518\n",
      "Iteration 22047, loss = 0.04961354\n",
      "Iteration 22048, loss = 0.04961019\n",
      "Iteration 22049, loss = 0.04960896\n",
      "Iteration 22050, loss = 0.04960729\n",
      "Iteration 22051, loss = 0.04960429\n",
      "Iteration 22052, loss = 0.04960331\n",
      "Iteration 22053, loss = 0.04960106\n",
      "Iteration 22054, loss = 0.04959582\n",
      "Iteration 22055, loss = 0.04959076\n",
      "Iteration 22056, loss = 0.04958711\n",
      "Iteration 22057, loss = 0.04958669\n",
      "Iteration 22058, loss = 0.04958526\n",
      "Iteration 22059, loss = 0.04958480\n",
      "Iteration 22060, loss = 0.04958382\n",
      "Iteration 22061, loss = 0.04957881\n",
      "Iteration 22062, loss = 0.04957974\n",
      "Iteration 22063, loss = 0.04957880\n",
      "Iteration 22064, loss = 0.04957863\n",
      "Iteration 22065, loss = 0.04957426\n",
      "Iteration 22066, loss = 0.04956948\n",
      "Iteration 22067, loss = 0.04956485\n",
      "Iteration 22068, loss = 0.04956865\n",
      "Iteration 22069, loss = 0.04956232\n",
      "Iteration 22070, loss = 0.04955690\n",
      "Iteration 22071, loss = 0.04956212\n",
      "Iteration 22072, loss = 0.04956289\n",
      "Iteration 22073, loss = 0.04956067\n",
      "Iteration 22074, loss = 0.04955672\n",
      "Iteration 22075, loss = 0.04955583\n",
      "Iteration 22076, loss = 0.04954978\n",
      "Iteration 22077, loss = 0.04954804\n",
      "Iteration 22078, loss = 0.04954205\n",
      "Iteration 22079, loss = 0.04954598\n",
      "Iteration 22080, loss = 0.04954547\n",
      "Iteration 22081, loss = 0.04953905\n",
      "Iteration 22082, loss = 0.04953748\n",
      "Iteration 22083, loss = 0.04953702\n",
      "Iteration 22084, loss = 0.04953231\n",
      "Iteration 22085, loss = 0.04952523\n",
      "Iteration 22086, loss = 0.04951753\n",
      "Iteration 22087, loss = 0.04952213\n",
      "Iteration 22088, loss = 0.04952421\n",
      "Iteration 22089, loss = 0.04951698\n",
      "Iteration 22090, loss = 0.04950712\n",
      "Iteration 22091, loss = 0.04951111\n",
      "Iteration 22092, loss = 0.04951240\n",
      "Iteration 22093, loss = 0.04951086\n",
      "Iteration 22094, loss = 0.04950852\n",
      "Iteration 22095, loss = 0.04950115\n",
      "Iteration 22096, loss = 0.04949600\n",
      "Iteration 22097, loss = 0.04949675\n",
      "Iteration 22098, loss = 0.04948993\n",
      "Iteration 22099, loss = 0.04948818\n",
      "Iteration 22100, loss = 0.04948288\n",
      "Iteration 22101, loss = 0.04947804\n",
      "Iteration 22102, loss = 0.04947827\n",
      "Iteration 22103, loss = 0.04947898\n",
      "Iteration 22104, loss = 0.04947281\n",
      "Iteration 22105, loss = 0.04947012\n",
      "Iteration 22106, loss = 0.04946659\n",
      "Iteration 22107, loss = 0.04946518\n",
      "Iteration 22108, loss = 0.04946123\n",
      "Iteration 22109, loss = 0.04946148\n",
      "Iteration 22110, loss = 0.04946064\n",
      "Iteration 22111, loss = 0.04945714\n",
      "Iteration 22112, loss = 0.04945415\n",
      "Iteration 22113, loss = 0.04944682\n",
      "Iteration 22114, loss = 0.04945407\n",
      "Iteration 22115, loss = 0.04945302\n",
      "Iteration 22116, loss = 0.04945014\n",
      "Iteration 22117, loss = 0.04944686\n",
      "Iteration 22118, loss = 0.04944489\n",
      "Iteration 22119, loss = 0.04944609\n",
      "Iteration 22120, loss = 0.04943778\n",
      "Iteration 22121, loss = 0.04943031\n",
      "Iteration 22122, loss = 0.04942996\n",
      "Iteration 22123, loss = 0.04942748\n",
      "Iteration 22124, loss = 0.04942394\n",
      "Iteration 22125, loss = 0.04942546\n",
      "Iteration 22126, loss = 0.04942309\n",
      "Iteration 22127, loss = 0.04941693\n",
      "Iteration 22128, loss = 0.04941316\n",
      "Iteration 22129, loss = 0.04941381\n",
      "Iteration 22130, loss = 0.04941143\n",
      "Iteration 22131, loss = 0.04941023\n",
      "Iteration 22132, loss = 0.04940445\n",
      "Iteration 22133, loss = 0.04940253\n",
      "Iteration 22134, loss = 0.04939976\n",
      "Iteration 22135, loss = 0.04939800\n",
      "Iteration 22136, loss = 0.04939838\n",
      "Iteration 22137, loss = 0.04939340\n",
      "Iteration 22138, loss = 0.04939031\n",
      "Iteration 22139, loss = 0.04939160\n",
      "Iteration 22140, loss = 0.04938557\n",
      "Iteration 22141, loss = 0.04938133\n",
      "Iteration 22142, loss = 0.04938550\n",
      "Iteration 22143, loss = 0.04938373\n",
      "Iteration 22144, loss = 0.04937531\n",
      "Iteration 22145, loss = 0.04938452\n",
      "Iteration 22146, loss = 0.04938222\n",
      "Iteration 22147, loss = 0.04937673\n",
      "Iteration 22148, loss = 0.04937134\n",
      "Iteration 22149, loss = 0.04936809\n",
      "Iteration 22150, loss = 0.04937108\n",
      "Iteration 22151, loss = 0.04936808\n",
      "Iteration 22152, loss = 0.04936241\n",
      "Iteration 22153, loss = 0.04935493\n",
      "Iteration 22154, loss = 0.04935158\n",
      "Iteration 22155, loss = 0.04934677\n",
      "Iteration 22156, loss = 0.04934871\n",
      "Iteration 22157, loss = 0.04934380\n",
      "Iteration 22158, loss = 0.04933733\n",
      "Iteration 22159, loss = 0.04933895\n",
      "Iteration 22160, loss = 0.04933816\n",
      "Iteration 22161, loss = 0.04933001\n",
      "Iteration 22162, loss = 0.04933276\n",
      "Iteration 22163, loss = 0.04932785\n",
      "Iteration 22164, loss = 0.04932702\n",
      "Iteration 22165, loss = 0.04932472\n",
      "Iteration 22166, loss = 0.04932541\n",
      "Iteration 22167, loss = 0.04932252\n",
      "Iteration 22168, loss = 0.04932055\n",
      "Iteration 22169, loss = 0.04931642\n",
      "Iteration 22170, loss = 0.04931202\n",
      "Iteration 22171, loss = 0.04931175\n",
      "Iteration 22172, loss = 0.04930408\n",
      "Iteration 22173, loss = 0.04930569\n",
      "Iteration 22174, loss = 0.04930211\n",
      "Iteration 22175, loss = 0.04929901\n",
      "Iteration 22176, loss = 0.04929752\n",
      "Iteration 22177, loss = 0.04929604\n",
      "Iteration 22178, loss = 0.04929255\n",
      "Iteration 22179, loss = 0.04928927\n",
      "Iteration 22180, loss = 0.04928514\n",
      "Iteration 22181, loss = 0.04928131\n",
      "Iteration 22182, loss = 0.04928154\n",
      "Iteration 22183, loss = 0.04927960\n",
      "Iteration 22184, loss = 0.04927495\n",
      "Iteration 22185, loss = 0.04927453\n",
      "Iteration 22186, loss = 0.04926980\n",
      "Iteration 22187, loss = 0.04926630\n",
      "Iteration 22188, loss = 0.04926787\n",
      "Iteration 22189, loss = 0.04926602\n",
      "Iteration 22190, loss = 0.04926434\n",
      "Iteration 22191, loss = 0.04925771\n",
      "Iteration 22192, loss = 0.04925596\n",
      "Iteration 22193, loss = 0.04925707\n",
      "Iteration 22194, loss = 0.04925327\n",
      "Iteration 22195, loss = 0.04925523\n",
      "Iteration 22196, loss = 0.04924688\n",
      "Iteration 22197, loss = 0.04925180\n",
      "Iteration 22198, loss = 0.04925013\n",
      "Iteration 22199, loss = 0.04924499\n",
      "Iteration 22200, loss = 0.04924756\n",
      "Iteration 22201, loss = 0.04924630\n",
      "Iteration 22202, loss = 0.04924099\n",
      "Iteration 22203, loss = 0.04924053\n",
      "Iteration 22204, loss = 0.04924126\n",
      "Iteration 22205, loss = 0.04923536\n",
      "Iteration 22206, loss = 0.04922890\n",
      "Iteration 22207, loss = 0.04922938\n",
      "Iteration 22208, loss = 0.04922476\n",
      "Iteration 22209, loss = 0.04922723\n",
      "Iteration 22210, loss = 0.04922351\n",
      "Iteration 22211, loss = 0.04921926\n",
      "Iteration 22212, loss = 0.04921828\n",
      "Iteration 22213, loss = 0.04921361\n",
      "Iteration 22214, loss = 0.04920808\n",
      "Iteration 22215, loss = 0.04920401\n",
      "Iteration 22216, loss = 0.04920255\n",
      "Iteration 22217, loss = 0.04919825\n",
      "Iteration 22218, loss = 0.04919764\n",
      "Iteration 22219, loss = 0.04919432\n",
      "Iteration 22220, loss = 0.04919437\n",
      "Iteration 22221, loss = 0.04919213\n",
      "Iteration 22222, loss = 0.04918704\n",
      "Iteration 22223, loss = 0.04919036\n",
      "Iteration 22224, loss = 0.04918666\n",
      "Iteration 22225, loss = 0.04917760\n",
      "Iteration 22226, loss = 0.04917657\n",
      "Iteration 22227, loss = 0.04917633\n",
      "Iteration 22228, loss = 0.04917237\n",
      "Iteration 22229, loss = 0.04917140\n",
      "Iteration 22230, loss = 0.04916573\n",
      "Iteration 22231, loss = 0.04916360\n",
      "Iteration 22232, loss = 0.04916234\n",
      "Iteration 22233, loss = 0.04916040\n",
      "Iteration 22234, loss = 0.04915852\n",
      "Iteration 22235, loss = 0.04915623\n",
      "Iteration 22236, loss = 0.04915336\n",
      "Iteration 22237, loss = 0.04915712\n",
      "Iteration 22238, loss = 0.04915439\n",
      "Iteration 22239, loss = 0.04914827\n",
      "Iteration 22240, loss = 0.04914515\n",
      "Iteration 22241, loss = 0.04914605\n",
      "Iteration 22242, loss = 0.04914672\n",
      "Iteration 22243, loss = 0.04914848\n",
      "Iteration 22244, loss = 0.04914472\n",
      "Iteration 22245, loss = 0.04913997\n",
      "Iteration 22246, loss = 0.04913486\n",
      "Iteration 22247, loss = 0.04912870\n",
      "Iteration 22248, loss = 0.04913114\n",
      "Iteration 22249, loss = 0.04912699\n",
      "Iteration 22250, loss = 0.04912104\n",
      "Iteration 22251, loss = 0.04912199\n",
      "Iteration 22252, loss = 0.04911909\n",
      "Iteration 22253, loss = 0.04911632\n",
      "Iteration 22254, loss = 0.04911468\n",
      "Iteration 22255, loss = 0.04911062\n",
      "Iteration 22256, loss = 0.04910697\n",
      "Iteration 22257, loss = 0.04910251\n",
      "Iteration 22258, loss = 0.04910088\n",
      "Iteration 22259, loss = 0.04910529\n",
      "Iteration 22260, loss = 0.04909778\n",
      "Iteration 22261, loss = 0.04909294\n",
      "Iteration 22262, loss = 0.04909145\n",
      "Iteration 22263, loss = 0.04909420\n",
      "Iteration 22264, loss = 0.04909094\n",
      "Iteration 22265, loss = 0.04908594\n",
      "Iteration 22266, loss = 0.04908387\n",
      "Iteration 22267, loss = 0.04908400\n",
      "Iteration 22268, loss = 0.04908050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 22269, loss = 0.04907743\n",
      "Iteration 22270, loss = 0.04907083\n",
      "Iteration 22271, loss = 0.04907263\n",
      "Iteration 22272, loss = 0.04907152\n",
      "Iteration 22273, loss = 0.04907051\n",
      "Iteration 22274, loss = 0.04906699\n",
      "Iteration 22275, loss = 0.04906370\n",
      "Iteration 22276, loss = 0.04906033\n",
      "Iteration 22277, loss = 0.04905696\n",
      "Iteration 22278, loss = 0.04905310\n",
      "Iteration 22279, loss = 0.04905117\n",
      "Iteration 22280, loss = 0.04904671\n",
      "Iteration 22281, loss = 0.04904368\n",
      "Iteration 22282, loss = 0.04904315\n",
      "Iteration 22283, loss = 0.04903992\n",
      "Iteration 22284, loss = 0.04903930\n",
      "Iteration 22285, loss = 0.04904165\n",
      "Iteration 22286, loss = 0.04904360\n",
      "Iteration 22287, loss = 0.04903529\n",
      "Iteration 22288, loss = 0.04903198\n",
      "Iteration 22289, loss = 0.04903097\n",
      "Iteration 22290, loss = 0.04903126\n",
      "Iteration 22291, loss = 0.04902564\n",
      "Iteration 22292, loss = 0.04901894\n",
      "Iteration 22293, loss = 0.04901971\n",
      "Iteration 22294, loss = 0.04901990\n",
      "Iteration 22295, loss = 0.04901804\n",
      "Iteration 22296, loss = 0.04901515\n",
      "Iteration 22297, loss = 0.04901327\n",
      "Iteration 22298, loss = 0.04900783\n",
      "Iteration 22299, loss = 0.04900160\n",
      "Iteration 22300, loss = 0.04900193\n",
      "Iteration 22301, loss = 0.04899865\n",
      "Iteration 22302, loss = 0.04899641\n",
      "Iteration 22303, loss = 0.04899632\n",
      "Iteration 22304, loss = 0.04899045\n",
      "Iteration 22305, loss = 0.04898982\n",
      "Iteration 22306, loss = 0.04898912\n",
      "Iteration 22307, loss = 0.04898637\n",
      "Iteration 22308, loss = 0.04898401\n",
      "Iteration 22309, loss = 0.04897879\n",
      "Iteration 22310, loss = 0.04897555\n",
      "Iteration 22311, loss = 0.04898171\n",
      "Iteration 22312, loss = 0.04897924\n",
      "Iteration 22313, loss = 0.04897238\n",
      "Iteration 22314, loss = 0.04897281\n",
      "Iteration 22315, loss = 0.04897335\n",
      "Iteration 22316, loss = 0.04897065\n",
      "Iteration 22317, loss = 0.04896872\n",
      "Iteration 22318, loss = 0.04896592\n",
      "Iteration 22319, loss = 0.04896295\n",
      "Iteration 22320, loss = 0.04895857\n",
      "Iteration 22321, loss = 0.04895517\n",
      "Iteration 22322, loss = 0.04895573\n",
      "Iteration 22323, loss = 0.04895049\n",
      "Iteration 22324, loss = 0.04894739\n",
      "Iteration 22325, loss = 0.04894282\n",
      "Iteration 22326, loss = 0.04894174\n",
      "Iteration 22327, loss = 0.04893849\n",
      "Iteration 22328, loss = 0.04893608\n",
      "Iteration 22329, loss = 0.04892900\n",
      "Iteration 22330, loss = 0.04893361\n",
      "Iteration 22331, loss = 0.04893287\n",
      "Iteration 22332, loss = 0.04893281\n",
      "Iteration 22333, loss = 0.04892480\n",
      "Iteration 22334, loss = 0.04892173\n",
      "Iteration 22335, loss = 0.04892018\n",
      "Iteration 22336, loss = 0.04892006\n",
      "Iteration 22337, loss = 0.04891455\n",
      "Iteration 22338, loss = 0.04891367\n",
      "Iteration 22339, loss = 0.04890989\n",
      "Iteration 22340, loss = 0.04891155\n",
      "Iteration 22341, loss = 0.04890707\n",
      "Iteration 22342, loss = 0.04890556\n",
      "Iteration 22343, loss = 0.04890251\n",
      "Iteration 22344, loss = 0.04889835\n",
      "Iteration 22345, loss = 0.04889983\n",
      "Iteration 22346, loss = 0.04889547\n",
      "Iteration 22347, loss = 0.04888775\n",
      "Iteration 22348, loss = 0.04889584\n",
      "Iteration 22349, loss = 0.04889348\n",
      "Iteration 22350, loss = 0.04888581\n",
      "Iteration 22351, loss = 0.04888565\n",
      "Iteration 22352, loss = 0.04888559\n",
      "Iteration 22353, loss = 0.04888605\n",
      "Iteration 22354, loss = 0.04888445\n",
      "Iteration 22355, loss = 0.04888017\n",
      "Iteration 22356, loss = 0.04887285\n",
      "Iteration 22357, loss = 0.04886642\n",
      "Iteration 22358, loss = 0.04886510\n",
      "Iteration 22359, loss = 0.04886360\n",
      "Iteration 22360, loss = 0.04886071\n",
      "Iteration 22361, loss = 0.04886256\n",
      "Iteration 22362, loss = 0.04885883\n",
      "Iteration 22363, loss = 0.04885415\n",
      "Iteration 22364, loss = 0.04885512\n",
      "Iteration 22365, loss = 0.04884893\n",
      "Iteration 22366, loss = 0.04884782\n",
      "Iteration 22367, loss = 0.04884856\n",
      "Iteration 22368, loss = 0.04884830\n",
      "Iteration 22369, loss = 0.04884149\n",
      "Iteration 22370, loss = 0.04884087\n",
      "Iteration 22371, loss = 0.04883629\n",
      "Iteration 22372, loss = 0.04883348\n",
      "Iteration 22373, loss = 0.04883662\n",
      "Iteration 22374, loss = 0.04882862\n",
      "Iteration 22375, loss = 0.04882452\n",
      "Iteration 22376, loss = 0.04882237\n",
      "Iteration 22377, loss = 0.04881906\n",
      "Iteration 22378, loss = 0.04881676\n",
      "Iteration 22379, loss = 0.04882113\n",
      "Iteration 22380, loss = 0.04881785\n",
      "Iteration 22381, loss = 0.04880937\n",
      "Iteration 22382, loss = 0.04880694\n",
      "Iteration 22383, loss = 0.04880752\n",
      "Iteration 22384, loss = 0.04880329\n",
      "Iteration 22385, loss = 0.04880204\n",
      "Iteration 22386, loss = 0.04880023\n",
      "Iteration 22387, loss = 0.04879781\n",
      "Iteration 22388, loss = 0.04879165\n",
      "Iteration 22389, loss = 0.04879556\n",
      "Iteration 22390, loss = 0.04879757\n",
      "Iteration 22391, loss = 0.04879410\n",
      "Iteration 22392, loss = 0.04878835\n",
      "Iteration 22393, loss = 0.04878363\n",
      "Iteration 22394, loss = 0.04878114\n",
      "Iteration 22395, loss = 0.04878431\n",
      "Iteration 22396, loss = 0.04877816\n",
      "Iteration 22397, loss = 0.04877247\n",
      "Iteration 22398, loss = 0.04877456\n",
      "Iteration 22399, loss = 0.04877478\n",
      "Iteration 22400, loss = 0.04877225\n",
      "Iteration 22401, loss = 0.04877045\n",
      "Iteration 22402, loss = 0.04877343\n",
      "Iteration 22403, loss = 0.04876667\n",
      "Iteration 22404, loss = 0.04875772\n",
      "Iteration 22405, loss = 0.04875319\n",
      "Iteration 22406, loss = 0.04875783\n",
      "Iteration 22407, loss = 0.04875543\n",
      "Iteration 22408, loss = 0.04875119\n",
      "Iteration 22409, loss = 0.04874624\n",
      "Iteration 22410, loss = 0.04874327\n",
      "Iteration 22411, loss = 0.04874262\n",
      "Iteration 22412, loss = 0.04873777\n",
      "Iteration 22413, loss = 0.04874125\n",
      "Iteration 22414, loss = 0.04873820\n",
      "Iteration 22415, loss = 0.04873195\n",
      "Iteration 22416, loss = 0.04873004\n",
      "Iteration 22417, loss = 0.04872612\n",
      "Iteration 22418, loss = 0.04872047\n",
      "Iteration 22419, loss = 0.04872261\n",
      "Iteration 22420, loss = 0.04871967\n",
      "Iteration 22421, loss = 0.04872107\n",
      "Iteration 22422, loss = 0.04871675\n",
      "Iteration 22423, loss = 0.04871274\n",
      "Iteration 22424, loss = 0.04871201\n",
      "Iteration 22425, loss = 0.04870787\n",
      "Iteration 22426, loss = 0.04870922\n",
      "Iteration 22427, loss = 0.04870604\n",
      "Iteration 22428, loss = 0.04870441\n",
      "Iteration 22429, loss = 0.04869950\n",
      "Iteration 22430, loss = 0.04869535\n",
      "Iteration 22431, loss = 0.04869721\n",
      "Iteration 22432, loss = 0.04869221\n",
      "Iteration 22433, loss = 0.04869125\n",
      "Iteration 22434, loss = 0.04868933\n",
      "Iteration 22435, loss = 0.04868762\n",
      "Iteration 22436, loss = 0.04868375\n",
      "Iteration 22437, loss = 0.04867959\n",
      "Iteration 22438, loss = 0.04868075\n",
      "Iteration 22439, loss = 0.04867495\n",
      "Iteration 22440, loss = 0.04867263\n",
      "Iteration 22441, loss = 0.04867351\n",
      "Iteration 22442, loss = 0.04866709\n",
      "Iteration 22443, loss = 0.04866505\n",
      "Iteration 22444, loss = 0.04866439\n",
      "Iteration 22445, loss = 0.04866458\n",
      "Iteration 22446, loss = 0.04866138\n",
      "Iteration 22447, loss = 0.04865752\n",
      "Iteration 22448, loss = 0.04865423\n",
      "Iteration 22449, loss = 0.04864961\n",
      "Iteration 22450, loss = 0.04865515\n",
      "Iteration 22451, loss = 0.04865179\n",
      "Iteration 22452, loss = 0.04865020\n",
      "Iteration 22453, loss = 0.04864949\n",
      "Iteration 22454, loss = 0.04864392\n",
      "Iteration 22455, loss = 0.04864215\n",
      "Iteration 22456, loss = 0.04863785\n",
      "Iteration 22457, loss = 0.04863173\n",
      "Iteration 22458, loss = 0.04863251\n",
      "Iteration 22459, loss = 0.04863439\n",
      "Iteration 22460, loss = 0.04863041\n",
      "Iteration 22461, loss = 0.04862165\n",
      "Iteration 22462, loss = 0.04861850\n",
      "Iteration 22463, loss = 0.04862317\n",
      "Iteration 22464, loss = 0.04862196\n",
      "Iteration 22465, loss = 0.04861784\n",
      "Iteration 22466, loss = 0.04861462\n",
      "Iteration 22467, loss = 0.04860637\n",
      "Iteration 22468, loss = 0.04860902\n",
      "Iteration 22469, loss = 0.04861090\n",
      "Iteration 22470, loss = 0.04860401\n",
      "Iteration 22471, loss = 0.04860338\n",
      "Iteration 22472, loss = 0.04860216\n",
      "Iteration 22473, loss = 0.04860451\n",
      "Iteration 22474, loss = 0.04860575\n",
      "Iteration 22475, loss = 0.04860016\n",
      "Iteration 22476, loss = 0.04859773\n",
      "Iteration 22477, loss = 0.04859362\n",
      "Iteration 22478, loss = 0.04858456\n",
      "Iteration 22479, loss = 0.04858459\n",
      "Iteration 22480, loss = 0.04858458\n",
      "Iteration 22481, loss = 0.04858023\n",
      "Iteration 22482, loss = 0.04857814\n",
      "Iteration 22483, loss = 0.04857761\n",
      "Iteration 22484, loss = 0.04857756\n",
      "Iteration 22485, loss = 0.04857793\n",
      "Iteration 22486, loss = 0.04857736\n",
      "Iteration 22487, loss = 0.04857074\n",
      "Iteration 22488, loss = 0.04856500\n",
      "Iteration 22489, loss = 0.04856025\n",
      "Iteration 22490, loss = 0.04855829\n",
      "Iteration 22491, loss = 0.04855689\n",
      "Iteration 22492, loss = 0.04855605\n",
      "Iteration 22493, loss = 0.04854868\n",
      "Iteration 22494, loss = 0.04854812\n",
      "Iteration 22495, loss = 0.04854804\n",
      "Iteration 22496, loss = 0.04854803\n",
      "Iteration 22497, loss = 0.04854995\n",
      "Iteration 22498, loss = 0.04854749\n",
      "Iteration 22499, loss = 0.04853953\n",
      "Iteration 22500, loss = 0.04853303\n",
      "Iteration 22501, loss = 0.04853187\n",
      "Iteration 22502, loss = 0.04853398\n",
      "Iteration 22503, loss = 0.04853178\n",
      "Iteration 22504, loss = 0.04852184\n",
      "Iteration 22505, loss = 0.04851954\n",
      "Iteration 22506, loss = 0.04852088\n",
      "Iteration 22507, loss = 0.04851474\n",
      "Iteration 22508, loss = 0.04851396\n",
      "Iteration 22509, loss = 0.04851427\n",
      "Iteration 22510, loss = 0.04850827\n",
      "Iteration 22511, loss = 0.04850997\n",
      "Iteration 22512, loss = 0.04850966\n",
      "Iteration 22513, loss = 0.04850512\n",
      "Iteration 22514, loss = 0.04849736\n",
      "Iteration 22515, loss = 0.04850403\n",
      "Iteration 22516, loss = 0.04850167\n",
      "Iteration 22517, loss = 0.04849823\n",
      "Iteration 22518, loss = 0.04849875\n",
      "Iteration 22519, loss = 0.04849340\n",
      "Iteration 22520, loss = 0.04848658\n",
      "Iteration 22521, loss = 0.04848460\n",
      "Iteration 22522, loss = 0.04848445\n",
      "Iteration 22523, loss = 0.04848024\n",
      "Iteration 22524, loss = 0.04848068\n",
      "Iteration 22525, loss = 0.04847157\n",
      "Iteration 22526, loss = 0.04847100\n",
      "Iteration 22527, loss = 0.04846773\n",
      "Iteration 22528, loss = 0.04846554\n",
      "Iteration 22529, loss = 0.04846756\n",
      "Iteration 22530, loss = 0.04846008\n",
      "Iteration 22531, loss = 0.04845988\n",
      "Iteration 22532, loss = 0.04845978\n",
      "Iteration 22533, loss = 0.04846002\n",
      "Iteration 22534, loss = 0.04845848\n",
      "Iteration 22535, loss = 0.04845375\n",
      "Iteration 22536, loss = 0.04844886\n",
      "Iteration 22537, loss = 0.04844861\n",
      "Iteration 22538, loss = 0.04844553\n",
      "Iteration 22539, loss = 0.04844400\n",
      "Iteration 22540, loss = 0.04844623\n",
      "Iteration 22541, loss = 0.04843994\n",
      "Iteration 22542, loss = 0.04844140\n",
      "Iteration 22543, loss = 0.04844073\n",
      "Iteration 22544, loss = 0.04843731\n",
      "Iteration 22545, loss = 0.04843671\n",
      "Iteration 22546, loss = 0.04842876\n",
      "Iteration 22547, loss = 0.04842677\n",
      "Iteration 22548, loss = 0.04842380\n",
      "Iteration 22549, loss = 0.04841731\n",
      "Iteration 22550, loss = 0.04841835\n",
      "Iteration 22551, loss = 0.04841639\n",
      "Iteration 22552, loss = 0.04841478\n",
      "Iteration 22553, loss = 0.04841037\n",
      "Iteration 22554, loss = 0.04840773\n",
      "Iteration 22555, loss = 0.04840528\n",
      "Iteration 22556, loss = 0.04839917\n",
      "Iteration 22557, loss = 0.04840005\n",
      "Iteration 22558, loss = 0.04839822\n",
      "Iteration 22559, loss = 0.04839856\n",
      "Iteration 22560, loss = 0.04839233\n",
      "Iteration 22561, loss = 0.04839328\n",
      "Iteration 22562, loss = 0.04839230\n",
      "Iteration 22563, loss = 0.04838856\n",
      "Iteration 22564, loss = 0.04838783\n",
      "Iteration 22565, loss = 0.04838570\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 22566, loss = 0.04838329\n",
      "Iteration 22567, loss = 0.04838086\n",
      "Iteration 22568, loss = 0.04837653\n",
      "Iteration 22569, loss = 0.04837958\n",
      "Iteration 22570, loss = 0.04837634\n",
      "Iteration 22571, loss = 0.04837035\n",
      "Iteration 22572, loss = 0.04836916\n",
      "Iteration 22573, loss = 0.04836973\n",
      "Iteration 22574, loss = 0.04836965\n",
      "Iteration 22575, loss = 0.04836382\n",
      "Iteration 22576, loss = 0.04836491\n",
      "Iteration 22577, loss = 0.04836576\n",
      "Iteration 22578, loss = 0.04836066\n",
      "Iteration 22579, loss = 0.04835114\n",
      "Iteration 22580, loss = 0.04835576\n",
      "Iteration 22581, loss = 0.04835756\n",
      "Iteration 22582, loss = 0.04835200\n",
      "Iteration 22583, loss = 0.04834153\n",
      "Iteration 22584, loss = 0.04833917\n",
      "Iteration 22585, loss = 0.04833775\n",
      "Iteration 22586, loss = 0.04833340\n",
      "Iteration 22587, loss = 0.04833337\n",
      "Iteration 22588, loss = 0.04833119\n",
      "Iteration 22589, loss = 0.04833315\n",
      "Iteration 22590, loss = 0.04833245\n",
      "Iteration 22591, loss = 0.04832614\n",
      "Iteration 22592, loss = 0.04832483\n",
      "Iteration 22593, loss = 0.04833007\n",
      "Iteration 22594, loss = 0.04832531\n",
      "Iteration 22595, loss = 0.04832035\n",
      "Iteration 22596, loss = 0.04831749\n",
      "Iteration 22597, loss = 0.04832127\n",
      "Iteration 22598, loss = 0.04831926\n",
      "Iteration 22599, loss = 0.04830919\n",
      "Iteration 22600, loss = 0.04830650\n",
      "Iteration 22601, loss = 0.04831266\n",
      "Iteration 22602, loss = 0.04831106\n",
      "Iteration 22603, loss = 0.04830105\n",
      "Iteration 22604, loss = 0.04829739\n",
      "Iteration 22605, loss = 0.04829471\n",
      "Iteration 22606, loss = 0.04829149\n",
      "Iteration 22607, loss = 0.04829006\n",
      "Iteration 22608, loss = 0.04828955\n",
      "Iteration 22609, loss = 0.04829068\n",
      "Iteration 22610, loss = 0.04828569\n",
      "Iteration 22611, loss = 0.04828203\n",
      "Iteration 22612, loss = 0.04828712\n",
      "Iteration 22613, loss = 0.04828469\n",
      "Iteration 22614, loss = 0.04828729\n",
      "Iteration 22615, loss = 0.04828654\n",
      "Iteration 22616, loss = 0.04828061\n",
      "Iteration 22617, loss = 0.04827131\n",
      "Iteration 22618, loss = 0.04827707\n",
      "Iteration 22619, loss = 0.04827149\n",
      "Iteration 22620, loss = 0.04827416\n",
      "Iteration 22621, loss = 0.04827235\n",
      "Iteration 22622, loss = 0.04826705\n",
      "Iteration 22623, loss = 0.04825832\n",
      "Iteration 22624, loss = 0.04825896\n",
      "Iteration 22625, loss = 0.04826018\n",
      "Iteration 22626, loss = 0.04825794\n",
      "Iteration 22627, loss = 0.04825322\n",
      "Iteration 22628, loss = 0.04824490\n",
      "Iteration 22629, loss = 0.04824291\n",
      "Iteration 22630, loss = 0.04824357\n",
      "Iteration 22631, loss = 0.04824042\n",
      "Iteration 22632, loss = 0.04823431\n",
      "Iteration 22633, loss = 0.04823308\n",
      "Iteration 22634, loss = 0.04823504\n",
      "Iteration 22635, loss = 0.04823673\n",
      "Iteration 22636, loss = 0.04823157\n",
      "Iteration 22637, loss = 0.04822490\n",
      "Iteration 22638, loss = 0.04822683\n",
      "Iteration 22639, loss = 0.04821813\n",
      "Iteration 22640, loss = 0.04822334\n",
      "Iteration 22641, loss = 0.04822321\n",
      "Iteration 22642, loss = 0.04821913\n",
      "Iteration 22643, loss = 0.04821264\n",
      "Iteration 22644, loss = 0.04820535\n",
      "Iteration 22645, loss = 0.04821116\n",
      "Iteration 22646, loss = 0.04820407\n",
      "Iteration 22647, loss = 0.04820213\n",
      "Iteration 22648, loss = 0.04820044\n",
      "Iteration 22649, loss = 0.04820019\n",
      "Iteration 22650, loss = 0.04819962\n",
      "Iteration 22651, loss = 0.04819651\n",
      "Iteration 22652, loss = 0.04818864\n",
      "Iteration 22653, loss = 0.04818807\n",
      "Iteration 22654, loss = 0.04818692\n",
      "Iteration 22655, loss = 0.04818182\n",
      "Iteration 22656, loss = 0.04817664\n",
      "Iteration 22657, loss = 0.04817680\n",
      "Iteration 22658, loss = 0.04817556\n",
      "Iteration 22659, loss = 0.04817709\n",
      "Iteration 22660, loss = 0.04817248\n",
      "Iteration 22661, loss = 0.04817099\n",
      "Iteration 22662, loss = 0.04817157\n",
      "Iteration 22663, loss = 0.04816416\n",
      "Iteration 22664, loss = 0.04815895\n",
      "Iteration 22665, loss = 0.04816557\n",
      "Iteration 22666, loss = 0.04816381\n",
      "Iteration 22667, loss = 0.04815526\n",
      "Iteration 22668, loss = 0.04815334\n",
      "Iteration 22669, loss = 0.04815432\n",
      "Iteration 22670, loss = 0.04814861\n",
      "Iteration 22671, loss = 0.04815290\n",
      "Iteration 22672, loss = 0.04815111\n",
      "Iteration 22673, loss = 0.04814473\n",
      "Iteration 22674, loss = 0.04814100\n",
      "Iteration 22675, loss = 0.04814078\n",
      "Iteration 22676, loss = 0.04814360\n",
      "Iteration 22677, loss = 0.04813951\n",
      "Iteration 22678, loss = 0.04813287\n",
      "Iteration 22679, loss = 0.04813044\n",
      "Iteration 22680, loss = 0.04812830\n",
      "Iteration 22681, loss = 0.04812839\n",
      "Iteration 22682, loss = 0.04812984\n",
      "Iteration 22683, loss = 0.04812339\n",
      "Iteration 22684, loss = 0.04811860\n",
      "Iteration 22685, loss = 0.04812344\n",
      "Iteration 22686, loss = 0.04812202\n",
      "Iteration 22687, loss = 0.04811906\n",
      "Iteration 22688, loss = 0.04811865\n",
      "Iteration 22689, loss = 0.04811603\n",
      "Iteration 22690, loss = 0.04811217\n",
      "Iteration 22691, loss = 0.04810701\n",
      "Iteration 22692, loss = 0.04810151\n",
      "Iteration 22693, loss = 0.04810166\n",
      "Iteration 22694, loss = 0.04810061\n",
      "Iteration 22695, loss = 0.04809809\n",
      "Iteration 22696, loss = 0.04809013\n",
      "Iteration 22697, loss = 0.04808623\n",
      "Iteration 22698, loss = 0.04809015\n",
      "Iteration 22699, loss = 0.04808889\n",
      "Iteration 22700, loss = 0.04808391\n",
      "Iteration 22701, loss = 0.04808135\n",
      "Iteration 22702, loss = 0.04808022\n",
      "Iteration 22703, loss = 0.04808174\n",
      "Iteration 22704, loss = 0.04807731\n",
      "Iteration 22705, loss = 0.04807674\n",
      "Iteration 22706, loss = 0.04807234\n",
      "Iteration 22707, loss = 0.04806763\n",
      "Iteration 22708, loss = 0.04806752\n",
      "Iteration 22709, loss = 0.04806426\n",
      "Iteration 22710, loss = 0.04806237\n",
      "Iteration 22711, loss = 0.04806136\n",
      "Iteration 22712, loss = 0.04805724\n",
      "Iteration 22713, loss = 0.04805695\n",
      "Iteration 22714, loss = 0.04805685\n",
      "Iteration 22715, loss = 0.04805353\n",
      "Iteration 22716, loss = 0.04804945\n",
      "Iteration 22717, loss = 0.04804636\n",
      "Iteration 22718, loss = 0.04804285\n",
      "Iteration 22719, loss = 0.04804391\n",
      "Iteration 22720, loss = 0.04804034\n",
      "Iteration 22721, loss = 0.04803875\n",
      "Iteration 22722, loss = 0.04803540\n",
      "Iteration 22723, loss = 0.04803478\n",
      "Iteration 22724, loss = 0.04803699\n",
      "Iteration 22725, loss = 0.04803552\n",
      "Iteration 22726, loss = 0.04803415\n",
      "Iteration 22727, loss = 0.04803207\n",
      "Iteration 22728, loss = 0.04801986\n",
      "Iteration 22729, loss = 0.04802372\n",
      "Iteration 22730, loss = 0.04802440\n",
      "Iteration 22731, loss = 0.04802204\n",
      "Iteration 22732, loss = 0.04801719\n",
      "Iteration 22733, loss = 0.04801458\n",
      "Iteration 22734, loss = 0.04801491\n",
      "Iteration 22735, loss = 0.04800881\n",
      "Iteration 22736, loss = 0.04800756\n",
      "Iteration 22737, loss = 0.04800727\n",
      "Iteration 22738, loss = 0.04800234\n",
      "Iteration 22739, loss = 0.04800164\n",
      "Iteration 22740, loss = 0.04799859\n",
      "Iteration 22741, loss = 0.04799304\n",
      "Iteration 22742, loss = 0.04799361\n",
      "Iteration 22743, loss = 0.04799201\n",
      "Iteration 22744, loss = 0.04798568\n",
      "Iteration 22745, loss = 0.04798785\n",
      "Iteration 22746, loss = 0.04798699\n",
      "Iteration 22747, loss = 0.04798626\n",
      "Iteration 22748, loss = 0.04798799\n",
      "Iteration 22749, loss = 0.04798587\n",
      "Iteration 22750, loss = 0.04798366\n",
      "Iteration 22751, loss = 0.04797942\n",
      "Iteration 22752, loss = 0.04797628\n",
      "Iteration 22753, loss = 0.04797472\n",
      "Iteration 22754, loss = 0.04797554\n",
      "Iteration 22755, loss = 0.04797431\n",
      "Iteration 22756, loss = 0.04796551\n",
      "Iteration 22757, loss = 0.04796296\n",
      "Iteration 22758, loss = 0.04795800\n",
      "Iteration 22759, loss = 0.04796290\n",
      "Iteration 22760, loss = 0.04795642\n",
      "Iteration 22761, loss = 0.04794803\n",
      "Iteration 22762, loss = 0.04795458\n",
      "Iteration 22763, loss = 0.04795694\n",
      "Iteration 22764, loss = 0.04794932\n",
      "Iteration 22765, loss = 0.04795040\n",
      "Iteration 22766, loss = 0.04795083\n",
      "Iteration 22767, loss = 0.04794488\n",
      "Iteration 22768, loss = 0.04794128\n",
      "Iteration 22769, loss = 0.04794169\n",
      "Iteration 22770, loss = 0.04793812\n",
      "Iteration 22771, loss = 0.04793705\n",
      "Iteration 22772, loss = 0.04793036\n",
      "Iteration 22773, loss = 0.04792863\n",
      "Iteration 22774, loss = 0.04792715\n",
      "Iteration 22775, loss = 0.04792344\n",
      "Iteration 22776, loss = 0.04792986\n",
      "Iteration 22777, loss = 0.04792689\n",
      "Iteration 22778, loss = 0.04791448\n",
      "Iteration 22779, loss = 0.04791540\n",
      "Iteration 22780, loss = 0.04791999\n",
      "Iteration 22781, loss = 0.04792172\n",
      "Iteration 22782, loss = 0.04791539\n",
      "Iteration 22783, loss = 0.04790950\n",
      "Iteration 22784, loss = 0.04790744\n",
      "Iteration 22785, loss = 0.04790231\n",
      "Iteration 22786, loss = 0.04790224\n",
      "Iteration 22787, loss = 0.04790041\n",
      "Iteration 22788, loss = 0.04789692\n",
      "Iteration 22789, loss = 0.04789278\n",
      "Iteration 22790, loss = 0.04789038\n",
      "Iteration 22791, loss = 0.04789537\n",
      "Iteration 22792, loss = 0.04789701\n",
      "Iteration 22793, loss = 0.04789281\n",
      "Iteration 22794, loss = 0.04788540\n",
      "Iteration 22795, loss = 0.04788581\n",
      "Iteration 22796, loss = 0.04788061\n",
      "Iteration 22797, loss = 0.04788145\n",
      "Iteration 22798, loss = 0.04788454\n",
      "Iteration 22799, loss = 0.04788542\n",
      "Iteration 22800, loss = 0.04788217\n",
      "Iteration 22801, loss = 0.04787490\n",
      "Iteration 22802, loss = 0.04787509\n",
      "Iteration 22803, loss = 0.04787548\n",
      "Iteration 22804, loss = 0.04786897\n",
      "Iteration 22805, loss = 0.04786263\n",
      "Iteration 22806, loss = 0.04786422\n",
      "Iteration 22807, loss = 0.04786498\n",
      "Iteration 22808, loss = 0.04785645\n",
      "Iteration 22809, loss = 0.04785379\n",
      "Iteration 22810, loss = 0.04785385\n",
      "Iteration 22811, loss = 0.04784975\n",
      "Iteration 22812, loss = 0.04784756\n",
      "Iteration 22813, loss = 0.04784838\n",
      "Iteration 22814, loss = 0.04784471\n",
      "Iteration 22815, loss = 0.04784364\n",
      "Iteration 22816, loss = 0.04783892\n",
      "Iteration 22817, loss = 0.04783736\n",
      "Iteration 22818, loss = 0.04783770\n",
      "Iteration 22819, loss = 0.04783722\n",
      "Iteration 22820, loss = 0.04783244\n",
      "Iteration 22821, loss = 0.04782904\n",
      "Iteration 22822, loss = 0.04782811\n",
      "Iteration 22823, loss = 0.04782846\n",
      "Iteration 22824, loss = 0.04782297\n",
      "Iteration 22825, loss = 0.04781632\n",
      "Iteration 22826, loss = 0.04782256\n",
      "Iteration 22827, loss = 0.04782584\n",
      "Iteration 22828, loss = 0.04782070\n",
      "Iteration 22829, loss = 0.04780888\n",
      "Iteration 22830, loss = 0.04781197\n",
      "Iteration 22831, loss = 0.04781317\n",
      "Iteration 22832, loss = 0.04781388\n",
      "Iteration 22833, loss = 0.04781107\n",
      "Iteration 22834, loss = 0.04780867\n",
      "Iteration 22835, loss = 0.04780947\n",
      "Iteration 22836, loss = 0.04780459\n",
      "Iteration 22837, loss = 0.04779704\n",
      "Iteration 22838, loss = 0.04779138\n",
      "Iteration 22839, loss = 0.04778994\n",
      "Iteration 22840, loss = 0.04779019\n",
      "Iteration 22841, loss = 0.04778841\n",
      "Iteration 22842, loss = 0.04778426\n",
      "Iteration 22843, loss = 0.04778231\n",
      "Iteration 22844, loss = 0.04778155\n",
      "Iteration 22845, loss = 0.04777922\n",
      "Iteration 22846, loss = 0.04777507\n",
      "Iteration 22847, loss = 0.04777160\n",
      "Iteration 22848, loss = 0.04777171\n",
      "Iteration 22849, loss = 0.04776577\n",
      "Iteration 22850, loss = 0.04776342\n",
      "Iteration 22851, loss = 0.04776117\n",
      "Iteration 22852, loss = 0.04775809\n",
      "Iteration 22853, loss = 0.04775765\n",
      "Iteration 22854, loss = 0.04775926\n",
      "Iteration 22855, loss = 0.04775618\n",
      "Iteration 22856, loss = 0.04775411\n",
      "Iteration 22857, loss = 0.04775452\n",
      "Iteration 22858, loss = 0.04775219\n",
      "Iteration 22859, loss = 0.04774728\n",
      "Iteration 22860, loss = 0.04774995\n",
      "Iteration 22861, loss = 0.04774799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 22862, loss = 0.04774748\n",
      "Iteration 22863, loss = 0.04774337\n",
      "Iteration 22864, loss = 0.04773885\n",
      "Iteration 22865, loss = 0.04773450\n",
      "Iteration 22866, loss = 0.04773727\n",
      "Iteration 22867, loss = 0.04773360\n",
      "Iteration 22868, loss = 0.04772980\n",
      "Iteration 22869, loss = 0.04772755\n",
      "Iteration 22870, loss = 0.04772383\n",
      "Iteration 22871, loss = 0.04772571\n",
      "Iteration 22872, loss = 0.04772002\n",
      "Iteration 22873, loss = 0.04771984\n",
      "Iteration 22874, loss = 0.04771807\n",
      "Iteration 22875, loss = 0.04771413\n",
      "Iteration 22876, loss = 0.04771474\n",
      "Iteration 22877, loss = 0.04771588\n",
      "Iteration 22878, loss = 0.04771278\n",
      "Iteration 22879, loss = 0.04771160\n",
      "Iteration 22880, loss = 0.04770886\n",
      "Iteration 22881, loss = 0.04770694\n",
      "Iteration 22882, loss = 0.04770241\n",
      "Iteration 22883, loss = 0.04769941\n",
      "Iteration 22884, loss = 0.04769864\n",
      "Iteration 22885, loss = 0.04769873\n",
      "Iteration 22886, loss = 0.04769001\n",
      "Iteration 22887, loss = 0.04768564\n",
      "Iteration 22888, loss = 0.04768636\n",
      "Iteration 22889, loss = 0.04768742\n",
      "Iteration 22890, loss = 0.04768033\n",
      "Iteration 22891, loss = 0.04767724\n",
      "Iteration 22892, loss = 0.04768244\n",
      "Iteration 22893, loss = 0.04768121\n",
      "Iteration 22894, loss = 0.04767568\n",
      "Iteration 22895, loss = 0.04767672\n",
      "Iteration 22896, loss = 0.04767049\n",
      "Iteration 22897, loss = 0.04766659\n",
      "Iteration 22898, loss = 0.04766428\n",
      "Iteration 22899, loss = 0.04766686\n",
      "Iteration 22900, loss = 0.04766124\n",
      "Iteration 22901, loss = 0.04765642\n",
      "Iteration 22902, loss = 0.04766076\n",
      "Iteration 22903, loss = 0.04766376\n",
      "Iteration 22904, loss = 0.04765814\n",
      "Iteration 22905, loss = 0.04765502\n",
      "Iteration 22906, loss = 0.04765141\n",
      "Iteration 22907, loss = 0.04764741\n",
      "Iteration 22908, loss = 0.04764885\n",
      "Iteration 22909, loss = 0.04764451\n",
      "Iteration 22910, loss = 0.04764218\n",
      "Iteration 22911, loss = 0.04763919\n",
      "Iteration 22912, loss = 0.04764043\n",
      "Iteration 22913, loss = 0.04763956\n",
      "Iteration 22914, loss = 0.04764056\n",
      "Iteration 22915, loss = 0.04763779\n",
      "Iteration 22916, loss = 0.04763685\n",
      "Iteration 22917, loss = 0.04763423\n",
      "Iteration 22918, loss = 0.04763137\n",
      "Iteration 22919, loss = 0.04762247\n",
      "Iteration 22920, loss = 0.04762091\n",
      "Iteration 22921, loss = 0.04762093\n",
      "Iteration 22922, loss = 0.04761807\n",
      "Iteration 22923, loss = 0.04761220\n",
      "Iteration 22924, loss = 0.04761255\n",
      "Iteration 22925, loss = 0.04761344\n",
      "Iteration 22926, loss = 0.04760839\n",
      "Iteration 22927, loss = 0.04760668\n",
      "Iteration 22928, loss = 0.04760790\n",
      "Iteration 22929, loss = 0.04760406\n",
      "Iteration 22930, loss = 0.04760616\n",
      "Iteration 22931, loss = 0.04760568\n",
      "Iteration 22932, loss = 0.04760303\n",
      "Iteration 22933, loss = 0.04759523\n",
      "Iteration 22934, loss = 0.04759558\n",
      "Iteration 22935, loss = 0.04759786\n",
      "Iteration 22936, loss = 0.04758871\n",
      "Iteration 22937, loss = 0.04758694\n",
      "Iteration 22938, loss = 0.04758535\n",
      "Iteration 22939, loss = 0.04758937\n",
      "Iteration 22940, loss = 0.04758728\n",
      "Iteration 22941, loss = 0.04758366\n",
      "Iteration 22942, loss = 0.04758061\n",
      "Iteration 22943, loss = 0.04757780\n",
      "Iteration 22944, loss = 0.04758017\n",
      "Iteration 22945, loss = 0.04757225\n",
      "Iteration 22946, loss = 0.04756828\n",
      "Iteration 22947, loss = 0.04756132\n",
      "Iteration 22948, loss = 0.04756516\n",
      "Iteration 22949, loss = 0.04756506\n",
      "Iteration 22950, loss = 0.04756061\n",
      "Iteration 22951, loss = 0.04755814\n",
      "Iteration 22952, loss = 0.04756008\n",
      "Iteration 22953, loss = 0.04755186\n",
      "Iteration 22954, loss = 0.04755160\n",
      "Iteration 22955, loss = 0.04755296\n",
      "Iteration 22956, loss = 0.04754656\n",
      "Iteration 22957, loss = 0.04754874\n",
      "Iteration 22958, loss = 0.04754777\n",
      "Iteration 22959, loss = 0.04754519\n",
      "Iteration 22960, loss = 0.04754266\n",
      "Iteration 22961, loss = 0.04754299\n",
      "Iteration 22962, loss = 0.04754001\n",
      "Iteration 22963, loss = 0.04753021\n",
      "Iteration 22964, loss = 0.04752843\n",
      "Iteration 22965, loss = 0.04753215\n",
      "Iteration 22966, loss = 0.04752579\n",
      "Iteration 22967, loss = 0.04752336\n",
      "Iteration 22968, loss = 0.04752169\n",
      "Iteration 22969, loss = 0.04752278\n",
      "Iteration 22970, loss = 0.04751873\n",
      "Iteration 22971, loss = 0.04751744\n",
      "Iteration 22972, loss = 0.04751828\n",
      "Iteration 22973, loss = 0.04751008\n",
      "Iteration 22974, loss = 0.04750902\n",
      "Iteration 22975, loss = 0.04751198\n",
      "Iteration 22976, loss = 0.04750776\n",
      "Iteration 22977, loss = 0.04750302\n",
      "Iteration 22978, loss = 0.04750322\n",
      "Iteration 22979, loss = 0.04749961\n",
      "Iteration 22980, loss = 0.04750234\n",
      "Iteration 22981, loss = 0.04749928\n",
      "Iteration 22982, loss = 0.04750012\n",
      "Iteration 22983, loss = 0.04749332\n",
      "Iteration 22984, loss = 0.04748818\n",
      "Iteration 22985, loss = 0.04748840\n",
      "Iteration 22986, loss = 0.04749038\n",
      "Iteration 22987, loss = 0.04748832\n",
      "Iteration 22988, loss = 0.04748674\n",
      "Iteration 22989, loss = 0.04748311\n",
      "Iteration 22990, loss = 0.04748238\n",
      "Iteration 22991, loss = 0.04748032\n",
      "Iteration 22992, loss = 0.04747351\n",
      "Iteration 22993, loss = 0.04747353\n",
      "Iteration 22994, loss = 0.04747093\n",
      "Iteration 22995, loss = 0.04746592\n",
      "Iteration 22996, loss = 0.04746906\n",
      "Iteration 22997, loss = 0.04746545\n",
      "Iteration 22998, loss = 0.04746499\n",
      "Iteration 22999, loss = 0.04746481\n",
      "Iteration 23000, loss = 0.04746177\n",
      "Iteration 23001, loss = 0.04745893\n",
      "Iteration 23002, loss = 0.04745426\n",
      "Iteration 23003, loss = 0.04745015\n",
      "Iteration 23004, loss = 0.04745202\n",
      "Iteration 23005, loss = 0.04745422\n",
      "Iteration 23006, loss = 0.04745040\n",
      "Iteration 23007, loss = 0.04744402\n",
      "Iteration 23008, loss = 0.04744089\n",
      "Iteration 23009, loss = 0.04743881\n",
      "Iteration 23010, loss = 0.04744169\n",
      "Iteration 23011, loss = 0.04743916\n",
      "Iteration 23012, loss = 0.04743051\n",
      "Iteration 23013, loss = 0.04743098\n",
      "Iteration 23014, loss = 0.04743262\n",
      "Iteration 23015, loss = 0.04743025\n",
      "Iteration 23016, loss = 0.04742901\n",
      "Iteration 23017, loss = 0.04742305\n",
      "Iteration 23018, loss = 0.04742420\n",
      "Iteration 23019, loss = 0.04742443\n",
      "Iteration 23020, loss = 0.04742270\n",
      "Iteration 23021, loss = 0.04741871\n",
      "Iteration 23022, loss = 0.04741017\n",
      "Iteration 23023, loss = 0.04741199\n",
      "Iteration 23024, loss = 0.04741231\n",
      "Iteration 23025, loss = 0.04740996\n",
      "Iteration 23026, loss = 0.04740548\n",
      "Iteration 23027, loss = 0.04740300\n",
      "Iteration 23028, loss = 0.04739677\n",
      "Iteration 23029, loss = 0.04739663\n",
      "Iteration 23030, loss = 0.04739504\n",
      "Iteration 23031, loss = 0.04739510\n",
      "Iteration 23032, loss = 0.04738977\n",
      "Iteration 23033, loss = 0.04738838\n",
      "Iteration 23034, loss = 0.04738856\n",
      "Iteration 23035, loss = 0.04738895\n",
      "Iteration 23036, loss = 0.04738804\n",
      "Iteration 23037, loss = 0.04738441\n",
      "Iteration 23038, loss = 0.04738139\n",
      "Iteration 23039, loss = 0.04737714\n",
      "Iteration 23040, loss = 0.04738073\n",
      "Iteration 23041, loss = 0.04738022\n",
      "Iteration 23042, loss = 0.04737137\n",
      "Iteration 23043, loss = 0.04736890\n",
      "Iteration 23044, loss = 0.04736745\n",
      "Iteration 23045, loss = 0.04736757\n",
      "Iteration 23046, loss = 0.04736831\n",
      "Iteration 23047, loss = 0.04736751\n",
      "Iteration 23048, loss = 0.04736421\n",
      "Iteration 23049, loss = 0.04736014\n",
      "Iteration 23050, loss = 0.04736072\n",
      "Iteration 23051, loss = 0.04735816\n",
      "Iteration 23052, loss = 0.04735397\n",
      "Iteration 23053, loss = 0.04735068\n",
      "Iteration 23054, loss = 0.04735094\n",
      "Iteration 23055, loss = 0.04735300\n",
      "Iteration 23056, loss = 0.04734908\n",
      "Iteration 23057, loss = 0.04734422\n",
      "Iteration 23058, loss = 0.04734264\n",
      "Iteration 23059, loss = 0.04734438\n",
      "Iteration 23060, loss = 0.04733672\n",
      "Iteration 23061, loss = 0.04733676\n",
      "Iteration 23062, loss = 0.04733925\n",
      "Iteration 23063, loss = 0.04733404\n",
      "Iteration 23064, loss = 0.04733062\n",
      "Iteration 23065, loss = 0.04732942\n",
      "Iteration 23066, loss = 0.04733367\n",
      "Iteration 23067, loss = 0.04732832\n",
      "Iteration 23068, loss = 0.04731809\n",
      "Iteration 23069, loss = 0.04731730\n",
      "Iteration 23070, loss = 0.04731757\n",
      "Iteration 23071, loss = 0.04731126\n",
      "Iteration 23072, loss = 0.04731256\n",
      "Iteration 23073, loss = 0.04731452\n",
      "Iteration 23074, loss = 0.04730559\n",
      "Iteration 23075, loss = 0.04730725\n",
      "Iteration 23076, loss = 0.04730353\n",
      "Iteration 23077, loss = 0.04730008\n",
      "Iteration 23078, loss = 0.04729815\n",
      "Iteration 23079, loss = 0.04729683\n",
      "Iteration 23080, loss = 0.04729526\n",
      "Iteration 23081, loss = 0.04729249\n",
      "Iteration 23082, loss = 0.04728766\n",
      "Iteration 23083, loss = 0.04728745\n",
      "Iteration 23084, loss = 0.04728867\n",
      "Iteration 23085, loss = 0.04728543\n",
      "Iteration 23086, loss = 0.04728136\n",
      "Iteration 23087, loss = 0.04728587\n",
      "Iteration 23088, loss = 0.04728437\n",
      "Iteration 23089, loss = 0.04728041\n",
      "Iteration 23090, loss = 0.04727542\n",
      "Iteration 23091, loss = 0.04727489\n",
      "Iteration 23092, loss = 0.04727555\n",
      "Iteration 23093, loss = 0.04727190\n",
      "Iteration 23094, loss = 0.04726929\n",
      "Iteration 23095, loss = 0.04726928\n",
      "Iteration 23096, loss = 0.04726307\n",
      "Iteration 23097, loss = 0.04726478\n",
      "Iteration 23098, loss = 0.04726129\n",
      "Iteration 23099, loss = 0.04725656\n",
      "Iteration 23100, loss = 0.04725493\n",
      "Iteration 23101, loss = 0.04725384\n",
      "Iteration 23102, loss = 0.04725186\n",
      "Iteration 23103, loss = 0.04724865\n",
      "Iteration 23104, loss = 0.04724916\n",
      "Iteration 23105, loss = 0.04724518\n",
      "Iteration 23106, loss = 0.04724456\n",
      "Iteration 23107, loss = 0.04724544\n",
      "Iteration 23108, loss = 0.04724116\n",
      "Iteration 23109, loss = 0.04724152\n",
      "Iteration 23110, loss = 0.04723619\n",
      "Iteration 23111, loss = 0.04723130\n",
      "Iteration 23112, loss = 0.04723573\n",
      "Iteration 23113, loss = 0.04723169\n",
      "Iteration 23114, loss = 0.04722786\n",
      "Iteration 23115, loss = 0.04722494\n",
      "Iteration 23116, loss = 0.04722674\n",
      "Iteration 23117, loss = 0.04722902\n",
      "Iteration 23118, loss = 0.04722845\n",
      "Iteration 23119, loss = 0.04722359\n",
      "Iteration 23120, loss = 0.04722090\n",
      "Iteration 23121, loss = 0.04721194\n",
      "Iteration 23122, loss = 0.04721382\n",
      "Iteration 23123, loss = 0.04721716\n",
      "Iteration 23124, loss = 0.04721364\n",
      "Iteration 23125, loss = 0.04721055\n",
      "Iteration 23126, loss = 0.04721024\n",
      "Iteration 23127, loss = 0.04720723\n",
      "Iteration 23128, loss = 0.04720680\n",
      "Iteration 23129, loss = 0.04720323\n",
      "Iteration 23130, loss = 0.04719836\n",
      "Iteration 23131, loss = 0.04719600\n",
      "Iteration 23132, loss = 0.04719736\n",
      "Iteration 23133, loss = 0.04719605\n",
      "Iteration 23134, loss = 0.04719556\n",
      "Iteration 23135, loss = 0.04718710\n",
      "Iteration 23136, loss = 0.04718342\n",
      "Iteration 23137, loss = 0.04718517\n",
      "Iteration 23138, loss = 0.04718419\n",
      "Iteration 23139, loss = 0.04718369\n",
      "Iteration 23140, loss = 0.04717731\n",
      "Iteration 23141, loss = 0.04717918\n",
      "Iteration 23142, loss = 0.04717814\n",
      "Iteration 23143, loss = 0.04717226\n",
      "Iteration 23144, loss = 0.04717229\n",
      "Iteration 23145, loss = 0.04717001\n",
      "Iteration 23146, loss = 0.04716289\n",
      "Iteration 23147, loss = 0.04716776\n",
      "Iteration 23148, loss = 0.04716625\n",
      "Iteration 23149, loss = 0.04715778\n",
      "Iteration 23150, loss = 0.04715798\n",
      "Iteration 23151, loss = 0.04715512\n",
      "Iteration 23152, loss = 0.04715690\n",
      "Iteration 23153, loss = 0.04715474\n",
      "Iteration 23154, loss = 0.04714898\n",
      "Iteration 23155, loss = 0.04714688\n",
      "Iteration 23156, loss = 0.04714868\n",
      "Iteration 23157, loss = 0.04714759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 23158, loss = 0.04714284\n",
      "Iteration 23159, loss = 0.04714111\n",
      "Iteration 23160, loss = 0.04714323\n",
      "Iteration 23161, loss = 0.04713640\n",
      "Iteration 23162, loss = 0.04713213\n",
      "Iteration 23163, loss = 0.04714132\n",
      "Iteration 23164, loss = 0.04713854\n",
      "Iteration 23165, loss = 0.04712695\n",
      "Iteration 23166, loss = 0.04712709\n",
      "Iteration 23167, loss = 0.04713374\n",
      "Iteration 23168, loss = 0.04713125\n",
      "Iteration 23169, loss = 0.04712845\n",
      "Iteration 23170, loss = 0.04712277\n",
      "Iteration 23171, loss = 0.04711959\n",
      "Iteration 23172, loss = 0.04711367\n",
      "Iteration 23173, loss = 0.04711587\n",
      "Iteration 23174, loss = 0.04711726\n",
      "Iteration 23175, loss = 0.04711391\n",
      "Iteration 23176, loss = 0.04710467\n",
      "Iteration 23177, loss = 0.04710986\n",
      "Iteration 23178, loss = 0.04711060\n",
      "Iteration 23179, loss = 0.04710478\n",
      "Iteration 23180, loss = 0.04710533\n",
      "Iteration 23181, loss = 0.04710107\n",
      "Iteration 23182, loss = 0.04710306\n",
      "Iteration 23183, loss = 0.04710654\n",
      "Iteration 23184, loss = 0.04710690\n",
      "Iteration 23185, loss = 0.04709629\n",
      "Iteration 23186, loss = 0.04708956\n",
      "Iteration 23187, loss = 0.04708770\n",
      "Iteration 23188, loss = 0.04709365\n",
      "Iteration 23189, loss = 0.04709290\n",
      "Iteration 23190, loss = 0.04708613\n",
      "Iteration 23191, loss = 0.04707616\n",
      "Iteration 23192, loss = 0.04707724\n",
      "Iteration 23193, loss = 0.04707669\n",
      "Iteration 23194, loss = 0.04707418\n",
      "Iteration 23195, loss = 0.04707019\n",
      "Iteration 23196, loss = 0.04706942\n",
      "Iteration 23197, loss = 0.04706309\n",
      "Iteration 23198, loss = 0.04706535\n",
      "Iteration 23199, loss = 0.04706532\n",
      "Iteration 23200, loss = 0.04706522\n",
      "Iteration 23201, loss = 0.04706208\n",
      "Iteration 23202, loss = 0.04705539\n",
      "Iteration 23203, loss = 0.04705470\n",
      "Iteration 23204, loss = 0.04705242\n",
      "Iteration 23205, loss = 0.04705225\n",
      "Iteration 23206, loss = 0.04705339\n",
      "Iteration 23207, loss = 0.04704939\n",
      "Iteration 23208, loss = 0.04704137\n",
      "Iteration 23209, loss = 0.04704317\n",
      "Iteration 23210, loss = 0.04704143\n",
      "Iteration 23211, loss = 0.04703586\n",
      "Iteration 23212, loss = 0.04703895\n",
      "Iteration 23213, loss = 0.04703855\n",
      "Iteration 23214, loss = 0.04703590\n",
      "Iteration 23215, loss = 0.04703417\n",
      "Iteration 23216, loss = 0.04702963\n",
      "Iteration 23217, loss = 0.04702429\n",
      "Iteration 23218, loss = 0.04702292\n",
      "Iteration 23219, loss = 0.04702516\n",
      "Iteration 23220, loss = 0.04702929\n",
      "Iteration 23221, loss = 0.04701956\n",
      "Iteration 23222, loss = 0.04701774\n",
      "Iteration 23223, loss = 0.04701472\n",
      "Iteration 23224, loss = 0.04701400\n",
      "Iteration 23225, loss = 0.04701512\n",
      "Iteration 23226, loss = 0.04701219\n",
      "Iteration 23227, loss = 0.04701116\n",
      "Iteration 23228, loss = 0.04700870\n",
      "Iteration 23229, loss = 0.04701090\n",
      "Iteration 23230, loss = 0.04700587\n",
      "Iteration 23231, loss = 0.04700193\n",
      "Iteration 23232, loss = 0.04700230\n",
      "Iteration 23233, loss = 0.04700243\n",
      "Iteration 23234, loss = 0.04700298\n",
      "Iteration 23235, loss = 0.04699803\n",
      "Iteration 23236, loss = 0.04699205\n",
      "Iteration 23237, loss = 0.04699128\n",
      "Iteration 23238, loss = 0.04699017\n",
      "Iteration 23239, loss = 0.04698984\n",
      "Iteration 23240, loss = 0.04698937\n",
      "Iteration 23241, loss = 0.04699038\n",
      "Iteration 23242, loss = 0.04698294\n",
      "Iteration 23243, loss = 0.04698409\n",
      "Iteration 23244, loss = 0.04698437\n",
      "Iteration 23245, loss = 0.04698315\n",
      "Iteration 23246, loss = 0.04698402\n",
      "Iteration 23247, loss = 0.04697869\n",
      "Iteration 23248, loss = 0.04696965\n",
      "Iteration 23249, loss = 0.04696828\n",
      "Iteration 23250, loss = 0.04697230\n",
      "Iteration 23251, loss = 0.04696455\n",
      "Iteration 23252, loss = 0.04696221\n",
      "Iteration 23253, loss = 0.04696263\n",
      "Iteration 23254, loss = 0.04696704\n",
      "Iteration 23255, loss = 0.04696421\n",
      "Iteration 23256, loss = 0.04696346\n",
      "Iteration 23257, loss = 0.04695750\n",
      "Iteration 23258, loss = 0.04694903\n",
      "Iteration 23259, loss = 0.04694428\n",
      "Iteration 23260, loss = 0.04694647\n",
      "Iteration 23261, loss = 0.04694512\n",
      "Iteration 23262, loss = 0.04693991\n",
      "Iteration 23263, loss = 0.04693743\n",
      "Iteration 23264, loss = 0.04693459\n",
      "Iteration 23265, loss = 0.04693467\n",
      "Iteration 23266, loss = 0.04693114\n",
      "Iteration 23267, loss = 0.04693234\n",
      "Iteration 23268, loss = 0.04693138\n",
      "Iteration 23269, loss = 0.04693149\n",
      "Iteration 23270, loss = 0.04692941\n",
      "Iteration 23271, loss = 0.04692680\n",
      "Iteration 23272, loss = 0.04692065\n",
      "Iteration 23273, loss = 0.04692257\n",
      "Iteration 23274, loss = 0.04692369\n",
      "Iteration 23275, loss = 0.04692001\n",
      "Iteration 23276, loss = 0.04691588\n",
      "Iteration 23277, loss = 0.04691478\n",
      "Iteration 23278, loss = 0.04691198\n",
      "Iteration 23279, loss = 0.04691472\n",
      "Iteration 23280, loss = 0.04691193\n",
      "Iteration 23281, loss = 0.04690795\n",
      "Iteration 23282, loss = 0.04690786\n",
      "Iteration 23283, loss = 0.04690306\n",
      "Iteration 23284, loss = 0.04690809\n",
      "Iteration 23285, loss = 0.04690365\n",
      "Iteration 23286, loss = 0.04689472\n",
      "Iteration 23287, loss = 0.04689122\n",
      "Iteration 23288, loss = 0.04689008\n",
      "Iteration 23289, loss = 0.04689108\n",
      "Iteration 23290, loss = 0.04688940\n",
      "Iteration 23291, loss = 0.04688817\n",
      "Iteration 23292, loss = 0.04688555\n",
      "Iteration 23293, loss = 0.04688210\n",
      "Iteration 23294, loss = 0.04688177\n",
      "Iteration 23295, loss = 0.04687932\n",
      "Iteration 23296, loss = 0.04687604\n",
      "Iteration 23297, loss = 0.04687647\n",
      "Iteration 23298, loss = 0.04687327\n",
      "Iteration 23299, loss = 0.04686685\n",
      "Iteration 23300, loss = 0.04687103\n",
      "Iteration 23301, loss = 0.04686961\n",
      "Iteration 23302, loss = 0.04686523\n",
      "Iteration 23303, loss = 0.04686463\n",
      "Iteration 23304, loss = 0.04686172\n",
      "Iteration 23305, loss = 0.04686284\n",
      "Iteration 23306, loss = 0.04686157\n",
      "Iteration 23307, loss = 0.04685645\n",
      "Iteration 23308, loss = 0.04685218\n",
      "Iteration 23309, loss = 0.04685394\n",
      "Iteration 23310, loss = 0.04685557\n",
      "Iteration 23311, loss = 0.04684778\n",
      "Iteration 23312, loss = 0.04684507\n",
      "Iteration 23313, loss = 0.04685070\n",
      "Iteration 23314, loss = 0.04685100\n",
      "Iteration 23315, loss = 0.04684550\n",
      "Iteration 23316, loss = 0.04684111\n",
      "Iteration 23317, loss = 0.04684019\n",
      "Iteration 23318, loss = 0.04683820\n",
      "Iteration 23319, loss = 0.04683523\n",
      "Iteration 23320, loss = 0.04683457\n",
      "Iteration 23321, loss = 0.04682962\n",
      "Iteration 23322, loss = 0.04682915\n",
      "Iteration 23323, loss = 0.04683004\n",
      "Iteration 23324, loss = 0.04682593\n",
      "Iteration 23325, loss = 0.04682213\n",
      "Iteration 23326, loss = 0.04681872\n",
      "Iteration 23327, loss = 0.04681490\n",
      "Iteration 23328, loss = 0.04682003\n",
      "Iteration 23329, loss = 0.04682436\n",
      "Iteration 23330, loss = 0.04681329\n",
      "Iteration 23331, loss = 0.04681076\n",
      "Iteration 23332, loss = 0.04680841\n",
      "Iteration 23333, loss = 0.04680768\n",
      "Iteration 23334, loss = 0.04680420\n",
      "Iteration 23335, loss = 0.04680335\n",
      "Iteration 23336, loss = 0.04679917\n",
      "Iteration 23337, loss = 0.04679467\n",
      "Iteration 23338, loss = 0.04679600\n",
      "Iteration 23339, loss = 0.04679827\n",
      "Iteration 23340, loss = 0.04679507\n",
      "Iteration 23341, loss = 0.04678829\n",
      "Iteration 23342, loss = 0.04679233\n",
      "Iteration 23343, loss = 0.04678866\n",
      "Iteration 23344, loss = 0.04678760\n",
      "Iteration 23345, loss = 0.04678557\n",
      "Iteration 23346, loss = 0.04678284\n",
      "Iteration 23347, loss = 0.04678248\n",
      "Iteration 23348, loss = 0.04678168\n",
      "Iteration 23349, loss = 0.04677558\n",
      "Iteration 23350, loss = 0.04677725\n",
      "Iteration 23351, loss = 0.04677499\n",
      "Iteration 23352, loss = 0.04677209\n",
      "Iteration 23353, loss = 0.04677308\n",
      "Iteration 23354, loss = 0.04677054\n",
      "Iteration 23355, loss = 0.04676664\n",
      "Iteration 23356, loss = 0.04676888\n",
      "Iteration 23357, loss = 0.04676554\n",
      "Iteration 23358, loss = 0.04676120\n",
      "Iteration 23359, loss = 0.04675830\n",
      "Iteration 23360, loss = 0.04675691\n",
      "Iteration 23361, loss = 0.04675685\n",
      "Iteration 23362, loss = 0.04675397\n",
      "Iteration 23363, loss = 0.04675372\n",
      "Iteration 23364, loss = 0.04675328\n",
      "Iteration 23365, loss = 0.04674742\n",
      "Iteration 23366, loss = 0.04674901\n",
      "Iteration 23367, loss = 0.04674668\n",
      "Iteration 23368, loss = 0.04673957\n",
      "Iteration 23369, loss = 0.04673920\n",
      "Iteration 23370, loss = 0.04674450\n",
      "Iteration 23371, loss = 0.04674403\n",
      "Iteration 23372, loss = 0.04673522\n",
      "Iteration 23373, loss = 0.04673324\n",
      "Iteration 23374, loss = 0.04673855\n",
      "Iteration 23375, loss = 0.04673458\n",
      "Iteration 23376, loss = 0.04672810\n",
      "Iteration 23377, loss = 0.04673141\n",
      "Iteration 23378, loss = 0.04673446\n",
      "Iteration 23379, loss = 0.04673116\n",
      "Iteration 23380, loss = 0.04672341\n",
      "Iteration 23381, loss = 0.04672349\n",
      "Iteration 23382, loss = 0.04672378\n",
      "Iteration 23383, loss = 0.04671441\n",
      "Iteration 23384, loss = 0.04672039\n",
      "Iteration 23385, loss = 0.04672559\n",
      "Iteration 23386, loss = 0.04672269\n",
      "Iteration 23387, loss = 0.04671079\n",
      "Iteration 23388, loss = 0.04670711\n",
      "Iteration 23389, loss = 0.04671143\n",
      "Iteration 23390, loss = 0.04671721\n",
      "Iteration 23391, loss = 0.04671760\n",
      "Iteration 23392, loss = 0.04671113\n",
      "Iteration 23393, loss = 0.04670387\n",
      "Iteration 23394, loss = 0.04670176\n",
      "Iteration 23395, loss = 0.04669812\n",
      "Iteration 23396, loss = 0.04669381\n",
      "Iteration 23397, loss = 0.04669350\n",
      "Iteration 23398, loss = 0.04669639\n",
      "Iteration 23399, loss = 0.04668616\n",
      "Iteration 23400, loss = 0.04668160\n",
      "Iteration 23401, loss = 0.04668764\n",
      "Iteration 23402, loss = 0.04668462\n",
      "Iteration 23403, loss = 0.04668025\n",
      "Iteration 23404, loss = 0.04667810\n",
      "Iteration 23405, loss = 0.04667430\n",
      "Iteration 23406, loss = 0.04667065\n",
      "Iteration 23407, loss = 0.04667143\n",
      "Iteration 23408, loss = 0.04666720\n",
      "Iteration 23409, loss = 0.04666786\n",
      "Iteration 23410, loss = 0.04666503\n",
      "Iteration 23411, loss = 0.04666244\n",
      "Iteration 23412, loss = 0.04666078\n",
      "Iteration 23413, loss = 0.04665766\n",
      "Iteration 23414, loss = 0.04665440\n",
      "Iteration 23415, loss = 0.04666000\n",
      "Iteration 23416, loss = 0.04665936\n",
      "Iteration 23417, loss = 0.04665261\n",
      "Iteration 23418, loss = 0.04665020\n",
      "Iteration 23419, loss = 0.04665538\n",
      "Iteration 23420, loss = 0.04664783\n",
      "Iteration 23421, loss = 0.04665336\n",
      "Iteration 23422, loss = 0.04665526\n",
      "Iteration 23423, loss = 0.04665299\n",
      "Iteration 23424, loss = 0.04664321\n",
      "Iteration 23425, loss = 0.04664599\n",
      "Iteration 23426, loss = 0.04664384\n",
      "Iteration 23427, loss = 0.04663596\n",
      "Iteration 23428, loss = 0.04663356\n",
      "Iteration 23429, loss = 0.04663682\n",
      "Iteration 23430, loss = 0.04663571\n",
      "Iteration 23431, loss = 0.04663639\n",
      "Iteration 23432, loss = 0.04663235\n",
      "Iteration 23433, loss = 0.04662727\n",
      "Iteration 23434, loss = 0.04662604\n",
      "Iteration 23435, loss = 0.04662009\n",
      "Iteration 23436, loss = 0.04662353\n",
      "Iteration 23437, loss = 0.04662673\n",
      "Iteration 23438, loss = 0.04662115\n",
      "Iteration 23439, loss = 0.04661830\n",
      "Iteration 23440, loss = 0.04661983\n",
      "Iteration 23441, loss = 0.04661221\n",
      "Iteration 23442, loss = 0.04660697\n",
      "Iteration 23443, loss = 0.04660821\n",
      "Iteration 23444, loss = 0.04660726\n",
      "Iteration 23445, loss = 0.04660069\n",
      "Iteration 23446, loss = 0.04660363\n",
      "Iteration 23447, loss = 0.04660400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 23448, loss = 0.04659941\n",
      "Iteration 23449, loss = 0.04659650\n",
      "Iteration 23450, loss = 0.04659296\n",
      "Iteration 23451, loss = 0.04659603\n",
      "Iteration 23452, loss = 0.04659512\n",
      "Iteration 23453, loss = 0.04658334\n",
      "Iteration 23454, loss = 0.04658686\n",
      "Iteration 23455, loss = 0.04659547\n",
      "Iteration 23456, loss = 0.04659637\n",
      "Iteration 23457, loss = 0.04659451\n",
      "Iteration 23458, loss = 0.04658565\n",
      "Iteration 23459, loss = 0.04657805\n",
      "Iteration 23460, loss = 0.04657928\n",
      "Iteration 23461, loss = 0.04657496\n",
      "Iteration 23462, loss = 0.04657640\n",
      "Iteration 23463, loss = 0.04657781\n",
      "Iteration 23464, loss = 0.04657294\n",
      "Iteration 23465, loss = 0.04656797\n",
      "Iteration 23466, loss = 0.04656814\n",
      "Iteration 23467, loss = 0.04656495\n",
      "Iteration 23468, loss = 0.04656470\n",
      "Iteration 23469, loss = 0.04656543\n",
      "Iteration 23470, loss = 0.04655965\n",
      "Iteration 23471, loss = 0.04656123\n",
      "Iteration 23472, loss = 0.04656089\n",
      "Iteration 23473, loss = 0.04655076\n",
      "Iteration 23474, loss = 0.04654691\n",
      "Iteration 23475, loss = 0.04655247\n",
      "Iteration 23476, loss = 0.04655172\n",
      "Iteration 23477, loss = 0.04654977\n",
      "Iteration 23478, loss = 0.04654204\n",
      "Iteration 23479, loss = 0.04654738\n",
      "Iteration 23480, loss = 0.04654651\n",
      "Iteration 23481, loss = 0.04653948\n",
      "Iteration 23482, loss = 0.04654490\n",
      "Iteration 23483, loss = 0.04654456\n",
      "Iteration 23484, loss = 0.04654079\n",
      "Iteration 23485, loss = 0.04653138\n",
      "Iteration 23486, loss = 0.04652883\n",
      "Iteration 23487, loss = 0.04652800\n",
      "Iteration 23488, loss = 0.04652957\n",
      "Iteration 23489, loss = 0.04652601\n",
      "Iteration 23490, loss = 0.04652309\n",
      "Iteration 23491, loss = 0.04651462\n",
      "Iteration 23492, loss = 0.04651491\n",
      "Iteration 23493, loss = 0.04651587\n",
      "Iteration 23494, loss = 0.04651473\n",
      "Iteration 23495, loss = 0.04651457\n",
      "Iteration 23496, loss = 0.04651559\n",
      "Iteration 23497, loss = 0.04651114\n",
      "Iteration 23498, loss = 0.04650817\n",
      "Iteration 23499, loss = 0.04650042\n",
      "Iteration 23500, loss = 0.04649839\n",
      "Iteration 23501, loss = 0.04650552\n",
      "Iteration 23502, loss = 0.04650561\n",
      "Iteration 23503, loss = 0.04649932\n",
      "Iteration 23504, loss = 0.04649280\n",
      "Iteration 23505, loss = 0.04649583\n",
      "Iteration 23506, loss = 0.04649591\n",
      "Iteration 23507, loss = 0.04649285\n",
      "Iteration 23508, loss = 0.04649191\n",
      "Iteration 23509, loss = 0.04648780\n",
      "Iteration 23510, loss = 0.04648608\n",
      "Iteration 23511, loss = 0.04648423\n",
      "Iteration 23512, loss = 0.04648166\n",
      "Iteration 23513, loss = 0.04647474\n",
      "Iteration 23514, loss = 0.04647974\n",
      "Iteration 23515, loss = 0.04648363\n",
      "Iteration 23516, loss = 0.04647606\n",
      "Iteration 23517, loss = 0.04646967\n",
      "Iteration 23518, loss = 0.04647239\n",
      "Iteration 23519, loss = 0.04647225\n",
      "Iteration 23520, loss = 0.04647248\n",
      "Iteration 23521, loss = 0.04647136\n",
      "Iteration 23522, loss = 0.04646728\n",
      "Iteration 23523, loss = 0.04646566\n",
      "Iteration 23524, loss = 0.04646217\n",
      "Iteration 23525, loss = 0.04645588\n",
      "Iteration 23526, loss = 0.04645223\n",
      "Iteration 23527, loss = 0.04646005\n",
      "Iteration 23528, loss = 0.04645862\n",
      "Iteration 23529, loss = 0.04645554\n",
      "Iteration 23530, loss = 0.04645141\n",
      "Iteration 23531, loss = 0.04644456\n",
      "Iteration 23532, loss = 0.04644552\n",
      "Iteration 23533, loss = 0.04644682\n",
      "Iteration 23534, loss = 0.04644612\n",
      "Iteration 23535, loss = 0.04644143\n",
      "Iteration 23536, loss = 0.04643944\n",
      "Iteration 23537, loss = 0.04643823\n",
      "Iteration 23538, loss = 0.04643069\n",
      "Iteration 23539, loss = 0.04642930\n",
      "Iteration 23540, loss = 0.04643142\n",
      "Iteration 23541, loss = 0.04643131\n",
      "Iteration 23542, loss = 0.04642563\n",
      "Iteration 23543, loss = 0.04642019\n",
      "Iteration 23544, loss = 0.04642162\n",
      "Iteration 23545, loss = 0.04642018\n",
      "Iteration 23546, loss = 0.04641959\n",
      "Iteration 23547, loss = 0.04642110\n",
      "Iteration 23548, loss = 0.04641709\n",
      "Iteration 23549, loss = 0.04641752\n",
      "Iteration 23550, loss = 0.04641465\n",
      "Iteration 23551, loss = 0.04641136\n",
      "Iteration 23552, loss = 0.04640926\n",
      "Iteration 23553, loss = 0.04640651\n",
      "Iteration 23554, loss = 0.04640815\n",
      "Iteration 23555, loss = 0.04640223\n",
      "Iteration 23556, loss = 0.04639611\n",
      "Iteration 23557, loss = 0.04639936\n",
      "Iteration 23558, loss = 0.04639747\n",
      "Iteration 23559, loss = 0.04639924\n",
      "Iteration 23560, loss = 0.04639737\n",
      "Iteration 23561, loss = 0.04639687\n",
      "Iteration 23562, loss = 0.04639346\n",
      "Iteration 23563, loss = 0.04638731\n",
      "Iteration 23564, loss = 0.04638419\n",
      "Iteration 23565, loss = 0.04637969\n",
      "Iteration 23566, loss = 0.04638484\n",
      "Iteration 23567, loss = 0.04638131\n",
      "Iteration 23568, loss = 0.04638091\n",
      "Iteration 23569, loss = 0.04637098\n",
      "Iteration 23570, loss = 0.04637450\n",
      "Iteration 23571, loss = 0.04637309\n",
      "Iteration 23572, loss = 0.04637268\n",
      "Iteration 23573, loss = 0.04636595\n",
      "Iteration 23574, loss = 0.04636304\n",
      "Iteration 23575, loss = 0.04636790\n",
      "Iteration 23576, loss = 0.04636673\n",
      "Iteration 23577, loss = 0.04636463\n",
      "Iteration 23578, loss = 0.04636014\n",
      "Iteration 23579, loss = 0.04635431\n",
      "Iteration 23580, loss = 0.04635448\n",
      "Iteration 23581, loss = 0.04635596\n",
      "Iteration 23582, loss = 0.04635323\n",
      "Iteration 23583, loss = 0.04635070\n",
      "Iteration 23584, loss = 0.04635358\n",
      "Iteration 23585, loss = 0.04634888\n",
      "Iteration 23586, loss = 0.04634846\n",
      "Iteration 23587, loss = 0.04634561\n",
      "Iteration 23588, loss = 0.04634179\n",
      "Iteration 23589, loss = 0.04634419\n",
      "Iteration 23590, loss = 0.04634440\n",
      "Iteration 23591, loss = 0.04634159\n",
      "Iteration 23592, loss = 0.04633575\n",
      "Iteration 23593, loss = 0.04633085\n",
      "Iteration 23594, loss = 0.04633007\n",
      "Iteration 23595, loss = 0.04633276\n",
      "Iteration 23596, loss = 0.04632830\n",
      "Iteration 23597, loss = 0.04632846\n",
      "Iteration 23598, loss = 0.04632496\n",
      "Iteration 23599, loss = 0.04632268\n",
      "Iteration 23600, loss = 0.04631929\n",
      "Iteration 23601, loss = 0.04631880\n",
      "Iteration 23602, loss = 0.04631817\n",
      "Iteration 23603, loss = 0.04631203\n",
      "Iteration 23604, loss = 0.04631139\n",
      "Iteration 23605, loss = 0.04631262\n",
      "Iteration 23606, loss = 0.04631011\n",
      "Iteration 23607, loss = 0.04630608\n",
      "Iteration 23608, loss = 0.04630788\n",
      "Iteration 23609, loss = 0.04630717\n",
      "Iteration 23610, loss = 0.04630048\n",
      "Iteration 23611, loss = 0.04630387\n",
      "Iteration 23612, loss = 0.04630105\n",
      "Iteration 23613, loss = 0.04629654\n",
      "Iteration 23614, loss = 0.04629230\n",
      "Iteration 23615, loss = 0.04629198\n",
      "Iteration 23616, loss = 0.04628979\n",
      "Iteration 23617, loss = 0.04629163\n",
      "Iteration 23618, loss = 0.04628769\n",
      "Iteration 23619, loss = 0.04628680\n",
      "Iteration 23620, loss = 0.04628330\n",
      "Iteration 23621, loss = 0.04628037\n",
      "Iteration 23622, loss = 0.04627984\n",
      "Iteration 23623, loss = 0.04628241\n",
      "Iteration 23624, loss = 0.04627859\n",
      "Iteration 23625, loss = 0.04627564\n",
      "Iteration 23626, loss = 0.04627918\n",
      "Iteration 23627, loss = 0.04627690\n",
      "Iteration 23628, loss = 0.04627579\n",
      "Iteration 23629, loss = 0.04627493\n",
      "Iteration 23630, loss = 0.04627401\n",
      "Iteration 23631, loss = 0.04627199\n",
      "Iteration 23632, loss = 0.04626600\n",
      "Iteration 23633, loss = 0.04626055\n",
      "Iteration 23634, loss = 0.04625979\n",
      "Iteration 23635, loss = 0.04625734\n",
      "Iteration 23636, loss = 0.04625655\n",
      "Iteration 23637, loss = 0.04624955\n",
      "Iteration 23638, loss = 0.04625263\n",
      "Iteration 23639, loss = 0.04625062\n",
      "Iteration 23640, loss = 0.04624693\n",
      "Iteration 23641, loss = 0.04624656\n",
      "Iteration 23642, loss = 0.04624686\n",
      "Iteration 23643, loss = 0.04624781\n",
      "Iteration 23644, loss = 0.04624376\n",
      "Iteration 23645, loss = 0.04623633\n",
      "Iteration 23646, loss = 0.04623616\n",
      "Iteration 23647, loss = 0.04623526\n",
      "Iteration 23648, loss = 0.04623320\n",
      "Iteration 23649, loss = 0.04622979\n",
      "Iteration 23650, loss = 0.04623196\n",
      "Iteration 23651, loss = 0.04623161\n",
      "Iteration 23652, loss = 0.04622733\n",
      "Iteration 23653, loss = 0.04622614\n",
      "Iteration 23654, loss = 0.04622650\n",
      "Iteration 23655, loss = 0.04622434\n",
      "Iteration 23656, loss = 0.04622193\n",
      "Iteration 23657, loss = 0.04622271\n",
      "Iteration 23658, loss = 0.04621835\n",
      "Iteration 23659, loss = 0.04622141\n",
      "Iteration 23660, loss = 0.04621907\n",
      "Iteration 23661, loss = 0.04620727\n",
      "Iteration 23662, loss = 0.04621468\n",
      "Iteration 23663, loss = 0.04621909\n",
      "Iteration 23664, loss = 0.04621108\n",
      "Iteration 23665, loss = 0.04621033\n",
      "Iteration 23666, loss = 0.04620713\n",
      "Iteration 23667, loss = 0.04620792\n",
      "Iteration 23668, loss = 0.04620575\n",
      "Iteration 23669, loss = 0.04620340\n",
      "Iteration 23670, loss = 0.04619899\n",
      "Iteration 23671, loss = 0.04619535\n",
      "Iteration 23672, loss = 0.04619338\n",
      "Iteration 23673, loss = 0.04619283\n",
      "Iteration 23674, loss = 0.04618717\n",
      "Iteration 23675, loss = 0.04618687\n",
      "Iteration 23676, loss = 0.04618209\n",
      "Iteration 23677, loss = 0.04618124\n",
      "Iteration 23678, loss = 0.04618246\n",
      "Iteration 23679, loss = 0.04618231\n",
      "Iteration 23680, loss = 0.04617967\n",
      "Iteration 23681, loss = 0.04617308\n",
      "Iteration 23682, loss = 0.04617067\n",
      "Iteration 23683, loss = 0.04617193\n",
      "Iteration 23684, loss = 0.04617246\n",
      "Iteration 23685, loss = 0.04616929\n",
      "Iteration 23686, loss = 0.04616543\n",
      "Iteration 23687, loss = 0.04616440\n",
      "Iteration 23688, loss = 0.04616288\n",
      "Iteration 23689, loss = 0.04616044\n",
      "Iteration 23690, loss = 0.04616344\n",
      "Iteration 23691, loss = 0.04616240\n",
      "Iteration 23692, loss = 0.04615586\n",
      "Iteration 23693, loss = 0.04615665\n",
      "Iteration 23694, loss = 0.04615433\n",
      "Iteration 23695, loss = 0.04615161\n",
      "Iteration 23696, loss = 0.04615469\n",
      "Iteration 23697, loss = 0.04615291\n",
      "Iteration 23698, loss = 0.04615316\n",
      "Iteration 23699, loss = 0.04614898\n",
      "Iteration 23700, loss = 0.04614252\n",
      "Iteration 23701, loss = 0.04614644\n",
      "Iteration 23702, loss = 0.04614471\n",
      "Iteration 23703, loss = 0.04614185\n",
      "Iteration 23704, loss = 0.04613691\n",
      "Iteration 23705, loss = 0.04613708\n",
      "Iteration 23706, loss = 0.04614114\n",
      "Iteration 23707, loss = 0.04613242\n",
      "Iteration 23708, loss = 0.04613088\n",
      "Iteration 23709, loss = 0.04613021\n",
      "Iteration 23710, loss = 0.04612728\n",
      "Iteration 23711, loss = 0.04612522\n",
      "Iteration 23712, loss = 0.04612500\n",
      "Iteration 23713, loss = 0.04612024\n",
      "Iteration 23714, loss = 0.04611616\n",
      "Iteration 23715, loss = 0.04611576\n",
      "Iteration 23716, loss = 0.04611596\n",
      "Iteration 23717, loss = 0.04611406\n",
      "Iteration 23718, loss = 0.04610986\n",
      "Iteration 23719, loss = 0.04610568\n",
      "Iteration 23720, loss = 0.04610980\n",
      "Iteration 23721, loss = 0.04610771\n",
      "Iteration 23722, loss = 0.04610434\n",
      "Iteration 23723, loss = 0.04610730\n",
      "Iteration 23724, loss = 0.04610118\n",
      "Iteration 23725, loss = 0.04609885\n",
      "Iteration 23726, loss = 0.04609794\n",
      "Iteration 23727, loss = 0.04609822\n",
      "Iteration 23728, loss = 0.04609355\n",
      "Iteration 23729, loss = 0.04608751\n",
      "Iteration 23730, loss = 0.04608457\n",
      "Iteration 23731, loss = 0.04608446\n",
      "Iteration 23732, loss = 0.04608475\n",
      "Iteration 23733, loss = 0.04608289\n",
      "Iteration 23734, loss = 0.04608518\n",
      "Iteration 23735, loss = 0.04607885\n",
      "Iteration 23736, loss = 0.04607792\n",
      "Iteration 23737, loss = 0.04607656\n",
      "Iteration 23738, loss = 0.04607745\n",
      "Iteration 23739, loss = 0.04607507\n",
      "Iteration 23740, loss = 0.04607448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 23741, loss = 0.04607358\n",
      "Iteration 23742, loss = 0.04607425\n",
      "Iteration 23743, loss = 0.04607390\n",
      "Iteration 23744, loss = 0.04606885\n",
      "Iteration 23745, loss = 0.04606077\n",
      "Iteration 23746, loss = 0.04606415\n",
      "Iteration 23747, loss = 0.04606802\n",
      "Iteration 23748, loss = 0.04606713\n",
      "Iteration 23749, loss = 0.04606104\n",
      "Iteration 23750, loss = 0.04605630\n",
      "Iteration 23751, loss = 0.04605532\n",
      "Iteration 23752, loss = 0.04605904\n",
      "Iteration 23753, loss = 0.04605472\n",
      "Iteration 23754, loss = 0.04604878\n",
      "Iteration 23755, loss = 0.04604987\n",
      "Iteration 23756, loss = 0.04605066\n",
      "Iteration 23757, loss = 0.04604875\n",
      "Iteration 23758, loss = 0.04604064\n",
      "Iteration 23759, loss = 0.04604141\n",
      "Iteration 23760, loss = 0.04604128\n",
      "Iteration 23761, loss = 0.04603965\n",
      "Iteration 23762, loss = 0.04603513\n",
      "Iteration 23763, loss = 0.04603232\n",
      "Iteration 23764, loss = 0.04603441\n",
      "Iteration 23765, loss = 0.04602945\n",
      "Iteration 23766, loss = 0.04602555\n",
      "Iteration 23767, loss = 0.04602625\n",
      "Iteration 23768, loss = 0.04603004\n",
      "Iteration 23769, loss = 0.04602500\n",
      "Iteration 23770, loss = 0.04602456\n",
      "Iteration 23771, loss = 0.04602301\n",
      "Iteration 23772, loss = 0.04601967\n",
      "Iteration 23773, loss = 0.04601869\n",
      "Iteration 23774, loss = 0.04602110\n",
      "Iteration 23775, loss = 0.04601797\n",
      "Iteration 23776, loss = 0.04601955\n",
      "Iteration 23777, loss = 0.04601831\n",
      "Iteration 23778, loss = 0.04601251\n",
      "Iteration 23779, loss = 0.04601059\n",
      "Iteration 23780, loss = 0.04601097\n",
      "Iteration 23781, loss = 0.04600607\n",
      "Iteration 23782, loss = 0.04600592\n",
      "Iteration 23783, loss = 0.04600321\n",
      "Iteration 23784, loss = 0.04600343\n",
      "Iteration 23785, loss = 0.04599615\n",
      "Iteration 23786, loss = 0.04599631\n",
      "Iteration 23787, loss = 0.04599578\n",
      "Iteration 23788, loss = 0.04599742\n",
      "Iteration 23789, loss = 0.04599532\n",
      "Iteration 23790, loss = 0.04599266\n",
      "Iteration 23791, loss = 0.04599070\n",
      "Iteration 23792, loss = 0.04598583\n",
      "Iteration 23793, loss = 0.04597717\n",
      "Iteration 23794, loss = 0.04598115\n",
      "Iteration 23795, loss = 0.04597843\n",
      "Iteration 23796, loss = 0.04597779\n",
      "Iteration 23797, loss = 0.04597562\n",
      "Iteration 23798, loss = 0.04597439\n",
      "Iteration 23799, loss = 0.04597373\n",
      "Iteration 23800, loss = 0.04596965\n",
      "Iteration 23801, loss = 0.04596897\n",
      "Iteration 23802, loss = 0.04596463\n",
      "Iteration 23803, loss = 0.04596556\n",
      "Iteration 23804, loss = 0.04596458\n",
      "Iteration 23805, loss = 0.04596333\n",
      "Iteration 23806, loss = 0.04595891\n",
      "Iteration 23807, loss = 0.04596034\n",
      "Iteration 23808, loss = 0.04596029\n",
      "Iteration 23809, loss = 0.04595982\n",
      "Iteration 23810, loss = 0.04595574\n",
      "Iteration 23811, loss = 0.04595204\n",
      "Iteration 23812, loss = 0.04594549\n",
      "Iteration 23813, loss = 0.04594479\n",
      "Iteration 23814, loss = 0.04594329\n",
      "Iteration 23815, loss = 0.04594169\n",
      "Iteration 23816, loss = 0.04593921\n",
      "Iteration 23817, loss = 0.04593856\n",
      "Iteration 23818, loss = 0.04593421\n",
      "Iteration 23819, loss = 0.04593295\n",
      "Iteration 23820, loss = 0.04593600\n",
      "Iteration 23821, loss = 0.04593057\n",
      "Iteration 23822, loss = 0.04593059\n",
      "Iteration 23823, loss = 0.04593674\n",
      "Iteration 23824, loss = 0.04593221\n",
      "Iteration 23825, loss = 0.04593083\n",
      "Iteration 23826, loss = 0.04592949\n",
      "Iteration 23827, loss = 0.04592774\n",
      "Iteration 23828, loss = 0.04592689\n",
      "Iteration 23829, loss = 0.04592304\n",
      "Iteration 23830, loss = 0.04592089\n",
      "Iteration 23831, loss = 0.04591575\n",
      "Iteration 23832, loss = 0.04590999\n",
      "Iteration 23833, loss = 0.04591260\n",
      "Iteration 23834, loss = 0.04591283\n",
      "Iteration 23835, loss = 0.04590920\n",
      "Iteration 23836, loss = 0.04590620\n",
      "Iteration 23837, loss = 0.04590745\n",
      "Iteration 23838, loss = 0.04590587\n",
      "Iteration 23839, loss = 0.04590043\n",
      "Iteration 23840, loss = 0.04590106\n",
      "Iteration 23841, loss = 0.04590046\n",
      "Iteration 23842, loss = 0.04589570\n",
      "Iteration 23843, loss = 0.04589397\n",
      "Iteration 23844, loss = 0.04589236\n",
      "Iteration 23845, loss = 0.04589481\n",
      "Iteration 23846, loss = 0.04589321\n",
      "Iteration 23847, loss = 0.04588787\n",
      "Iteration 23848, loss = 0.04588725\n",
      "Iteration 23849, loss = 0.04588526\n",
      "Iteration 23850, loss = 0.04587976\n",
      "Iteration 23851, loss = 0.04588291\n",
      "Iteration 23852, loss = 0.04588110\n",
      "Iteration 23853, loss = 0.04588597\n",
      "Iteration 23854, loss = 0.04587753\n",
      "Iteration 23855, loss = 0.04587272\n",
      "Iteration 23856, loss = 0.04587564\n",
      "Iteration 23857, loss = 0.04587539\n",
      "Iteration 23858, loss = 0.04587212\n",
      "Iteration 23859, loss = 0.04587227\n",
      "Iteration 23860, loss = 0.04586865\n",
      "Iteration 23861, loss = 0.04586722\n",
      "Iteration 23862, loss = 0.04586356\n",
      "Iteration 23863, loss = 0.04585880\n",
      "Iteration 23864, loss = 0.04585730\n",
      "Iteration 23865, loss = 0.04585626\n",
      "Iteration 23866, loss = 0.04585900\n",
      "Iteration 23867, loss = 0.04585575\n",
      "Iteration 23868, loss = 0.04584866\n",
      "Iteration 23869, loss = 0.04584786\n",
      "Iteration 23870, loss = 0.04585089\n",
      "Iteration 23871, loss = 0.04585212\n",
      "Iteration 23872, loss = 0.04584893\n",
      "Iteration 23873, loss = 0.04584496\n",
      "Iteration 23874, loss = 0.04584366\n",
      "Iteration 23875, loss = 0.04584850\n",
      "Iteration 23876, loss = 0.04584680\n",
      "Iteration 23877, loss = 0.04584180\n",
      "Iteration 23878, loss = 0.04583801\n",
      "Iteration 23879, loss = 0.04583566\n",
      "Iteration 23880, loss = 0.04583139\n",
      "Iteration 23881, loss = 0.04582603\n",
      "Iteration 23882, loss = 0.04582753\n",
      "Iteration 23883, loss = 0.04582418\n",
      "Iteration 23884, loss = 0.04581977\n",
      "Iteration 23885, loss = 0.04582229\n",
      "Iteration 23886, loss = 0.04581874\n",
      "Iteration 23887, loss = 0.04581504\n",
      "Iteration 23888, loss = 0.04582101\n",
      "Iteration 23889, loss = 0.04582051\n",
      "Iteration 23890, loss = 0.04581692\n",
      "Iteration 23891, loss = 0.04581017\n",
      "Iteration 23892, loss = 0.04581151\n",
      "Iteration 23893, loss = 0.04581073\n",
      "Iteration 23894, loss = 0.04580367\n",
      "Iteration 23895, loss = 0.04580716\n",
      "Iteration 23896, loss = 0.04580792\n",
      "Iteration 23897, loss = 0.04580358\n",
      "Iteration 23898, loss = 0.04580234\n",
      "Iteration 23899, loss = 0.04580247\n",
      "Iteration 23900, loss = 0.04579891\n",
      "Iteration 23901, loss = 0.04579451\n",
      "Iteration 23902, loss = 0.04579476\n",
      "Iteration 23903, loss = 0.04579338\n",
      "Iteration 23904, loss = 0.04579020\n",
      "Iteration 23905, loss = 0.04579393\n",
      "Iteration 23906, loss = 0.04578980\n",
      "Iteration 23907, loss = 0.04578542\n",
      "Iteration 23908, loss = 0.04578359\n",
      "Iteration 23909, loss = 0.04578074\n",
      "Iteration 23910, loss = 0.04577991\n",
      "Iteration 23911, loss = 0.04577774\n",
      "Iteration 23912, loss = 0.04577440\n",
      "Iteration 23913, loss = 0.04577707\n",
      "Iteration 23914, loss = 0.04577709\n",
      "Iteration 23915, loss = 0.04577171\n",
      "Iteration 23916, loss = 0.04576990\n",
      "Iteration 23917, loss = 0.04576680\n",
      "Iteration 23918, loss = 0.04576457\n",
      "Iteration 23919, loss = 0.04576652\n",
      "Iteration 23920, loss = 0.04576589\n",
      "Iteration 23921, loss = 0.04576327\n",
      "Iteration 23922, loss = 0.04575798\n",
      "Iteration 23923, loss = 0.04575596\n",
      "Iteration 23924, loss = 0.04575256\n",
      "Iteration 23925, loss = 0.04575211\n",
      "Iteration 23926, loss = 0.04575245\n",
      "Iteration 23927, loss = 0.04575025\n",
      "Iteration 23928, loss = 0.04574868\n",
      "Iteration 23929, loss = 0.04575126\n",
      "Iteration 23930, loss = 0.04574974\n",
      "Iteration 23931, loss = 0.04574465\n",
      "Iteration 23932, loss = 0.04574283\n",
      "Iteration 23933, loss = 0.04574329\n",
      "Iteration 23934, loss = 0.04574395\n",
      "Iteration 23935, loss = 0.04573604\n",
      "Iteration 23936, loss = 0.04573555\n",
      "Iteration 23937, loss = 0.04573588\n",
      "Iteration 23938, loss = 0.04573480\n",
      "Iteration 23939, loss = 0.04573635\n",
      "Iteration 23940, loss = 0.04573467\n",
      "Iteration 23941, loss = 0.04572872\n",
      "Iteration 23942, loss = 0.04572519\n",
      "Iteration 23943, loss = 0.04572517\n",
      "Iteration 23944, loss = 0.04572261\n",
      "Iteration 23945, loss = 0.04571836\n",
      "Iteration 23946, loss = 0.04571705\n",
      "Iteration 23947, loss = 0.04571785\n",
      "Iteration 23948, loss = 0.04572691\n",
      "Iteration 23949, loss = 0.04571805\n",
      "Iteration 23950, loss = 0.04571737\n",
      "Iteration 23951, loss = 0.04571990\n",
      "Iteration 23952, loss = 0.04571975\n",
      "Iteration 23953, loss = 0.04571804\n",
      "Iteration 23954, loss = 0.04571178\n",
      "Iteration 23955, loss = 0.04570305\n",
      "Iteration 23956, loss = 0.04570679\n",
      "Iteration 23957, loss = 0.04570958\n",
      "Iteration 23958, loss = 0.04570404\n",
      "Iteration 23959, loss = 0.04569216\n",
      "Iteration 23960, loss = 0.04569694\n",
      "Iteration 23961, loss = 0.04570266\n",
      "Iteration 23962, loss = 0.04570266\n",
      "Iteration 23963, loss = 0.04569962\n",
      "Iteration 23964, loss = 0.04569538\n",
      "Iteration 23965, loss = 0.04568577\n",
      "Iteration 23966, loss = 0.04568919\n",
      "Iteration 23967, loss = 0.04569658\n",
      "Iteration 23968, loss = 0.04569355\n",
      "Iteration 23969, loss = 0.04568819\n",
      "Iteration 23970, loss = 0.04568205\n",
      "Iteration 23971, loss = 0.04568293\n",
      "Iteration 23972, loss = 0.04568844\n",
      "Iteration 23973, loss = 0.04568736\n",
      "Iteration 23974, loss = 0.04568437\n",
      "Iteration 23975, loss = 0.04567879\n",
      "Iteration 23976, loss = 0.04567284\n",
      "Iteration 23977, loss = 0.04567503\n",
      "Iteration 23978, loss = 0.04567373\n",
      "Iteration 23979, loss = 0.04567611\n",
      "Iteration 23980, loss = 0.04567049\n",
      "Iteration 23981, loss = 0.04566356\n",
      "Iteration 23982, loss = 0.04566698\n",
      "Iteration 23983, loss = 0.04566424\n",
      "Iteration 23984, loss = 0.04566331\n",
      "Iteration 23985, loss = 0.04565987\n",
      "Iteration 23986, loss = 0.04565568\n",
      "Iteration 23987, loss = 0.04565435\n",
      "Iteration 23988, loss = 0.04565480\n",
      "Iteration 23989, loss = 0.04565247\n",
      "Iteration 23990, loss = 0.04565379\n",
      "Iteration 23991, loss = 0.04565094\n",
      "Iteration 23992, loss = 0.04565512\n",
      "Iteration 23993, loss = 0.04565131\n",
      "Iteration 23994, loss = 0.04564192\n",
      "Iteration 23995, loss = 0.04564154\n",
      "Iteration 23996, loss = 0.04564175\n",
      "Iteration 23997, loss = 0.04563549\n",
      "Iteration 23998, loss = 0.04563073\n",
      "Iteration 23999, loss = 0.04563099\n",
      "Iteration 24000, loss = 0.04562708\n",
      "Iteration 24001, loss = 0.04563145\n",
      "Iteration 24002, loss = 0.04562871\n",
      "Iteration 24003, loss = 0.04562313\n",
      "Iteration 24004, loss = 0.04562610\n",
      "Iteration 24005, loss = 0.04562498\n",
      "Iteration 24006, loss = 0.04562307\n",
      "Iteration 24007, loss = 0.04562068\n",
      "Iteration 24008, loss = 0.04561739\n",
      "Iteration 24009, loss = 0.04561116\n",
      "Iteration 24010, loss = 0.04561452\n",
      "Iteration 24011, loss = 0.04561695\n",
      "Iteration 24012, loss = 0.04561241\n",
      "Iteration 24013, loss = 0.04560661\n",
      "Iteration 24014, loss = 0.04560783\n",
      "Iteration 24015, loss = 0.04560653\n",
      "Iteration 24016, loss = 0.04560266\n",
      "Iteration 24017, loss = 0.04560420\n",
      "Iteration 24018, loss = 0.04560236\n",
      "Iteration 24019, loss = 0.04560451\n",
      "Iteration 24020, loss = 0.04560046\n",
      "Iteration 24021, loss = 0.04559979\n",
      "Iteration 24022, loss = 0.04559591\n",
      "Iteration 24023, loss = 0.04559586\n",
      "Iteration 24024, loss = 0.04559663\n",
      "Iteration 24025, loss = 0.04559348\n",
      "Iteration 24026, loss = 0.04558963\n",
      "Iteration 24027, loss = 0.04558433\n",
      "Iteration 24028, loss = 0.04558568\n",
      "Iteration 24029, loss = 0.04558502\n",
      "Iteration 24030, loss = 0.04558311\n",
      "Iteration 24031, loss = 0.04558061\n",
      "Iteration 24032, loss = 0.04557729\n",
      "Iteration 24033, loss = 0.04557520\n",
      "Iteration 24034, loss = 0.04557746\n",
      "Iteration 24035, loss = 0.04557215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 24036, loss = 0.04556714\n",
      "Iteration 24037, loss = 0.04556838\n",
      "Iteration 24038, loss = 0.04556634\n",
      "Iteration 24039, loss = 0.04556185\n",
      "Iteration 24040, loss = 0.04556364\n",
      "Iteration 24041, loss = 0.04556192\n",
      "Iteration 24042, loss = 0.04556192\n",
      "Iteration 24043, loss = 0.04555888\n",
      "Iteration 24044, loss = 0.04555932\n",
      "Iteration 24045, loss = 0.04555658\n",
      "Iteration 24046, loss = 0.04555393\n",
      "Iteration 24047, loss = 0.04555061\n",
      "Iteration 24048, loss = 0.04554631\n",
      "Iteration 24049, loss = 0.04554718\n",
      "Iteration 24050, loss = 0.04554434\n",
      "Iteration 24051, loss = 0.04554056\n",
      "Iteration 24052, loss = 0.04554504\n",
      "Iteration 24053, loss = 0.04554351\n",
      "Iteration 24054, loss = 0.04554002\n",
      "Iteration 24055, loss = 0.04553687\n",
      "Iteration 24056, loss = 0.04553452\n",
      "Iteration 24057, loss = 0.04553394\n",
      "Iteration 24058, loss = 0.04553127\n",
      "Iteration 24059, loss = 0.04553388\n",
      "Iteration 24060, loss = 0.04553311\n",
      "Iteration 24061, loss = 0.04552884\n",
      "Iteration 24062, loss = 0.04552542\n",
      "Iteration 24063, loss = 0.04553202\n",
      "Iteration 24064, loss = 0.04552512\n",
      "Iteration 24065, loss = 0.04551975\n",
      "Iteration 24066, loss = 0.04552327\n",
      "Iteration 24067, loss = 0.04552332\n",
      "Iteration 24068, loss = 0.04551583\n",
      "Iteration 24069, loss = 0.04551801\n",
      "Iteration 24070, loss = 0.04551646\n",
      "Iteration 24071, loss = 0.04551286\n",
      "Iteration 24072, loss = 0.04551139\n",
      "Iteration 24073, loss = 0.04551748\n",
      "Iteration 24074, loss = 0.04551895\n",
      "Iteration 24075, loss = 0.04551327\n",
      "Iteration 24076, loss = 0.04550784\n",
      "Iteration 24077, loss = 0.04550250\n",
      "Iteration 24078, loss = 0.04549921\n",
      "Iteration 24079, loss = 0.04550213\n",
      "Iteration 24080, loss = 0.04549762\n",
      "Iteration 24081, loss = 0.04549530\n",
      "Iteration 24082, loss = 0.04549631\n",
      "Iteration 24083, loss = 0.04549400\n",
      "Iteration 24084, loss = 0.04549003\n",
      "Iteration 24085, loss = 0.04549306\n",
      "Iteration 24086, loss = 0.04548648\n",
      "Iteration 24087, loss = 0.04549021\n",
      "Iteration 24088, loss = 0.04549023\n",
      "Iteration 24089, loss = 0.04549077\n",
      "Iteration 24090, loss = 0.04548633\n",
      "Iteration 24091, loss = 0.04547939\n",
      "Iteration 24092, loss = 0.04547911\n",
      "Iteration 24093, loss = 0.04548167\n",
      "Iteration 24094, loss = 0.04548492\n",
      "Iteration 24095, loss = 0.04547790\n",
      "Iteration 24096, loss = 0.04546928\n",
      "Iteration 24097, loss = 0.04547048\n",
      "Iteration 24098, loss = 0.04547104\n",
      "Iteration 24099, loss = 0.04547009\n",
      "Iteration 24100, loss = 0.04546227\n",
      "Iteration 24101, loss = 0.04546673\n",
      "Iteration 24102, loss = 0.04547291\n",
      "Iteration 24103, loss = 0.04547119\n",
      "Iteration 24104, loss = 0.04545785\n",
      "Iteration 24105, loss = 0.04545649\n",
      "Iteration 24106, loss = 0.04545818\n",
      "Iteration 24107, loss = 0.04545427\n",
      "Iteration 24108, loss = 0.04545610\n",
      "Iteration 24109, loss = 0.04545228\n",
      "Iteration 24110, loss = 0.04544790\n",
      "Iteration 24111, loss = 0.04545214\n",
      "Iteration 24112, loss = 0.04544777\n",
      "Iteration 24113, loss = 0.04544093\n",
      "Iteration 24114, loss = 0.04543951\n",
      "Iteration 24115, loss = 0.04544102\n",
      "Iteration 24116, loss = 0.04544193\n",
      "Iteration 24117, loss = 0.04544173\n",
      "Iteration 24118, loss = 0.04543919\n",
      "Iteration 24119, loss = 0.04543654\n",
      "Iteration 24120, loss = 0.04543718\n",
      "Iteration 24121, loss = 0.04543514\n",
      "Iteration 24122, loss = 0.04543324\n",
      "Iteration 24123, loss = 0.04542831\n",
      "Iteration 24124, loss = 0.04542612\n",
      "Iteration 24125, loss = 0.04542819\n",
      "Iteration 24126, loss = 0.04541893\n",
      "Iteration 24127, loss = 0.04541593\n",
      "Iteration 24128, loss = 0.04541668\n",
      "Iteration 24129, loss = 0.04541638\n",
      "Iteration 24130, loss = 0.04541205\n",
      "Iteration 24131, loss = 0.04540338\n",
      "Iteration 24132, loss = 0.04540660\n",
      "Iteration 24133, loss = 0.04540289\n",
      "Iteration 24134, loss = 0.04539697\n",
      "Iteration 24135, loss = 0.04539265\n",
      "Iteration 24136, loss = 0.04539010\n",
      "Iteration 24137, loss = 0.04538402\n",
      "Iteration 24138, loss = 0.04538313\n",
      "Iteration 24139, loss = 0.04538137\n",
      "Iteration 24140, loss = 0.04537469\n",
      "Iteration 24141, loss = 0.04537526\n",
      "Iteration 24142, loss = 0.04537294\n",
      "Iteration 24143, loss = 0.04537093\n",
      "Iteration 24144, loss = 0.04536698\n",
      "Iteration 24145, loss = 0.04536057\n",
      "Iteration 24146, loss = 0.04535596\n",
      "Iteration 24147, loss = 0.04536033\n",
      "Iteration 24148, loss = 0.04535874\n",
      "Iteration 24149, loss = 0.04535229\n",
      "Iteration 24150, loss = 0.04534408\n",
      "Iteration 24151, loss = 0.04534340\n",
      "Iteration 24152, loss = 0.04533607\n",
      "Iteration 24153, loss = 0.04533741\n",
      "Iteration 24154, loss = 0.04533414\n",
      "Iteration 24155, loss = 0.04532929\n",
      "Iteration 24156, loss = 0.04532234\n",
      "Iteration 24157, loss = 0.04532088\n",
      "Iteration 24158, loss = 0.04531590\n",
      "Iteration 24159, loss = 0.04531427\n",
      "Iteration 24160, loss = 0.04531668\n",
      "Iteration 24161, loss = 0.04531280\n",
      "Iteration 24162, loss = 0.04530650\n",
      "Iteration 24163, loss = 0.04530313\n",
      "Iteration 24164, loss = 0.04529911\n",
      "Iteration 24165, loss = 0.04529775\n",
      "Iteration 24166, loss = 0.04529446\n",
      "Iteration 24167, loss = 0.04529699\n",
      "Iteration 24168, loss = 0.04529438\n",
      "Iteration 24169, loss = 0.04528758\n",
      "Iteration 24170, loss = 0.04528597\n",
      "Iteration 24171, loss = 0.04528160\n",
      "Iteration 24172, loss = 0.04528348\n",
      "Iteration 24173, loss = 0.04528182\n",
      "Iteration 24174, loss = 0.04527449\n",
      "Iteration 24175, loss = 0.04526477\n",
      "Iteration 24176, loss = 0.04526365\n",
      "Iteration 24177, loss = 0.04527125\n",
      "Iteration 24178, loss = 0.04526952\n",
      "Iteration 24179, loss = 0.04525823\n",
      "Iteration 24180, loss = 0.04525331\n",
      "Iteration 24181, loss = 0.04525775\n",
      "Iteration 24182, loss = 0.04525826\n",
      "Iteration 24183, loss = 0.04524821\n",
      "Iteration 24184, loss = 0.04524284\n",
      "Iteration 24185, loss = 0.04523882\n",
      "Iteration 24186, loss = 0.04523244\n",
      "Iteration 24187, loss = 0.04523778\n",
      "Iteration 24188, loss = 0.04523618\n",
      "Iteration 24189, loss = 0.04522350\n",
      "Iteration 24190, loss = 0.04522969\n",
      "Iteration 24191, loss = 0.04522971\n",
      "Iteration 24192, loss = 0.04522707\n",
      "Iteration 24193, loss = 0.04522641\n",
      "Iteration 24194, loss = 0.04522453\n",
      "Iteration 24195, loss = 0.04521735\n",
      "Iteration 24196, loss = 0.04521116\n",
      "Iteration 24197, loss = 0.04520957\n",
      "Iteration 24198, loss = 0.04520997\n",
      "Iteration 24199, loss = 0.04520895\n",
      "Iteration 24200, loss = 0.04520749\n",
      "Iteration 24201, loss = 0.04520197\n",
      "Iteration 24202, loss = 0.04519940\n",
      "Iteration 24203, loss = 0.04520031\n",
      "Iteration 24204, loss = 0.04519911\n",
      "Iteration 24205, loss = 0.04519262\n",
      "Iteration 24206, loss = 0.04518425\n",
      "Iteration 24207, loss = 0.04518418\n",
      "Iteration 24208, loss = 0.04518502\n",
      "Iteration 24209, loss = 0.04517481\n",
      "Iteration 24210, loss = 0.04517360\n",
      "Iteration 24211, loss = 0.04517367\n",
      "Iteration 24212, loss = 0.04517103\n",
      "Iteration 24213, loss = 0.04516602\n",
      "Iteration 24214, loss = 0.04515762\n",
      "Iteration 24215, loss = 0.04515326\n",
      "Iteration 24216, loss = 0.04515597\n",
      "Iteration 24217, loss = 0.04515438\n",
      "Iteration 24218, loss = 0.04514961\n",
      "Iteration 24219, loss = 0.04514493\n",
      "Iteration 24220, loss = 0.04513776\n",
      "Iteration 24221, loss = 0.04513849\n",
      "Iteration 24222, loss = 0.04513498\n",
      "Iteration 24223, loss = 0.04512861\n",
      "Iteration 24224, loss = 0.04512272\n",
      "Iteration 24225, loss = 0.04512420\n",
      "Iteration 24226, loss = 0.04512107\n",
      "Iteration 24227, loss = 0.04511201\n",
      "Iteration 24228, loss = 0.04510807\n",
      "Iteration 24229, loss = 0.04511035\n",
      "Iteration 24230, loss = 0.04510983\n",
      "Iteration 24231, loss = 0.04510673\n",
      "Iteration 24232, loss = 0.04510519\n",
      "Iteration 24233, loss = 0.04509465\n",
      "Iteration 24234, loss = 0.04508895\n",
      "Iteration 24235, loss = 0.04509275\n",
      "Iteration 24236, loss = 0.04509064\n",
      "Iteration 24237, loss = 0.04508833\n",
      "Iteration 24238, loss = 0.04507893\n",
      "Iteration 24239, loss = 0.04506895\n",
      "Iteration 24240, loss = 0.04507080\n",
      "Iteration 24241, loss = 0.04507468\n",
      "Iteration 24242, loss = 0.04507542\n",
      "Iteration 24243, loss = 0.04506955\n",
      "Iteration 24244, loss = 0.04506483\n",
      "Iteration 24245, loss = 0.04505587\n",
      "Iteration 24246, loss = 0.04504887\n",
      "Iteration 24247, loss = 0.04505223\n",
      "Iteration 24248, loss = 0.04504989\n",
      "Iteration 24249, loss = 0.04504520\n",
      "Iteration 24250, loss = 0.04504099\n",
      "Iteration 24251, loss = 0.04503484\n",
      "Iteration 24252, loss = 0.04503716\n",
      "Iteration 24253, loss = 0.04503715\n",
      "Iteration 24254, loss = 0.04503085\n",
      "Iteration 24255, loss = 0.04502650\n",
      "Iteration 24256, loss = 0.04502737\n",
      "Iteration 24257, loss = 0.04501749\n",
      "Iteration 24258, loss = 0.04501627\n",
      "Iteration 24259, loss = 0.04501487\n",
      "Iteration 24260, loss = 0.04501158\n",
      "Iteration 24261, loss = 0.04500749\n",
      "Iteration 24262, loss = 0.04500176\n",
      "Iteration 24263, loss = 0.04500471\n",
      "Iteration 24264, loss = 0.04500405\n",
      "Iteration 24265, loss = 0.04499579\n",
      "Iteration 24266, loss = 0.04499054\n",
      "Iteration 24267, loss = 0.04499428\n",
      "Iteration 24268, loss = 0.04498883\n",
      "Iteration 24269, loss = 0.04497807\n",
      "Iteration 24270, loss = 0.04497145\n",
      "Iteration 24271, loss = 0.04496699\n",
      "Iteration 24272, loss = 0.04495977\n",
      "Iteration 24273, loss = 0.04495160\n",
      "Iteration 24274, loss = 0.04494175\n",
      "Iteration 24275, loss = 0.04493619\n",
      "Iteration 24276, loss = 0.04492789\n",
      "Iteration 24277, loss = 0.04491786\n",
      "Iteration 24278, loss = 0.04490718\n",
      "Iteration 24279, loss = 0.04490378\n",
      "Iteration 24280, loss = 0.04489715\n",
      "Iteration 24281, loss = 0.04488330\n",
      "Iteration 24282, loss = 0.04487468\n",
      "Iteration 24283, loss = 0.04487001\n",
      "Iteration 24284, loss = 0.04486205\n",
      "Iteration 24285, loss = 0.04485278\n",
      "Iteration 24286, loss = 0.04484068\n",
      "Iteration 24287, loss = 0.04483288\n",
      "Iteration 24288, loss = 0.04482827\n",
      "Iteration 24289, loss = 0.04481569\n",
      "Iteration 24290, loss = 0.04480320\n",
      "Iteration 24291, loss = 0.04479362\n",
      "Iteration 24292, loss = 0.04478782\n",
      "Iteration 24293, loss = 0.04477769\n",
      "Iteration 24294, loss = 0.04476331\n",
      "Iteration 24295, loss = 0.04475649\n",
      "Iteration 24296, loss = 0.04474709\n",
      "Iteration 24297, loss = 0.04474204\n",
      "Iteration 24298, loss = 0.04472907\n",
      "Iteration 24299, loss = 0.04471914\n",
      "Iteration 24300, loss = 0.04471310\n",
      "Iteration 24301, loss = 0.04470230\n",
      "Iteration 24302, loss = 0.04469005\n",
      "Iteration 24303, loss = 0.04467730\n",
      "Iteration 24304, loss = 0.04467334\n",
      "Iteration 24305, loss = 0.04466960\n",
      "Iteration 24306, loss = 0.04465693\n",
      "Iteration 24307, loss = 0.04464541\n",
      "Iteration 24308, loss = 0.04464039\n",
      "Iteration 24309, loss = 0.04463524\n",
      "Iteration 24310, loss = 0.04462675\n",
      "Iteration 24311, loss = 0.04461666\n",
      "Iteration 24312, loss = 0.04460390\n",
      "Iteration 24313, loss = 0.04459132\n",
      "Iteration 24314, loss = 0.04458844\n",
      "Iteration 24315, loss = 0.04458199\n",
      "Iteration 24316, loss = 0.04457480\n",
      "Iteration 24317, loss = 0.04456616\n",
      "Iteration 24318, loss = 0.04455274\n",
      "Iteration 24319, loss = 0.04454913\n",
      "Iteration 24320, loss = 0.04454441\n",
      "Iteration 24321, loss = 0.04453558\n",
      "Iteration 24322, loss = 0.04452823\n",
      "Iteration 24323, loss = 0.04452090\n",
      "Iteration 24324, loss = 0.04450675\n",
      "Iteration 24325, loss = 0.04449453\n",
      "Iteration 24326, loss = 0.04449020\n",
      "Iteration 24327, loss = 0.04449059\n",
      "Iteration 24328, loss = 0.04448158\n",
      "Iteration 24329, loss = 0.04446864\n",
      "Iteration 24330, loss = 0.04445263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 24331, loss = 0.04444693\n",
      "Iteration 24332, loss = 0.04444190\n",
      "Iteration 24333, loss = 0.04443414\n",
      "Iteration 24334, loss = 0.04442553\n",
      "Iteration 24335, loss = 0.04441284\n",
      "Iteration 24336, loss = 0.04440710\n",
      "Iteration 24337, loss = 0.04440337\n",
      "Iteration 24338, loss = 0.04439510\n",
      "Iteration 24339, loss = 0.04438495\n",
      "Iteration 24340, loss = 0.04438024\n",
      "Iteration 24341, loss = 0.04436969\n",
      "Iteration 24342, loss = 0.04436598\n",
      "Iteration 24343, loss = 0.04435613\n",
      "Iteration 24344, loss = 0.04434875\n",
      "Iteration 24345, loss = 0.04434175\n",
      "Iteration 24346, loss = 0.04433851\n",
      "Iteration 24347, loss = 0.04433452\n",
      "Iteration 24348, loss = 0.04433110\n",
      "Iteration 24349, loss = 0.04432745\n",
      "Iteration 24350, loss = 0.04432384\n",
      "Iteration 24351, loss = 0.04432467\n",
      "Iteration 24352, loss = 0.04432229\n",
      "Iteration 24353, loss = 0.04431494\n",
      "Iteration 24354, loss = 0.04430937\n",
      "Iteration 24355, loss = 0.04430133\n",
      "Iteration 24356, loss = 0.04430573\n",
      "Iteration 24357, loss = 0.04430264\n",
      "Iteration 24358, loss = 0.04429924\n",
      "Iteration 24359, loss = 0.04429198\n",
      "Iteration 24360, loss = 0.04428399\n",
      "Iteration 24361, loss = 0.04428609\n",
      "Iteration 24362, loss = 0.04428756\n",
      "Iteration 24363, loss = 0.04428410\n",
      "Iteration 24364, loss = 0.04427723\n",
      "Iteration 24365, loss = 0.04427305\n",
      "Iteration 24366, loss = 0.04426658\n",
      "Iteration 24367, loss = 0.04425979\n",
      "Iteration 24368, loss = 0.04426033\n",
      "Iteration 24369, loss = 0.04426163\n",
      "Iteration 24370, loss = 0.04426068\n",
      "Iteration 24371, loss = 0.04425575\n",
      "Iteration 24372, loss = 0.04424623\n",
      "Iteration 24373, loss = 0.04424582\n",
      "Iteration 24374, loss = 0.04424464\n",
      "Iteration 24375, loss = 0.04424381\n",
      "Iteration 24376, loss = 0.04424300\n",
      "Iteration 24377, loss = 0.04423980\n",
      "Iteration 24378, loss = 0.04423196\n",
      "Iteration 24379, loss = 0.04422974\n",
      "Iteration 24380, loss = 0.04423168\n",
      "Iteration 24381, loss = 0.04422508\n",
      "Iteration 24382, loss = 0.04421852\n",
      "Iteration 24383, loss = 0.04421935\n",
      "Iteration 24384, loss = 0.04421804\n",
      "Iteration 24385, loss = 0.04421701\n",
      "Iteration 24386, loss = 0.04421347\n",
      "Iteration 24387, loss = 0.04420534\n",
      "Iteration 24388, loss = 0.04420440\n",
      "Iteration 24389, loss = 0.04420091\n",
      "Iteration 24390, loss = 0.04420355\n",
      "Iteration 24391, loss = 0.04419735\n",
      "Iteration 24392, loss = 0.04418932\n",
      "Iteration 24393, loss = 0.04418732\n",
      "Iteration 24394, loss = 0.04418420\n",
      "Iteration 24395, loss = 0.04418528\n",
      "Iteration 24396, loss = 0.04418752\n",
      "Iteration 24397, loss = 0.04418163\n",
      "Iteration 24398, loss = 0.04417050\n",
      "Iteration 24399, loss = 0.04416836\n",
      "Iteration 24400, loss = 0.04416859\n",
      "Iteration 24401, loss = 0.04416343\n",
      "Iteration 24402, loss = 0.04416300\n",
      "Iteration 24403, loss = 0.04415783\n",
      "Iteration 24404, loss = 0.04415343\n",
      "Iteration 24405, loss = 0.04415393\n",
      "Iteration 24406, loss = 0.04415715\n",
      "Iteration 24407, loss = 0.04415027\n",
      "Iteration 24408, loss = 0.04414562\n",
      "Iteration 24409, loss = 0.04414637\n",
      "Iteration 24410, loss = 0.04414667\n",
      "Iteration 24411, loss = 0.04414106\n",
      "Iteration 24412, loss = 0.04413570\n",
      "Iteration 24413, loss = 0.04413161\n",
      "Iteration 24414, loss = 0.04412664\n",
      "Iteration 24415, loss = 0.04412374\n",
      "Iteration 24416, loss = 0.04412379\n",
      "Iteration 24417, loss = 0.04412109\n",
      "Iteration 24418, loss = 0.04411619\n",
      "Iteration 24419, loss = 0.04411484\n",
      "Iteration 24420, loss = 0.04411558\n",
      "Iteration 24421, loss = 0.04411152\n",
      "Iteration 24422, loss = 0.04410955\n",
      "Iteration 24423, loss = 0.04410915\n",
      "Iteration 24424, loss = 0.04410216\n",
      "Iteration 24425, loss = 0.04410454\n",
      "Iteration 24426, loss = 0.04410577\n",
      "Iteration 24427, loss = 0.04409914\n",
      "Iteration 24428, loss = 0.04410054\n",
      "Iteration 24429, loss = 0.04410135\n",
      "Iteration 24430, loss = 0.04409101\n",
      "Iteration 24431, loss = 0.04409522\n",
      "Iteration 24432, loss = 0.04409558\n",
      "Iteration 24433, loss = 0.04409519\n",
      "Iteration 24434, loss = 0.04408914\n",
      "Iteration 24435, loss = 0.04409177\n",
      "Iteration 24436, loss = 0.04408800\n",
      "Iteration 24437, loss = 0.04408100\n",
      "Iteration 24438, loss = 0.04407969\n",
      "Iteration 24439, loss = 0.04407412\n",
      "Iteration 24440, loss = 0.04407139\n",
      "Iteration 24441, loss = 0.04407466\n",
      "Iteration 24442, loss = 0.04407150\n",
      "Iteration 24443, loss = 0.04406829\n",
      "Iteration 24444, loss = 0.04407175\n",
      "Iteration 24445, loss = 0.04406964\n",
      "Iteration 24446, loss = 0.04406467\n",
      "Iteration 24447, loss = 0.04406438\n",
      "Iteration 24448, loss = 0.04406210\n",
      "Iteration 24449, loss = 0.04406419\n",
      "Iteration 24450, loss = 0.04405838\n",
      "Iteration 24451, loss = 0.04405595\n",
      "Iteration 24452, loss = 0.04405999\n",
      "Iteration 24453, loss = 0.04405920\n",
      "Iteration 24454, loss = 0.04405644\n",
      "Iteration 24455, loss = 0.04405080\n",
      "Iteration 24456, loss = 0.04405004\n",
      "Iteration 24457, loss = 0.04405407\n",
      "Iteration 24458, loss = 0.04404763\n",
      "Iteration 24459, loss = 0.04405222\n",
      "Iteration 24460, loss = 0.04404769\n",
      "Iteration 24461, loss = 0.04404075\n",
      "Iteration 24462, loss = 0.04403962\n",
      "Iteration 24463, loss = 0.04403623\n",
      "Iteration 24464, loss = 0.04403500\n",
      "Iteration 24465, loss = 0.04402955\n",
      "Iteration 24466, loss = 0.04402565\n",
      "Iteration 24467, loss = 0.04403103\n",
      "Iteration 24468, loss = 0.04403010\n",
      "Iteration 24469, loss = 0.04402348\n",
      "Iteration 24470, loss = 0.04402098\n",
      "Iteration 24471, loss = 0.04402043\n",
      "Iteration 24472, loss = 0.04402351\n",
      "Iteration 24473, loss = 0.04402197\n",
      "Iteration 24474, loss = 0.04401522\n",
      "Iteration 24475, loss = 0.04401461\n",
      "Iteration 24476, loss = 0.04401877\n",
      "Iteration 24477, loss = 0.04401664\n",
      "Iteration 24478, loss = 0.04400852\n",
      "Iteration 24479, loss = 0.04400578\n",
      "Iteration 24480, loss = 0.04401143\n",
      "Iteration 24481, loss = 0.04401284\n",
      "Iteration 24482, loss = 0.04401197\n",
      "Iteration 24483, loss = 0.04400657\n",
      "Iteration 24484, loss = 0.04400302\n",
      "Iteration 24485, loss = 0.04401200\n",
      "Iteration 24486, loss = 0.04401212\n",
      "Iteration 24487, loss = 0.04400240\n",
      "Iteration 24488, loss = 0.04399975\n",
      "Iteration 24489, loss = 0.04399375\n",
      "Iteration 24490, loss = 0.04399115\n",
      "Iteration 24491, loss = 0.04399137\n",
      "Iteration 24492, loss = 0.04399297\n",
      "Iteration 24493, loss = 0.04399094\n",
      "Iteration 24494, loss = 0.04398759\n",
      "Iteration 24495, loss = 0.04398696\n",
      "Iteration 24496, loss = 0.04399094\n",
      "Iteration 24497, loss = 0.04399057\n",
      "Iteration 24498, loss = 0.04398125\n",
      "Iteration 24499, loss = 0.04397291\n",
      "Iteration 24500, loss = 0.04397912\n",
      "Iteration 24501, loss = 0.04398025\n",
      "Iteration 24502, loss = 0.04397459\n",
      "Iteration 24503, loss = 0.04396963\n",
      "Iteration 24504, loss = 0.04397376\n",
      "Iteration 24505, loss = 0.04397303\n",
      "Iteration 24506, loss = 0.04396890\n",
      "Iteration 24507, loss = 0.04396897\n",
      "Iteration 24508, loss = 0.04396110\n",
      "Iteration 24509, loss = 0.04396213\n",
      "Iteration 24510, loss = 0.04396210\n",
      "Iteration 24511, loss = 0.04396048\n",
      "Iteration 24512, loss = 0.04395502\n",
      "Iteration 24513, loss = 0.04395318\n",
      "Iteration 24514, loss = 0.04395340\n",
      "Iteration 24515, loss = 0.04395356\n",
      "Iteration 24516, loss = 0.04395056\n",
      "Iteration 24517, loss = 0.04394792\n",
      "Iteration 24518, loss = 0.04394404\n",
      "Iteration 24519, loss = 0.04394131\n",
      "Iteration 24520, loss = 0.04393913\n",
      "Iteration 24521, loss = 0.04393531\n",
      "Iteration 24522, loss = 0.04393416\n",
      "Iteration 24523, loss = 0.04393600\n",
      "Iteration 24524, loss = 0.04393649\n",
      "Iteration 24525, loss = 0.04393415\n",
      "Iteration 24526, loss = 0.04393073\n",
      "Iteration 24527, loss = 0.04392675\n",
      "Iteration 24528, loss = 0.04392758\n",
      "Iteration 24529, loss = 0.04392661\n",
      "Iteration 24530, loss = 0.04392216\n",
      "Iteration 24531, loss = 0.04391910\n",
      "Iteration 24532, loss = 0.04391952\n",
      "Iteration 24533, loss = 0.04391940\n",
      "Iteration 24534, loss = 0.04391638\n",
      "Iteration 24535, loss = 0.04391417\n",
      "Iteration 24536, loss = 0.04391484\n",
      "Iteration 24537, loss = 0.04391343\n",
      "Iteration 24538, loss = 0.04391133\n",
      "Iteration 24539, loss = 0.04390656\n",
      "Iteration 24540, loss = 0.04390657\n",
      "Iteration 24541, loss = 0.04390489\n",
      "Iteration 24542, loss = 0.04390546\n",
      "Iteration 24543, loss = 0.04390212\n",
      "Iteration 24544, loss = 0.04389911\n",
      "Iteration 24545, loss = 0.04389858\n",
      "Iteration 24546, loss = 0.04390214\n",
      "Iteration 24547, loss = 0.04390255\n",
      "Iteration 24548, loss = 0.04389783\n",
      "Iteration 24549, loss = 0.04389243\n",
      "Iteration 24550, loss = 0.04389322\n",
      "Iteration 24551, loss = 0.04389334\n",
      "Iteration 24552, loss = 0.04389411\n",
      "Iteration 24553, loss = 0.04388859\n",
      "Iteration 24554, loss = 0.04388487\n",
      "Iteration 24555, loss = 0.04388492\n",
      "Iteration 24556, loss = 0.04388919\n",
      "Iteration 24557, loss = 0.04388266\n",
      "Iteration 24558, loss = 0.04387837\n",
      "Iteration 24559, loss = 0.04387744\n",
      "Iteration 24560, loss = 0.04388076\n",
      "Iteration 24561, loss = 0.04388091\n",
      "Iteration 24562, loss = 0.04387484\n",
      "Iteration 24563, loss = 0.04387239\n",
      "Iteration 24564, loss = 0.04387096\n",
      "Iteration 24565, loss = 0.04387186\n",
      "Iteration 24566, loss = 0.04387044\n",
      "Iteration 24567, loss = 0.04386751\n",
      "Iteration 24568, loss = 0.04386367\n",
      "Iteration 24569, loss = 0.04386163\n",
      "Iteration 24570, loss = 0.04386516\n",
      "Iteration 24571, loss = 0.04386187\n",
      "Iteration 24572, loss = 0.04385630\n",
      "Iteration 24573, loss = 0.04385626\n",
      "Iteration 24574, loss = 0.04385544\n",
      "Iteration 24575, loss = 0.04384968\n",
      "Iteration 24576, loss = 0.04385163\n",
      "Iteration 24577, loss = 0.04385322\n",
      "Iteration 24578, loss = 0.04385080\n",
      "Iteration 24579, loss = 0.04385015\n",
      "Iteration 24580, loss = 0.04384568\n",
      "Iteration 24581, loss = 0.04384179\n",
      "Iteration 24582, loss = 0.04384582\n",
      "Iteration 24583, loss = 0.04384608\n",
      "Iteration 24584, loss = 0.04384114\n",
      "Iteration 24585, loss = 0.04384447\n",
      "Iteration 24586, loss = 0.04384142\n",
      "Iteration 24587, loss = 0.04384056\n",
      "Iteration 24588, loss = 0.04383994\n",
      "Iteration 24589, loss = 0.04383370\n",
      "Iteration 24590, loss = 0.04383363\n",
      "Iteration 24591, loss = 0.04382973\n",
      "Iteration 24592, loss = 0.04382920\n",
      "Iteration 24593, loss = 0.04382926\n",
      "Iteration 24594, loss = 0.04382425\n",
      "Iteration 24595, loss = 0.04382227\n",
      "Iteration 24596, loss = 0.04382127\n",
      "Iteration 24597, loss = 0.04381818\n",
      "Iteration 24598, loss = 0.04381833\n",
      "Iteration 24599, loss = 0.04381403\n",
      "Iteration 24600, loss = 0.04381309\n",
      "Iteration 24601, loss = 0.04381159\n",
      "Iteration 24602, loss = 0.04380931\n",
      "Iteration 24603, loss = 0.04380742\n",
      "Iteration 24604, loss = 0.04380647\n",
      "Iteration 24605, loss = 0.04380320\n",
      "Iteration 24606, loss = 0.04380213\n",
      "Iteration 24607, loss = 0.04380230\n",
      "Iteration 24608, loss = 0.04380117\n",
      "Iteration 24609, loss = 0.04379825\n",
      "Iteration 24610, loss = 0.04379514\n",
      "Iteration 24611, loss = 0.04380135\n",
      "Iteration 24612, loss = 0.04379937\n",
      "Iteration 24613, loss = 0.04380203\n",
      "Iteration 24614, loss = 0.04379760\n",
      "Iteration 24615, loss = 0.04378703\n",
      "Iteration 24616, loss = 0.04379679\n",
      "Iteration 24617, loss = 0.04379804\n",
      "Iteration 24618, loss = 0.04378984\n",
      "Iteration 24619, loss = 0.04379104\n",
      "Iteration 24620, loss = 0.04379216\n",
      "Iteration 24621, loss = 0.04378730\n",
      "Iteration 24622, loss = 0.04378179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 24623, loss = 0.04378330\n",
      "Iteration 24624, loss = 0.04378174\n",
      "Iteration 24625, loss = 0.04378559\n",
      "Iteration 24626, loss = 0.04378363\n",
      "Iteration 24627, loss = 0.04377898\n",
      "Iteration 24628, loss = 0.04377901\n",
      "Iteration 24629, loss = 0.04377419\n",
      "Iteration 24630, loss = 0.04377159\n",
      "Iteration 24631, loss = 0.04377734\n",
      "Iteration 24632, loss = 0.04377912\n",
      "Iteration 24633, loss = 0.04377037\n",
      "Iteration 24634, loss = 0.04375898\n",
      "Iteration 24635, loss = 0.04375949\n",
      "Iteration 24636, loss = 0.04376093\n",
      "Iteration 24637, loss = 0.04376176\n",
      "Iteration 24638, loss = 0.04375525\n",
      "Iteration 24639, loss = 0.04375779\n",
      "Iteration 24640, loss = 0.04375177\n",
      "Iteration 24641, loss = 0.04374974\n",
      "Iteration 24642, loss = 0.04374536\n",
      "Iteration 24643, loss = 0.04374591\n",
      "Iteration 24644, loss = 0.04374913\n",
      "Iteration 24645, loss = 0.04374756\n",
      "Iteration 24646, loss = 0.04374533\n",
      "Iteration 24647, loss = 0.04373317\n",
      "Iteration 24648, loss = 0.04374398\n",
      "Iteration 24649, loss = 0.04374966\n",
      "Iteration 24650, loss = 0.04374072\n",
      "Iteration 24651, loss = 0.04373338\n",
      "Iteration 24652, loss = 0.04373694\n",
      "Iteration 24653, loss = 0.04373843\n",
      "Iteration 24654, loss = 0.04373713\n",
      "Iteration 24655, loss = 0.04373204\n",
      "Iteration 24656, loss = 0.04372923\n",
      "Iteration 24657, loss = 0.04372700\n",
      "Iteration 24658, loss = 0.04372799\n",
      "Iteration 24659, loss = 0.04372399\n",
      "Iteration 24660, loss = 0.04372026\n",
      "Iteration 24661, loss = 0.04372038\n",
      "Iteration 24662, loss = 0.04371831\n",
      "Iteration 24663, loss = 0.04371475\n",
      "Iteration 24664, loss = 0.04371412\n",
      "Iteration 24665, loss = 0.04370991\n",
      "Iteration 24666, loss = 0.04370479\n",
      "Iteration 24667, loss = 0.04370795\n",
      "Iteration 24668, loss = 0.04370503\n",
      "Iteration 24669, loss = 0.04370294\n",
      "Iteration 24670, loss = 0.04370197\n",
      "Iteration 24671, loss = 0.04370130\n",
      "Iteration 24672, loss = 0.04370025\n",
      "Iteration 24673, loss = 0.04369989\n",
      "Iteration 24674, loss = 0.04370101\n",
      "Iteration 24675, loss = 0.04369839\n",
      "Iteration 24676, loss = 0.04369511\n",
      "Iteration 24677, loss = 0.04369441\n",
      "Iteration 24678, loss = 0.04369753\n",
      "Iteration 24679, loss = 0.04369427\n",
      "Iteration 24680, loss = 0.04369230\n",
      "Iteration 24681, loss = 0.04368658\n",
      "Iteration 24682, loss = 0.04368556\n",
      "Iteration 24683, loss = 0.04368425\n",
      "Iteration 24684, loss = 0.04368438\n",
      "Iteration 24685, loss = 0.04367877\n",
      "Iteration 24686, loss = 0.04368102\n",
      "Iteration 24687, loss = 0.04367868\n",
      "Iteration 24688, loss = 0.04367325\n",
      "Iteration 24689, loss = 0.04367170\n",
      "Iteration 24690, loss = 0.04367576\n",
      "Iteration 24691, loss = 0.04367682\n",
      "Iteration 24692, loss = 0.04367833\n",
      "Iteration 24693, loss = 0.04366995\n",
      "Iteration 24694, loss = 0.04366166\n",
      "Iteration 24695, loss = 0.04366701\n",
      "Iteration 24696, loss = 0.04366530\n",
      "Iteration 24697, loss = 0.04366059\n",
      "Iteration 24698, loss = 0.04366136\n",
      "Iteration 24699, loss = 0.04366058\n",
      "Iteration 24700, loss = 0.04365397\n",
      "Iteration 24701, loss = 0.04365368\n",
      "Iteration 24702, loss = 0.04365055\n",
      "Iteration 24703, loss = 0.04365037\n",
      "Iteration 24704, loss = 0.04364836\n",
      "Iteration 24705, loss = 0.04364441\n",
      "Iteration 24706, loss = 0.04364438\n",
      "Iteration 24707, loss = 0.04364136\n",
      "Iteration 24708, loss = 0.04364300\n",
      "Iteration 24709, loss = 0.04363917\n",
      "Iteration 24710, loss = 0.04364105\n",
      "Iteration 24711, loss = 0.04364315\n",
      "Iteration 24712, loss = 0.04363922\n",
      "Iteration 24713, loss = 0.04363916\n",
      "Iteration 24714, loss = 0.04364086\n",
      "Iteration 24715, loss = 0.04363996\n",
      "Iteration 24716, loss = 0.04363900\n",
      "Iteration 24717, loss = 0.04363656\n",
      "Iteration 24718, loss = 0.04363632\n",
      "Iteration 24719, loss = 0.04362793\n",
      "Iteration 24720, loss = 0.04362894\n",
      "Iteration 24721, loss = 0.04363526\n",
      "Iteration 24722, loss = 0.04363477\n",
      "Iteration 24723, loss = 0.04362445\n",
      "Iteration 24724, loss = 0.04361591\n",
      "Iteration 24725, loss = 0.04362680\n",
      "Iteration 24726, loss = 0.04362763\n",
      "Iteration 24727, loss = 0.04362329\n",
      "Iteration 24728, loss = 0.04361888\n",
      "Iteration 24729, loss = 0.04362119\n",
      "Iteration 24730, loss = 0.04361991\n",
      "Iteration 24731, loss = 0.04361056\n",
      "Iteration 24732, loss = 0.04360765\n",
      "Iteration 24733, loss = 0.04360886\n",
      "Iteration 24734, loss = 0.04360808\n",
      "Iteration 24735, loss = 0.04360522\n",
      "Iteration 24736, loss = 0.04360317\n",
      "Iteration 24737, loss = 0.04360382\n",
      "Iteration 24738, loss = 0.04360333\n",
      "Iteration 24739, loss = 0.04359661\n",
      "Iteration 24740, loss = 0.04359327\n",
      "Iteration 24741, loss = 0.04359726\n",
      "Iteration 24742, loss = 0.04359709\n",
      "Iteration 24743, loss = 0.04359002\n",
      "Iteration 24744, loss = 0.04359252\n",
      "Iteration 24745, loss = 0.04359017\n",
      "Iteration 24746, loss = 0.04358866\n",
      "Iteration 24747, loss = 0.04358678\n",
      "Iteration 24748, loss = 0.04358053\n",
      "Iteration 24749, loss = 0.04358301\n",
      "Iteration 24750, loss = 0.04358599\n",
      "Iteration 24751, loss = 0.04358104\n",
      "Iteration 24752, loss = 0.04357862\n",
      "Iteration 24753, loss = 0.04357666\n",
      "Iteration 24754, loss = 0.04357679\n",
      "Iteration 24755, loss = 0.04357599\n",
      "Iteration 24756, loss = 0.04357353\n",
      "Iteration 24757, loss = 0.04357002\n",
      "Iteration 24758, loss = 0.04356694\n",
      "Iteration 24759, loss = 0.04356694\n",
      "Iteration 24760, loss = 0.04356448\n",
      "Iteration 24761, loss = 0.04356208\n",
      "Iteration 24762, loss = 0.04355873\n",
      "Iteration 24763, loss = 0.04356111\n",
      "Iteration 24764, loss = 0.04355342\n",
      "Iteration 24765, loss = 0.04355343\n",
      "Iteration 24766, loss = 0.04355685\n",
      "Iteration 24767, loss = 0.04355553\n",
      "Iteration 24768, loss = 0.04355072\n",
      "Iteration 24769, loss = 0.04354534\n",
      "Iteration 24770, loss = 0.04354970\n",
      "Iteration 24771, loss = 0.04354927\n",
      "Iteration 24772, loss = 0.04354275\n",
      "Iteration 24773, loss = 0.04354239\n",
      "Iteration 24774, loss = 0.04354423\n",
      "Iteration 24775, loss = 0.04354023\n",
      "Iteration 24776, loss = 0.04354343\n",
      "Iteration 24777, loss = 0.04353711\n",
      "Iteration 24778, loss = 0.04353561\n",
      "Iteration 24779, loss = 0.04353146\n",
      "Iteration 24780, loss = 0.04353189\n",
      "Iteration 24781, loss = 0.04353190\n",
      "Iteration 24782, loss = 0.04352913\n",
      "Iteration 24783, loss = 0.04352861\n",
      "Iteration 24784, loss = 0.04352698\n",
      "Iteration 24785, loss = 0.04352406\n",
      "Iteration 24786, loss = 0.04352287\n",
      "Iteration 24787, loss = 0.04352114\n",
      "Iteration 24788, loss = 0.04352083\n",
      "Iteration 24789, loss = 0.04352544\n",
      "Iteration 24790, loss = 0.04351414\n",
      "Iteration 24791, loss = 0.04351756\n",
      "Iteration 24792, loss = 0.04352005\n",
      "Iteration 24793, loss = 0.04351828\n",
      "Iteration 24794, loss = 0.04351199\n",
      "Iteration 24795, loss = 0.04350901\n",
      "Iteration 24796, loss = 0.04350819\n",
      "Iteration 24797, loss = 0.04350595\n",
      "Iteration 24798, loss = 0.04350102\n",
      "Iteration 24799, loss = 0.04349783\n",
      "Iteration 24800, loss = 0.04349684\n",
      "Iteration 24801, loss = 0.04349732\n",
      "Iteration 24802, loss = 0.04349728\n",
      "Iteration 24803, loss = 0.04349586\n",
      "Iteration 24804, loss = 0.04349053\n",
      "Iteration 24805, loss = 0.04349458\n",
      "Iteration 24806, loss = 0.04350105\n",
      "Iteration 24807, loss = 0.04349407\n",
      "Iteration 24808, loss = 0.04348529\n",
      "Iteration 24809, loss = 0.04348601\n",
      "Iteration 24810, loss = 0.04348650\n",
      "Iteration 24811, loss = 0.04348568\n",
      "Iteration 24812, loss = 0.04348280\n",
      "Iteration 24813, loss = 0.04348008\n",
      "Iteration 24814, loss = 0.04347531\n",
      "Iteration 24815, loss = 0.04347484\n",
      "Iteration 24816, loss = 0.04347767\n",
      "Iteration 24817, loss = 0.04347919\n",
      "Iteration 24818, loss = 0.04347155\n",
      "Iteration 24819, loss = 0.04346481\n",
      "Iteration 24820, loss = 0.04346756\n",
      "Iteration 24821, loss = 0.04346696\n",
      "Iteration 24822, loss = 0.04346069\n",
      "Iteration 24823, loss = 0.04345810\n",
      "Iteration 24824, loss = 0.04345804\n",
      "Iteration 24825, loss = 0.04345372\n",
      "Iteration 24826, loss = 0.04345662\n",
      "Iteration 24827, loss = 0.04345244\n",
      "Iteration 24828, loss = 0.04345421\n",
      "Iteration 24829, loss = 0.04344950\n",
      "Iteration 24830, loss = 0.04344995\n",
      "Iteration 24831, loss = 0.04344693\n",
      "Iteration 24832, loss = 0.04344917\n",
      "Iteration 24833, loss = 0.04344768\n",
      "Iteration 24834, loss = 0.04344367\n",
      "Iteration 24835, loss = 0.04344280\n",
      "Iteration 24836, loss = 0.04343739\n",
      "Iteration 24837, loss = 0.04343734\n",
      "Iteration 24838, loss = 0.04343445\n",
      "Iteration 24839, loss = 0.04343100\n",
      "Iteration 24840, loss = 0.04343321\n",
      "Iteration 24841, loss = 0.04342949\n",
      "Iteration 24842, loss = 0.04342530\n",
      "Iteration 24843, loss = 0.04343025\n",
      "Iteration 24844, loss = 0.04342612\n",
      "Iteration 24845, loss = 0.04342630\n",
      "Iteration 24846, loss = 0.04342458\n",
      "Iteration 24847, loss = 0.04342474\n",
      "Iteration 24848, loss = 0.04342594\n",
      "Iteration 24849, loss = 0.04342016\n",
      "Iteration 24850, loss = 0.04342270\n",
      "Iteration 24851, loss = 0.04342334\n",
      "Iteration 24852, loss = 0.04341329\n",
      "Iteration 24853, loss = 0.04341803\n",
      "Iteration 24854, loss = 0.04341957\n",
      "Iteration 24855, loss = 0.04341190\n",
      "Iteration 24856, loss = 0.04341513\n",
      "Iteration 24857, loss = 0.04341360\n",
      "Iteration 24858, loss = 0.04340535\n",
      "Iteration 24859, loss = 0.04340689\n",
      "Iteration 24860, loss = 0.04340388\n",
      "Iteration 24861, loss = 0.04340142\n",
      "Iteration 24862, loss = 0.04340031\n",
      "Iteration 24863, loss = 0.04339543\n",
      "Iteration 24864, loss = 0.04339193\n",
      "Iteration 24865, loss = 0.04339220\n",
      "Iteration 24866, loss = 0.04339403\n",
      "Iteration 24867, loss = 0.04339214\n",
      "Iteration 24868, loss = 0.04339082\n",
      "Iteration 24869, loss = 0.04338758\n",
      "Iteration 24870, loss = 0.04338812\n",
      "Iteration 24871, loss = 0.04338414\n",
      "Iteration 24872, loss = 0.04337331\n",
      "Iteration 24873, loss = 0.04338160\n",
      "Iteration 24874, loss = 0.04338617\n",
      "Iteration 24875, loss = 0.04338031\n",
      "Iteration 24876, loss = 0.04337413\n",
      "Iteration 24877, loss = 0.04337392\n",
      "Iteration 24878, loss = 0.04337064\n",
      "Iteration 24879, loss = 0.04336360\n",
      "Iteration 24880, loss = 0.04336378\n",
      "Iteration 24881, loss = 0.04336421\n",
      "Iteration 24882, loss = 0.04336523\n",
      "Iteration 24883, loss = 0.04336645\n",
      "Iteration 24884, loss = 0.04336378\n",
      "Iteration 24885, loss = 0.04336315\n",
      "Iteration 24886, loss = 0.04335602\n",
      "Iteration 24887, loss = 0.04335622\n",
      "Iteration 24888, loss = 0.04335962\n",
      "Iteration 24889, loss = 0.04335586\n",
      "Iteration 24890, loss = 0.04334846\n",
      "Iteration 24891, loss = 0.04335190\n",
      "Iteration 24892, loss = 0.04335387\n",
      "Iteration 24893, loss = 0.04335227\n",
      "Iteration 24894, loss = 0.04334504\n",
      "Iteration 24895, loss = 0.04334294\n",
      "Iteration 24896, loss = 0.04333820\n",
      "Iteration 24897, loss = 0.04334160\n",
      "Iteration 24898, loss = 0.04334311\n",
      "Iteration 24899, loss = 0.04334100\n",
      "Iteration 24900, loss = 0.04333608\n",
      "Iteration 24901, loss = 0.04333029\n",
      "Iteration 24902, loss = 0.04333049\n",
      "Iteration 24903, loss = 0.04332926\n",
      "Iteration 24904, loss = 0.04332510\n",
      "Iteration 24905, loss = 0.04331929\n",
      "Iteration 24906, loss = 0.04331881\n",
      "Iteration 24907, loss = 0.04331583\n",
      "Iteration 24908, loss = 0.04332061\n",
      "Iteration 24909, loss = 0.04331794\n",
      "Iteration 24910, loss = 0.04331037\n",
      "Iteration 24911, loss = 0.04330909\n",
      "Iteration 24912, loss = 0.04330859\n",
      "Iteration 24913, loss = 0.04331199\n",
      "Iteration 24914, loss = 0.04330922\n",
      "Iteration 24915, loss = 0.04330744\n",
      "Iteration 24916, loss = 0.04330779\n",
      "Iteration 24917, loss = 0.04330752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 24918, loss = 0.04330316\n",
      "Iteration 24919, loss = 0.04330092\n",
      "Iteration 24920, loss = 0.04330405\n",
      "Iteration 24921, loss = 0.04329870\n",
      "Iteration 24922, loss = 0.04329177\n",
      "Iteration 24923, loss = 0.04329714\n",
      "Iteration 24924, loss = 0.04329620\n",
      "Iteration 24925, loss = 0.04329172\n",
      "Iteration 24926, loss = 0.04328811\n",
      "Iteration 24927, loss = 0.04328450\n",
      "Iteration 24928, loss = 0.04328202\n",
      "Iteration 24929, loss = 0.04327869\n",
      "Iteration 24930, loss = 0.04327790\n",
      "Iteration 24931, loss = 0.04327882\n",
      "Iteration 24932, loss = 0.04327405\n",
      "Iteration 24933, loss = 0.04327415\n",
      "Iteration 24934, loss = 0.04327650\n",
      "Iteration 24935, loss = 0.04327346\n",
      "Iteration 24936, loss = 0.04327206\n",
      "Iteration 24937, loss = 0.04326199\n",
      "Iteration 24938, loss = 0.04327023\n",
      "Iteration 24939, loss = 0.04327094\n",
      "Iteration 24940, loss = 0.04326435\n",
      "Iteration 24941, loss = 0.04326479\n",
      "Iteration 24942, loss = 0.04326478\n",
      "Iteration 24943, loss = 0.04326063\n",
      "Iteration 24944, loss = 0.04326319\n",
      "Iteration 24945, loss = 0.04326196\n",
      "Iteration 24946, loss = 0.04325237\n",
      "Iteration 24947, loss = 0.04324810\n",
      "Iteration 24948, loss = 0.04325220\n",
      "Iteration 24949, loss = 0.04324855\n",
      "Iteration 24950, loss = 0.04324556\n",
      "Iteration 24951, loss = 0.04324322\n",
      "Iteration 24952, loss = 0.04324498\n",
      "Iteration 24953, loss = 0.04324539\n",
      "Iteration 24954, loss = 0.04323603\n",
      "Iteration 24955, loss = 0.04323397\n",
      "Iteration 24956, loss = 0.04323650\n",
      "Iteration 24957, loss = 0.04323817\n",
      "Iteration 24958, loss = 0.04323438\n",
      "Iteration 24959, loss = 0.04322992\n",
      "Iteration 24960, loss = 0.04322559\n",
      "Iteration 24961, loss = 0.04322894\n",
      "Iteration 24962, loss = 0.04322645\n",
      "Iteration 24963, loss = 0.04322138\n",
      "Iteration 24964, loss = 0.04321802\n",
      "Iteration 24965, loss = 0.04321998\n",
      "Iteration 24966, loss = 0.04322273\n",
      "Iteration 24967, loss = 0.04322062\n",
      "Iteration 24968, loss = 0.04321126\n",
      "Iteration 24969, loss = 0.04320843\n",
      "Iteration 24970, loss = 0.04321121\n",
      "Iteration 24971, loss = 0.04321119\n",
      "Iteration 24972, loss = 0.04320798\n",
      "Iteration 24973, loss = 0.04320434\n",
      "Iteration 24974, loss = 0.04320595\n",
      "Iteration 24975, loss = 0.04320018\n",
      "Iteration 24976, loss = 0.04319765\n",
      "Iteration 24977, loss = 0.04319719\n",
      "Iteration 24978, loss = 0.04319507\n",
      "Iteration 24979, loss = 0.04319460\n",
      "Iteration 24980, loss = 0.04318501\n",
      "Iteration 24981, loss = 0.04318232\n",
      "Iteration 24982, loss = 0.04318591\n",
      "Iteration 24983, loss = 0.04318309\n",
      "Iteration 24984, loss = 0.04317872\n",
      "Iteration 24985, loss = 0.04318135\n",
      "Iteration 24986, loss = 0.04317830\n",
      "Iteration 24987, loss = 0.04317682\n",
      "Iteration 24988, loss = 0.04318017\n",
      "Iteration 24989, loss = 0.04317532\n",
      "Iteration 24990, loss = 0.04317279\n",
      "Iteration 24991, loss = 0.04317397\n",
      "Iteration 24992, loss = 0.04316889\n",
      "Iteration 24993, loss = 0.04317070\n",
      "Iteration 24994, loss = 0.04317351\n",
      "Iteration 24995, loss = 0.04317070\n",
      "Iteration 24996, loss = 0.04316041\n",
      "Iteration 24997, loss = 0.04316588\n",
      "Iteration 24998, loss = 0.04317075\n",
      "Iteration 24999, loss = 0.04316803\n",
      "Iteration 25000, loss = 0.04316982\n",
      "Iteration 25001, loss = 0.04316506\n",
      "Iteration 25002, loss = 0.04316403\n",
      "Iteration 25003, loss = 0.04316113\n",
      "Iteration 25004, loss = 0.04315449\n",
      "Iteration 25005, loss = 0.04315161\n",
      "Iteration 25006, loss = 0.04314773\n",
      "Iteration 25007, loss = 0.04314909\n",
      "Iteration 25008, loss = 0.04314790\n",
      "Iteration 25009, loss = 0.04314799\n",
      "Iteration 25010, loss = 0.04314044\n",
      "Iteration 25011, loss = 0.04313881\n",
      "Iteration 25012, loss = 0.04314153\n",
      "Iteration 25013, loss = 0.04314231\n",
      "Iteration 25014, loss = 0.04313837\n",
      "Iteration 25015, loss = 0.04313780\n",
      "Iteration 25016, loss = 0.04313069\n",
      "Iteration 25017, loss = 0.04313071\n",
      "Iteration 25018, loss = 0.04312539\n",
      "Iteration 25019, loss = 0.04312235\n",
      "Iteration 25020, loss = 0.04312650\n",
      "Iteration 25021, loss = 0.04312176\n",
      "Iteration 25022, loss = 0.04311607\n",
      "Iteration 25023, loss = 0.04311536\n",
      "Iteration 25024, loss = 0.04311181\n",
      "Iteration 25025, loss = 0.04311110\n",
      "Iteration 25026, loss = 0.04311279\n",
      "Iteration 25027, loss = 0.04310792\n",
      "Iteration 25028, loss = 0.04311159\n",
      "Iteration 25029, loss = 0.04310566\n",
      "Iteration 25030, loss = 0.04310513\n",
      "Iteration 25031, loss = 0.04310651\n",
      "Iteration 25032, loss = 0.04310200\n",
      "Iteration 25033, loss = 0.04310519\n",
      "Iteration 25034, loss = 0.04309902\n",
      "Iteration 25035, loss = 0.04309784\n",
      "Iteration 25036, loss = 0.04310037\n",
      "Iteration 25037, loss = 0.04310090\n",
      "Iteration 25038, loss = 0.04309487\n",
      "Iteration 25039, loss = 0.04309185\n",
      "Iteration 25040, loss = 0.04308866\n",
      "Iteration 25041, loss = 0.04308858\n",
      "Iteration 25042, loss = 0.04308609\n",
      "Iteration 25043, loss = 0.04308250\n",
      "Iteration 25044, loss = 0.04307890\n",
      "Iteration 25045, loss = 0.04307931\n",
      "Iteration 25046, loss = 0.04308080\n",
      "Iteration 25047, loss = 0.04308134\n",
      "Iteration 25048, loss = 0.04307894\n",
      "Iteration 25049, loss = 0.04307079\n",
      "Iteration 25050, loss = 0.04306890\n",
      "Iteration 25051, loss = 0.04307125\n",
      "Iteration 25052, loss = 0.04306712\n",
      "Iteration 25053, loss = 0.04305891\n",
      "Iteration 25054, loss = 0.04306500\n",
      "Iteration 25055, loss = 0.04306134\n",
      "Iteration 25056, loss = 0.04305961\n",
      "Iteration 25057, loss = 0.04305885\n",
      "Iteration 25058, loss = 0.04305429\n",
      "Iteration 25059, loss = 0.04305367\n",
      "Iteration 25060, loss = 0.04305078\n",
      "Iteration 25061, loss = 0.04304978\n",
      "Iteration 25062, loss = 0.04304829\n",
      "Iteration 25063, loss = 0.04304480\n",
      "Iteration 25064, loss = 0.04304721\n",
      "Iteration 25065, loss = 0.04304454\n",
      "Iteration 25066, loss = 0.04304037\n",
      "Iteration 25067, loss = 0.04303891\n",
      "Iteration 25068, loss = 0.04303537\n",
      "Iteration 25069, loss = 0.04303701\n",
      "Iteration 25070, loss = 0.04303625\n",
      "Iteration 25071, loss = 0.04303397\n",
      "Iteration 25072, loss = 0.04302801\n",
      "Iteration 25073, loss = 0.04302987\n",
      "Iteration 25074, loss = 0.04302537\n",
      "Iteration 25075, loss = 0.04302775\n",
      "Iteration 25076, loss = 0.04302712\n",
      "Iteration 25077, loss = 0.04302158\n",
      "Iteration 25078, loss = 0.04301752\n",
      "Iteration 25079, loss = 0.04301693\n",
      "Iteration 25080, loss = 0.04301761\n",
      "Iteration 25081, loss = 0.04301790\n",
      "Iteration 25082, loss = 0.04301474\n",
      "Iteration 25083, loss = 0.04301616\n",
      "Iteration 25084, loss = 0.04301236\n",
      "Iteration 25085, loss = 0.04301023\n",
      "Iteration 25086, loss = 0.04301191\n",
      "Iteration 25087, loss = 0.04300913\n",
      "Iteration 25088, loss = 0.04301144\n",
      "Iteration 25089, loss = 0.04301052\n",
      "Iteration 25090, loss = 0.04300120\n",
      "Iteration 25091, loss = 0.04300446\n",
      "Iteration 25092, loss = 0.04300896\n",
      "Iteration 25093, loss = 0.04301016\n",
      "Iteration 25094, loss = 0.04300255\n",
      "Iteration 25095, loss = 0.04299873\n",
      "Iteration 25096, loss = 0.04299795\n",
      "Iteration 25097, loss = 0.04299882\n",
      "Iteration 25098, loss = 0.04299424\n",
      "Iteration 25099, loss = 0.04298699\n",
      "Iteration 25100, loss = 0.04298194\n",
      "Iteration 25101, loss = 0.04298337\n",
      "Iteration 25102, loss = 0.04298515\n",
      "Iteration 25103, loss = 0.04298012\n",
      "Iteration 25104, loss = 0.04298391\n",
      "Iteration 25105, loss = 0.04298459\n",
      "Iteration 25106, loss = 0.04298447\n",
      "Iteration 25107, loss = 0.04298390\n",
      "Iteration 25108, loss = 0.04297948\n",
      "Iteration 25109, loss = 0.04297112\n",
      "Iteration 25110, loss = 0.04296842\n",
      "Iteration 25111, loss = 0.04297154\n",
      "Iteration 25112, loss = 0.04297225\n",
      "Iteration 25113, loss = 0.04296403\n",
      "Iteration 25114, loss = 0.04295457\n",
      "Iteration 25115, loss = 0.04295835\n",
      "Iteration 25116, loss = 0.04295860\n",
      "Iteration 25117, loss = 0.04295775\n",
      "Iteration 25118, loss = 0.04295838\n",
      "Iteration 25119, loss = 0.04295864\n",
      "Iteration 25120, loss = 0.04295474\n",
      "Iteration 25121, loss = 0.04295204\n",
      "Iteration 25122, loss = 0.04295156\n",
      "Iteration 25123, loss = 0.04295054\n",
      "Iteration 25124, loss = 0.04294706\n",
      "Iteration 25125, loss = 0.04294143\n",
      "Iteration 25126, loss = 0.04294230\n",
      "Iteration 25127, loss = 0.04294258\n",
      "Iteration 25128, loss = 0.04294065\n",
      "Iteration 25129, loss = 0.04293703\n",
      "Iteration 25130, loss = 0.04293070\n",
      "Iteration 25131, loss = 0.04293023\n",
      "Iteration 25132, loss = 0.04292919\n",
      "Iteration 25133, loss = 0.04292750\n",
      "Iteration 25134, loss = 0.04292070\n",
      "Iteration 25135, loss = 0.04292459\n",
      "Iteration 25136, loss = 0.04292593\n",
      "Iteration 25137, loss = 0.04291952\n",
      "Iteration 25138, loss = 0.04291523\n",
      "Iteration 25139, loss = 0.04292185\n",
      "Iteration 25140, loss = 0.04292206\n",
      "Iteration 25141, loss = 0.04291979\n",
      "Iteration 25142, loss = 0.04291580\n",
      "Iteration 25143, loss = 0.04290634\n",
      "Iteration 25144, loss = 0.04290314\n",
      "Iteration 25145, loss = 0.04290614\n",
      "Iteration 25146, loss = 0.04290528\n",
      "Iteration 25147, loss = 0.04290884\n",
      "Iteration 25148, loss = 0.04290337\n",
      "Iteration 25149, loss = 0.04289539\n",
      "Iteration 25150, loss = 0.04289564\n",
      "Iteration 25151, loss = 0.04289309\n",
      "Iteration 25152, loss = 0.04289493\n",
      "Iteration 25153, loss = 0.04289799\n",
      "Iteration 25154, loss = 0.04288971\n",
      "Iteration 25155, loss = 0.04288591\n",
      "Iteration 25156, loss = 0.04288648\n",
      "Iteration 25157, loss = 0.04288420\n",
      "Iteration 25158, loss = 0.04288305\n",
      "Iteration 25159, loss = 0.04288440\n",
      "Iteration 25160, loss = 0.04288557\n",
      "Iteration 25161, loss = 0.04288352\n",
      "Iteration 25162, loss = 0.04287559\n",
      "Iteration 25163, loss = 0.04287588\n",
      "Iteration 25164, loss = 0.04287700\n",
      "Iteration 25165, loss = 0.04287475\n",
      "Iteration 25166, loss = 0.04287386\n",
      "Iteration 25167, loss = 0.04287645\n",
      "Iteration 25168, loss = 0.04287472\n",
      "Iteration 25169, loss = 0.04287128\n",
      "Iteration 25170, loss = 0.04287051\n",
      "Iteration 25171, loss = 0.04286421\n",
      "Iteration 25172, loss = 0.04286521\n",
      "Iteration 25173, loss = 0.04286716\n",
      "Iteration 25174, loss = 0.04286408\n",
      "Iteration 25175, loss = 0.04286499\n",
      "Iteration 25176, loss = 0.04286564\n",
      "Iteration 25177, loss = 0.04285707\n",
      "Iteration 25178, loss = 0.04285146\n",
      "Iteration 25179, loss = 0.04285189\n",
      "Iteration 25180, loss = 0.04285125\n",
      "Iteration 25181, loss = 0.04284548\n",
      "Iteration 25182, loss = 0.04284549\n",
      "Iteration 25183, loss = 0.04284337\n",
      "Iteration 25184, loss = 0.04284157\n",
      "Iteration 25185, loss = 0.04284166\n",
      "Iteration 25186, loss = 0.04283956\n",
      "Iteration 25187, loss = 0.04283214\n",
      "Iteration 25188, loss = 0.04283553\n",
      "Iteration 25189, loss = 0.04283646\n",
      "Iteration 25190, loss = 0.04283038\n",
      "Iteration 25191, loss = 0.04283176\n",
      "Iteration 25192, loss = 0.04283002\n",
      "Iteration 25193, loss = 0.04282614\n",
      "Iteration 25194, loss = 0.04282712\n",
      "Iteration 25195, loss = 0.04283187\n",
      "Iteration 25196, loss = 0.04282833\n",
      "Iteration 25197, loss = 0.04282505\n",
      "Iteration 25198, loss = 0.04281937\n",
      "Iteration 25199, loss = 0.04281784\n",
      "Iteration 25200, loss = 0.04281908\n",
      "Iteration 25201, loss = 0.04281136\n",
      "Iteration 25202, loss = 0.04281025\n",
      "Iteration 25203, loss = 0.04280871\n",
      "Iteration 25204, loss = 0.04280599\n",
      "Iteration 25205, loss = 0.04280539\n",
      "Iteration 25206, loss = 0.04280567\n",
      "Iteration 25207, loss = 0.04280056\n",
      "Iteration 25208, loss = 0.04279864\n",
      "Iteration 25209, loss = 0.04279829\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 25210, loss = 0.04279606\n",
      "Iteration 25211, loss = 0.04279348\n",
      "Iteration 25212, loss = 0.04278732\n",
      "Iteration 25213, loss = 0.04278992\n",
      "Iteration 25214, loss = 0.04278769\n",
      "Iteration 25215, loss = 0.04278612\n",
      "Iteration 25216, loss = 0.04278514\n",
      "Iteration 25217, loss = 0.04278349\n",
      "Iteration 25218, loss = 0.04278254\n",
      "Iteration 25219, loss = 0.04278175\n",
      "Iteration 25220, loss = 0.04277485\n",
      "Iteration 25221, loss = 0.04277449\n",
      "Iteration 25222, loss = 0.04277722\n",
      "Iteration 25223, loss = 0.04277536\n",
      "Iteration 25224, loss = 0.04277277\n",
      "Iteration 25225, loss = 0.04277224\n",
      "Iteration 25226, loss = 0.04276610\n",
      "Iteration 25227, loss = 0.04276734\n",
      "Iteration 25228, loss = 0.04277641\n",
      "Iteration 25229, loss = 0.04277221\n",
      "Iteration 25230, loss = 0.04276543\n",
      "Iteration 25231, loss = 0.04276788\n",
      "Iteration 25232, loss = 0.04276589\n",
      "Iteration 25233, loss = 0.04276286\n",
      "Iteration 25234, loss = 0.04276111\n",
      "Iteration 25235, loss = 0.04276062\n",
      "Iteration 25236, loss = 0.04275614\n",
      "Iteration 25237, loss = 0.04275540\n",
      "Iteration 25238, loss = 0.04275669\n",
      "Iteration 25239, loss = 0.04275173\n",
      "Iteration 25240, loss = 0.04275498\n",
      "Iteration 25241, loss = 0.04275299\n",
      "Iteration 25242, loss = 0.04274708\n",
      "Iteration 25243, loss = 0.04275101\n",
      "Iteration 25244, loss = 0.04274998\n",
      "Iteration 25245, loss = 0.04274202\n",
      "Iteration 25246, loss = 0.04273543\n",
      "Iteration 25247, loss = 0.04273822\n",
      "Iteration 25248, loss = 0.04273897\n",
      "Iteration 25249, loss = 0.04273798\n",
      "Iteration 25250, loss = 0.04273412\n",
      "Iteration 25251, loss = 0.04273328\n",
      "Iteration 25252, loss = 0.04272984\n",
      "Iteration 25253, loss = 0.04272829\n",
      "Iteration 25254, loss = 0.04271747\n",
      "Iteration 25255, loss = 0.04272127\n",
      "Iteration 25256, loss = 0.04272479\n",
      "Iteration 25257, loss = 0.04272341\n",
      "Iteration 25258, loss = 0.04271791\n",
      "Iteration 25259, loss = 0.04271312\n",
      "Iteration 25260, loss = 0.04271528\n",
      "Iteration 25261, loss = 0.04271539\n",
      "Iteration 25262, loss = 0.04271281\n",
      "Iteration 25263, loss = 0.04270995\n",
      "Iteration 25264, loss = 0.04271257\n",
      "Iteration 25265, loss = 0.04271074\n",
      "Iteration 25266, loss = 0.04270715\n",
      "Iteration 25267, loss = 0.04270264\n",
      "Iteration 25268, loss = 0.04270687\n",
      "Iteration 25269, loss = 0.04270271\n",
      "Iteration 25270, loss = 0.04269196\n",
      "Iteration 25271, loss = 0.04269482\n",
      "Iteration 25272, loss = 0.04269709\n",
      "Iteration 25273, loss = 0.04269935\n",
      "Iteration 25274, loss = 0.04269284\n",
      "Iteration 25275, loss = 0.04269158\n",
      "Iteration 25276, loss = 0.04269202\n",
      "Iteration 25277, loss = 0.04269010\n",
      "Iteration 25278, loss = 0.04268615\n",
      "Iteration 25279, loss = 0.04268507\n",
      "Iteration 25280, loss = 0.04268455\n",
      "Iteration 25281, loss = 0.04268758\n",
      "Iteration 25282, loss = 0.04268514\n",
      "Iteration 25283, loss = 0.04267708\n",
      "Iteration 25284, loss = 0.04267327\n",
      "Iteration 25285, loss = 0.04267787\n",
      "Iteration 25286, loss = 0.04268045\n",
      "Iteration 25287, loss = 0.04267772\n",
      "Iteration 25288, loss = 0.04267073\n",
      "Iteration 25289, loss = 0.04266171\n",
      "Iteration 25290, loss = 0.04266932\n",
      "Iteration 25291, loss = 0.04267449\n",
      "Iteration 25292, loss = 0.04267187\n",
      "Iteration 25293, loss = 0.04266441\n",
      "Iteration 25294, loss = 0.04266398\n",
      "Iteration 25295, loss = 0.04266038\n",
      "Iteration 25296, loss = 0.04265317\n",
      "Iteration 25297, loss = 0.04265889\n",
      "Iteration 25298, loss = 0.04265989\n",
      "Iteration 25299, loss = 0.04265612\n",
      "Iteration 25300, loss = 0.04265192\n",
      "Iteration 25301, loss = 0.04264540\n",
      "Iteration 25302, loss = 0.04264236\n",
      "Iteration 25303, loss = 0.04264640\n",
      "Iteration 25304, loss = 0.04264751\n",
      "Iteration 25305, loss = 0.04264517\n",
      "Iteration 25306, loss = 0.04263924\n",
      "Iteration 25307, loss = 0.04263357\n",
      "Iteration 25308, loss = 0.04263389\n",
      "Iteration 25309, loss = 0.04262955\n",
      "Iteration 25310, loss = 0.04263583\n",
      "Iteration 25311, loss = 0.04263243\n",
      "Iteration 25312, loss = 0.04262891\n",
      "Iteration 25313, loss = 0.04262965\n",
      "Iteration 25314, loss = 0.04262957\n",
      "Iteration 25315, loss = 0.04263010\n",
      "Iteration 25316, loss = 0.04262135\n",
      "Iteration 25317, loss = 0.04262277\n",
      "Iteration 25318, loss = 0.04262371\n",
      "Iteration 25319, loss = 0.04262432\n",
      "Iteration 25320, loss = 0.04261483\n",
      "Iteration 25321, loss = 0.04261256\n",
      "Iteration 25322, loss = 0.04261521\n",
      "Iteration 25323, loss = 0.04261465\n",
      "Iteration 25324, loss = 0.04261035\n",
      "Iteration 25325, loss = 0.04260471\n",
      "Iteration 25326, loss = 0.04260197\n",
      "Iteration 25327, loss = 0.04260327\n",
      "Iteration 25328, loss = 0.04259889\n",
      "Iteration 25329, loss = 0.04259911\n",
      "Iteration 25330, loss = 0.04259579\n",
      "Iteration 25331, loss = 0.04259695\n",
      "Iteration 25332, loss = 0.04259532\n",
      "Iteration 25333, loss = 0.04259303\n",
      "Iteration 25334, loss = 0.04259352\n",
      "Iteration 25335, loss = 0.04259013\n",
      "Iteration 25336, loss = 0.04258533\n",
      "Iteration 25337, loss = 0.04258837\n",
      "Iteration 25338, loss = 0.04258812\n",
      "Iteration 25339, loss = 0.04258050\n",
      "Iteration 25340, loss = 0.04258511\n",
      "Iteration 25341, loss = 0.04258718\n",
      "Iteration 25342, loss = 0.04258605\n",
      "Iteration 25343, loss = 0.04258046\n",
      "Iteration 25344, loss = 0.04258001\n",
      "Iteration 25345, loss = 0.04257395\n",
      "Iteration 25346, loss = 0.04257265\n",
      "Iteration 25347, loss = 0.04257690\n",
      "Iteration 25348, loss = 0.04257286\n",
      "Iteration 25349, loss = 0.04257122\n",
      "Iteration 25350, loss = 0.04256839\n",
      "Iteration 25351, loss = 0.04256995\n",
      "Iteration 25352, loss = 0.04257339\n",
      "Iteration 25353, loss = 0.04257214\n",
      "Iteration 25354, loss = 0.04257156\n",
      "Iteration 25355, loss = 0.04257493\n",
      "Iteration 25356, loss = 0.04257104\n",
      "Iteration 25357, loss = 0.04255912\n",
      "Iteration 25358, loss = 0.04254839\n",
      "Iteration 25359, loss = 0.04256153\n",
      "Iteration 25360, loss = 0.04256526\n",
      "Iteration 25361, loss = 0.04256346\n",
      "Iteration 25362, loss = 0.04255533\n",
      "Iteration 25363, loss = 0.04254953\n",
      "Iteration 25364, loss = 0.04255138\n",
      "Iteration 25365, loss = 0.04254851\n",
      "Iteration 25366, loss = 0.04254373\n",
      "Iteration 25367, loss = 0.04254582\n",
      "Iteration 25368, loss = 0.04253820\n",
      "Iteration 25369, loss = 0.04253907\n",
      "Iteration 25370, loss = 0.04254006\n",
      "Iteration 25371, loss = 0.04253628\n",
      "Iteration 25372, loss = 0.04253465\n",
      "Iteration 25373, loss = 0.04253119\n",
      "Iteration 25374, loss = 0.04252233\n",
      "Iteration 25375, loss = 0.04252508\n",
      "Iteration 25376, loss = 0.04252376\n",
      "Iteration 25377, loss = 0.04252356\n",
      "Iteration 25378, loss = 0.04251549\n",
      "Iteration 25379, loss = 0.04251657\n",
      "Iteration 25380, loss = 0.04252042\n",
      "Iteration 25381, loss = 0.04251754\n",
      "Iteration 25382, loss = 0.04251575\n",
      "Iteration 25383, loss = 0.04251518\n",
      "Iteration 25384, loss = 0.04251543\n",
      "Iteration 25385, loss = 0.04250732\n",
      "Iteration 25386, loss = 0.04251132\n",
      "Iteration 25387, loss = 0.04251092\n",
      "Iteration 25388, loss = 0.04250094\n",
      "Iteration 25389, loss = 0.04250358\n",
      "Iteration 25390, loss = 0.04250422\n",
      "Iteration 25391, loss = 0.04250507\n",
      "Iteration 25392, loss = 0.04249842\n",
      "Iteration 25393, loss = 0.04249459\n",
      "Iteration 25394, loss = 0.04249127\n",
      "Iteration 25395, loss = 0.04249365\n",
      "Iteration 25396, loss = 0.04249157\n",
      "Iteration 25397, loss = 0.04249044\n",
      "Iteration 25398, loss = 0.04248846\n",
      "Iteration 25399, loss = 0.04248781\n",
      "Iteration 25400, loss = 0.04249269\n",
      "Iteration 25401, loss = 0.04249054\n",
      "Iteration 25402, loss = 0.04248739\n",
      "Iteration 25403, loss = 0.04248440\n",
      "Iteration 25404, loss = 0.04247760\n",
      "Iteration 25405, loss = 0.04247428\n",
      "Iteration 25406, loss = 0.04247785\n",
      "Iteration 25407, loss = 0.04247686\n",
      "Iteration 25408, loss = 0.04247028\n",
      "Iteration 25409, loss = 0.04247211\n",
      "Iteration 25410, loss = 0.04247194\n",
      "Iteration 25411, loss = 0.04247011\n",
      "Iteration 25412, loss = 0.04247032\n",
      "Iteration 25413, loss = 0.04246621\n",
      "Iteration 25414, loss = 0.04246269\n",
      "Iteration 25415, loss = 0.04245763\n",
      "Iteration 25416, loss = 0.04245647\n",
      "Iteration 25417, loss = 0.04245739\n",
      "Iteration 25418, loss = 0.04245723\n",
      "Iteration 25419, loss = 0.04245015\n",
      "Iteration 25420, loss = 0.04245251\n",
      "Iteration 25421, loss = 0.04245145\n",
      "Iteration 25422, loss = 0.04245391\n",
      "Iteration 25423, loss = 0.04245233\n",
      "Iteration 25424, loss = 0.04245098\n",
      "Iteration 25425, loss = 0.04244578\n",
      "Iteration 25426, loss = 0.04244274\n",
      "Iteration 25427, loss = 0.04243813\n",
      "Iteration 25428, loss = 0.04243882\n",
      "Iteration 25429, loss = 0.04243655\n",
      "Iteration 25430, loss = 0.04243774\n",
      "Iteration 25431, loss = 0.04243260\n",
      "Iteration 25432, loss = 0.04243020\n",
      "Iteration 25433, loss = 0.04243296\n",
      "Iteration 25434, loss = 0.04243059\n",
      "Iteration 25435, loss = 0.04242535\n",
      "Iteration 25436, loss = 0.04242685\n",
      "Iteration 25437, loss = 0.04242960\n",
      "Iteration 25438, loss = 0.04242962\n",
      "Iteration 25439, loss = 0.04241987\n",
      "Iteration 25440, loss = 0.04242115\n",
      "Iteration 25441, loss = 0.04242256\n",
      "Iteration 25442, loss = 0.04241950\n",
      "Iteration 25443, loss = 0.04242260\n",
      "Iteration 25444, loss = 0.04241925\n",
      "Iteration 25445, loss = 0.04241430\n",
      "Iteration 25446, loss = 0.04241169\n",
      "Iteration 25447, loss = 0.04241419\n",
      "Iteration 25448, loss = 0.04240688\n",
      "Iteration 25449, loss = 0.04241014\n",
      "Iteration 25450, loss = 0.04241028\n",
      "Iteration 25451, loss = 0.04241100\n",
      "Iteration 25452, loss = 0.04240555\n",
      "Iteration 25453, loss = 0.04239830\n",
      "Iteration 25454, loss = 0.04239227\n",
      "Iteration 25455, loss = 0.04239675\n",
      "Iteration 25456, loss = 0.04240438\n",
      "Iteration 25457, loss = 0.04239905\n",
      "Iteration 25458, loss = 0.04238552\n",
      "Iteration 25459, loss = 0.04239507\n",
      "Iteration 25460, loss = 0.04239863\n",
      "Iteration 25461, loss = 0.04239867\n",
      "Iteration 25462, loss = 0.04239702\n",
      "Iteration 25463, loss = 0.04239574\n",
      "Iteration 25464, loss = 0.04239516\n",
      "Iteration 25465, loss = 0.04238809\n",
      "Iteration 25466, loss = 0.04238609\n",
      "Iteration 25467, loss = 0.04238206\n",
      "Iteration 25468, loss = 0.04238126\n",
      "Iteration 25469, loss = 0.04238012\n",
      "Iteration 25470, loss = 0.04237710\n",
      "Iteration 25471, loss = 0.04237240\n",
      "Iteration 25472, loss = 0.04236565\n",
      "Iteration 25473, loss = 0.04236940\n",
      "Iteration 25474, loss = 0.04237224\n",
      "Iteration 25475, loss = 0.04236648\n",
      "Iteration 25476, loss = 0.04235916\n",
      "Iteration 25477, loss = 0.04235971\n",
      "Iteration 25478, loss = 0.04235703\n",
      "Iteration 25479, loss = 0.04235833\n",
      "Iteration 25480, loss = 0.04235374\n",
      "Iteration 25481, loss = 0.04235297\n",
      "Iteration 25482, loss = 0.04235151\n",
      "Iteration 25483, loss = 0.04235355\n",
      "Iteration 25484, loss = 0.04235421\n",
      "Iteration 25485, loss = 0.04234759\n",
      "Iteration 25486, loss = 0.04234404\n",
      "Iteration 25487, loss = 0.04234597\n",
      "Iteration 25488, loss = 0.04235102\n",
      "Iteration 25489, loss = 0.04234971\n",
      "Iteration 25490, loss = 0.04234034\n",
      "Iteration 25491, loss = 0.04233757\n",
      "Iteration 25492, loss = 0.04233584\n",
      "Iteration 25493, loss = 0.04233586\n",
      "Iteration 25494, loss = 0.04233221\n",
      "Iteration 25495, loss = 0.04233265\n",
      "Iteration 25496, loss = 0.04233169\n",
      "Iteration 25497, loss = 0.04233098\n",
      "Iteration 25498, loss = 0.04232528\n",
      "Iteration 25499, loss = 0.04232547\n",
      "Iteration 25500, loss = 0.04231961\n",
      "Iteration 25501, loss = 0.04231855\n",
      "Iteration 25502, loss = 0.04232069\n",
      "Iteration 25503, loss = 0.04231685\n",
      "Iteration 25504, loss = 0.04231896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 25505, loss = 0.04231697\n",
      "Iteration 25506, loss = 0.04231761\n",
      "Iteration 25507, loss = 0.04231736\n",
      "Iteration 25508, loss = 0.04231552\n",
      "Iteration 25509, loss = 0.04231601\n",
      "Iteration 25510, loss = 0.04231276\n",
      "Iteration 25511, loss = 0.04230720\n",
      "Iteration 25512, loss = 0.04230330\n",
      "Iteration 25513, loss = 0.04229878\n",
      "Iteration 25514, loss = 0.04230073\n",
      "Iteration 25515, loss = 0.04229856\n",
      "Iteration 25516, loss = 0.04229526\n",
      "Iteration 25517, loss = 0.04229168\n",
      "Iteration 25518, loss = 0.04229512\n",
      "Iteration 25519, loss = 0.04229702\n",
      "Iteration 25520, loss = 0.04229531\n",
      "Iteration 25521, loss = 0.04228693\n",
      "Iteration 25522, loss = 0.04228610\n",
      "Iteration 25523, loss = 0.04228562\n",
      "Iteration 25524, loss = 0.04228214\n",
      "Iteration 25525, loss = 0.04227983\n",
      "Iteration 25526, loss = 0.04228358\n",
      "Iteration 25527, loss = 0.04227975\n",
      "Iteration 25528, loss = 0.04227628\n",
      "Iteration 25529, loss = 0.04227261\n",
      "Iteration 25530, loss = 0.04227605\n",
      "Iteration 25531, loss = 0.04227956\n",
      "Iteration 25532, loss = 0.04227308\n",
      "Iteration 25533, loss = 0.04226992\n",
      "Iteration 25534, loss = 0.04226929\n",
      "Iteration 25535, loss = 0.04226685\n",
      "Iteration 25536, loss = 0.04227071\n",
      "Iteration 25537, loss = 0.04226801\n",
      "Iteration 25538, loss = 0.04226460\n",
      "Iteration 25539, loss = 0.04226043\n",
      "Iteration 25540, loss = 0.04225607\n",
      "Iteration 25541, loss = 0.04225632\n",
      "Iteration 25542, loss = 0.04225438\n",
      "Iteration 25543, loss = 0.04225210\n",
      "Iteration 25544, loss = 0.04225221\n",
      "Iteration 25545, loss = 0.04225276\n",
      "Iteration 25546, loss = 0.04224703\n",
      "Iteration 25547, loss = 0.04224812\n",
      "Iteration 25548, loss = 0.04224528\n",
      "Iteration 25549, loss = 0.04224324\n",
      "Iteration 25550, loss = 0.04224424\n",
      "Iteration 25551, loss = 0.04224152\n",
      "Iteration 25552, loss = 0.04223940\n",
      "Iteration 25553, loss = 0.04223717\n",
      "Iteration 25554, loss = 0.04223819\n",
      "Iteration 25555, loss = 0.04223794\n",
      "Iteration 25556, loss = 0.04223508\n",
      "Iteration 25557, loss = 0.04223774\n",
      "Iteration 25558, loss = 0.04223928\n",
      "Iteration 25559, loss = 0.04223539\n",
      "Iteration 25560, loss = 0.04223199\n",
      "Iteration 25561, loss = 0.04223001\n",
      "Iteration 25562, loss = 0.04222474\n",
      "Iteration 25563, loss = 0.04222778\n",
      "Iteration 25564, loss = 0.04223059\n",
      "Iteration 25565, loss = 0.04221933\n",
      "Iteration 25566, loss = 0.04222354\n",
      "Iteration 25567, loss = 0.04222675\n",
      "Iteration 25568, loss = 0.04221997\n",
      "Iteration 25569, loss = 0.04221929\n",
      "Iteration 25570, loss = 0.04221487\n",
      "Iteration 25571, loss = 0.04221959\n",
      "Iteration 25572, loss = 0.04222262\n",
      "Iteration 25573, loss = 0.04221422\n",
      "Iteration 25574, loss = 0.04220937\n",
      "Iteration 25575, loss = 0.04221197\n",
      "Iteration 25576, loss = 0.04221729\n",
      "Iteration 25577, loss = 0.04221292\n",
      "Iteration 25578, loss = 0.04220867\n",
      "Iteration 25579, loss = 0.04220531\n",
      "Iteration 25580, loss = 0.04219838\n",
      "Iteration 25581, loss = 0.04220350\n",
      "Iteration 25582, loss = 0.04220417\n",
      "Iteration 25583, loss = 0.04219875\n",
      "Iteration 25584, loss = 0.04218979\n",
      "Iteration 25585, loss = 0.04218903\n",
      "Iteration 25586, loss = 0.04218897\n",
      "Iteration 25587, loss = 0.04218847\n",
      "Iteration 25588, loss = 0.04218652\n",
      "Iteration 25589, loss = 0.04218696\n",
      "Iteration 25590, loss = 0.04218119\n",
      "Iteration 25591, loss = 0.04217640\n",
      "Iteration 25592, loss = 0.04218190\n",
      "Iteration 25593, loss = 0.04218366\n",
      "Iteration 25594, loss = 0.04217398\n",
      "Iteration 25595, loss = 0.04217032\n",
      "Iteration 25596, loss = 0.04217370\n",
      "Iteration 25597, loss = 0.04217340\n",
      "Iteration 25598, loss = 0.04217351\n",
      "Iteration 25599, loss = 0.04217134\n",
      "Iteration 25600, loss = 0.04216996\n",
      "Iteration 25601, loss = 0.04217177\n",
      "Iteration 25602, loss = 0.04216939\n",
      "Iteration 25603, loss = 0.04216693\n",
      "Iteration 25604, loss = 0.04216172\n",
      "Iteration 25605, loss = 0.04216027\n",
      "Iteration 25606, loss = 0.04216183\n",
      "Iteration 25607, loss = 0.04216103\n",
      "Iteration 25608, loss = 0.04215529\n",
      "Iteration 25609, loss = 0.04215457\n",
      "Iteration 25610, loss = 0.04215340\n",
      "Iteration 25611, loss = 0.04215384\n",
      "Iteration 25612, loss = 0.04214905\n",
      "Iteration 25613, loss = 0.04214974\n",
      "Iteration 25614, loss = 0.04214614\n",
      "Iteration 25615, loss = 0.04214152\n",
      "Iteration 25616, loss = 0.04214351\n",
      "Iteration 25617, loss = 0.04214011\n",
      "Iteration 25618, loss = 0.04213858\n",
      "Iteration 25619, loss = 0.04213526\n",
      "Iteration 25620, loss = 0.04213574\n",
      "Iteration 25621, loss = 0.04213504\n",
      "Iteration 25622, loss = 0.04213033\n",
      "Iteration 25623, loss = 0.04213217\n",
      "Iteration 25624, loss = 0.04213179\n",
      "Iteration 25625, loss = 0.04213033\n",
      "Iteration 25626, loss = 0.04212629\n",
      "Iteration 25627, loss = 0.04212359\n",
      "Iteration 25628, loss = 0.04212347\n",
      "Iteration 25629, loss = 0.04212314\n",
      "Iteration 25630, loss = 0.04211882\n",
      "Iteration 25631, loss = 0.04211519\n",
      "Iteration 25632, loss = 0.04211274\n",
      "Iteration 25633, loss = 0.04211341\n",
      "Iteration 25634, loss = 0.04211490\n",
      "Iteration 25635, loss = 0.04211036\n",
      "Iteration 25636, loss = 0.04210990\n",
      "Iteration 25637, loss = 0.04210996\n",
      "Iteration 25638, loss = 0.04210839\n",
      "Iteration 25639, loss = 0.04210839\n",
      "Iteration 25640, loss = 0.04210106\n",
      "Iteration 25641, loss = 0.04209995\n",
      "Iteration 25642, loss = 0.04210687\n",
      "Iteration 25643, loss = 0.04210396\n",
      "Iteration 25644, loss = 0.04209432\n",
      "Iteration 25645, loss = 0.04209717\n",
      "Iteration 25646, loss = 0.04209557\n",
      "Iteration 25647, loss = 0.04209000\n",
      "Iteration 25648, loss = 0.04209108\n",
      "Iteration 25649, loss = 0.04208720\n",
      "Iteration 25650, loss = 0.04208670\n",
      "Iteration 25651, loss = 0.04209242\n",
      "Iteration 25652, loss = 0.04209278\n",
      "Iteration 25653, loss = 0.04208487\n",
      "Iteration 25654, loss = 0.04208500\n",
      "Iteration 25655, loss = 0.04208531\n",
      "Iteration 25656, loss = 0.04207910\n",
      "Iteration 25657, loss = 0.04208343\n",
      "Iteration 25658, loss = 0.04208323\n",
      "Iteration 25659, loss = 0.04207534\n",
      "Iteration 25660, loss = 0.04207195\n",
      "Iteration 25661, loss = 0.04207300\n",
      "Iteration 25662, loss = 0.04207393\n",
      "Iteration 25663, loss = 0.04207252\n",
      "Iteration 25664, loss = 0.04206921\n",
      "Iteration 25665, loss = 0.04206377\n",
      "Iteration 25666, loss = 0.04206337\n",
      "Iteration 25667, loss = 0.04207118\n",
      "Iteration 25668, loss = 0.04206897\n",
      "Iteration 25669, loss = 0.04206324\n",
      "Iteration 25670, loss = 0.04206406\n",
      "Iteration 25671, loss = 0.04206031\n",
      "Iteration 25672, loss = 0.04206204\n",
      "Iteration 25673, loss = 0.04206493\n",
      "Iteration 25674, loss = 0.04206156\n",
      "Iteration 25675, loss = 0.04205160\n",
      "Iteration 25676, loss = 0.04204954\n",
      "Iteration 25677, loss = 0.04205116\n",
      "Iteration 25678, loss = 0.04204910\n",
      "Iteration 25679, loss = 0.04204372\n",
      "Iteration 25680, loss = 0.04204226\n",
      "Iteration 25681, loss = 0.04204217\n",
      "Iteration 25682, loss = 0.04204410\n",
      "Iteration 25683, loss = 0.04203906\n",
      "Iteration 25684, loss = 0.04203144\n",
      "Iteration 25685, loss = 0.04203800\n",
      "Iteration 25686, loss = 0.04204116\n",
      "Iteration 25687, loss = 0.04203246\n",
      "Iteration 25688, loss = 0.04203055\n",
      "Iteration 25689, loss = 0.04203128\n",
      "Iteration 25690, loss = 0.04202924\n",
      "Iteration 25691, loss = 0.04202968\n",
      "Iteration 25692, loss = 0.04202346\n",
      "Iteration 25693, loss = 0.04201881\n",
      "Iteration 25694, loss = 0.04201834\n",
      "Iteration 25695, loss = 0.04201998\n",
      "Iteration 25696, loss = 0.04201806\n",
      "Iteration 25697, loss = 0.04201517\n",
      "Iteration 25698, loss = 0.04201463\n",
      "Iteration 25699, loss = 0.04201013\n",
      "Iteration 25700, loss = 0.04201182\n",
      "Iteration 25701, loss = 0.04201020\n",
      "Iteration 25702, loss = 0.04200466\n",
      "Iteration 25703, loss = 0.04200705\n",
      "Iteration 25704, loss = 0.04200678\n",
      "Iteration 25705, loss = 0.04200329\n",
      "Iteration 25706, loss = 0.04200211\n",
      "Iteration 25707, loss = 0.04199746\n",
      "Iteration 25708, loss = 0.04200067\n",
      "Iteration 25709, loss = 0.04200204\n",
      "Iteration 25710, loss = 0.04200021\n",
      "Iteration 25711, loss = 0.04199547\n",
      "Iteration 25712, loss = 0.04199453\n",
      "Iteration 25713, loss = 0.04199795\n",
      "Iteration 25714, loss = 0.04199667\n",
      "Iteration 25715, loss = 0.04199032\n",
      "Iteration 25716, loss = 0.04198894\n",
      "Iteration 25717, loss = 0.04198949\n",
      "Iteration 25718, loss = 0.04198712\n",
      "Iteration 25719, loss = 0.04198571\n",
      "Iteration 25720, loss = 0.04198118\n",
      "Iteration 25721, loss = 0.04197630\n",
      "Iteration 25722, loss = 0.04197854\n",
      "Iteration 25723, loss = 0.04197817\n",
      "Iteration 25724, loss = 0.04197488\n",
      "Iteration 25725, loss = 0.04196966\n",
      "Iteration 25726, loss = 0.04197159\n",
      "Iteration 25727, loss = 0.04197445\n",
      "Iteration 25728, loss = 0.04197197\n",
      "Iteration 25729, loss = 0.04196952\n",
      "Iteration 25730, loss = 0.04196702\n",
      "Iteration 25731, loss = 0.04196100\n",
      "Iteration 25732, loss = 0.04196157\n",
      "Iteration 25733, loss = 0.04196828\n",
      "Iteration 25734, loss = 0.04196405\n",
      "Iteration 25735, loss = 0.04196046\n",
      "Iteration 25736, loss = 0.04195270\n",
      "Iteration 25737, loss = 0.04195891\n",
      "Iteration 25738, loss = 0.04196047\n",
      "Iteration 25739, loss = 0.04195762\n",
      "Iteration 25740, loss = 0.04195826\n",
      "Iteration 25741, loss = 0.04195447\n",
      "Iteration 25742, loss = 0.04195202\n",
      "Iteration 25743, loss = 0.04194326\n",
      "Iteration 25744, loss = 0.04194742\n",
      "Iteration 25745, loss = 0.04195219\n",
      "Iteration 25746, loss = 0.04195168\n",
      "Iteration 25747, loss = 0.04194961\n",
      "Iteration 25748, loss = 0.04193849\n",
      "Iteration 25749, loss = 0.04193674\n",
      "Iteration 25750, loss = 0.04194089\n",
      "Iteration 25751, loss = 0.04193608\n",
      "Iteration 25752, loss = 0.04193415\n",
      "Iteration 25753, loss = 0.04193533\n",
      "Iteration 25754, loss = 0.04192806\n",
      "Iteration 25755, loss = 0.04193075\n",
      "Iteration 25756, loss = 0.04193365\n",
      "Iteration 25757, loss = 0.04192978\n",
      "Iteration 25758, loss = 0.04192112\n",
      "Iteration 25759, loss = 0.04191712\n",
      "Iteration 25760, loss = 0.04191620\n",
      "Iteration 25761, loss = 0.04191853\n",
      "Iteration 25762, loss = 0.04191650\n",
      "Iteration 25763, loss = 0.04191201\n",
      "Iteration 25764, loss = 0.04191073\n",
      "Iteration 25765, loss = 0.04190838\n",
      "Iteration 25766, loss = 0.04190578\n",
      "Iteration 25767, loss = 0.04190738\n",
      "Iteration 25768, loss = 0.04190756\n",
      "Iteration 25769, loss = 0.04190274\n",
      "Iteration 25770, loss = 0.04190590\n",
      "Iteration 25771, loss = 0.04190066\n",
      "Iteration 25772, loss = 0.04190273\n",
      "Iteration 25773, loss = 0.04190455\n",
      "Iteration 25774, loss = 0.04190224\n",
      "Iteration 25775, loss = 0.04189654\n",
      "Iteration 25776, loss = 0.04189327\n",
      "Iteration 25777, loss = 0.04189490\n",
      "Iteration 25778, loss = 0.04189562\n",
      "Iteration 25779, loss = 0.04189135\n",
      "Iteration 25780, loss = 0.04188653\n",
      "Iteration 25781, loss = 0.04188769\n",
      "Iteration 25782, loss = 0.04188448\n",
      "Iteration 25783, loss = 0.04188322\n",
      "Iteration 25784, loss = 0.04187837\n",
      "Iteration 25785, loss = 0.04188261\n",
      "Iteration 25786, loss = 0.04188281\n",
      "Iteration 25787, loss = 0.04187332\n",
      "Iteration 25788, loss = 0.04187310\n",
      "Iteration 25789, loss = 0.04187515\n",
      "Iteration 25790, loss = 0.04187559\n",
      "Iteration 25791, loss = 0.04187063\n",
      "Iteration 25792, loss = 0.04187008\n",
      "Iteration 25793, loss = 0.04186522\n",
      "Iteration 25794, loss = 0.04186848\n",
      "Iteration 25795, loss = 0.04186482\n",
      "Iteration 25796, loss = 0.04185902\n",
      "Iteration 25797, loss = 0.04185926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 25798, loss = 0.04185948\n",
      "Iteration 25799, loss = 0.04185822\n",
      "Iteration 25800, loss = 0.04185726\n",
      "Iteration 25801, loss = 0.04185398\n",
      "Iteration 25802, loss = 0.04184564\n",
      "Iteration 25803, loss = 0.04185391\n",
      "Iteration 25804, loss = 0.04185702\n",
      "Iteration 25805, loss = 0.04184999\n",
      "Iteration 25806, loss = 0.04184382\n",
      "Iteration 25807, loss = 0.04184482\n",
      "Iteration 25808, loss = 0.04184618\n",
      "Iteration 25809, loss = 0.04184443\n",
      "Iteration 25810, loss = 0.04184690\n",
      "Iteration 25811, loss = 0.04184423\n",
      "Iteration 25812, loss = 0.04183563\n",
      "Iteration 25813, loss = 0.04183301\n",
      "Iteration 25814, loss = 0.04183110\n",
      "Iteration 25815, loss = 0.04182890\n",
      "Iteration 25816, loss = 0.04182961\n",
      "Iteration 25817, loss = 0.04182778\n",
      "Iteration 25818, loss = 0.04182574\n",
      "Iteration 25819, loss = 0.04181961\n",
      "Iteration 25820, loss = 0.04182844\n",
      "Iteration 25821, loss = 0.04183218\n",
      "Iteration 25822, loss = 0.04182364\n",
      "Iteration 25823, loss = 0.04181451\n",
      "Iteration 25824, loss = 0.04182007\n",
      "Iteration 25825, loss = 0.04182205\n",
      "Iteration 25826, loss = 0.04181441\n",
      "Iteration 25827, loss = 0.04181256\n",
      "Iteration 25828, loss = 0.04181130\n",
      "Iteration 25829, loss = 0.04180822\n",
      "Iteration 25830, loss = 0.04180458\n",
      "Iteration 25831, loss = 0.04180211\n",
      "Iteration 25832, loss = 0.04180352\n",
      "Iteration 25833, loss = 0.04179716\n",
      "Iteration 25834, loss = 0.04179874\n",
      "Iteration 25835, loss = 0.04179983\n",
      "Iteration 25836, loss = 0.04179837\n",
      "Iteration 25837, loss = 0.04178941\n",
      "Iteration 25838, loss = 0.04179643\n",
      "Iteration 25839, loss = 0.04179517\n",
      "Iteration 25840, loss = 0.04179309\n",
      "Iteration 25841, loss = 0.04179118\n",
      "Iteration 25842, loss = 0.04178908\n",
      "Iteration 25843, loss = 0.04179280\n",
      "Iteration 25844, loss = 0.04178905\n",
      "Iteration 25845, loss = 0.04179028\n",
      "Iteration 25846, loss = 0.04178672\n",
      "Iteration 25847, loss = 0.04177853\n",
      "Iteration 25848, loss = 0.04178852\n",
      "Iteration 25849, loss = 0.04179027\n",
      "Iteration 25850, loss = 0.04177967\n",
      "Iteration 25851, loss = 0.04177302\n",
      "Iteration 25852, loss = 0.04177652\n",
      "Iteration 25853, loss = 0.04178390\n",
      "Iteration 25854, loss = 0.04177810\n",
      "Iteration 25855, loss = 0.04176827\n",
      "Iteration 25856, loss = 0.04176920\n",
      "Iteration 25857, loss = 0.04177237\n",
      "Iteration 25858, loss = 0.04177183\n",
      "Iteration 25859, loss = 0.04176414\n",
      "Iteration 25860, loss = 0.04176067\n",
      "Iteration 25861, loss = 0.04175885\n",
      "Iteration 25862, loss = 0.04175655\n",
      "Iteration 25863, loss = 0.04174874\n",
      "Iteration 25864, loss = 0.04175626\n",
      "Iteration 25865, loss = 0.04175551\n",
      "Iteration 25866, loss = 0.04175214\n",
      "Iteration 25867, loss = 0.04174875\n",
      "Iteration 25868, loss = 0.04174853\n",
      "Iteration 25869, loss = 0.04174871\n",
      "Iteration 25870, loss = 0.04174268\n",
      "Iteration 25871, loss = 0.04174292\n",
      "Iteration 25872, loss = 0.04174305\n",
      "Iteration 25873, loss = 0.04173298\n",
      "Iteration 25874, loss = 0.04173851\n",
      "Iteration 25875, loss = 0.04174190\n",
      "Iteration 25876, loss = 0.04174447\n",
      "Iteration 25877, loss = 0.04174462\n",
      "Iteration 25878, loss = 0.04173789\n",
      "Iteration 25879, loss = 0.04173283\n",
      "Iteration 25880, loss = 0.04173497\n",
      "Iteration 25881, loss = 0.04173884\n",
      "Iteration 25882, loss = 0.04173731\n",
      "Iteration 25883, loss = 0.04173473\n",
      "Iteration 25884, loss = 0.04173168\n",
      "Iteration 25885, loss = 0.04172393\n",
      "Iteration 25886, loss = 0.04172075\n",
      "Iteration 25887, loss = 0.04172099\n",
      "Iteration 25888, loss = 0.04171927\n",
      "Iteration 25889, loss = 0.04171488\n",
      "Iteration 25890, loss = 0.04171116\n",
      "Iteration 25891, loss = 0.04171264\n",
      "Iteration 25892, loss = 0.04170753\n",
      "Iteration 25893, loss = 0.04170644\n",
      "Iteration 25894, loss = 0.04170334\n",
      "Iteration 25895, loss = 0.04170212\n",
      "Iteration 25896, loss = 0.04169792\n",
      "Iteration 25897, loss = 0.04170114\n",
      "Iteration 25898, loss = 0.04170348\n",
      "Iteration 25899, loss = 0.04170116\n",
      "Iteration 25900, loss = 0.04169746\n",
      "Iteration 25901, loss = 0.04169242\n",
      "Iteration 25902, loss = 0.04169481\n",
      "Iteration 25903, loss = 0.04169181\n",
      "Iteration 25904, loss = 0.04168535\n",
      "Iteration 25905, loss = 0.04168481\n",
      "Iteration 25906, loss = 0.04168080\n",
      "Iteration 25907, loss = 0.04168142\n",
      "Iteration 25908, loss = 0.04167914\n",
      "Iteration 25909, loss = 0.04168046\n",
      "Iteration 25910, loss = 0.04167671\n",
      "Iteration 25911, loss = 0.04167719\n",
      "Iteration 25912, loss = 0.04167659\n",
      "Iteration 25913, loss = 0.04167515\n",
      "Iteration 25914, loss = 0.04167241\n",
      "Iteration 25915, loss = 0.04167187\n",
      "Iteration 25916, loss = 0.04166940\n",
      "Iteration 25917, loss = 0.04166464\n",
      "Iteration 25918, loss = 0.04166846\n",
      "Iteration 25919, loss = 0.04166447\n",
      "Iteration 25920, loss = 0.04166286\n",
      "Iteration 25921, loss = 0.04166336\n",
      "Iteration 25922, loss = 0.04166222\n",
      "Iteration 25923, loss = 0.04166293\n",
      "Iteration 25924, loss = 0.04165660\n",
      "Iteration 25925, loss = 0.04165292\n",
      "Iteration 25926, loss = 0.04165570\n",
      "Iteration 25927, loss = 0.04165363\n",
      "Iteration 25928, loss = 0.04165320\n",
      "Iteration 25929, loss = 0.04165048\n",
      "Iteration 25930, loss = 0.04164718\n",
      "Iteration 25931, loss = 0.04164875\n",
      "Iteration 25932, loss = 0.04164646\n",
      "Iteration 25933, loss = 0.04164990\n",
      "Iteration 25934, loss = 0.04164511\n",
      "Iteration 25935, loss = 0.04164345\n",
      "Iteration 25936, loss = 0.04164374\n",
      "Iteration 25937, loss = 0.04164011\n",
      "Iteration 25938, loss = 0.04163985\n",
      "Iteration 25939, loss = 0.04163716\n",
      "Iteration 25940, loss = 0.04163943\n",
      "Iteration 25941, loss = 0.04163723\n",
      "Iteration 25942, loss = 0.04163122\n",
      "Iteration 25943, loss = 0.04162801\n",
      "Iteration 25944, loss = 0.04163222\n",
      "Iteration 25945, loss = 0.04162765\n",
      "Iteration 25946, loss = 0.04162270\n",
      "Iteration 25947, loss = 0.04162507\n",
      "Iteration 25948, loss = 0.04162572\n",
      "Iteration 25949, loss = 0.04162005\n",
      "Iteration 25950, loss = 0.04161314\n",
      "Iteration 25951, loss = 0.04161379\n",
      "Iteration 25952, loss = 0.04161447\n",
      "Iteration 25953, loss = 0.04161443\n",
      "Iteration 25954, loss = 0.04160995\n",
      "Iteration 25955, loss = 0.04160702\n",
      "Iteration 25956, loss = 0.04160626\n",
      "Iteration 25957, loss = 0.04160367\n",
      "Iteration 25958, loss = 0.04159898\n",
      "Iteration 25959, loss = 0.04160418\n",
      "Iteration 25960, loss = 0.04160171\n",
      "Iteration 25961, loss = 0.04159807\n",
      "Iteration 25962, loss = 0.04160164\n",
      "Iteration 25963, loss = 0.04159672\n",
      "Iteration 25964, loss = 0.04159472\n",
      "Iteration 25965, loss = 0.04159431\n",
      "Iteration 25966, loss = 0.04159590\n",
      "Iteration 25967, loss = 0.04159468\n",
      "Iteration 25968, loss = 0.04158933\n",
      "Iteration 25969, loss = 0.04158518\n",
      "Iteration 25970, loss = 0.04158532\n",
      "Iteration 25971, loss = 0.04158938\n",
      "Iteration 25972, loss = 0.04158631\n",
      "Iteration 25973, loss = 0.04158219\n",
      "Iteration 25974, loss = 0.04157821\n",
      "Iteration 25975, loss = 0.04157872\n",
      "Iteration 25976, loss = 0.04157503\n",
      "Iteration 25977, loss = 0.04157438\n",
      "Iteration 25978, loss = 0.04157888\n",
      "Iteration 25979, loss = 0.04157627\n",
      "Iteration 25980, loss = 0.04156922\n",
      "Iteration 25981, loss = 0.04157592\n",
      "Iteration 25982, loss = 0.04157382\n",
      "Iteration 25983, loss = 0.04157361\n",
      "Iteration 25984, loss = 0.04157362\n",
      "Iteration 25985, loss = 0.04156705\n",
      "Iteration 25986, loss = 0.04156058\n",
      "Iteration 25987, loss = 0.04156518\n",
      "Iteration 25988, loss = 0.04156543\n",
      "Iteration 25989, loss = 0.04156447\n",
      "Iteration 25990, loss = 0.04155579\n",
      "Iteration 25991, loss = 0.04155049\n",
      "Iteration 25992, loss = 0.04155772\n",
      "Iteration 25993, loss = 0.04155684\n",
      "Iteration 25994, loss = 0.04155541\n",
      "Iteration 25995, loss = 0.04155544\n",
      "Iteration 25996, loss = 0.04155438\n",
      "Iteration 25997, loss = 0.04154651\n",
      "Iteration 25998, loss = 0.04153811\n",
      "Iteration 25999, loss = 0.04154459\n",
      "Iteration 26000, loss = 0.04154826\n",
      "Iteration 26001, loss = 0.04154867\n",
      "Iteration 26002, loss = 0.04154588\n",
      "Iteration 26003, loss = 0.04153518\n",
      "Iteration 26004, loss = 0.04153801\n",
      "Iteration 26005, loss = 0.04153580\n",
      "Iteration 26006, loss = 0.04153396\n",
      "Iteration 26007, loss = 0.04153384\n",
      "Iteration 26008, loss = 0.04153091\n",
      "Iteration 26009, loss = 0.04152135\n",
      "Iteration 26010, loss = 0.04152316\n",
      "Iteration 26011, loss = 0.04153314\n",
      "Iteration 26012, loss = 0.04153494\n",
      "Iteration 26013, loss = 0.04152576\n",
      "Iteration 26014, loss = 0.04151441\n",
      "Iteration 26015, loss = 0.04151940\n",
      "Iteration 26016, loss = 0.04151608\n",
      "Iteration 26017, loss = 0.04151309\n",
      "Iteration 26018, loss = 0.04151064\n",
      "Iteration 26019, loss = 0.04151210\n",
      "Iteration 26020, loss = 0.04150697\n",
      "Iteration 26021, loss = 0.04150315\n",
      "Iteration 26022, loss = 0.04150581\n",
      "Iteration 26023, loss = 0.04150364\n",
      "Iteration 26024, loss = 0.04150887\n",
      "Iteration 26025, loss = 0.04150349\n",
      "Iteration 26026, loss = 0.04149498\n",
      "Iteration 26027, loss = 0.04149865\n",
      "Iteration 26028, loss = 0.04149494\n",
      "Iteration 26029, loss = 0.04149104\n",
      "Iteration 26030, loss = 0.04149325\n",
      "Iteration 26031, loss = 0.04148985\n",
      "Iteration 26032, loss = 0.04148923\n",
      "Iteration 26033, loss = 0.04148888\n",
      "Iteration 26034, loss = 0.04148266\n",
      "Iteration 26035, loss = 0.04148534\n",
      "Iteration 26036, loss = 0.04148443\n",
      "Iteration 26037, loss = 0.04148062\n",
      "Iteration 26038, loss = 0.04147908\n",
      "Iteration 26039, loss = 0.04147503\n",
      "Iteration 26040, loss = 0.04147618\n",
      "Iteration 26041, loss = 0.04147264\n",
      "Iteration 26042, loss = 0.04147162\n",
      "Iteration 26043, loss = 0.04146811\n",
      "Iteration 26044, loss = 0.04146993\n",
      "Iteration 26045, loss = 0.04146740\n",
      "Iteration 26046, loss = 0.04146515\n",
      "Iteration 26047, loss = 0.04146365\n",
      "Iteration 26048, loss = 0.04146333\n",
      "Iteration 26049, loss = 0.04145955\n",
      "Iteration 26050, loss = 0.04145714\n",
      "Iteration 26051, loss = 0.04145781\n",
      "Iteration 26052, loss = 0.04145427\n",
      "Iteration 26053, loss = 0.04145539\n",
      "Iteration 26054, loss = 0.04145367\n",
      "Iteration 26055, loss = 0.04145783\n",
      "Iteration 26056, loss = 0.04145259\n",
      "Iteration 26057, loss = 0.04145093\n",
      "Iteration 26058, loss = 0.04145648\n",
      "Iteration 26059, loss = 0.04145548\n",
      "Iteration 26060, loss = 0.04145049\n",
      "Iteration 26061, loss = 0.04144091\n",
      "Iteration 26062, loss = 0.04144044\n",
      "Iteration 26063, loss = 0.04144596\n",
      "Iteration 26064, loss = 0.04144446\n",
      "Iteration 26065, loss = 0.04144282\n",
      "Iteration 26066, loss = 0.04143689\n",
      "Iteration 26067, loss = 0.04143284\n",
      "Iteration 26068, loss = 0.04143665\n",
      "Iteration 26069, loss = 0.04144030\n",
      "Iteration 26070, loss = 0.04143539\n",
      "Iteration 26071, loss = 0.04143141\n",
      "Iteration 26072, loss = 0.04142647\n",
      "Iteration 26073, loss = 0.04142968\n",
      "Iteration 26074, loss = 0.04143191\n",
      "Iteration 26075, loss = 0.04142699\n",
      "Iteration 26076, loss = 0.04142508\n",
      "Iteration 26077, loss = 0.04142274\n",
      "Iteration 26078, loss = 0.04142207\n",
      "Iteration 26079, loss = 0.04141892\n",
      "Iteration 26080, loss = 0.04141863\n",
      "Iteration 26081, loss = 0.04141435\n",
      "Iteration 26082, loss = 0.04141413\n",
      "Iteration 26083, loss = 0.04141804\n",
      "Iteration 26084, loss = 0.04141097\n",
      "Iteration 26085, loss = 0.04141174\n",
      "Iteration 26086, loss = 0.04141352\n",
      "Iteration 26087, loss = 0.04140863\n",
      "Iteration 26088, loss = 0.04140051\n",
      "Iteration 26089, loss = 0.04139837\n",
      "Iteration 26090, loss = 0.04140019\n",
      "Iteration 26091, loss = 0.04140090\n",
      "Iteration 26092, loss = 0.04139786\n",
      "Iteration 26093, loss = 0.04139822\n",
      "Iteration 26094, loss = 0.04139640\n",
      "Iteration 26095, loss = 0.04139591\n",
      "Iteration 26096, loss = 0.04139236\n",
      "Iteration 26097, loss = 0.04139367\n",
      "Iteration 26098, loss = 0.04139066\n",
      "Iteration 26099, loss = 0.04138409\n",
      "Iteration 26100, loss = 0.04138851\n",
      "Iteration 26101, loss = 0.04139206\n",
      "Iteration 26102, loss = 0.04138640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 26103, loss = 0.04137765\n",
      "Iteration 26104, loss = 0.04138687\n",
      "Iteration 26105, loss = 0.04138629\n",
      "Iteration 26106, loss = 0.04137631\n",
      "Iteration 26107, loss = 0.04137716\n",
      "Iteration 26108, loss = 0.04137886\n",
      "Iteration 26109, loss = 0.04137283\n",
      "Iteration 26110, loss = 0.04137062\n",
      "Iteration 26111, loss = 0.04137609\n",
      "Iteration 26112, loss = 0.04136982\n",
      "Iteration 26113, loss = 0.04136829\n",
      "Iteration 26114, loss = 0.04136595\n",
      "Iteration 26115, loss = 0.04136587\n",
      "Iteration 26116, loss = 0.04136086\n",
      "Iteration 26117, loss = 0.04135577\n",
      "Iteration 26118, loss = 0.04135244\n",
      "Iteration 26119, loss = 0.04135763\n",
      "Iteration 26120, loss = 0.04135904\n",
      "Iteration 26121, loss = 0.04135434\n",
      "Iteration 26122, loss = 0.04135126\n",
      "Iteration 26123, loss = 0.04134972\n",
      "Iteration 26124, loss = 0.04134773\n",
      "Iteration 26125, loss = 0.04134778\n",
      "Iteration 26126, loss = 0.04134389\n",
      "Iteration 26127, loss = 0.04134351\n",
      "Iteration 26128, loss = 0.04134785\n",
      "Iteration 26129, loss = 0.04134566\n",
      "Iteration 26130, loss = 0.04134506\n",
      "Iteration 26131, loss = 0.04134161\n",
      "Iteration 26132, loss = 0.04133974\n",
      "Iteration 26133, loss = 0.04134749\n",
      "Iteration 26134, loss = 0.04134469\n",
      "Iteration 26135, loss = 0.04133755\n",
      "Iteration 26136, loss = 0.04133846\n",
      "Iteration 26137, loss = 0.04133822\n",
      "Iteration 26138, loss = 0.04134035\n",
      "Iteration 26139, loss = 0.04134200\n",
      "Iteration 26140, loss = 0.04133417\n",
      "Iteration 26141, loss = 0.04132724\n",
      "Iteration 26142, loss = 0.04132704\n",
      "Iteration 26143, loss = 0.04133335\n",
      "Iteration 26144, loss = 0.04133672\n",
      "Iteration 26145, loss = 0.04133068\n",
      "Iteration 26146, loss = 0.04131344\n",
      "Iteration 26147, loss = 0.04131213\n",
      "Iteration 26148, loss = 0.04131892\n",
      "Iteration 26149, loss = 0.04132343\n",
      "Iteration 26150, loss = 0.04131769\n",
      "Iteration 26151, loss = 0.04131123\n",
      "Iteration 26152, loss = 0.04130631\n",
      "Iteration 26153, loss = 0.04129964\n",
      "Iteration 26154, loss = 0.04130681\n",
      "Iteration 26155, loss = 0.04130531\n",
      "Iteration 26156, loss = 0.04130803\n",
      "Iteration 26157, loss = 0.04130596\n",
      "Iteration 26158, loss = 0.04129985\n",
      "Iteration 26159, loss = 0.04129513\n",
      "Iteration 26160, loss = 0.04129371\n",
      "Iteration 26161, loss = 0.04129907\n",
      "Iteration 26162, loss = 0.04129354\n",
      "Iteration 26163, loss = 0.04128758\n",
      "Iteration 26164, loss = 0.04128700\n",
      "Iteration 26165, loss = 0.04129344\n",
      "Iteration 26166, loss = 0.04129070\n",
      "Iteration 26167, loss = 0.04128572\n",
      "Iteration 26168, loss = 0.04128090\n",
      "Iteration 26169, loss = 0.04127722\n",
      "Iteration 26170, loss = 0.04128052\n",
      "Iteration 26171, loss = 0.04128103\n",
      "Iteration 26172, loss = 0.04128133\n",
      "Iteration 26173, loss = 0.04128014\n",
      "Iteration 26174, loss = 0.04127841\n",
      "Iteration 26175, loss = 0.04127416\n",
      "Iteration 26176, loss = 0.04126571\n",
      "Iteration 26177, loss = 0.04126733\n",
      "Iteration 26178, loss = 0.04127329\n",
      "Iteration 26179, loss = 0.04127589\n",
      "Iteration 26180, loss = 0.04126711\n",
      "Iteration 26181, loss = 0.04126256\n",
      "Iteration 26182, loss = 0.04126102\n",
      "Iteration 26183, loss = 0.04126063\n",
      "Iteration 26184, loss = 0.04126406\n",
      "Iteration 26185, loss = 0.04126220\n",
      "Iteration 26186, loss = 0.04125428\n",
      "Iteration 26187, loss = 0.04125035\n",
      "Iteration 26188, loss = 0.04126003\n",
      "Iteration 26189, loss = 0.04125959\n",
      "Iteration 26190, loss = 0.04125579\n",
      "Iteration 26191, loss = 0.04125322\n",
      "Iteration 26192, loss = 0.04124565\n",
      "Iteration 26193, loss = 0.04124744\n",
      "Iteration 26194, loss = 0.04124388\n",
      "Iteration 26195, loss = 0.04123824\n",
      "Iteration 26196, loss = 0.04123982\n",
      "Iteration 26197, loss = 0.04123982\n",
      "Iteration 26198, loss = 0.04123180\n",
      "Iteration 26199, loss = 0.04122736\n",
      "Iteration 26200, loss = 0.04123650\n",
      "Iteration 26201, loss = 0.04123334\n",
      "Iteration 26202, loss = 0.04122887\n",
      "Iteration 26203, loss = 0.04123011\n",
      "Iteration 26204, loss = 0.04122668\n",
      "Iteration 26205, loss = 0.04123132\n",
      "Iteration 26206, loss = 0.04123002\n",
      "Iteration 26207, loss = 0.04122751\n",
      "Iteration 26208, loss = 0.04122213\n",
      "Iteration 26209, loss = 0.04121549\n",
      "Iteration 26210, loss = 0.04122045\n",
      "Iteration 26211, loss = 0.04122329\n",
      "Iteration 26212, loss = 0.04122024\n",
      "Iteration 26213, loss = 0.04121377\n",
      "Iteration 26214, loss = 0.04121061\n",
      "Iteration 26215, loss = 0.04120623\n",
      "Iteration 26216, loss = 0.04121043\n",
      "Iteration 26217, loss = 0.04120875\n",
      "Iteration 26218, loss = 0.04120385\n",
      "Iteration 26219, loss = 0.04120327\n",
      "Iteration 26220, loss = 0.04120215\n",
      "Iteration 26221, loss = 0.04120136\n",
      "Iteration 26222, loss = 0.04120472\n",
      "Iteration 26223, loss = 0.04120449\n",
      "Iteration 26224, loss = 0.04120024\n",
      "Iteration 26225, loss = 0.04119562\n",
      "Iteration 26226, loss = 0.04118978\n",
      "Iteration 26227, loss = 0.04119122\n",
      "Iteration 26228, loss = 0.04119395\n",
      "Iteration 26229, loss = 0.04119369\n",
      "Iteration 26230, loss = 0.04119009\n",
      "Iteration 26231, loss = 0.04118350\n",
      "Iteration 26232, loss = 0.04118350\n",
      "Iteration 26233, loss = 0.04118128\n",
      "Iteration 26234, loss = 0.04117668\n",
      "Iteration 26235, loss = 0.04117192\n",
      "Iteration 26236, loss = 0.04117693\n",
      "Iteration 26237, loss = 0.04118140\n",
      "Iteration 26238, loss = 0.04117578\n",
      "Iteration 26239, loss = 0.04116802\n",
      "Iteration 26240, loss = 0.04117132\n",
      "Iteration 26241, loss = 0.04117178\n",
      "Iteration 26242, loss = 0.04116939\n",
      "Iteration 26243, loss = 0.04116815\n",
      "Iteration 26244, loss = 0.04116398\n",
      "Iteration 26245, loss = 0.04116264\n",
      "Iteration 26246, loss = 0.04116194\n",
      "Iteration 26247, loss = 0.04116400\n",
      "Iteration 26248, loss = 0.04115899\n",
      "Iteration 26249, loss = 0.04115369\n",
      "Iteration 26250, loss = 0.04115575\n",
      "Iteration 26251, loss = 0.04115562\n",
      "Iteration 26252, loss = 0.04115372\n",
      "Iteration 26253, loss = 0.04115285\n",
      "Iteration 26254, loss = 0.04115061\n",
      "Iteration 26255, loss = 0.04114490\n",
      "Iteration 26256, loss = 0.04114928\n",
      "Iteration 26257, loss = 0.04115059\n",
      "Iteration 26258, loss = 0.04114356\n",
      "Iteration 26259, loss = 0.04113899\n",
      "Iteration 26260, loss = 0.04114326\n",
      "Iteration 26261, loss = 0.04113896\n",
      "Iteration 26262, loss = 0.04113620\n",
      "Iteration 26263, loss = 0.04113605\n",
      "Iteration 26264, loss = 0.04113697\n",
      "Iteration 26265, loss = 0.04112958\n",
      "Iteration 26266, loss = 0.04113492\n",
      "Iteration 26267, loss = 0.04113666\n",
      "Iteration 26268, loss = 0.04113456\n",
      "Iteration 26269, loss = 0.04113005\n",
      "Iteration 26270, loss = 0.04112290\n",
      "Iteration 26271, loss = 0.04112313\n",
      "Iteration 26272, loss = 0.04112063\n",
      "Iteration 26273, loss = 0.04111732\n",
      "Iteration 26274, loss = 0.04111944\n",
      "Iteration 26275, loss = 0.04111858\n",
      "Iteration 26276, loss = 0.04111693\n",
      "Iteration 26277, loss = 0.04111371\n",
      "Iteration 26278, loss = 0.04111824\n",
      "Iteration 26279, loss = 0.04111355\n",
      "Iteration 26280, loss = 0.04111026\n",
      "Iteration 26281, loss = 0.04111042\n",
      "Iteration 26282, loss = 0.04111181\n",
      "Iteration 26283, loss = 0.04110972\n",
      "Iteration 26284, loss = 0.04111401\n",
      "Iteration 26285, loss = 0.04111162\n",
      "Iteration 26286, loss = 0.04110066\n",
      "Iteration 26287, loss = 0.04109774\n",
      "Iteration 26288, loss = 0.04110064\n",
      "Iteration 26289, loss = 0.04109731\n",
      "Iteration 26290, loss = 0.04109293\n",
      "Iteration 26291, loss = 0.04109451\n",
      "Iteration 26292, loss = 0.04109513\n",
      "Iteration 26293, loss = 0.04109143\n",
      "Iteration 26294, loss = 0.04108756\n",
      "Iteration 26295, loss = 0.04108725\n",
      "Iteration 26296, loss = 0.04108776\n",
      "Iteration 26297, loss = 0.04108334\n",
      "Iteration 26298, loss = 0.04108462\n",
      "Iteration 26299, loss = 0.04108632\n",
      "Iteration 26300, loss = 0.04108171\n",
      "Iteration 26301, loss = 0.04108109\n",
      "Iteration 26302, loss = 0.04107860\n",
      "Iteration 26303, loss = 0.04107341\n",
      "Iteration 26304, loss = 0.04107567\n",
      "Iteration 26305, loss = 0.04107902\n",
      "Iteration 26306, loss = 0.04107697\n",
      "Iteration 26307, loss = 0.04106979\n",
      "Iteration 26308, loss = 0.04107054\n",
      "Iteration 26309, loss = 0.04107438\n",
      "Iteration 26310, loss = 0.04107417\n",
      "Iteration 26311, loss = 0.04107029\n",
      "Iteration 26312, loss = 0.04106656\n",
      "Iteration 26313, loss = 0.04106133\n",
      "Iteration 26314, loss = 0.04106464\n",
      "Iteration 26315, loss = 0.04106234\n",
      "Iteration 26316, loss = 0.04105841\n",
      "Iteration 26317, loss = 0.04106066\n",
      "Iteration 26318, loss = 0.04105590\n",
      "Iteration 26319, loss = 0.04105359\n",
      "Iteration 26320, loss = 0.04105435\n",
      "Iteration 26321, loss = 0.04105454\n",
      "Iteration 26322, loss = 0.04105218\n",
      "Iteration 26323, loss = 0.04105180\n",
      "Iteration 26324, loss = 0.04104908\n",
      "Iteration 26325, loss = 0.04104238\n",
      "Iteration 26326, loss = 0.04104166\n",
      "Iteration 26327, loss = 0.04104280\n",
      "Iteration 26328, loss = 0.04103969\n",
      "Iteration 26329, loss = 0.04103591\n",
      "Iteration 26330, loss = 0.04103993\n",
      "Iteration 26331, loss = 0.04103887\n",
      "Iteration 26332, loss = 0.04103320\n",
      "Iteration 26333, loss = 0.04102815\n",
      "Iteration 26334, loss = 0.04102918\n",
      "Iteration 26335, loss = 0.04103042\n",
      "Iteration 26336, loss = 0.04102582\n",
      "Iteration 26337, loss = 0.04102550\n",
      "Iteration 26338, loss = 0.04102870\n",
      "Iteration 26339, loss = 0.04102805\n",
      "Iteration 26340, loss = 0.04102310\n",
      "Iteration 26341, loss = 0.04102142\n",
      "Iteration 26342, loss = 0.04101477\n",
      "Iteration 26343, loss = 0.04101722\n",
      "Iteration 26344, loss = 0.04101449\n",
      "Iteration 26345, loss = 0.04100968\n",
      "Iteration 26346, loss = 0.04100948\n",
      "Iteration 26347, loss = 0.04100997\n",
      "Iteration 26348, loss = 0.04100660\n",
      "Iteration 26349, loss = 0.04100961\n",
      "Iteration 26350, loss = 0.04100592\n",
      "Iteration 26351, loss = 0.04100546\n",
      "Iteration 26352, loss = 0.04100765\n",
      "Iteration 26353, loss = 0.04101049\n",
      "Iteration 26354, loss = 0.04100789\n",
      "Iteration 26355, loss = 0.04099879\n",
      "Iteration 26356, loss = 0.04100236\n",
      "Iteration 26357, loss = 0.04100558\n",
      "Iteration 26358, loss = 0.04100547\n",
      "Iteration 26359, loss = 0.04100132\n",
      "Iteration 26360, loss = 0.04099697\n",
      "Iteration 26361, loss = 0.04099384\n",
      "Iteration 26362, loss = 0.04099312\n",
      "Iteration 26363, loss = 0.04099112\n",
      "Iteration 26364, loss = 0.04098353\n",
      "Iteration 26365, loss = 0.04098199\n",
      "Iteration 26366, loss = 0.04098650\n",
      "Iteration 26367, loss = 0.04098318\n",
      "Iteration 26368, loss = 0.04097679\n",
      "Iteration 26369, loss = 0.04097704\n",
      "Iteration 26370, loss = 0.04098115\n",
      "Iteration 26371, loss = 0.04098054\n",
      "Iteration 26372, loss = 0.04097606\n",
      "Iteration 26373, loss = 0.04097002\n",
      "Iteration 26374, loss = 0.04096960\n",
      "Iteration 26375, loss = 0.04097200\n",
      "Iteration 26376, loss = 0.04096539\n",
      "Iteration 26377, loss = 0.04096554\n",
      "Iteration 26378, loss = 0.04096706\n",
      "Iteration 26379, loss = 0.04096285\n",
      "Iteration 26380, loss = 0.04096237\n",
      "Iteration 26381, loss = 0.04096366\n",
      "Iteration 26382, loss = 0.04096240\n",
      "Iteration 26383, loss = 0.04095795\n",
      "Iteration 26384, loss = 0.04095326\n",
      "Iteration 26385, loss = 0.04095221\n",
      "Iteration 26386, loss = 0.04095141\n",
      "Iteration 26387, loss = 0.04095085\n",
      "Iteration 26388, loss = 0.04094817\n",
      "Iteration 26389, loss = 0.04095178\n",
      "Iteration 26390, loss = 0.04094575\n",
      "Iteration 26391, loss = 0.04094505\n",
      "Iteration 26392, loss = 0.04094442\n",
      "Iteration 26393, loss = 0.04094011\n",
      "Iteration 26394, loss = 0.04094436\n",
      "Iteration 26395, loss = 0.04094602\n",
      "Iteration 26396, loss = 0.04093523\n",
      "Iteration 26397, loss = 0.04093709\n",
      "Iteration 26398, loss = 0.04093885\n",
      "Iteration 26399, loss = 0.04093890\n",
      "Iteration 26400, loss = 0.04093943\n",
      "Iteration 26401, loss = 0.04093790\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 26402, loss = 0.04093612\n",
      "Iteration 26403, loss = 0.04093076\n",
      "Iteration 26404, loss = 0.04092871\n",
      "Iteration 26405, loss = 0.04093170\n",
      "Iteration 26406, loss = 0.04092968\n",
      "Iteration 26407, loss = 0.04092145\n",
      "Iteration 26408, loss = 0.04092309\n",
      "Iteration 26409, loss = 0.04092322\n",
      "Iteration 26410, loss = 0.04091502\n",
      "Iteration 26411, loss = 0.04091053\n",
      "Iteration 26412, loss = 0.04091823\n",
      "Iteration 26413, loss = 0.04091960\n",
      "Iteration 26414, loss = 0.04091236\n",
      "Iteration 26415, loss = 0.04090709\n",
      "Iteration 26416, loss = 0.04091184\n",
      "Iteration 26417, loss = 0.04090867\n",
      "Iteration 26418, loss = 0.04090664\n",
      "Iteration 26419, loss = 0.04090950\n",
      "Iteration 26420, loss = 0.04090595\n",
      "Iteration 26421, loss = 0.04089959\n",
      "Iteration 26422, loss = 0.04089925\n",
      "Iteration 26423, loss = 0.04089905\n",
      "Iteration 26424, loss = 0.04089629\n",
      "Iteration 26425, loss = 0.04090068\n",
      "Iteration 26426, loss = 0.04089892\n",
      "Iteration 26427, loss = 0.04089115\n",
      "Iteration 26428, loss = 0.04088974\n",
      "Iteration 26429, loss = 0.04089060\n",
      "Iteration 26430, loss = 0.04089090\n",
      "Iteration 26431, loss = 0.04089020\n",
      "Iteration 26432, loss = 0.04088547\n",
      "Iteration 26433, loss = 0.04088124\n",
      "Iteration 26434, loss = 0.04088285\n",
      "Iteration 26435, loss = 0.04088144\n",
      "Iteration 26436, loss = 0.04088230\n",
      "Iteration 26437, loss = 0.04087842\n",
      "Iteration 26438, loss = 0.04087504\n",
      "Iteration 26439, loss = 0.04087326\n",
      "Iteration 26440, loss = 0.04088182\n",
      "Iteration 26441, loss = 0.04088231\n",
      "Iteration 26442, loss = 0.04087440\n",
      "Iteration 26443, loss = 0.04087293\n",
      "Iteration 26444, loss = 0.04087322\n",
      "Iteration 26445, loss = 0.04087038\n",
      "Iteration 26446, loss = 0.04086735\n",
      "Iteration 26447, loss = 0.04086838\n",
      "Iteration 26448, loss = 0.04086520\n",
      "Iteration 26449, loss = 0.04086583\n",
      "Iteration 26450, loss = 0.04086901\n",
      "Iteration 26451, loss = 0.04086442\n",
      "Iteration 26452, loss = 0.04085500\n",
      "Iteration 26453, loss = 0.04085789\n",
      "Iteration 26454, loss = 0.04085995\n",
      "Iteration 26455, loss = 0.04085552\n",
      "Iteration 26456, loss = 0.04085790\n",
      "Iteration 26457, loss = 0.04084968\n",
      "Iteration 26458, loss = 0.04085080\n",
      "Iteration 26459, loss = 0.04085930\n",
      "Iteration 26460, loss = 0.04086180\n",
      "Iteration 26461, loss = 0.04085794\n",
      "Iteration 26462, loss = 0.04085315\n",
      "Iteration 26463, loss = 0.04084768\n",
      "Iteration 26464, loss = 0.04084085\n",
      "Iteration 26465, loss = 0.04084797\n",
      "Iteration 26466, loss = 0.04084323\n",
      "Iteration 26467, loss = 0.04083413\n",
      "Iteration 26468, loss = 0.04083678\n",
      "Iteration 26469, loss = 0.04083749\n",
      "Iteration 26470, loss = 0.04083923\n",
      "Iteration 26471, loss = 0.04083766\n",
      "Iteration 26472, loss = 0.04083200\n",
      "Iteration 26473, loss = 0.04083064\n",
      "Iteration 26474, loss = 0.04082639\n",
      "Iteration 26475, loss = 0.04082293\n",
      "Iteration 26476, loss = 0.04082374\n",
      "Iteration 26477, loss = 0.04082387\n",
      "Iteration 26478, loss = 0.04081964\n",
      "Iteration 26479, loss = 0.04082042\n",
      "Iteration 26480, loss = 0.04081744\n",
      "Iteration 26481, loss = 0.04081286\n",
      "Iteration 26482, loss = 0.04081213\n",
      "Iteration 26483, loss = 0.04081190\n",
      "Iteration 26484, loss = 0.04080727\n",
      "Iteration 26485, loss = 0.04080858\n",
      "Iteration 26486, loss = 0.04080815\n",
      "Iteration 26487, loss = 0.04081147\n",
      "Iteration 26488, loss = 0.04080569\n",
      "Iteration 26489, loss = 0.04080388\n",
      "Iteration 26490, loss = 0.04080470\n",
      "Iteration 26491, loss = 0.04080358\n",
      "Iteration 26492, loss = 0.04079585\n",
      "Iteration 26493, loss = 0.04079813\n",
      "Iteration 26494, loss = 0.04079784\n",
      "Iteration 26495, loss = 0.04079838\n",
      "Iteration 26496, loss = 0.04079857\n",
      "Iteration 26497, loss = 0.04079216\n",
      "Iteration 26498, loss = 0.04078461\n",
      "Iteration 26499, loss = 0.04078998\n",
      "Iteration 26500, loss = 0.04079554\n",
      "Iteration 26501, loss = 0.04079579\n",
      "Iteration 26502, loss = 0.04078683\n",
      "Iteration 26503, loss = 0.04078194\n",
      "Iteration 26504, loss = 0.04078679\n",
      "Iteration 26505, loss = 0.04078981\n",
      "Iteration 26506, loss = 0.04079090\n",
      "Iteration 26507, loss = 0.04078574\n",
      "Iteration 26508, loss = 0.04078397\n",
      "Iteration 26509, loss = 0.04077827\n",
      "Iteration 26510, loss = 0.04077555\n",
      "Iteration 26511, loss = 0.04077844\n",
      "Iteration 26512, loss = 0.04077457\n",
      "Iteration 26513, loss = 0.04077509\n",
      "Iteration 26514, loss = 0.04077919\n",
      "Iteration 26515, loss = 0.04077651\n",
      "Iteration 26516, loss = 0.04077002\n",
      "Iteration 26517, loss = 0.04076510\n",
      "Iteration 26518, loss = 0.04076564\n",
      "Iteration 26519, loss = 0.04076240\n",
      "Iteration 26520, loss = 0.04076215\n",
      "Iteration 26521, loss = 0.04076314\n",
      "Iteration 26522, loss = 0.04075947\n",
      "Iteration 26523, loss = 0.04075855\n",
      "Iteration 26524, loss = 0.04075963\n",
      "Iteration 26525, loss = 0.04075583\n",
      "Iteration 26526, loss = 0.04075428\n",
      "Iteration 26527, loss = 0.04075123\n",
      "Iteration 26528, loss = 0.04074526\n",
      "Iteration 26529, loss = 0.04074525\n",
      "Iteration 26530, loss = 0.04074244\n",
      "Iteration 26531, loss = 0.04074074\n",
      "Iteration 26532, loss = 0.04074150\n",
      "Iteration 26533, loss = 0.04073858\n",
      "Iteration 26534, loss = 0.04073674\n",
      "Iteration 26535, loss = 0.04073434\n",
      "Iteration 26536, loss = 0.04074145\n",
      "Iteration 26537, loss = 0.04073870\n",
      "Iteration 26538, loss = 0.04073197\n",
      "Iteration 26539, loss = 0.04073553\n",
      "Iteration 26540, loss = 0.04073979\n",
      "Iteration 26541, loss = 0.04073681\n",
      "Iteration 26542, loss = 0.04073651\n",
      "Iteration 26543, loss = 0.04073373\n",
      "Iteration 26544, loss = 0.04072421\n",
      "Iteration 26545, loss = 0.04073113\n",
      "Iteration 26546, loss = 0.04073158\n",
      "Iteration 26547, loss = 0.04072243\n",
      "Iteration 26548, loss = 0.04072304\n",
      "Iteration 26549, loss = 0.04072794\n",
      "Iteration 26550, loss = 0.04073042\n",
      "Iteration 26551, loss = 0.04072764\n",
      "Iteration 26552, loss = 0.04071820\n",
      "Iteration 26553, loss = 0.04071513\n",
      "Iteration 26554, loss = 0.04071235\n",
      "Iteration 26555, loss = 0.04071219\n",
      "Iteration 26556, loss = 0.04070556\n",
      "Iteration 26557, loss = 0.04070553\n",
      "Iteration 26558, loss = 0.04070667\n",
      "Iteration 26559, loss = 0.04070209\n",
      "Iteration 26560, loss = 0.04069877\n",
      "Iteration 26561, loss = 0.04069958\n",
      "Iteration 26562, loss = 0.04069615\n",
      "Iteration 26563, loss = 0.04069428\n",
      "Iteration 26564, loss = 0.04070020\n",
      "Iteration 26565, loss = 0.04069811\n",
      "Iteration 26566, loss = 0.04069537\n",
      "Iteration 26567, loss = 0.04069296\n",
      "Iteration 26568, loss = 0.04069465\n",
      "Iteration 26569, loss = 0.04069416\n",
      "Iteration 26570, loss = 0.04068852\n",
      "Iteration 26571, loss = 0.04069137\n",
      "Iteration 26572, loss = 0.04069310\n",
      "Iteration 26573, loss = 0.04069219\n",
      "Iteration 26574, loss = 0.04069273\n",
      "Iteration 26575, loss = 0.04068328\n",
      "Iteration 26576, loss = 0.04067647\n",
      "Iteration 26577, loss = 0.04068609\n",
      "Iteration 26578, loss = 0.04068468\n",
      "Iteration 26579, loss = 0.04068010\n",
      "Iteration 26580, loss = 0.04067263\n",
      "Iteration 26581, loss = 0.04067234\n",
      "Iteration 26582, loss = 0.04067347\n",
      "Iteration 26583, loss = 0.04067255\n",
      "Iteration 26584, loss = 0.04067535\n",
      "Iteration 26585, loss = 0.04067396\n",
      "Iteration 26586, loss = 0.04067175\n",
      "Iteration 26587, loss = 0.04066317\n",
      "Iteration 26588, loss = 0.04066178\n",
      "Iteration 26589, loss = 0.04066792\n",
      "Iteration 26590, loss = 0.04066300\n",
      "Iteration 26591, loss = 0.04065186\n",
      "Iteration 26592, loss = 0.04065778\n",
      "Iteration 26593, loss = 0.04065850\n",
      "Iteration 26594, loss = 0.04065784\n",
      "Iteration 26595, loss = 0.04065439\n",
      "Iteration 26596, loss = 0.04065263\n",
      "Iteration 26597, loss = 0.04065034\n",
      "Iteration 26598, loss = 0.04064450\n",
      "Iteration 26599, loss = 0.04064456\n",
      "Iteration 26600, loss = 0.04065226\n",
      "Iteration 26601, loss = 0.04064374\n",
      "Iteration 26602, loss = 0.04064200\n",
      "Iteration 26603, loss = 0.04064600\n",
      "Iteration 26604, loss = 0.04064681\n",
      "Iteration 26605, loss = 0.04064038\n",
      "Iteration 26606, loss = 0.04063323\n",
      "Iteration 26607, loss = 0.04063522\n",
      "Iteration 26608, loss = 0.04063758\n",
      "Iteration 26609, loss = 0.04063483\n",
      "Iteration 26610, loss = 0.04063749\n",
      "Iteration 26611, loss = 0.04063187\n",
      "Iteration 26612, loss = 0.04063385\n",
      "Iteration 26613, loss = 0.04063638\n",
      "Iteration 26614, loss = 0.04063689\n",
      "Iteration 26615, loss = 0.04063112\n",
      "Iteration 26616, loss = 0.04062846\n",
      "Iteration 26617, loss = 0.04063049\n",
      "Iteration 26618, loss = 0.04062356\n",
      "Iteration 26619, loss = 0.04062960\n",
      "Iteration 26620, loss = 0.04063157\n",
      "Iteration 26621, loss = 0.04062702\n",
      "Iteration 26622, loss = 0.04061330\n",
      "Iteration 26623, loss = 0.04061744\n",
      "Iteration 26624, loss = 0.04061764\n",
      "Iteration 26625, loss = 0.04061527\n",
      "Iteration 26626, loss = 0.04060972\n",
      "Iteration 26627, loss = 0.04061153\n",
      "Iteration 26628, loss = 0.04061143\n",
      "Iteration 26629, loss = 0.04060288\n",
      "Iteration 26630, loss = 0.04060084\n",
      "Iteration 26631, loss = 0.04060659\n",
      "Iteration 26632, loss = 0.04060815\n",
      "Iteration 26633, loss = 0.04060118\n",
      "Iteration 26634, loss = 0.04059862\n",
      "Iteration 26635, loss = 0.04059845\n",
      "Iteration 26636, loss = 0.04059867\n",
      "Iteration 26637, loss = 0.04059500\n",
      "Iteration 26638, loss = 0.04059747\n",
      "Iteration 26639, loss = 0.04059478\n",
      "Iteration 26640, loss = 0.04059179\n",
      "Iteration 26641, loss = 0.04059164\n",
      "Iteration 26642, loss = 0.04059208\n",
      "Iteration 26643, loss = 0.04059234\n",
      "Iteration 26644, loss = 0.04058691\n",
      "Iteration 26645, loss = 0.04058313\n",
      "Iteration 26646, loss = 0.04058584\n",
      "Iteration 26647, loss = 0.04058380\n",
      "Iteration 26648, loss = 0.04057708\n",
      "Iteration 26649, loss = 0.04057599\n",
      "Iteration 26650, loss = 0.04057763\n",
      "Iteration 26651, loss = 0.04057284\n",
      "Iteration 26652, loss = 0.04057599\n",
      "Iteration 26653, loss = 0.04057481\n",
      "Iteration 26654, loss = 0.04057521\n",
      "Iteration 26655, loss = 0.04057010\n",
      "Iteration 26656, loss = 0.04056441\n",
      "Iteration 26657, loss = 0.04056730\n",
      "Iteration 26658, loss = 0.04057663\n",
      "Iteration 26659, loss = 0.04057274\n",
      "Iteration 26660, loss = 0.04056295\n",
      "Iteration 26661, loss = 0.04056383\n",
      "Iteration 26662, loss = 0.04056558\n",
      "Iteration 26663, loss = 0.04056411\n",
      "Iteration 26664, loss = 0.04056003\n",
      "Iteration 26665, loss = 0.04055719\n",
      "Iteration 26666, loss = 0.04055389\n",
      "Iteration 26667, loss = 0.04055076\n",
      "Iteration 26668, loss = 0.04055281\n",
      "Iteration 26669, loss = 0.04054768\n",
      "Iteration 26670, loss = 0.04054780\n",
      "Iteration 26671, loss = 0.04054872\n",
      "Iteration 26672, loss = 0.04054673\n",
      "Iteration 26673, loss = 0.04054425\n",
      "Iteration 26674, loss = 0.04054077\n",
      "Iteration 26675, loss = 0.04053702\n",
      "Iteration 26676, loss = 0.04053372\n",
      "Iteration 26677, loss = 0.04053556\n",
      "Iteration 26678, loss = 0.04053479\n",
      "Iteration 26679, loss = 0.04053893\n",
      "Iteration 26680, loss = 0.04053252\n",
      "Iteration 26681, loss = 0.04052895\n",
      "Iteration 26682, loss = 0.04052931\n",
      "Iteration 26683, loss = 0.04052992\n",
      "Iteration 26684, loss = 0.04052983\n",
      "Iteration 26685, loss = 0.04053022\n",
      "Iteration 26686, loss = 0.04052770\n",
      "Iteration 26687, loss = 0.04052465\n",
      "Iteration 26688, loss = 0.04051864\n",
      "Iteration 26689, loss = 0.04052772\n",
      "Iteration 26690, loss = 0.04052432\n",
      "Iteration 26691, loss = 0.04051889\n",
      "Iteration 26692, loss = 0.04051750\n",
      "Iteration 26693, loss = 0.04051875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 26694, loss = 0.04051451\n",
      "Iteration 26695, loss = 0.04051151\n",
      "Iteration 26696, loss = 0.04050801\n",
      "Iteration 26697, loss = 0.04051225\n",
      "Iteration 26698, loss = 0.04051443\n",
      "Iteration 26699, loss = 0.04050769\n",
      "Iteration 26700, loss = 0.04050985\n",
      "Iteration 26701, loss = 0.04050399\n",
      "Iteration 26702, loss = 0.04049850\n",
      "Iteration 26703, loss = 0.04050014\n",
      "Iteration 26704, loss = 0.04049666\n",
      "Iteration 26705, loss = 0.04049947\n",
      "Iteration 26706, loss = 0.04049879\n",
      "Iteration 26707, loss = 0.04049903\n",
      "Iteration 26708, loss = 0.04049499\n",
      "Iteration 26709, loss = 0.04049299\n",
      "Iteration 26710, loss = 0.04049296\n",
      "Iteration 26711, loss = 0.04049589\n",
      "Iteration 26712, loss = 0.04049520\n",
      "Iteration 26713, loss = 0.04048724\n",
      "Iteration 26714, loss = 0.04048895\n",
      "Iteration 26715, loss = 0.04048438\n",
      "Iteration 26716, loss = 0.04048025\n",
      "Iteration 26717, loss = 0.04048886\n",
      "Iteration 26718, loss = 0.04048319\n",
      "Iteration 26719, loss = 0.04047621\n",
      "Iteration 26720, loss = 0.04048083\n",
      "Iteration 26721, loss = 0.04047883\n",
      "Iteration 26722, loss = 0.04047091\n",
      "Iteration 26723, loss = 0.04047388\n",
      "Iteration 26724, loss = 0.04047786\n",
      "Iteration 26725, loss = 0.04047585\n",
      "Iteration 26726, loss = 0.04047008\n",
      "Iteration 26727, loss = 0.04046766\n",
      "Iteration 26728, loss = 0.04046494\n",
      "Iteration 26729, loss = 0.04046366\n",
      "Iteration 26730, loss = 0.04046504\n",
      "Iteration 26731, loss = 0.04047010\n",
      "Iteration 26732, loss = 0.04046813\n",
      "Iteration 26733, loss = 0.04046206\n",
      "Iteration 26734, loss = 0.04046129\n",
      "Iteration 26735, loss = 0.04045818\n",
      "Iteration 26736, loss = 0.04045517\n",
      "Iteration 26737, loss = 0.04045564\n",
      "Iteration 26738, loss = 0.04045405\n",
      "Iteration 26739, loss = 0.04045577\n",
      "Iteration 26740, loss = 0.04045160\n",
      "Iteration 26741, loss = 0.04045334\n",
      "Iteration 26742, loss = 0.04045144\n",
      "Iteration 26743, loss = 0.04045097\n",
      "Iteration 26744, loss = 0.04044738\n",
      "Iteration 26745, loss = 0.04044458\n",
      "Iteration 26746, loss = 0.04044423\n",
      "Iteration 26747, loss = 0.04044375\n",
      "Iteration 26748, loss = 0.04044091\n",
      "Iteration 26749, loss = 0.04043764\n",
      "Iteration 26750, loss = 0.04043355\n",
      "Iteration 26751, loss = 0.04043309\n",
      "Iteration 26752, loss = 0.04043120\n",
      "Iteration 26753, loss = 0.04043023\n",
      "Iteration 26754, loss = 0.04043043\n",
      "Iteration 26755, loss = 0.04042889\n",
      "Iteration 26756, loss = 0.04042555\n",
      "Iteration 26757, loss = 0.04042213\n",
      "Iteration 26758, loss = 0.04042710\n",
      "Iteration 26759, loss = 0.04042642\n",
      "Iteration 26760, loss = 0.04042316\n",
      "Iteration 26761, loss = 0.04041997\n",
      "Iteration 26762, loss = 0.04042093\n",
      "Iteration 26763, loss = 0.04042624\n",
      "Iteration 26764, loss = 0.04042408\n",
      "Iteration 26765, loss = 0.04041878\n",
      "Iteration 26766, loss = 0.04041677\n",
      "Iteration 26767, loss = 0.04041443\n",
      "Iteration 26768, loss = 0.04041636\n",
      "Iteration 26769, loss = 0.04041829\n",
      "Iteration 26770, loss = 0.04041405\n",
      "Iteration 26771, loss = 0.04040771\n",
      "Iteration 26772, loss = 0.04041031\n",
      "Iteration 26773, loss = 0.04040931\n",
      "Iteration 26774, loss = 0.04041515\n",
      "Iteration 26775, loss = 0.04041076\n",
      "Iteration 26776, loss = 0.04040682\n",
      "Iteration 26777, loss = 0.04040505\n",
      "Iteration 26778, loss = 0.04041182\n",
      "Iteration 26779, loss = 0.04041286\n",
      "Iteration 26780, loss = 0.04040525\n",
      "Iteration 26781, loss = 0.04039583\n",
      "Iteration 26782, loss = 0.04039976\n",
      "Iteration 26783, loss = 0.04040357\n",
      "Iteration 26784, loss = 0.04039650\n",
      "Iteration 26785, loss = 0.04039629\n",
      "Iteration 26786, loss = 0.04038970\n",
      "Iteration 26787, loss = 0.04038336\n",
      "Iteration 26788, loss = 0.04038397\n",
      "Iteration 26789, loss = 0.04038082\n",
      "Iteration 26790, loss = 0.04038474\n",
      "Iteration 26791, loss = 0.04038348\n",
      "Iteration 26792, loss = 0.04038175\n",
      "Iteration 26793, loss = 0.04038264\n",
      "Iteration 26794, loss = 0.04038315\n",
      "Iteration 26795, loss = 0.04038055\n",
      "Iteration 26796, loss = 0.04038002\n",
      "Iteration 26797, loss = 0.04037338\n",
      "Iteration 26798, loss = 0.04037449\n",
      "Iteration 26799, loss = 0.04038012\n",
      "Iteration 26800, loss = 0.04037626\n",
      "Iteration 26801, loss = 0.04037371\n",
      "Iteration 26802, loss = 0.04037105\n",
      "Iteration 26803, loss = 0.04036447\n",
      "Iteration 26804, loss = 0.04036612\n",
      "Iteration 26805, loss = 0.04037135\n",
      "Iteration 26806, loss = 0.04037249\n",
      "Iteration 26807, loss = 0.04036238\n",
      "Iteration 26808, loss = 0.04035633\n",
      "Iteration 26809, loss = 0.04036121\n",
      "Iteration 26810, loss = 0.04036394\n",
      "Iteration 26811, loss = 0.04036290\n",
      "Iteration 26812, loss = 0.04035983\n",
      "Iteration 26813, loss = 0.04035816\n",
      "Iteration 26814, loss = 0.04035844\n",
      "Iteration 26815, loss = 0.04035551\n",
      "Iteration 26816, loss = 0.04035236\n",
      "Iteration 26817, loss = 0.04034834\n",
      "Iteration 26818, loss = 0.04034595\n",
      "Iteration 26819, loss = 0.04035003\n",
      "Iteration 26820, loss = 0.04034964\n",
      "Iteration 26821, loss = 0.04034607\n",
      "Iteration 26822, loss = 0.04034275\n",
      "Iteration 26823, loss = 0.04034181\n",
      "Iteration 26824, loss = 0.04034673\n",
      "Iteration 26825, loss = 0.04034632\n",
      "Iteration 26826, loss = 0.04033950\n",
      "Iteration 26827, loss = 0.04033600\n",
      "Iteration 26828, loss = 0.04033743\n",
      "Iteration 26829, loss = 0.04033602\n",
      "Iteration 26830, loss = 0.04033328\n",
      "Iteration 26831, loss = 0.04033215\n",
      "Iteration 26832, loss = 0.04033206\n",
      "Iteration 26833, loss = 0.04033671\n",
      "Iteration 26834, loss = 0.04033340\n",
      "Iteration 26835, loss = 0.04032549\n",
      "Iteration 26836, loss = 0.04032022\n",
      "Iteration 26837, loss = 0.04032134\n",
      "Iteration 26838, loss = 0.04031620\n",
      "Iteration 26839, loss = 0.04031661\n",
      "Iteration 26840, loss = 0.04031561\n",
      "Iteration 26841, loss = 0.04031369\n",
      "Iteration 26842, loss = 0.04031743\n",
      "Iteration 26843, loss = 0.04031524\n",
      "Iteration 26844, loss = 0.04031378\n",
      "Iteration 26845, loss = 0.04031318\n",
      "Iteration 26846, loss = 0.04030716\n",
      "Iteration 26847, loss = 0.04030837\n",
      "Iteration 26848, loss = 0.04030806\n",
      "Iteration 26849, loss = 0.04030656\n",
      "Iteration 26850, loss = 0.04030242\n",
      "Iteration 26851, loss = 0.04030228\n",
      "Iteration 26852, loss = 0.04030339\n",
      "Iteration 26853, loss = 0.04030236\n",
      "Iteration 26854, loss = 0.04030097\n",
      "Iteration 26855, loss = 0.04030047\n",
      "Iteration 26856, loss = 0.04029798\n",
      "Iteration 26857, loss = 0.04029675\n",
      "Iteration 26858, loss = 0.04029628\n",
      "Iteration 26859, loss = 0.04029273\n",
      "Iteration 26860, loss = 0.04029712\n",
      "Iteration 26861, loss = 0.04029602\n",
      "Iteration 26862, loss = 0.04029610\n",
      "Iteration 26863, loss = 0.04029595\n",
      "Iteration 26864, loss = 0.04028536\n",
      "Iteration 26865, loss = 0.04028436\n",
      "Iteration 26866, loss = 0.04028871\n",
      "Iteration 26867, loss = 0.04028318\n",
      "Iteration 26868, loss = 0.04027784\n",
      "Iteration 26869, loss = 0.04027724\n",
      "Iteration 26870, loss = 0.04027571\n",
      "Iteration 26871, loss = 0.04027512\n",
      "Iteration 26872, loss = 0.04026993\n",
      "Iteration 26873, loss = 0.04026978\n",
      "Iteration 26874, loss = 0.04026968\n",
      "Iteration 26875, loss = 0.04026674\n",
      "Iteration 26876, loss = 0.04026798\n",
      "Iteration 26877, loss = 0.04026676\n",
      "Iteration 26878, loss = 0.04026791\n",
      "Iteration 26879, loss = 0.04026470\n",
      "Iteration 26880, loss = 0.04026298\n",
      "Iteration 26881, loss = 0.04026722\n",
      "Iteration 26882, loss = 0.04026221\n",
      "Iteration 26883, loss = 0.04025846\n",
      "Iteration 26884, loss = 0.04025977\n",
      "Iteration 26885, loss = 0.04026048\n",
      "Iteration 26886, loss = 0.04025911\n",
      "Iteration 26887, loss = 0.04025799\n",
      "Iteration 26888, loss = 0.04025403\n",
      "Iteration 26889, loss = 0.04025210\n",
      "Iteration 26890, loss = 0.04025237\n",
      "Iteration 26891, loss = 0.04024781\n",
      "Iteration 26892, loss = 0.04025328\n",
      "Iteration 26893, loss = 0.04025125\n",
      "Iteration 26894, loss = 0.04024424\n",
      "Iteration 26895, loss = 0.04024141\n",
      "Iteration 26896, loss = 0.04023732\n",
      "Iteration 26897, loss = 0.04023694\n",
      "Iteration 26898, loss = 0.04023713\n",
      "Iteration 26899, loss = 0.04023593\n",
      "Iteration 26900, loss = 0.04023797\n",
      "Iteration 26901, loss = 0.04023678\n",
      "Iteration 26902, loss = 0.04023122\n",
      "Iteration 26903, loss = 0.04022636\n",
      "Iteration 26904, loss = 0.04022540\n",
      "Iteration 26905, loss = 0.04022549\n",
      "Iteration 26906, loss = 0.04022779\n",
      "Iteration 26907, loss = 0.04022535\n",
      "Iteration 26908, loss = 0.04022558\n",
      "Iteration 26909, loss = 0.04022377\n",
      "Iteration 26910, loss = 0.04022470\n",
      "Iteration 26911, loss = 0.04022580\n",
      "Iteration 26912, loss = 0.04022178\n",
      "Iteration 26913, loss = 0.04022102\n",
      "Iteration 26914, loss = 0.04021857\n",
      "Iteration 26915, loss = 0.04021876\n",
      "Iteration 26916, loss = 0.04021973\n",
      "Iteration 26917, loss = 0.04021511\n",
      "Iteration 26918, loss = 0.04021258\n",
      "Iteration 26919, loss = 0.04020809\n",
      "Iteration 26920, loss = 0.04021020\n",
      "Iteration 26921, loss = 0.04021320\n",
      "Iteration 26922, loss = 0.04021120\n",
      "Iteration 26923, loss = 0.04020333\n",
      "Iteration 26924, loss = 0.04020428\n",
      "Iteration 26925, loss = 0.04021122\n",
      "Iteration 26926, loss = 0.04020942\n",
      "Iteration 26927, loss = 0.04020730\n",
      "Iteration 26928, loss = 0.04020459\n",
      "Iteration 26929, loss = 0.04019853\n",
      "Iteration 26930, loss = 0.04020167\n",
      "Iteration 26931, loss = 0.04019503\n",
      "Iteration 26932, loss = 0.04019547\n",
      "Iteration 26933, loss = 0.04019289\n",
      "Iteration 26934, loss = 0.04019323\n",
      "Iteration 26935, loss = 0.04018700\n",
      "Iteration 26936, loss = 0.04018638\n",
      "Iteration 26937, loss = 0.04019314\n",
      "Iteration 26938, loss = 0.04019274\n",
      "Iteration 26939, loss = 0.04018553\n",
      "Iteration 26940, loss = 0.04018309\n",
      "Iteration 26941, loss = 0.04018236\n",
      "Iteration 26942, loss = 0.04018391\n",
      "Iteration 26943, loss = 0.04017728\n",
      "Iteration 26944, loss = 0.04018033\n",
      "Iteration 26945, loss = 0.04018735\n",
      "Iteration 26946, loss = 0.04018332\n",
      "Iteration 26947, loss = 0.04017621\n",
      "Iteration 26948, loss = 0.04017487\n",
      "Iteration 26949, loss = 0.04017202\n",
      "Iteration 26950, loss = 0.04017579\n",
      "Iteration 26951, loss = 0.04017042\n",
      "Iteration 26952, loss = 0.04017264\n",
      "Iteration 26953, loss = 0.04017101\n",
      "Iteration 26954, loss = 0.04017006\n",
      "Iteration 26955, loss = 0.04017555\n",
      "Iteration 26956, loss = 0.04017266\n",
      "Iteration 26957, loss = 0.04016584\n",
      "Iteration 26958, loss = 0.04015748\n",
      "Iteration 26959, loss = 0.04016109\n",
      "Iteration 26960, loss = 0.04016087\n",
      "Iteration 26961, loss = 0.04015306\n",
      "Iteration 26962, loss = 0.04015393\n",
      "Iteration 26963, loss = 0.04015441\n",
      "Iteration 26964, loss = 0.04015444\n",
      "Iteration 26965, loss = 0.04015581\n",
      "Iteration 26966, loss = 0.04014952\n",
      "Iteration 26967, loss = 0.04014849\n",
      "Iteration 26968, loss = 0.04014688\n",
      "Iteration 26969, loss = 0.04014489\n",
      "Iteration 26970, loss = 0.04014026\n",
      "Iteration 26971, loss = 0.04014123\n",
      "Iteration 26972, loss = 0.04014241\n",
      "Iteration 26973, loss = 0.04014095\n",
      "Iteration 26974, loss = 0.04013679\n",
      "Iteration 26975, loss = 0.04013705\n",
      "Iteration 26976, loss = 0.04013498\n",
      "Iteration 26977, loss = 0.04013514\n",
      "Iteration 26978, loss = 0.04013116\n",
      "Iteration 26979, loss = 0.04012846\n",
      "Iteration 26980, loss = 0.04012745\n",
      "Iteration 26981, loss = 0.04012337\n",
      "Iteration 26982, loss = 0.04012398\n",
      "Iteration 26983, loss = 0.04012514\n",
      "Iteration 26984, loss = 0.04012894\n",
      "Iteration 26985, loss = 0.04012441\n",
      "Iteration 26986, loss = 0.04012271\n",
      "Iteration 26987, loss = 0.04012315\n",
      "Iteration 26988, loss = 0.04012491\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 26989, loss = 0.04012025\n",
      "Iteration 26990, loss = 0.04012104\n",
      "Iteration 26991, loss = 0.04011657\n",
      "Iteration 26992, loss = 0.04011863\n",
      "Iteration 26993, loss = 0.04011903\n",
      "Iteration 26994, loss = 0.04011477\n",
      "Iteration 26995, loss = 0.04011518\n",
      "Iteration 26996, loss = 0.04010993\n",
      "Iteration 26997, loss = 0.04011719\n",
      "Iteration 26998, loss = 0.04012011\n",
      "Iteration 26999, loss = 0.04011924\n",
      "Iteration 27000, loss = 0.04011431\n",
      "Iteration 27001, loss = 0.04011055\n",
      "Iteration 27002, loss = 0.04010713\n",
      "Iteration 27003, loss = 0.04011014\n",
      "Iteration 27004, loss = 0.04010571\n",
      "Iteration 27005, loss = 0.04011027\n",
      "Iteration 27006, loss = 0.04010449\n",
      "Iteration 27007, loss = 0.04009357\n",
      "Iteration 27008, loss = 0.04010095\n",
      "Iteration 27009, loss = 0.04010490\n",
      "Iteration 27010, loss = 0.04010330\n",
      "Iteration 27011, loss = 0.04010095\n",
      "Iteration 27012, loss = 0.04009891\n",
      "Iteration 27013, loss = 0.04009051\n",
      "Iteration 27014, loss = 0.04008599\n",
      "Iteration 27015, loss = 0.04008808\n",
      "Iteration 27016, loss = 0.04008688\n",
      "Iteration 27017, loss = 0.04008146\n",
      "Iteration 27018, loss = 0.04008071\n",
      "Iteration 27019, loss = 0.04008461\n",
      "Iteration 27020, loss = 0.04008199\n",
      "Iteration 27021, loss = 0.04007591\n",
      "Iteration 27022, loss = 0.04007319\n",
      "Iteration 27023, loss = 0.04007580\n",
      "Iteration 27024, loss = 0.04007712\n",
      "Iteration 27025, loss = 0.04007541\n",
      "Iteration 27026, loss = 0.04006912\n",
      "Iteration 27027, loss = 0.04006902\n",
      "Iteration 27028, loss = 0.04006693\n",
      "Iteration 27029, loss = 0.04006887\n",
      "Iteration 27030, loss = 0.04006786\n",
      "Iteration 27031, loss = 0.04006389\n",
      "Iteration 27032, loss = 0.04006107\n",
      "Iteration 27033, loss = 0.04006462\n",
      "Iteration 27034, loss = 0.04006111\n",
      "Iteration 27035, loss = 0.04005324\n",
      "Iteration 27036, loss = 0.04005875\n",
      "Iteration 27037, loss = 0.04005863\n",
      "Iteration 27038, loss = 0.04005655\n",
      "Iteration 27039, loss = 0.04005230\n",
      "Iteration 27040, loss = 0.04004728\n",
      "Iteration 27041, loss = 0.04004797\n",
      "Iteration 27042, loss = 0.04004736\n",
      "Iteration 27043, loss = 0.04004449\n",
      "Iteration 27044, loss = 0.04004383\n",
      "Iteration 27045, loss = 0.04004060\n",
      "Iteration 27046, loss = 0.04004004\n",
      "Iteration 27047, loss = 0.04004247\n",
      "Iteration 27048, loss = 0.04003997\n",
      "Iteration 27049, loss = 0.04003686\n",
      "Iteration 27050, loss = 0.04003828\n",
      "Iteration 27051, loss = 0.04003791\n",
      "Iteration 27052, loss = 0.04003631\n",
      "Iteration 27053, loss = 0.04003810\n",
      "Iteration 27054, loss = 0.04002914\n",
      "Iteration 27055, loss = 0.04003169\n",
      "Iteration 27056, loss = 0.04003911\n",
      "Iteration 27057, loss = 0.04003338\n",
      "Iteration 27058, loss = 0.04002415\n",
      "Iteration 27059, loss = 0.04002199\n",
      "Iteration 27060, loss = 0.04002318\n",
      "Iteration 27061, loss = 0.04002252\n",
      "Iteration 27062, loss = 0.04002211\n",
      "Iteration 27063, loss = 0.04001863\n",
      "Iteration 27064, loss = 0.04001412\n",
      "Iteration 27065, loss = 0.04001790\n",
      "Iteration 27066, loss = 0.04001954\n",
      "Iteration 27067, loss = 0.04001880\n",
      "Iteration 27068, loss = 0.04001367\n",
      "Iteration 27069, loss = 0.04001150\n",
      "Iteration 27070, loss = 0.04001317\n",
      "Iteration 27071, loss = 0.04001028\n",
      "Iteration 27072, loss = 0.04000538\n",
      "Iteration 27073, loss = 0.04000508\n",
      "Iteration 27074, loss = 0.04000511\n",
      "Iteration 27075, loss = 0.04000241\n",
      "Iteration 27076, loss = 0.04000062\n",
      "Iteration 27077, loss = 0.03999984\n",
      "Iteration 27078, loss = 0.04000255\n",
      "Iteration 27079, loss = 0.03999815\n",
      "Iteration 27080, loss = 0.03999579\n",
      "Iteration 27081, loss = 0.03999834\n",
      "Iteration 27082, loss = 0.03999988\n",
      "Iteration 27083, loss = 0.03999390\n",
      "Iteration 27084, loss = 0.03999114\n",
      "Iteration 27085, loss = 0.03999447\n",
      "Iteration 27086, loss = 0.03999009\n",
      "Iteration 27087, loss = 0.03999260\n",
      "Iteration 27088, loss = 0.03999155\n",
      "Iteration 27089, loss = 0.03999306\n",
      "Iteration 27090, loss = 0.03999230\n",
      "Iteration 27091, loss = 0.03998602\n",
      "Iteration 27092, loss = 0.03997806\n",
      "Iteration 27093, loss = 0.03999029\n",
      "Iteration 27094, loss = 0.03999425\n",
      "Iteration 27095, loss = 0.03998473\n",
      "Iteration 27096, loss = 0.03997875\n",
      "Iteration 27097, loss = 0.03998155\n",
      "Iteration 27098, loss = 0.03997949\n",
      "Iteration 27099, loss = 0.03997804\n",
      "Iteration 27100, loss = 0.03997506\n",
      "Iteration 27101, loss = 0.03996909\n",
      "Iteration 27102, loss = 0.03997071\n",
      "Iteration 27103, loss = 0.03996972\n",
      "Iteration 27104, loss = 0.03996676\n",
      "Iteration 27105, loss = 0.03996333\n",
      "Iteration 27106, loss = 0.03996203\n",
      "Iteration 27107, loss = 0.03996197\n",
      "Iteration 27108, loss = 0.03995803\n",
      "Iteration 27109, loss = 0.03995565\n",
      "Iteration 27110, loss = 0.03995734\n",
      "Iteration 27111, loss = 0.03996199\n",
      "Iteration 27112, loss = 0.03996062\n",
      "Iteration 27113, loss = 0.03995382\n",
      "Iteration 27114, loss = 0.03995284\n",
      "Iteration 27115, loss = 0.03996020\n",
      "Iteration 27116, loss = 0.03995859\n",
      "Iteration 27117, loss = 0.03994931\n",
      "Iteration 27118, loss = 0.03995721\n",
      "Iteration 27119, loss = 0.03995278\n",
      "Iteration 27120, loss = 0.03994580\n",
      "Iteration 27121, loss = 0.03994491\n",
      "Iteration 27122, loss = 0.03994703\n",
      "Iteration 27123, loss = 0.03994339\n",
      "Iteration 27124, loss = 0.03994346\n",
      "Iteration 27125, loss = 0.03994344\n",
      "Iteration 27126, loss = 0.03994292\n",
      "Iteration 27127, loss = 0.03994221\n",
      "Iteration 27128, loss = 0.03993930\n",
      "Iteration 27129, loss = 0.03993517\n",
      "Iteration 27130, loss = 0.03993386\n",
      "Iteration 27131, loss = 0.03993009\n",
      "Iteration 27132, loss = 0.03992753\n",
      "Iteration 27133, loss = 0.03992578\n",
      "Iteration 27134, loss = 0.03993236\n",
      "Iteration 27135, loss = 0.03992958\n",
      "Iteration 27136, loss = 0.03992942\n",
      "Iteration 27137, loss = 0.03992946\n",
      "Iteration 27138, loss = 0.03992668\n",
      "Iteration 27139, loss = 0.03992718\n",
      "Iteration 27140, loss = 0.03992905\n",
      "Iteration 27141, loss = 0.03992408\n",
      "Iteration 27142, loss = 0.03991921\n",
      "Iteration 27143, loss = 0.03991231\n",
      "Iteration 27144, loss = 0.03991893\n",
      "Iteration 27145, loss = 0.03991861\n",
      "Iteration 27146, loss = 0.03991748\n",
      "Iteration 27147, loss = 0.03991036\n",
      "Iteration 27148, loss = 0.03990362\n",
      "Iteration 27149, loss = 0.03990462\n",
      "Iteration 27150, loss = 0.03990810\n",
      "Iteration 27151, loss = 0.03990311\n",
      "Iteration 27152, loss = 0.03990018\n",
      "Iteration 27153, loss = 0.03989876\n",
      "Iteration 27154, loss = 0.03989892\n",
      "Iteration 27155, loss = 0.03989436\n",
      "Iteration 27156, loss = 0.03990162\n",
      "Iteration 27157, loss = 0.03990226\n",
      "Iteration 27158, loss = 0.03990185\n",
      "Iteration 27159, loss = 0.03990108\n",
      "Iteration 27160, loss = 0.03989703\n",
      "Iteration 27161, loss = 0.03989214\n",
      "Iteration 27162, loss = 0.03989372\n",
      "Iteration 27163, loss = 0.03989642\n",
      "Iteration 27164, loss = 0.03989210\n",
      "Iteration 27165, loss = 0.03988638\n",
      "Iteration 27166, loss = 0.03988846\n",
      "Iteration 27167, loss = 0.03988795\n",
      "Iteration 27168, loss = 0.03988690\n",
      "Iteration 27169, loss = 0.03988329\n",
      "Iteration 27170, loss = 0.03988534\n",
      "Iteration 27171, loss = 0.03987643\n",
      "Iteration 27172, loss = 0.03987600\n",
      "Iteration 27173, loss = 0.03988545\n",
      "Iteration 27174, loss = 0.03988302\n",
      "Iteration 27175, loss = 0.03987609\n",
      "Iteration 27176, loss = 0.03986886\n",
      "Iteration 27177, loss = 0.03986578\n",
      "Iteration 27178, loss = 0.03986587\n",
      "Iteration 27179, loss = 0.03986497\n",
      "Iteration 27180, loss = 0.03986575\n",
      "Iteration 27181, loss = 0.03986153\n",
      "Iteration 27182, loss = 0.03986207\n",
      "Iteration 27183, loss = 0.03986083\n",
      "Iteration 27184, loss = 0.03985812\n",
      "Iteration 27185, loss = 0.03985911\n",
      "Iteration 27186, loss = 0.03985711\n",
      "Iteration 27187, loss = 0.03985608\n",
      "Iteration 27188, loss = 0.03985929\n",
      "Iteration 27189, loss = 0.03985770\n",
      "Iteration 27190, loss = 0.03985214\n",
      "Iteration 27191, loss = 0.03985185\n",
      "Iteration 27192, loss = 0.03985048\n",
      "Iteration 27193, loss = 0.03985330\n",
      "Iteration 27194, loss = 0.03984992\n",
      "Iteration 27195, loss = 0.03984813\n",
      "Iteration 27196, loss = 0.03984846\n",
      "Iteration 27197, loss = 0.03984672\n",
      "Iteration 27198, loss = 0.03984825\n",
      "Iteration 27199, loss = 0.03984447\n",
      "Iteration 27200, loss = 0.03984522\n",
      "Iteration 27201, loss = 0.03984259\n",
      "Iteration 27202, loss = 0.03983839\n",
      "Iteration 27203, loss = 0.03984030\n",
      "Iteration 27204, loss = 0.03984118\n",
      "Iteration 27205, loss = 0.03984153\n",
      "Iteration 27206, loss = 0.03983673\n",
      "Iteration 27207, loss = 0.03984013\n",
      "Iteration 27208, loss = 0.03983671\n",
      "Iteration 27209, loss = 0.03983554\n",
      "Iteration 27210, loss = 0.03983256\n",
      "Iteration 27211, loss = 0.03982921\n",
      "Iteration 27212, loss = 0.03982788\n",
      "Iteration 27213, loss = 0.03982496\n",
      "Iteration 27214, loss = 0.03982750\n",
      "Iteration 27215, loss = 0.03982690\n",
      "Iteration 27216, loss = 0.03982057\n",
      "Iteration 27217, loss = 0.03981799\n",
      "Iteration 27218, loss = 0.03981867\n",
      "Iteration 27219, loss = 0.03981863\n",
      "Iteration 27220, loss = 0.03982012\n",
      "Iteration 27221, loss = 0.03981913\n",
      "Iteration 27222, loss = 0.03981562\n",
      "Iteration 27223, loss = 0.03981699\n",
      "Iteration 27224, loss = 0.03981380\n",
      "Iteration 27225, loss = 0.03981380\n",
      "Iteration 27226, loss = 0.03981051\n",
      "Iteration 27227, loss = 0.03981232\n",
      "Iteration 27228, loss = 0.03980947\n",
      "Iteration 27229, loss = 0.03979996\n",
      "Iteration 27230, loss = 0.03980153\n",
      "Iteration 27231, loss = 0.03980275\n",
      "Iteration 27232, loss = 0.03980573\n",
      "Iteration 27233, loss = 0.03980083\n",
      "Iteration 27234, loss = 0.03980644\n",
      "Iteration 27235, loss = 0.03980568\n",
      "Iteration 27236, loss = 0.03979845\n",
      "Iteration 27237, loss = 0.03979796\n",
      "Iteration 27238, loss = 0.03979613\n",
      "Iteration 27239, loss = 0.03979921\n",
      "Iteration 27240, loss = 0.03979542\n",
      "Iteration 27241, loss = 0.03979007\n",
      "Iteration 27242, loss = 0.03978814\n",
      "Iteration 27243, loss = 0.03979088\n",
      "Iteration 27244, loss = 0.03979503\n",
      "Iteration 27245, loss = 0.03979384\n",
      "Iteration 27246, loss = 0.03978608\n",
      "Iteration 27247, loss = 0.03977813\n",
      "Iteration 27248, loss = 0.03978549\n",
      "Iteration 27249, loss = 0.03978951\n",
      "Iteration 27250, loss = 0.03978156\n",
      "Iteration 27251, loss = 0.03977090\n",
      "Iteration 27252, loss = 0.03977456\n",
      "Iteration 27253, loss = 0.03977321\n",
      "Iteration 27254, loss = 0.03976874\n",
      "Iteration 27255, loss = 0.03976788\n",
      "Iteration 27256, loss = 0.03976718\n",
      "Iteration 27257, loss = 0.03976323\n",
      "Iteration 27258, loss = 0.03976106\n",
      "Iteration 27259, loss = 0.03976172\n",
      "Iteration 27260, loss = 0.03976134\n",
      "Iteration 27261, loss = 0.03975676\n",
      "Iteration 27262, loss = 0.03976293\n",
      "Iteration 27263, loss = 0.03976341\n",
      "Iteration 27264, loss = 0.03976213\n",
      "Iteration 27265, loss = 0.03975875\n",
      "Iteration 27266, loss = 0.03975065\n",
      "Iteration 27267, loss = 0.03974904\n",
      "Iteration 27268, loss = 0.03975014\n",
      "Iteration 27269, loss = 0.03974747\n",
      "Iteration 27270, loss = 0.03975009\n",
      "Iteration 27271, loss = 0.03974968\n",
      "Iteration 27272, loss = 0.03974468\n",
      "Iteration 27273, loss = 0.03974716\n",
      "Iteration 27274, loss = 0.03975459\n",
      "Iteration 27275, loss = 0.03974867\n",
      "Iteration 27276, loss = 0.03974250\n",
      "Iteration 27277, loss = 0.03974024\n",
      "Iteration 27278, loss = 0.03974136\n",
      "Iteration 27279, loss = 0.03973820\n",
      "Iteration 27280, loss = 0.03973943\n",
      "Iteration 27281, loss = 0.03973986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 27282, loss = 0.03973615\n",
      "Iteration 27283, loss = 0.03973535\n",
      "Iteration 27284, loss = 0.03972963\n",
      "Iteration 27285, loss = 0.03972834\n",
      "Iteration 27286, loss = 0.03973169\n",
      "Iteration 27287, loss = 0.03972917\n",
      "Iteration 27288, loss = 0.03972732\n",
      "Iteration 27289, loss = 0.03972459\n",
      "Iteration 27290, loss = 0.03972883\n",
      "Iteration 27291, loss = 0.03973039\n",
      "Iteration 27292, loss = 0.03972538\n",
      "Iteration 27293, loss = 0.03972480\n",
      "Iteration 27294, loss = 0.03972086\n",
      "Iteration 27295, loss = 0.03971753\n",
      "Iteration 27296, loss = 0.03971936\n",
      "Iteration 27297, loss = 0.03971847\n",
      "Iteration 27298, loss = 0.03971966\n",
      "Iteration 27299, loss = 0.03971942\n",
      "Iteration 27300, loss = 0.03972452\n",
      "Iteration 27301, loss = 0.03971994\n",
      "Iteration 27302, loss = 0.03971179\n",
      "Iteration 27303, loss = 0.03970655\n",
      "Iteration 27304, loss = 0.03970360\n",
      "Iteration 27305, loss = 0.03970882\n",
      "Iteration 27306, loss = 0.03970682\n",
      "Iteration 27307, loss = 0.03969761\n",
      "Iteration 27308, loss = 0.03970002\n",
      "Iteration 27309, loss = 0.03970069\n",
      "Iteration 27310, loss = 0.03969855\n",
      "Iteration 27311, loss = 0.03969354\n",
      "Iteration 27312, loss = 0.03969504\n",
      "Iteration 27313, loss = 0.03969297\n",
      "Iteration 27314, loss = 0.03969102\n",
      "Iteration 27315, loss = 0.03969086\n",
      "Iteration 27316, loss = 0.03968925\n",
      "Iteration 27317, loss = 0.03968670\n",
      "Iteration 27318, loss = 0.03968215\n",
      "Iteration 27319, loss = 0.03968402\n",
      "Iteration 27320, loss = 0.03968476\n",
      "Iteration 27321, loss = 0.03967838\n",
      "Iteration 27322, loss = 0.03967988\n",
      "Iteration 27323, loss = 0.03967791\n",
      "Iteration 27324, loss = 0.03967943\n",
      "Iteration 27325, loss = 0.03968076\n",
      "Iteration 27326, loss = 0.03967916\n",
      "Iteration 27327, loss = 0.03967480\n",
      "Iteration 27328, loss = 0.03967268\n",
      "Iteration 27329, loss = 0.03967206\n",
      "Iteration 27330, loss = 0.03967535\n",
      "Iteration 27331, loss = 0.03967342\n",
      "Iteration 27332, loss = 0.03966902\n",
      "Iteration 27333, loss = 0.03966503\n",
      "Iteration 27334, loss = 0.03966376\n",
      "Iteration 27335, loss = 0.03966162\n",
      "Iteration 27336, loss = 0.03966425\n",
      "Iteration 27337, loss = 0.03966321\n",
      "Iteration 27338, loss = 0.03965896\n",
      "Iteration 27339, loss = 0.03965644\n",
      "Iteration 27340, loss = 0.03966192\n",
      "Iteration 27341, loss = 0.03966018\n",
      "Iteration 27342, loss = 0.03966262\n",
      "Iteration 27343, loss = 0.03966262\n",
      "Iteration 27344, loss = 0.03965841\n",
      "Iteration 27345, loss = 0.03965082\n",
      "Iteration 27346, loss = 0.03965565\n",
      "Iteration 27347, loss = 0.03965847\n",
      "Iteration 27348, loss = 0.03966033\n",
      "Iteration 27349, loss = 0.03965497\n",
      "Iteration 27350, loss = 0.03965043\n",
      "Iteration 27351, loss = 0.03964967\n",
      "Iteration 27352, loss = 0.03964858\n",
      "Iteration 27353, loss = 0.03964755\n",
      "Iteration 27354, loss = 0.03964335\n",
      "Iteration 27355, loss = 0.03963874\n",
      "Iteration 27356, loss = 0.03963960\n",
      "Iteration 27357, loss = 0.03963420\n",
      "Iteration 27358, loss = 0.03963787\n",
      "Iteration 27359, loss = 0.03963623\n",
      "Iteration 27360, loss = 0.03963027\n",
      "Iteration 27361, loss = 0.03963181\n",
      "Iteration 27362, loss = 0.03962886\n",
      "Iteration 27363, loss = 0.03962548\n",
      "Iteration 27364, loss = 0.03962799\n",
      "Iteration 27365, loss = 0.03962857\n",
      "Iteration 27366, loss = 0.03962156\n",
      "Iteration 27367, loss = 0.03962438\n",
      "Iteration 27368, loss = 0.03962447\n",
      "Iteration 27369, loss = 0.03961907\n",
      "Iteration 27370, loss = 0.03961810\n",
      "Iteration 27371, loss = 0.03961844\n",
      "Iteration 27372, loss = 0.03961481\n",
      "Iteration 27373, loss = 0.03962065\n",
      "Iteration 27374, loss = 0.03961941\n",
      "Iteration 27375, loss = 0.03961799\n",
      "Iteration 27376, loss = 0.03961282\n",
      "Iteration 27377, loss = 0.03960822\n",
      "Iteration 27378, loss = 0.03961449\n",
      "Iteration 27379, loss = 0.03961608\n",
      "Iteration 27380, loss = 0.03960667\n",
      "Iteration 27381, loss = 0.03960460\n",
      "Iteration 27382, loss = 0.03960794\n",
      "Iteration 27383, loss = 0.03960985\n",
      "Iteration 27384, loss = 0.03960592\n",
      "Iteration 27385, loss = 0.03960217\n",
      "Iteration 27386, loss = 0.03959779\n",
      "Iteration 27387, loss = 0.03959774\n",
      "Iteration 27388, loss = 0.03960611\n",
      "Iteration 27389, loss = 0.03960109\n",
      "Iteration 27390, loss = 0.03959436\n",
      "Iteration 27391, loss = 0.03959473\n",
      "Iteration 27392, loss = 0.03959333\n",
      "Iteration 27393, loss = 0.03959042\n",
      "Iteration 27394, loss = 0.03958397\n",
      "Iteration 27395, loss = 0.03958714\n",
      "Iteration 27396, loss = 0.03959165\n",
      "Iteration 27397, loss = 0.03958996\n",
      "Iteration 27398, loss = 0.03958587\n",
      "Iteration 27399, loss = 0.03958266\n",
      "Iteration 27400, loss = 0.03958185\n",
      "Iteration 27401, loss = 0.03958307\n",
      "Iteration 27402, loss = 0.03958531\n",
      "Iteration 27403, loss = 0.03958526\n",
      "Iteration 27404, loss = 0.03957933\n",
      "Iteration 27405, loss = 0.03958005\n",
      "Iteration 27406, loss = 0.03958041\n",
      "Iteration 27407, loss = 0.03957636\n",
      "Iteration 27408, loss = 0.03957134\n",
      "Iteration 27409, loss = 0.03957097\n",
      "Iteration 27410, loss = 0.03957392\n",
      "Iteration 27411, loss = 0.03956902\n",
      "Iteration 27412, loss = 0.03956764\n",
      "Iteration 27413, loss = 0.03957428\n",
      "Iteration 27414, loss = 0.03956888\n",
      "Iteration 27415, loss = 0.03956422\n",
      "Iteration 27416, loss = 0.03956496\n",
      "Iteration 27417, loss = 0.03956849\n",
      "Iteration 27418, loss = 0.03956355\n",
      "Iteration 27419, loss = 0.03956086\n",
      "Iteration 27420, loss = 0.03956114\n",
      "Iteration 27421, loss = 0.03955489\n",
      "Iteration 27422, loss = 0.03956258\n",
      "Iteration 27423, loss = 0.03956143\n",
      "Iteration 27424, loss = 0.03955581\n",
      "Iteration 27425, loss = 0.03955456\n",
      "Iteration 27426, loss = 0.03955193\n",
      "Iteration 27427, loss = 0.03954673\n",
      "Iteration 27428, loss = 0.03954371\n",
      "Iteration 27429, loss = 0.03954909\n",
      "Iteration 27430, loss = 0.03954791\n",
      "Iteration 27431, loss = 0.03954512\n",
      "Iteration 27432, loss = 0.03953934\n",
      "Iteration 27433, loss = 0.03954365\n",
      "Iteration 27434, loss = 0.03954755\n",
      "Iteration 27435, loss = 0.03954421\n",
      "Iteration 27436, loss = 0.03954361\n",
      "Iteration 27437, loss = 0.03953990\n",
      "Iteration 27438, loss = 0.03953947\n",
      "Iteration 27439, loss = 0.03953553\n",
      "Iteration 27440, loss = 0.03953784\n",
      "Iteration 27441, loss = 0.03954123\n",
      "Iteration 27442, loss = 0.03953827\n",
      "Iteration 27443, loss = 0.03953108\n",
      "Iteration 27444, loss = 0.03953171\n",
      "Iteration 27445, loss = 0.03953060\n",
      "Iteration 27446, loss = 0.03953786\n",
      "Iteration 27447, loss = 0.03953494\n",
      "Iteration 27448, loss = 0.03953232\n",
      "Iteration 27449, loss = 0.03952547\n",
      "Iteration 27450, loss = 0.03951825\n",
      "Iteration 27451, loss = 0.03952539\n",
      "Iteration 27452, loss = 0.03952721\n",
      "Iteration 27453, loss = 0.03951755\n",
      "Iteration 27454, loss = 0.03951964\n",
      "Iteration 27455, loss = 0.03952004\n",
      "Iteration 27456, loss = 0.03951969\n",
      "Iteration 27457, loss = 0.03952235\n",
      "Iteration 27458, loss = 0.03952104\n",
      "Iteration 27459, loss = 0.03951754\n",
      "Iteration 27460, loss = 0.03951572\n",
      "Iteration 27461, loss = 0.03951419\n",
      "Iteration 27462, loss = 0.03951115\n",
      "Iteration 27463, loss = 0.03950454\n",
      "Iteration 27464, loss = 0.03950804\n",
      "Iteration 27465, loss = 0.03950604\n",
      "Iteration 27466, loss = 0.03950205\n",
      "Iteration 27467, loss = 0.03950005\n",
      "Iteration 27468, loss = 0.03950290\n",
      "Iteration 27469, loss = 0.03950064\n",
      "Iteration 27470, loss = 0.03950053\n",
      "Iteration 27471, loss = 0.03949677\n",
      "Iteration 27472, loss = 0.03949297\n",
      "Iteration 27473, loss = 0.03949009\n",
      "Iteration 27474, loss = 0.03949288\n",
      "Iteration 27475, loss = 0.03948997\n",
      "Iteration 27476, loss = 0.03948715\n",
      "Iteration 27477, loss = 0.03948469\n",
      "Iteration 27478, loss = 0.03948901\n",
      "Iteration 27479, loss = 0.03948610\n",
      "Iteration 27480, loss = 0.03947865\n",
      "Iteration 27481, loss = 0.03948195\n",
      "Iteration 27482, loss = 0.03947956\n",
      "Iteration 27483, loss = 0.03947967\n",
      "Iteration 27484, loss = 0.03947655\n",
      "Iteration 27485, loss = 0.03947579\n",
      "Iteration 27486, loss = 0.03947744\n",
      "Iteration 27487, loss = 0.03947909\n",
      "Iteration 27488, loss = 0.03947137\n",
      "Iteration 27489, loss = 0.03947118\n",
      "Iteration 27490, loss = 0.03947284\n",
      "Iteration 27491, loss = 0.03946862\n",
      "Iteration 27492, loss = 0.03946637\n",
      "Iteration 27493, loss = 0.03946616\n",
      "Iteration 27494, loss = 0.03946479\n",
      "Iteration 27495, loss = 0.03946887\n",
      "Iteration 27496, loss = 0.03946798\n",
      "Iteration 27497, loss = 0.03946466\n",
      "Iteration 27498, loss = 0.03945993\n",
      "Iteration 27499, loss = 0.03946428\n",
      "Iteration 27500, loss = 0.03946614\n",
      "Iteration 27501, loss = 0.03946121\n",
      "Iteration 27502, loss = 0.03945343\n",
      "Iteration 27503, loss = 0.03945494\n",
      "Iteration 27504, loss = 0.03945236\n",
      "Iteration 27505, loss = 0.03944846\n",
      "Iteration 27506, loss = 0.03945182\n",
      "Iteration 27507, loss = 0.03944881\n",
      "Iteration 27508, loss = 0.03944599\n",
      "Iteration 27509, loss = 0.03944464\n",
      "Iteration 27510, loss = 0.03944186\n",
      "Iteration 27511, loss = 0.03944463\n",
      "Iteration 27512, loss = 0.03944121\n",
      "Iteration 27513, loss = 0.03944168\n",
      "Iteration 27514, loss = 0.03944130\n",
      "Iteration 27515, loss = 0.03944156\n",
      "Iteration 27516, loss = 0.03943831\n",
      "Iteration 27517, loss = 0.03943607\n",
      "Iteration 27518, loss = 0.03943507\n",
      "Iteration 27519, loss = 0.03943495\n",
      "Iteration 27520, loss = 0.03942924\n",
      "Iteration 27521, loss = 0.03943027\n",
      "Iteration 27522, loss = 0.03943235\n",
      "Iteration 27523, loss = 0.03943576\n",
      "Iteration 27524, loss = 0.03943369\n",
      "Iteration 27525, loss = 0.03943310\n",
      "Iteration 27526, loss = 0.03942416\n",
      "Iteration 27527, loss = 0.03942389\n",
      "Iteration 27528, loss = 0.03942614\n",
      "Iteration 27529, loss = 0.03942427\n",
      "Iteration 27530, loss = 0.03942730\n",
      "Iteration 27531, loss = 0.03942258\n",
      "Iteration 27532, loss = 0.03942609\n",
      "Iteration 27533, loss = 0.03942864\n",
      "Iteration 27534, loss = 0.03942320\n",
      "Iteration 27535, loss = 0.03941848\n",
      "Iteration 27536, loss = 0.03942246\n",
      "Iteration 27537, loss = 0.03942184\n",
      "Iteration 27538, loss = 0.03942213\n",
      "Iteration 27539, loss = 0.03942110\n",
      "Iteration 27540, loss = 0.03941999\n",
      "Iteration 27541, loss = 0.03941388\n",
      "Iteration 27542, loss = 0.03940869\n",
      "Iteration 27543, loss = 0.03941153\n",
      "Iteration 27544, loss = 0.03941577\n",
      "Iteration 27545, loss = 0.03941672\n",
      "Iteration 27546, loss = 0.03940787\n",
      "Iteration 27547, loss = 0.03941048\n",
      "Iteration 27548, loss = 0.03940586\n",
      "Iteration 27549, loss = 0.03940151\n",
      "Iteration 27550, loss = 0.03940094\n",
      "Iteration 27551, loss = 0.03939930\n",
      "Iteration 27552, loss = 0.03939830\n",
      "Iteration 27553, loss = 0.03939594\n",
      "Iteration 27554, loss = 0.03939815\n",
      "Iteration 27555, loss = 0.03939647\n",
      "Iteration 27556, loss = 0.03939572\n",
      "Iteration 27557, loss = 0.03939296\n",
      "Iteration 27558, loss = 0.03939525\n",
      "Iteration 27559, loss = 0.03939745\n",
      "Iteration 27560, loss = 0.03939169\n",
      "Iteration 27561, loss = 0.03939275\n",
      "Iteration 27562, loss = 0.03938989\n",
      "Iteration 27563, loss = 0.03938945\n",
      "Iteration 27564, loss = 0.03938656\n",
      "Iteration 27565, loss = 0.03938372\n",
      "Iteration 27566, loss = 0.03938533\n",
      "Iteration 27567, loss = 0.03938754\n",
      "Iteration 27568, loss = 0.03938604\n",
      "Iteration 27569, loss = 0.03938063\n",
      "Iteration 27570, loss = 0.03938211\n",
      "Iteration 27571, loss = 0.03938356\n",
      "Iteration 27572, loss = 0.03938228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 27573, loss = 0.03937709\n",
      "Iteration 27574, loss = 0.03937854\n",
      "Iteration 27575, loss = 0.03937837\n",
      "Iteration 27576, loss = 0.03937104\n",
      "Iteration 27577, loss = 0.03937118\n",
      "Iteration 27578, loss = 0.03937541\n",
      "Iteration 27579, loss = 0.03937523\n",
      "Iteration 27580, loss = 0.03936971\n",
      "Iteration 27581, loss = 0.03936552\n",
      "Iteration 27582, loss = 0.03936861\n",
      "Iteration 27583, loss = 0.03936997\n",
      "Iteration 27584, loss = 0.03936489\n",
      "Iteration 27585, loss = 0.03936272\n",
      "Iteration 27586, loss = 0.03936370\n",
      "Iteration 27587, loss = 0.03936065\n",
      "Iteration 27588, loss = 0.03936114\n",
      "Iteration 27589, loss = 0.03936190\n",
      "Iteration 27590, loss = 0.03936177\n",
      "Iteration 27591, loss = 0.03935913\n",
      "Iteration 27592, loss = 0.03935429\n",
      "Iteration 27593, loss = 0.03934778\n",
      "Iteration 27594, loss = 0.03935128\n",
      "Iteration 27595, loss = 0.03935556\n",
      "Iteration 27596, loss = 0.03935524\n",
      "Iteration 27597, loss = 0.03935000\n",
      "Iteration 27598, loss = 0.03934807\n",
      "Iteration 27599, loss = 0.03934686\n",
      "Iteration 27600, loss = 0.03935362\n",
      "Iteration 27601, loss = 0.03934905\n",
      "Iteration 27602, loss = 0.03934200\n",
      "Iteration 27603, loss = 0.03933647\n",
      "Iteration 27604, loss = 0.03934405\n",
      "Iteration 27605, loss = 0.03934274\n",
      "Iteration 27606, loss = 0.03933739\n",
      "Iteration 27607, loss = 0.03933588\n",
      "Iteration 27608, loss = 0.03933473\n",
      "Iteration 27609, loss = 0.03933224\n",
      "Iteration 27610, loss = 0.03933397\n",
      "Iteration 27611, loss = 0.03933281\n",
      "Iteration 27612, loss = 0.03932985\n",
      "Iteration 27613, loss = 0.03932760\n",
      "Iteration 27614, loss = 0.03933006\n",
      "Iteration 27615, loss = 0.03932852\n",
      "Iteration 27616, loss = 0.03932162\n",
      "Iteration 27617, loss = 0.03932863\n",
      "Iteration 27618, loss = 0.03933119\n",
      "Iteration 27619, loss = 0.03932627\n",
      "Iteration 27620, loss = 0.03932481\n",
      "Iteration 27621, loss = 0.03932294\n",
      "Iteration 27622, loss = 0.03931867\n",
      "Iteration 27623, loss = 0.03931468\n",
      "Iteration 27624, loss = 0.03931903\n",
      "Iteration 27625, loss = 0.03931626\n",
      "Iteration 27626, loss = 0.03931285\n",
      "Iteration 27627, loss = 0.03931396\n",
      "Iteration 27628, loss = 0.03931684\n",
      "Iteration 27629, loss = 0.03931418\n",
      "Iteration 27630, loss = 0.03930926\n",
      "Iteration 27631, loss = 0.03930662\n",
      "Iteration 27632, loss = 0.03931116\n",
      "Iteration 27633, loss = 0.03930774\n",
      "Iteration 27634, loss = 0.03930629\n",
      "Iteration 27635, loss = 0.03930534\n",
      "Iteration 27636, loss = 0.03930423\n",
      "Iteration 27637, loss = 0.03929905\n",
      "Iteration 27638, loss = 0.03929738\n",
      "Iteration 27639, loss = 0.03929893\n",
      "Iteration 27640, loss = 0.03929783\n",
      "Iteration 27641, loss = 0.03929354\n",
      "Iteration 27642, loss = 0.03929945\n",
      "Iteration 27643, loss = 0.03929856\n",
      "Iteration 27644, loss = 0.03929779\n",
      "Iteration 27645, loss = 0.03928976\n",
      "Iteration 27646, loss = 0.03929944\n",
      "Iteration 27647, loss = 0.03930323\n",
      "Iteration 27648, loss = 0.03929634\n",
      "Iteration 27649, loss = 0.03929004\n",
      "Iteration 27650, loss = 0.03929279\n",
      "Iteration 27651, loss = 0.03928964\n",
      "Iteration 27652, loss = 0.03929042\n",
      "Iteration 27653, loss = 0.03929592\n",
      "Iteration 27654, loss = 0.03929969\n",
      "Iteration 27655, loss = 0.03929750\n",
      "Iteration 27656, loss = 0.03928863\n",
      "Iteration 27657, loss = 0.03928500\n",
      "Iteration 27658, loss = 0.03928570\n",
      "Iteration 27659, loss = 0.03928006\n",
      "Iteration 27660, loss = 0.03928240\n",
      "Iteration 27661, loss = 0.03928260\n",
      "Iteration 27662, loss = 0.03927375\n",
      "Iteration 27663, loss = 0.03927292\n",
      "Iteration 27664, loss = 0.03927738\n",
      "Iteration 27665, loss = 0.03927938\n",
      "Iteration 27666, loss = 0.03927820\n",
      "Iteration 27667, loss = 0.03927080\n",
      "Iteration 27668, loss = 0.03926954\n",
      "Iteration 27669, loss = 0.03926826\n",
      "Iteration 27670, loss = 0.03925997\n",
      "Iteration 27671, loss = 0.03926288\n",
      "Iteration 27672, loss = 0.03926433\n",
      "Iteration 27673, loss = 0.03926202\n",
      "Iteration 27674, loss = 0.03926104\n",
      "Iteration 27675, loss = 0.03925685\n",
      "Iteration 27676, loss = 0.03926170\n",
      "Iteration 27677, loss = 0.03926234\n",
      "Iteration 27678, loss = 0.03926192\n",
      "Iteration 27679, loss = 0.03925549\n",
      "Iteration 27680, loss = 0.03925778\n",
      "Iteration 27681, loss = 0.03925726\n",
      "Iteration 27682, loss = 0.03925786\n",
      "Iteration 27683, loss = 0.03925387\n",
      "Iteration 27684, loss = 0.03925172\n",
      "Iteration 27685, loss = 0.03925185\n",
      "Iteration 27686, loss = 0.03924488\n",
      "Iteration 27687, loss = 0.03925095\n",
      "Iteration 27688, loss = 0.03925194\n",
      "Iteration 27689, loss = 0.03924655\n",
      "Iteration 27690, loss = 0.03924077\n",
      "Iteration 27691, loss = 0.03924140\n",
      "Iteration 27692, loss = 0.03924495\n",
      "Iteration 27693, loss = 0.03924117\n",
      "Iteration 27694, loss = 0.03923425\n",
      "Iteration 27695, loss = 0.03923921\n",
      "Iteration 27696, loss = 0.03924308\n",
      "Iteration 27697, loss = 0.03923401\n",
      "Iteration 27698, loss = 0.03923776\n",
      "Iteration 27699, loss = 0.03924629\n",
      "Iteration 27700, loss = 0.03924459\n",
      "Iteration 27701, loss = 0.03923825\n",
      "Iteration 27702, loss = 0.03922828\n",
      "Iteration 27703, loss = 0.03922881\n",
      "Iteration 27704, loss = 0.03923402\n",
      "Iteration 27705, loss = 0.03923441\n",
      "Iteration 27706, loss = 0.03922751\n",
      "Iteration 27707, loss = 0.03922827\n",
      "Iteration 27708, loss = 0.03922270\n",
      "Iteration 27709, loss = 0.03921707\n",
      "Iteration 27710, loss = 0.03922184\n",
      "Iteration 27711, loss = 0.03922318\n",
      "Iteration 27712, loss = 0.03922131\n",
      "Iteration 27713, loss = 0.03922006\n",
      "Iteration 27714, loss = 0.03921631\n",
      "Iteration 27715, loss = 0.03921217\n",
      "Iteration 27716, loss = 0.03920658\n",
      "Iteration 27717, loss = 0.03921252\n",
      "Iteration 27718, loss = 0.03921423\n",
      "Iteration 27719, loss = 0.03920886\n",
      "Iteration 27720, loss = 0.03920724\n",
      "Iteration 27721, loss = 0.03921066\n",
      "Iteration 27722, loss = 0.03921581\n",
      "Iteration 27723, loss = 0.03921272\n",
      "Iteration 27724, loss = 0.03920412\n",
      "Iteration 27725, loss = 0.03920336\n",
      "Iteration 27726, loss = 0.03920654\n",
      "Iteration 27727, loss = 0.03920577\n",
      "Iteration 27728, loss = 0.03920285\n",
      "Iteration 27729, loss = 0.03920114\n",
      "Iteration 27730, loss = 0.03919816\n",
      "Iteration 27731, loss = 0.03920065\n",
      "Iteration 27732, loss = 0.03919939\n",
      "Iteration 27733, loss = 0.03919467\n",
      "Iteration 27734, loss = 0.03919944\n",
      "Iteration 27735, loss = 0.03919833\n",
      "Iteration 27736, loss = 0.03919805\n",
      "Iteration 27737, loss = 0.03919814\n",
      "Iteration 27738, loss = 0.03919176\n",
      "Iteration 27739, loss = 0.03918587\n",
      "Iteration 27740, loss = 0.03918919\n",
      "Iteration 27741, loss = 0.03918708\n",
      "Iteration 27742, loss = 0.03918280\n",
      "Iteration 27743, loss = 0.03917743\n",
      "Iteration 27744, loss = 0.03918008\n",
      "Iteration 27745, loss = 0.03917777\n",
      "Iteration 27746, loss = 0.03917584\n",
      "Iteration 27747, loss = 0.03918258\n",
      "Iteration 27748, loss = 0.03918271\n",
      "Iteration 27749, loss = 0.03917527\n",
      "Iteration 27750, loss = 0.03917354\n",
      "Iteration 27751, loss = 0.03917060\n",
      "Iteration 27752, loss = 0.03916978\n",
      "Iteration 27753, loss = 0.03917459\n",
      "Iteration 27754, loss = 0.03917159\n",
      "Iteration 27755, loss = 0.03916900\n",
      "Iteration 27756, loss = 0.03916311\n",
      "Iteration 27757, loss = 0.03917041\n",
      "Iteration 27758, loss = 0.03917096\n",
      "Iteration 27759, loss = 0.03917035\n",
      "Iteration 27760, loss = 0.03916176\n",
      "Iteration 27761, loss = 0.03916688\n",
      "Iteration 27762, loss = 0.03917268\n",
      "Iteration 27763, loss = 0.03917083\n",
      "Iteration 27764, loss = 0.03916130\n",
      "Iteration 27765, loss = 0.03916080\n",
      "Iteration 27766, loss = 0.03915970\n",
      "Iteration 27767, loss = 0.03915998\n",
      "Iteration 27768, loss = 0.03915533\n",
      "Iteration 27769, loss = 0.03914851\n",
      "Iteration 27770, loss = 0.03915556\n",
      "Iteration 27771, loss = 0.03916112\n",
      "Iteration 27772, loss = 0.03915626\n",
      "Iteration 27773, loss = 0.03915222\n",
      "Iteration 27774, loss = 0.03915131\n",
      "Iteration 27775, loss = 0.03915138\n",
      "Iteration 27776, loss = 0.03914952\n",
      "Iteration 27777, loss = 0.03914516\n",
      "Iteration 27778, loss = 0.03914262\n",
      "Iteration 27779, loss = 0.03914346\n",
      "Iteration 27780, loss = 0.03914006\n",
      "Iteration 27781, loss = 0.03914061\n",
      "Iteration 27782, loss = 0.03914454\n",
      "Iteration 27783, loss = 0.03913949\n",
      "Iteration 27784, loss = 0.03913851\n",
      "Iteration 27785, loss = 0.03913859\n",
      "Iteration 27786, loss = 0.03914528\n",
      "Iteration 27787, loss = 0.03914207\n",
      "Iteration 27788, loss = 0.03913220\n",
      "Iteration 27789, loss = 0.03913363\n",
      "Iteration 27790, loss = 0.03913399\n",
      "Iteration 27791, loss = 0.03913893\n",
      "Iteration 27792, loss = 0.03913821\n",
      "Iteration 27793, loss = 0.03913775\n",
      "Iteration 27794, loss = 0.03913135\n",
      "Iteration 27795, loss = 0.03912433\n",
      "Iteration 27796, loss = 0.03913117\n",
      "Iteration 27797, loss = 0.03913602\n",
      "Iteration 27798, loss = 0.03912903\n",
      "Iteration 27799, loss = 0.03911952\n",
      "Iteration 27800, loss = 0.03912283\n",
      "Iteration 27801, loss = 0.03912899\n",
      "Iteration 27802, loss = 0.03912712\n",
      "Iteration 27803, loss = 0.03912225\n",
      "Iteration 27804, loss = 0.03911680\n",
      "Iteration 27805, loss = 0.03911641\n",
      "Iteration 27806, loss = 0.03911630\n",
      "Iteration 27807, loss = 0.03911538\n",
      "Iteration 27808, loss = 0.03911430\n",
      "Iteration 27809, loss = 0.03910875\n",
      "Iteration 27810, loss = 0.03910660\n",
      "Iteration 27811, loss = 0.03910266\n",
      "Iteration 27812, loss = 0.03910431\n",
      "Iteration 27813, loss = 0.03910697\n",
      "Iteration 27814, loss = 0.03910319\n",
      "Iteration 27815, loss = 0.03910314\n",
      "Iteration 27816, loss = 0.03910450\n",
      "Iteration 27817, loss = 0.03910487\n",
      "Iteration 27818, loss = 0.03910757\n",
      "Iteration 27819, loss = 0.03910412\n",
      "Iteration 27820, loss = 0.03910156\n",
      "Iteration 27821, loss = 0.03910007\n",
      "Iteration 27822, loss = 0.03909679\n",
      "Iteration 27823, loss = 0.03909670\n",
      "Iteration 27824, loss = 0.03909472\n",
      "Iteration 27825, loss = 0.03908998\n",
      "Iteration 27826, loss = 0.03909429\n",
      "Iteration 27827, loss = 0.03909501\n",
      "Iteration 27828, loss = 0.03909045\n",
      "Iteration 27829, loss = 0.03909167\n",
      "Iteration 27830, loss = 0.03909730\n",
      "Iteration 27831, loss = 0.03910007\n",
      "Iteration 27832, loss = 0.03909653\n",
      "Iteration 27833, loss = 0.03908974\n",
      "Iteration 27834, loss = 0.03908397\n",
      "Iteration 27835, loss = 0.03908221\n",
      "Iteration 27836, loss = 0.03908976\n",
      "Iteration 27837, loss = 0.03908862\n",
      "Iteration 27838, loss = 0.03908103\n",
      "Iteration 27839, loss = 0.03907985\n",
      "Iteration 27840, loss = 0.03908381\n",
      "Iteration 27841, loss = 0.03908917\n",
      "Iteration 27842, loss = 0.03908570\n",
      "Iteration 27843, loss = 0.03907249\n",
      "Iteration 27844, loss = 0.03908066\n",
      "Iteration 27845, loss = 0.03908093\n",
      "Iteration 27846, loss = 0.03908274\n",
      "Iteration 27847, loss = 0.03908679\n",
      "Iteration 27848, loss = 0.03908277\n",
      "Iteration 27849, loss = 0.03907010\n",
      "Iteration 27850, loss = 0.03906703\n",
      "Iteration 27851, loss = 0.03906937\n",
      "Iteration 27852, loss = 0.03907500\n",
      "Iteration 27853, loss = 0.03907199\n",
      "Iteration 27854, loss = 0.03906213\n",
      "Iteration 27855, loss = 0.03906046\n",
      "Iteration 27856, loss = 0.03906420\n",
      "Iteration 27857, loss = 0.03905546\n",
      "Iteration 27858, loss = 0.03905566\n",
      "Iteration 27859, loss = 0.03905727\n",
      "Iteration 27860, loss = 0.03905613\n",
      "Iteration 27861, loss = 0.03905659\n",
      "Iteration 27862, loss = 0.03905605\n",
      "Iteration 27863, loss = 0.03904762\n",
      "Iteration 27864, loss = 0.03905310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 27865, loss = 0.03905923\n",
      "Iteration 27866, loss = 0.03904794\n",
      "Iteration 27867, loss = 0.03904810\n",
      "Iteration 27868, loss = 0.03905276\n",
      "Iteration 27869, loss = 0.03904882\n",
      "Iteration 27870, loss = 0.03905390\n",
      "Iteration 27871, loss = 0.03905438\n",
      "Iteration 27872, loss = 0.03904675\n",
      "Iteration 27873, loss = 0.03904076\n",
      "Iteration 27874, loss = 0.03904106\n",
      "Iteration 27875, loss = 0.03904033\n",
      "Iteration 27876, loss = 0.03903465\n",
      "Iteration 27877, loss = 0.03903810\n",
      "Iteration 27878, loss = 0.03904263\n",
      "Iteration 27879, loss = 0.03904238\n",
      "Iteration 27880, loss = 0.03903974\n",
      "Iteration 27881, loss = 0.03903603\n",
      "Iteration 27882, loss = 0.03903260\n",
      "Iteration 27883, loss = 0.03903411\n",
      "Iteration 27884, loss = 0.03903477\n",
      "Iteration 27885, loss = 0.03903289\n",
      "Iteration 27886, loss = 0.03902902\n",
      "Iteration 27887, loss = 0.03902842\n",
      "Iteration 27888, loss = 0.03903172\n",
      "Iteration 27889, loss = 0.03903317\n",
      "Iteration 27890, loss = 0.03902873\n",
      "Iteration 27891, loss = 0.03902463\n",
      "Iteration 27892, loss = 0.03903087\n",
      "Iteration 27893, loss = 0.03902520\n",
      "Iteration 27894, loss = 0.03902466\n",
      "Iteration 27895, loss = 0.03902673\n",
      "Iteration 27896, loss = 0.03902699\n",
      "Iteration 27897, loss = 0.03902612\n",
      "Iteration 27898, loss = 0.03902206\n",
      "Iteration 27899, loss = 0.03901245\n",
      "Iteration 27900, loss = 0.03901325\n",
      "Iteration 27901, loss = 0.03902267\n",
      "Iteration 27902, loss = 0.03901415\n",
      "Iteration 27903, loss = 0.03901411\n",
      "Iteration 27904, loss = 0.03901598\n",
      "Iteration 27905, loss = 0.03901887\n",
      "Iteration 27906, loss = 0.03901418\n",
      "Iteration 27907, loss = 0.03900561\n",
      "Iteration 27908, loss = 0.03900245\n",
      "Iteration 27909, loss = 0.03900071\n",
      "Iteration 27910, loss = 0.03899673\n",
      "Iteration 27911, loss = 0.03899472\n",
      "Iteration 27912, loss = 0.03899674\n",
      "Iteration 27913, loss = 0.03899694\n",
      "Iteration 27914, loss = 0.03899717\n",
      "Iteration 27915, loss = 0.03900032\n",
      "Iteration 27916, loss = 0.03899882\n",
      "Iteration 27917, loss = 0.03899437\n",
      "Iteration 27918, loss = 0.03898890\n",
      "Iteration 27919, loss = 0.03899230\n",
      "Iteration 27920, loss = 0.03899042\n",
      "Iteration 27921, loss = 0.03898804\n",
      "Iteration 27922, loss = 0.03899068\n",
      "Iteration 27923, loss = 0.03898892\n",
      "Iteration 27924, loss = 0.03898840\n",
      "Iteration 27925, loss = 0.03898253\n",
      "Iteration 27926, loss = 0.03898413\n",
      "Iteration 27927, loss = 0.03898598\n",
      "Iteration 27928, loss = 0.03898410\n",
      "Iteration 27929, loss = 0.03897842\n",
      "Iteration 27930, loss = 0.03898692\n",
      "Iteration 27931, loss = 0.03898639\n",
      "Iteration 27932, loss = 0.03898539\n",
      "Iteration 27933, loss = 0.03898530\n",
      "Iteration 27934, loss = 0.03898135\n",
      "Iteration 27935, loss = 0.03897559\n",
      "Iteration 27936, loss = 0.03898695\n",
      "Iteration 27937, loss = 0.03898415\n",
      "Iteration 27938, loss = 0.03897668\n",
      "Iteration 27939, loss = 0.03897951\n",
      "Iteration 27940, loss = 0.03897956\n",
      "Iteration 27941, loss = 0.03897568\n",
      "Iteration 27942, loss = 0.03896951\n",
      "Iteration 27943, loss = 0.03896829\n",
      "Iteration 27944, loss = 0.03896954\n",
      "Iteration 27945, loss = 0.03897071\n",
      "Iteration 27946, loss = 0.03896439\n",
      "Iteration 27947, loss = 0.03896300\n",
      "Iteration 27948, loss = 0.03896472\n",
      "Iteration 27949, loss = 0.03896680\n",
      "Iteration 27950, loss = 0.03896000\n",
      "Iteration 27951, loss = 0.03895565\n",
      "Iteration 27952, loss = 0.03895534\n",
      "Iteration 27953, loss = 0.03895370\n",
      "Iteration 27954, loss = 0.03895287\n",
      "Iteration 27955, loss = 0.03895171\n",
      "Iteration 27956, loss = 0.03895295\n",
      "Iteration 27957, loss = 0.03895683\n",
      "Iteration 27958, loss = 0.03895380\n",
      "Iteration 27959, loss = 0.03894549\n",
      "Iteration 27960, loss = 0.03895101\n",
      "Iteration 27961, loss = 0.03895042\n",
      "Iteration 27962, loss = 0.03894154\n",
      "Iteration 27963, loss = 0.03894474\n",
      "Iteration 27964, loss = 0.03894603\n",
      "Iteration 27965, loss = 0.03894167\n",
      "Iteration 27966, loss = 0.03894399\n",
      "Iteration 27967, loss = 0.03894589\n",
      "Iteration 27968, loss = 0.03894341\n",
      "Iteration 27969, loss = 0.03893986\n",
      "Iteration 27970, loss = 0.03893893\n",
      "Iteration 27971, loss = 0.03894333\n",
      "Iteration 27972, loss = 0.03893873\n",
      "Iteration 27973, loss = 0.03893525\n",
      "Iteration 27974, loss = 0.03894315\n",
      "Iteration 27975, loss = 0.03894016\n",
      "Iteration 27976, loss = 0.03892805\n",
      "Iteration 27977, loss = 0.03893603\n",
      "Iteration 27978, loss = 0.03893632\n",
      "Iteration 27979, loss = 0.03893197\n",
      "Iteration 27980, loss = 0.03892580\n",
      "Iteration 27981, loss = 0.03893275\n",
      "Iteration 27982, loss = 0.03893501\n",
      "Iteration 27983, loss = 0.03893151\n",
      "Iteration 27984, loss = 0.03892763\n",
      "Iteration 27985, loss = 0.03892715\n",
      "Iteration 27986, loss = 0.03892533\n",
      "Iteration 27987, loss = 0.03892035\n",
      "Iteration 27988, loss = 0.03892264\n",
      "Iteration 27989, loss = 0.03892308\n",
      "Iteration 27990, loss = 0.03892671\n",
      "Iteration 27991, loss = 0.03892124\n",
      "Iteration 27992, loss = 0.03891648\n",
      "Iteration 27993, loss = 0.03891490\n",
      "Iteration 27994, loss = 0.03892169\n",
      "Iteration 27995, loss = 0.03891845\n",
      "Iteration 27996, loss = 0.03891145\n",
      "Iteration 27997, loss = 0.03891094\n",
      "Iteration 27998, loss = 0.03891782\n",
      "Iteration 27999, loss = 0.03891762\n",
      "Iteration 28000, loss = 0.03891234\n",
      "Iteration 28001, loss = 0.03890510\n",
      "Iteration 28002, loss = 0.03890027\n",
      "Iteration 28003, loss = 0.03890386\n",
      "Iteration 28004, loss = 0.03890635\n",
      "Iteration 28005, loss = 0.03889944\n",
      "Iteration 28006, loss = 0.03889663\n",
      "Iteration 28007, loss = 0.03889776\n",
      "Iteration 28008, loss = 0.03890385\n",
      "Iteration 28009, loss = 0.03890622\n",
      "Iteration 28010, loss = 0.03890494\n",
      "Iteration 28011, loss = 0.03889995\n",
      "Iteration 28012, loss = 0.03889537\n",
      "Iteration 28013, loss = 0.03889747\n",
      "Iteration 28014, loss = 0.03889794\n",
      "Iteration 28015, loss = 0.03888861\n",
      "Iteration 28016, loss = 0.03889390\n",
      "Iteration 28017, loss = 0.03889794\n",
      "Iteration 28018, loss = 0.03889657\n",
      "Iteration 28019, loss = 0.03889528\n",
      "Iteration 28020, loss = 0.03889369\n",
      "Iteration 28021, loss = 0.03889266\n",
      "Iteration 28022, loss = 0.03889516\n",
      "Iteration 28023, loss = 0.03889197\n",
      "Iteration 28024, loss = 0.03888871\n",
      "Iteration 28025, loss = 0.03888931\n",
      "Iteration 28026, loss = 0.03888761\n",
      "Iteration 28027, loss = 0.03888653\n",
      "Iteration 28028, loss = 0.03889023\n",
      "Iteration 28029, loss = 0.03889435\n",
      "Iteration 28030, loss = 0.03888997\n",
      "Iteration 28031, loss = 0.03887989\n",
      "Iteration 28032, loss = 0.03887569\n",
      "Iteration 28033, loss = 0.03888541\n",
      "Iteration 28034, loss = 0.03888581\n",
      "Iteration 28035, loss = 0.03888015\n",
      "Iteration 28036, loss = 0.03887032\n",
      "Iteration 28037, loss = 0.03887337\n",
      "Iteration 28038, loss = 0.03888137\n",
      "Iteration 28039, loss = 0.03887794\n",
      "Iteration 28040, loss = 0.03886827\n",
      "Iteration 28041, loss = 0.03886595\n",
      "Iteration 28042, loss = 0.03887093\n",
      "Iteration 28043, loss = 0.03887233\n",
      "Iteration 28044, loss = 0.03886555\n",
      "Iteration 28045, loss = 0.03886016\n",
      "Iteration 28046, loss = 0.03886231\n",
      "Iteration 28047, loss = 0.03886125\n",
      "Iteration 28048, loss = 0.03885278\n",
      "Iteration 28049, loss = 0.03885559\n",
      "Iteration 28050, loss = 0.03886134\n",
      "Iteration 28051, loss = 0.03885983\n",
      "Iteration 28052, loss = 0.03885634\n",
      "Iteration 28053, loss = 0.03885559\n",
      "Iteration 28054, loss = 0.03885484\n",
      "Iteration 28055, loss = 0.03884646\n",
      "Iteration 28056, loss = 0.03885451\n",
      "Iteration 28057, loss = 0.03885495\n",
      "Iteration 28058, loss = 0.03885433\n",
      "Iteration 28059, loss = 0.03885133\n",
      "Iteration 28060, loss = 0.03884968\n",
      "Iteration 28061, loss = 0.03885317\n",
      "Iteration 28062, loss = 0.03884757\n",
      "Iteration 28063, loss = 0.03885010\n",
      "Iteration 28064, loss = 0.03884736\n",
      "Iteration 28065, loss = 0.03884696\n",
      "Iteration 28066, loss = 0.03885123\n",
      "Iteration 28067, loss = 0.03885266\n",
      "Iteration 28068, loss = 0.03885116\n",
      "Iteration 28069, loss = 0.03884478\n",
      "Iteration 28070, loss = 0.03884147\n",
      "Iteration 28071, loss = 0.03883987\n",
      "Iteration 28072, loss = 0.03885085\n",
      "Iteration 28073, loss = 0.03884950\n",
      "Iteration 28074, loss = 0.03884300\n",
      "Iteration 28075, loss = 0.03883310\n",
      "Iteration 28076, loss = 0.03883925\n",
      "Iteration 28077, loss = 0.03884832\n",
      "Iteration 28078, loss = 0.03885284\n",
      "Iteration 28079, loss = 0.03884948\n",
      "Iteration 28080, loss = 0.03884289\n",
      "Iteration 28081, loss = 0.03883213\n",
      "Iteration 28082, loss = 0.03883171\n",
      "Iteration 28083, loss = 0.03882616\n",
      "Iteration 28084, loss = 0.03882738\n",
      "Iteration 28085, loss = 0.03883503\n",
      "Iteration 28086, loss = 0.03883525\n",
      "Iteration 28087, loss = 0.03883146\n",
      "Iteration 28088, loss = 0.03882278\n",
      "Iteration 28089, loss = 0.03882562\n",
      "Iteration 28090, loss = 0.03882665\n",
      "Iteration 28091, loss = 0.03881943\n",
      "Iteration 28092, loss = 0.03883118\n",
      "Iteration 28093, loss = 0.03883378\n",
      "Iteration 28094, loss = 0.03882333\n",
      "Iteration 28095, loss = 0.03881311\n",
      "Iteration 28096, loss = 0.03881198\n",
      "Iteration 28097, loss = 0.03881892\n",
      "Iteration 28098, loss = 0.03881623\n",
      "Iteration 28099, loss = 0.03881613\n",
      "Iteration 28100, loss = 0.03881570\n",
      "Iteration 28101, loss = 0.03881622\n",
      "Iteration 28102, loss = 0.03881028\n",
      "Iteration 28103, loss = 0.03881390\n",
      "Iteration 28104, loss = 0.03881464\n",
      "Iteration 28105, loss = 0.03881525\n",
      "Iteration 28106, loss = 0.03880903\n",
      "Iteration 28107, loss = 0.03879855\n",
      "Iteration 28108, loss = 0.03879813\n",
      "Iteration 28109, loss = 0.03880108\n",
      "Iteration 28110, loss = 0.03879862\n",
      "Iteration 28111, loss = 0.03879586\n",
      "Iteration 28112, loss = 0.03879391\n",
      "Iteration 28113, loss = 0.03879184\n",
      "Iteration 28114, loss = 0.03878749\n",
      "Iteration 28115, loss = 0.03878839\n",
      "Iteration 28116, loss = 0.03878586\n",
      "Iteration 28117, loss = 0.03878828\n",
      "Iteration 28118, loss = 0.03879065\n",
      "Iteration 28119, loss = 0.03878936\n",
      "Iteration 28120, loss = 0.03878550\n",
      "Iteration 28121, loss = 0.03878353\n",
      "Iteration 28122, loss = 0.03878288\n",
      "Iteration 28123, loss = 0.03878557\n",
      "Iteration 28124, loss = 0.03878292\n",
      "Iteration 28125, loss = 0.03877912\n",
      "Iteration 28126, loss = 0.03878431\n",
      "Iteration 28127, loss = 0.03878323\n",
      "Iteration 28128, loss = 0.03877676\n",
      "Iteration 28129, loss = 0.03878087\n",
      "Iteration 28130, loss = 0.03877573\n",
      "Iteration 28131, loss = 0.03877386\n",
      "Iteration 28132, loss = 0.03877250\n",
      "Iteration 28133, loss = 0.03877059\n",
      "Iteration 28134, loss = 0.03876938\n",
      "Iteration 28135, loss = 0.03876836\n",
      "Iteration 28136, loss = 0.03876842\n",
      "Iteration 28137, loss = 0.03876655\n",
      "Iteration 28138, loss = 0.03876592\n",
      "Iteration 28139, loss = 0.03876511\n",
      "Iteration 28140, loss = 0.03876499\n",
      "Iteration 28141, loss = 0.03876192\n",
      "Iteration 28142, loss = 0.03876631\n",
      "Iteration 28143, loss = 0.03875889\n",
      "Iteration 28144, loss = 0.03876091\n",
      "Iteration 28145, loss = 0.03876391\n",
      "Iteration 28146, loss = 0.03876016\n",
      "Iteration 28147, loss = 0.03876509\n",
      "Iteration 28148, loss = 0.03876055\n",
      "Iteration 28149, loss = 0.03876278\n",
      "Iteration 28150, loss = 0.03876660\n",
      "Iteration 28151, loss = 0.03876303\n",
      "Iteration 28152, loss = 0.03875597\n",
      "Iteration 28153, loss = 0.03876077\n",
      "Iteration 28154, loss = 0.03876038\n",
      "Iteration 28155, loss = 0.03875589\n",
      "Iteration 28156, loss = 0.03874739\n",
      "Iteration 28157, loss = 0.03875416\n",
      "Iteration 28158, loss = 0.03875413\n",
      "Iteration 28159, loss = 0.03875544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 28160, loss = 0.03875100\n",
      "Iteration 28161, loss = 0.03875165\n",
      "Iteration 28162, loss = 0.03874660\n",
      "Iteration 28163, loss = 0.03874424\n",
      "Iteration 28164, loss = 0.03874043\n",
      "Iteration 28165, loss = 0.03874152\n",
      "Iteration 28166, loss = 0.03874239\n",
      "Iteration 28167, loss = 0.03874681\n",
      "Iteration 28168, loss = 0.03874027\n",
      "Iteration 28169, loss = 0.03873901\n",
      "Iteration 28170, loss = 0.03873666\n",
      "Iteration 28171, loss = 0.03873837\n",
      "Iteration 28172, loss = 0.03873443\n",
      "Iteration 28173, loss = 0.03873322\n",
      "Iteration 28174, loss = 0.03873270\n",
      "Iteration 28175, loss = 0.03872516\n",
      "Iteration 28176, loss = 0.03873417\n",
      "Iteration 28177, loss = 0.03873635\n",
      "Iteration 28178, loss = 0.03873850\n",
      "Iteration 28179, loss = 0.03873676\n",
      "Iteration 28180, loss = 0.03873050\n",
      "Iteration 28181, loss = 0.03873234\n",
      "Iteration 28182, loss = 0.03873093\n",
      "Iteration 28183, loss = 0.03872129\n",
      "Iteration 28184, loss = 0.03872316\n",
      "Iteration 28185, loss = 0.03872037\n",
      "Iteration 28186, loss = 0.03872228\n",
      "Iteration 28187, loss = 0.03872228\n",
      "Iteration 28188, loss = 0.03871968\n",
      "Iteration 28189, loss = 0.03871462\n",
      "Iteration 28190, loss = 0.03871786\n",
      "Iteration 28191, loss = 0.03871661\n",
      "Iteration 28192, loss = 0.03871593\n",
      "Iteration 28193, loss = 0.03871190\n",
      "Iteration 28194, loss = 0.03871327\n",
      "Iteration 28195, loss = 0.03871261\n",
      "Iteration 28196, loss = 0.03870733\n",
      "Iteration 28197, loss = 0.03871295\n",
      "Iteration 28198, loss = 0.03870640\n",
      "Iteration 28199, loss = 0.03871236\n",
      "Iteration 28200, loss = 0.03871523\n",
      "Iteration 28201, loss = 0.03871113\n",
      "Iteration 28202, loss = 0.03870860\n",
      "Iteration 28203, loss = 0.03870996\n",
      "Iteration 28204, loss = 0.03870491\n",
      "Iteration 28205, loss = 0.03870007\n",
      "Iteration 28206, loss = 0.03870308\n",
      "Iteration 28207, loss = 0.03870665\n",
      "Iteration 28208, loss = 0.03869922\n",
      "Iteration 28209, loss = 0.03869309\n",
      "Iteration 28210, loss = 0.03869646\n",
      "Iteration 28211, loss = 0.03869710\n",
      "Iteration 28212, loss = 0.03869783\n",
      "Iteration 28213, loss = 0.03869274\n",
      "Iteration 28214, loss = 0.03869609\n",
      "Iteration 28215, loss = 0.03869185\n",
      "Iteration 28216, loss = 0.03868820\n",
      "Iteration 28217, loss = 0.03869374\n",
      "Iteration 28218, loss = 0.03869487\n",
      "Iteration 28219, loss = 0.03869328\n",
      "Iteration 28220, loss = 0.03869233\n",
      "Iteration 28221, loss = 0.03869299\n",
      "Iteration 28222, loss = 0.03869262\n",
      "Iteration 28223, loss = 0.03869114\n",
      "Iteration 28224, loss = 0.03869243\n",
      "Iteration 28225, loss = 0.03868960\n",
      "Iteration 28226, loss = 0.03868979\n",
      "Iteration 28227, loss = 0.03868670\n",
      "Iteration 28228, loss = 0.03868489\n",
      "Iteration 28229, loss = 0.03868913\n",
      "Iteration 28230, loss = 0.03868391\n",
      "Iteration 28231, loss = 0.03867860\n",
      "Iteration 28232, loss = 0.03867423\n",
      "Iteration 28233, loss = 0.03867913\n",
      "Iteration 28234, loss = 0.03868058\n",
      "Iteration 28235, loss = 0.03867748\n",
      "Iteration 28236, loss = 0.03867430\n",
      "Iteration 28237, loss = 0.03866971\n",
      "Iteration 28238, loss = 0.03866739\n",
      "Iteration 28239, loss = 0.03867139\n",
      "Iteration 28240, loss = 0.03866849\n",
      "Iteration 28241, loss = 0.03866203\n",
      "Iteration 28242, loss = 0.03866319\n",
      "Iteration 28243, loss = 0.03866804\n",
      "Iteration 28244, loss = 0.03866440\n",
      "Iteration 28245, loss = 0.03866327\n",
      "Iteration 28246, loss = 0.03866234\n",
      "Iteration 28247, loss = 0.03866786\n",
      "Iteration 28248, loss = 0.03866119\n",
      "Iteration 28249, loss = 0.03865452\n",
      "Iteration 28250, loss = 0.03866368\n",
      "Iteration 28251, loss = 0.03866210\n",
      "Iteration 28252, loss = 0.03865582\n",
      "Iteration 28253, loss = 0.03865951\n",
      "Iteration 28254, loss = 0.03865751\n",
      "Iteration 28255, loss = 0.03865271\n",
      "Iteration 28256, loss = 0.03864851\n",
      "Iteration 28257, loss = 0.03865683\n",
      "Iteration 28258, loss = 0.03866000\n",
      "Iteration 28259, loss = 0.03865298\n",
      "Iteration 28260, loss = 0.03864266\n",
      "Iteration 28261, loss = 0.03864822\n",
      "Iteration 28262, loss = 0.03864867\n",
      "Iteration 28263, loss = 0.03864323\n",
      "Iteration 28264, loss = 0.03864185\n",
      "Iteration 28265, loss = 0.03864523\n",
      "Iteration 28266, loss = 0.03864041\n",
      "Iteration 28267, loss = 0.03863992\n",
      "Iteration 28268, loss = 0.03864025\n",
      "Iteration 28269, loss = 0.03864211\n",
      "Iteration 28270, loss = 0.03863620\n",
      "Iteration 28271, loss = 0.03864172\n",
      "Iteration 28272, loss = 0.03864604\n",
      "Iteration 28273, loss = 0.03864620\n",
      "Iteration 28274, loss = 0.03863865\n",
      "Iteration 28275, loss = 0.03863353\n",
      "Iteration 28276, loss = 0.03864200\n",
      "Iteration 28277, loss = 0.03864729\n",
      "Iteration 28278, loss = 0.03863664\n",
      "Iteration 28279, loss = 0.03862974\n",
      "Iteration 28280, loss = 0.03863518\n",
      "Iteration 28281, loss = 0.03864156\n",
      "Iteration 28282, loss = 0.03864139\n",
      "Iteration 28283, loss = 0.03863268\n",
      "Iteration 28284, loss = 0.03862871\n",
      "Iteration 28285, loss = 0.03863639\n",
      "Iteration 28286, loss = 0.03863930\n",
      "Iteration 28287, loss = 0.03863355\n",
      "Iteration 28288, loss = 0.03862167\n",
      "Iteration 28289, loss = 0.03862460\n",
      "Iteration 28290, loss = 0.03863312\n",
      "Iteration 28291, loss = 0.03863350\n",
      "Iteration 28292, loss = 0.03862404\n",
      "Iteration 28293, loss = 0.03862226\n",
      "Iteration 28294, loss = 0.03862372\n",
      "Iteration 28295, loss = 0.03861956\n",
      "Iteration 28296, loss = 0.03861995\n",
      "Iteration 28297, loss = 0.03861622\n",
      "Iteration 28298, loss = 0.03861364\n",
      "Iteration 28299, loss = 0.03861663\n",
      "Iteration 28300, loss = 0.03861310\n",
      "Iteration 28301, loss = 0.03860849\n",
      "Iteration 28302, loss = 0.03860823\n",
      "Iteration 28303, loss = 0.03860918\n",
      "Iteration 28304, loss = 0.03860386\n",
      "Iteration 28305, loss = 0.03860673\n",
      "Iteration 28306, loss = 0.03860085\n",
      "Iteration 28307, loss = 0.03860881\n",
      "Iteration 28308, loss = 0.03860817\n",
      "Iteration 28309, loss = 0.03860627\n",
      "Iteration 28310, loss = 0.03860648\n",
      "Iteration 28311, loss = 0.03860507\n",
      "Iteration 28312, loss = 0.03860481\n",
      "Iteration 28313, loss = 0.03859959\n",
      "Iteration 28314, loss = 0.03859281\n",
      "Iteration 28315, loss = 0.03859496\n",
      "Iteration 28316, loss = 0.03859284\n",
      "Iteration 28317, loss = 0.03859209\n",
      "Iteration 28318, loss = 0.03859352\n",
      "Iteration 28319, loss = 0.03859414\n",
      "Iteration 28320, loss = 0.03859014\n",
      "Iteration 28321, loss = 0.03858975\n",
      "Iteration 28322, loss = 0.03858634\n",
      "Iteration 28323, loss = 0.03858900\n",
      "Iteration 28324, loss = 0.03858724\n",
      "Iteration 28325, loss = 0.03858837\n",
      "Iteration 28326, loss = 0.03858570\n",
      "Iteration 28327, loss = 0.03858594\n",
      "Iteration 28328, loss = 0.03857922\n",
      "Iteration 28329, loss = 0.03858330\n",
      "Iteration 28330, loss = 0.03858698\n",
      "Iteration 28331, loss = 0.03858108\n",
      "Iteration 28332, loss = 0.03857541\n",
      "Iteration 28333, loss = 0.03858406\n",
      "Iteration 28334, loss = 0.03859136\n",
      "Iteration 28335, loss = 0.03858666\n",
      "Iteration 28336, loss = 0.03858690\n",
      "Iteration 28337, loss = 0.03858379\n",
      "Iteration 28338, loss = 0.03858524\n",
      "Iteration 28339, loss = 0.03857970\n",
      "Iteration 28340, loss = 0.03857785\n",
      "Iteration 28341, loss = 0.03858272\n",
      "Iteration 28342, loss = 0.03857899\n",
      "Iteration 28343, loss = 0.03856951\n",
      "Iteration 28344, loss = 0.03858106\n",
      "Iteration 28345, loss = 0.03858910\n",
      "Iteration 28346, loss = 0.03858714\n",
      "Iteration 28347, loss = 0.03858198\n",
      "Iteration 28348, loss = 0.03857670\n",
      "Iteration 28349, loss = 0.03856854\n",
      "Iteration 28350, loss = 0.03857389\n",
      "Iteration 28351, loss = 0.03857560\n",
      "Iteration 28352, loss = 0.03857031\n",
      "Iteration 28353, loss = 0.03857214\n",
      "Iteration 28354, loss = 0.03856695\n",
      "Iteration 28355, loss = 0.03856563\n",
      "Iteration 28356, loss = 0.03856756\n",
      "Iteration 28357, loss = 0.03856764\n",
      "Iteration 28358, loss = 0.03856166\n",
      "Iteration 28359, loss = 0.03856153\n",
      "Iteration 28360, loss = 0.03856269\n",
      "Iteration 28361, loss = 0.03856157\n",
      "Iteration 28362, loss = 0.03855693\n",
      "Iteration 28363, loss = 0.03856156\n",
      "Iteration 28364, loss = 0.03855852\n",
      "Iteration 28365, loss = 0.03855030\n",
      "Iteration 28366, loss = 0.03854990\n",
      "Iteration 28367, loss = 0.03855330\n",
      "Iteration 28368, loss = 0.03855540\n",
      "Iteration 28369, loss = 0.03855279\n",
      "Iteration 28370, loss = 0.03855056\n",
      "Iteration 28371, loss = 0.03854967\n",
      "Iteration 28372, loss = 0.03854817\n",
      "Iteration 28373, loss = 0.03854615\n",
      "Iteration 28374, loss = 0.03854236\n",
      "Iteration 28375, loss = 0.03853919\n",
      "Iteration 28376, loss = 0.03853462\n",
      "Iteration 28377, loss = 0.03853526\n",
      "Iteration 28378, loss = 0.03853682\n",
      "Iteration 28379, loss = 0.03854154\n",
      "Iteration 28380, loss = 0.03853462\n",
      "Iteration 28381, loss = 0.03853068\n",
      "Iteration 28382, loss = 0.03853274\n",
      "Iteration 28383, loss = 0.03853826\n",
      "Iteration 28384, loss = 0.03853763\n",
      "Iteration 28385, loss = 0.03853531\n",
      "Iteration 28386, loss = 0.03852865\n",
      "Iteration 28387, loss = 0.03853070\n",
      "Iteration 28388, loss = 0.03853103\n",
      "Iteration 28389, loss = 0.03852731\n",
      "Iteration 28390, loss = 0.03852115\n",
      "Iteration 28391, loss = 0.03852086\n",
      "Iteration 28392, loss = 0.03852446\n",
      "Iteration 28393, loss = 0.03852522\n",
      "Iteration 28394, loss = 0.03852620\n",
      "Iteration 28395, loss = 0.03852186\n",
      "Iteration 28396, loss = 0.03851792\n",
      "Iteration 28397, loss = 0.03852281\n",
      "Iteration 28398, loss = 0.03852176\n",
      "Iteration 28399, loss = 0.03851790\n",
      "Iteration 28400, loss = 0.03851708\n",
      "Iteration 28401, loss = 0.03851430\n",
      "Iteration 28402, loss = 0.03851427\n",
      "Iteration 28403, loss = 0.03851375\n",
      "Iteration 28404, loss = 0.03850790\n",
      "Iteration 28405, loss = 0.03850515\n",
      "Iteration 28406, loss = 0.03850808\n",
      "Iteration 28407, loss = 0.03850642\n",
      "Iteration 28408, loss = 0.03851026\n",
      "Iteration 28409, loss = 0.03850815\n",
      "Iteration 28410, loss = 0.03851028\n",
      "Iteration 28411, loss = 0.03850834\n",
      "Iteration 28412, loss = 0.03849996\n",
      "Iteration 28413, loss = 0.03850473\n",
      "Iteration 28414, loss = 0.03850250\n",
      "Iteration 28415, loss = 0.03849534\n",
      "Iteration 28416, loss = 0.03849587\n",
      "Iteration 28417, loss = 0.03849435\n",
      "Iteration 28418, loss = 0.03849556\n",
      "Iteration 28419, loss = 0.03849302\n",
      "Iteration 28420, loss = 0.03849031\n",
      "Iteration 28421, loss = 0.03849177\n",
      "Iteration 28422, loss = 0.03848947\n",
      "Iteration 28423, loss = 0.03848749\n",
      "Iteration 28424, loss = 0.03848958\n",
      "Iteration 28425, loss = 0.03848574\n",
      "Iteration 28426, loss = 0.03848688\n",
      "Iteration 28427, loss = 0.03848892\n",
      "Iteration 28428, loss = 0.03848481\n",
      "Iteration 28429, loss = 0.03849136\n",
      "Iteration 28430, loss = 0.03848743\n",
      "Iteration 28431, loss = 0.03848649\n",
      "Iteration 28432, loss = 0.03848462\n",
      "Iteration 28433, loss = 0.03848329\n",
      "Iteration 28434, loss = 0.03848443\n",
      "Iteration 28435, loss = 0.03848205\n",
      "Iteration 28436, loss = 0.03848960\n",
      "Iteration 28437, loss = 0.03848752\n",
      "Iteration 28438, loss = 0.03848314\n",
      "Iteration 28439, loss = 0.03847570\n",
      "Iteration 28440, loss = 0.03848032\n",
      "Iteration 28441, loss = 0.03847668\n",
      "Iteration 28442, loss = 0.03847375\n",
      "Iteration 28443, loss = 0.03847315\n",
      "Iteration 28444, loss = 0.03847246\n",
      "Iteration 28445, loss = 0.03846918\n",
      "Iteration 28446, loss = 0.03847003\n",
      "Iteration 28447, loss = 0.03847435\n",
      "Iteration 28448, loss = 0.03847738\n",
      "Iteration 28449, loss = 0.03846993\n",
      "Iteration 28450, loss = 0.03846187\n",
      "Iteration 28451, loss = 0.03846588\n",
      "Iteration 28452, loss = 0.03846618\n",
      "Iteration 28453, loss = 0.03846268\n",
      "Iteration 28454, loss = 0.03846193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 28455, loss = 0.03846192\n",
      "Iteration 28456, loss = 0.03845903\n",
      "Iteration 28457, loss = 0.03846159\n",
      "Iteration 28458, loss = 0.03846170\n",
      "Iteration 28459, loss = 0.03846504\n",
      "Iteration 28460, loss = 0.03846634\n",
      "Iteration 28461, loss = 0.03845837\n",
      "Iteration 28462, loss = 0.03845609\n",
      "Iteration 28463, loss = 0.03845907\n",
      "Iteration 28464, loss = 0.03845690\n",
      "Iteration 28465, loss = 0.03845577\n",
      "Iteration 28466, loss = 0.03845959\n",
      "Iteration 28467, loss = 0.03845597\n",
      "Iteration 28468, loss = 0.03846285\n",
      "Iteration 28469, loss = 0.03846225\n",
      "Iteration 28470, loss = 0.03845177\n",
      "Iteration 28471, loss = 0.03845080\n",
      "Iteration 28472, loss = 0.03846179\n",
      "Iteration 28473, loss = 0.03846051\n",
      "Iteration 28474, loss = 0.03845403\n",
      "Iteration 28475, loss = 0.03844573\n",
      "Iteration 28476, loss = 0.03844386\n",
      "Iteration 28477, loss = 0.03844977\n",
      "Iteration 28478, loss = 0.03844726\n",
      "Iteration 28479, loss = 0.03844588\n",
      "Iteration 28480, loss = 0.03844125\n",
      "Iteration 28481, loss = 0.03844545\n",
      "Iteration 28482, loss = 0.03844097\n",
      "Iteration 28483, loss = 0.03844311\n",
      "Iteration 28484, loss = 0.03844534\n",
      "Iteration 28485, loss = 0.03844253\n",
      "Iteration 28486, loss = 0.03844692\n",
      "Iteration 28487, loss = 0.03844437\n",
      "Iteration 28488, loss = 0.03843825\n",
      "Iteration 28489, loss = 0.03843109\n",
      "Iteration 28490, loss = 0.03843442\n",
      "Iteration 28491, loss = 0.03843486\n",
      "Iteration 28492, loss = 0.03842912\n",
      "Iteration 28493, loss = 0.03842868\n",
      "Iteration 28494, loss = 0.03843380\n",
      "Iteration 28495, loss = 0.03843141\n",
      "Iteration 28496, loss = 0.03842993\n",
      "Iteration 28497, loss = 0.03843117\n",
      "Iteration 28498, loss = 0.03843147\n",
      "Iteration 28499, loss = 0.03842518\n",
      "Iteration 28500, loss = 0.03842262\n",
      "Iteration 28501, loss = 0.03842176\n",
      "Iteration 28502, loss = 0.03842538\n",
      "Iteration 28503, loss = 0.03842157\n",
      "Iteration 28504, loss = 0.03841711\n",
      "Iteration 28505, loss = 0.03842192\n",
      "Iteration 28506, loss = 0.03842384\n",
      "Iteration 28507, loss = 0.03841613\n",
      "Iteration 28508, loss = 0.03841619\n",
      "Iteration 28509, loss = 0.03842417\n",
      "Iteration 28510, loss = 0.03842164\n",
      "Iteration 28511, loss = 0.03841522\n",
      "Iteration 28512, loss = 0.03842034\n",
      "Iteration 28513, loss = 0.03842137\n",
      "Iteration 28514, loss = 0.03841754\n",
      "Iteration 28515, loss = 0.03841086\n",
      "Iteration 28516, loss = 0.03840972\n",
      "Iteration 28517, loss = 0.03841564\n",
      "Iteration 28518, loss = 0.03841120\n",
      "Iteration 28519, loss = 0.03840641\n",
      "Iteration 28520, loss = 0.03840769\n",
      "Iteration 28521, loss = 0.03840955\n",
      "Iteration 28522, loss = 0.03840882\n",
      "Iteration 28523, loss = 0.03841099\n",
      "Iteration 28524, loss = 0.03840884\n",
      "Iteration 28525, loss = 0.03840091\n",
      "Iteration 28526, loss = 0.03839622\n",
      "Iteration 28527, loss = 0.03839993\n",
      "Iteration 28528, loss = 0.03840656\n",
      "Iteration 28529, loss = 0.03839632\n",
      "Iteration 28530, loss = 0.03838810\n",
      "Iteration 28531, loss = 0.03839305\n",
      "Iteration 28532, loss = 0.03839351\n",
      "Iteration 28533, loss = 0.03839680\n",
      "Iteration 28534, loss = 0.03839658\n",
      "Iteration 28535, loss = 0.03839126\n",
      "Iteration 28536, loss = 0.03838570\n",
      "Iteration 28537, loss = 0.03838668\n",
      "Iteration 28538, loss = 0.03839425\n",
      "Iteration 28539, loss = 0.03838898\n",
      "Iteration 28540, loss = 0.03838291\n",
      "Iteration 28541, loss = 0.03838821\n",
      "Iteration 28542, loss = 0.03838600\n",
      "Iteration 28543, loss = 0.03838974\n",
      "Iteration 28544, loss = 0.03838961\n",
      "Iteration 28545, loss = 0.03837922\n",
      "Iteration 28546, loss = 0.03837943\n",
      "Iteration 28547, loss = 0.03838433\n",
      "Iteration 28548, loss = 0.03838018\n",
      "Iteration 28549, loss = 0.03836883\n",
      "Iteration 28550, loss = 0.03838037\n",
      "Iteration 28551, loss = 0.03838261\n",
      "Iteration 28552, loss = 0.03837717\n",
      "Iteration 28553, loss = 0.03837746\n",
      "Iteration 28554, loss = 0.03837464\n",
      "Iteration 28555, loss = 0.03836877\n",
      "Iteration 28556, loss = 0.03836624\n",
      "Iteration 28557, loss = 0.03837340\n",
      "Iteration 28558, loss = 0.03836833\n",
      "Iteration 28559, loss = 0.03836950\n",
      "Iteration 28560, loss = 0.03836992\n",
      "Iteration 28561, loss = 0.03837777\n",
      "Iteration 28562, loss = 0.03837236\n",
      "Iteration 28563, loss = 0.03836458\n",
      "Iteration 28564, loss = 0.03836912\n",
      "Iteration 28565, loss = 0.03836900\n",
      "Iteration 28566, loss = 0.03836873\n",
      "Iteration 28567, loss = 0.03835972\n",
      "Iteration 28568, loss = 0.03836300\n",
      "Iteration 28569, loss = 0.03836988\n",
      "Iteration 28570, loss = 0.03836854\n",
      "Iteration 28571, loss = 0.03835684\n",
      "Iteration 28572, loss = 0.03835613\n",
      "Iteration 28573, loss = 0.03836077\n",
      "Iteration 28574, loss = 0.03836469\n",
      "Iteration 28575, loss = 0.03836501\n",
      "Iteration 28576, loss = 0.03835944\n",
      "Iteration 28577, loss = 0.03835032\n",
      "Iteration 28578, loss = 0.03835179\n",
      "Iteration 28579, loss = 0.03835330\n",
      "Iteration 28580, loss = 0.03835030\n",
      "Iteration 28581, loss = 0.03834682\n",
      "Iteration 28582, loss = 0.03834507\n",
      "Iteration 28583, loss = 0.03834266\n",
      "Iteration 28584, loss = 0.03834130\n",
      "Iteration 28585, loss = 0.03834216\n",
      "Iteration 28586, loss = 0.03833993\n",
      "Iteration 28587, loss = 0.03833980\n",
      "Iteration 28588, loss = 0.03834021\n",
      "Iteration 28589, loss = 0.03833794\n",
      "Iteration 28590, loss = 0.03833783\n",
      "Iteration 28591, loss = 0.03833659\n",
      "Iteration 28592, loss = 0.03834213\n",
      "Iteration 28593, loss = 0.03833788\n",
      "Iteration 28594, loss = 0.03833846\n",
      "Iteration 28595, loss = 0.03833738\n",
      "Iteration 28596, loss = 0.03833258\n",
      "Iteration 28597, loss = 0.03833667\n",
      "Iteration 28598, loss = 0.03833350\n",
      "Iteration 28599, loss = 0.03833447\n",
      "Iteration 28600, loss = 0.03832709\n",
      "Iteration 28601, loss = 0.03832946\n",
      "Iteration 28602, loss = 0.03833110\n",
      "Iteration 28603, loss = 0.03832838\n",
      "Iteration 28604, loss = 0.03832753\n",
      "Iteration 28605, loss = 0.03832531\n",
      "Iteration 28606, loss = 0.03832677\n",
      "Iteration 28607, loss = 0.03832697\n",
      "Iteration 28608, loss = 0.03831944\n",
      "Iteration 28609, loss = 0.03832251\n",
      "Iteration 28610, loss = 0.03831870\n",
      "Iteration 28611, loss = 0.03831905\n",
      "Iteration 28612, loss = 0.03831886\n",
      "Iteration 28613, loss = 0.03831641\n",
      "Iteration 28614, loss = 0.03831324\n",
      "Iteration 28615, loss = 0.03831709\n",
      "Iteration 28616, loss = 0.03831018\n",
      "Iteration 28617, loss = 0.03831603\n",
      "Iteration 28618, loss = 0.03831900\n",
      "Iteration 28619, loss = 0.03832056\n",
      "Iteration 28620, loss = 0.03831797\n",
      "Iteration 28621, loss = 0.03830730\n",
      "Iteration 28622, loss = 0.03831049\n",
      "Iteration 28623, loss = 0.03832395\n",
      "Iteration 28624, loss = 0.03831991\n",
      "Iteration 28625, loss = 0.03831239\n",
      "Iteration 28626, loss = 0.03830672\n",
      "Iteration 28627, loss = 0.03830903\n",
      "Iteration 28628, loss = 0.03830684\n",
      "Iteration 28629, loss = 0.03830475\n",
      "Iteration 28630, loss = 0.03830362\n",
      "Iteration 28631, loss = 0.03830176\n",
      "Iteration 28632, loss = 0.03829666\n",
      "Iteration 28633, loss = 0.03829426\n",
      "Iteration 28634, loss = 0.03829824\n",
      "Iteration 28635, loss = 0.03829903\n",
      "Iteration 28636, loss = 0.03829385\n",
      "Iteration 28637, loss = 0.03829504\n",
      "Iteration 28638, loss = 0.03830146\n",
      "Iteration 28639, loss = 0.03829567\n",
      "Iteration 28640, loss = 0.03829302\n",
      "Iteration 28641, loss = 0.03829733\n",
      "Iteration 28642, loss = 0.03829369\n",
      "Iteration 28643, loss = 0.03829201\n",
      "Iteration 28644, loss = 0.03829010\n",
      "Iteration 28645, loss = 0.03828999\n",
      "Iteration 28646, loss = 0.03829216\n",
      "Iteration 28647, loss = 0.03828617\n",
      "Iteration 28648, loss = 0.03828028\n",
      "Iteration 28649, loss = 0.03828438\n",
      "Iteration 28650, loss = 0.03828523\n",
      "Iteration 28651, loss = 0.03828130\n",
      "Iteration 28652, loss = 0.03828139\n",
      "Iteration 28653, loss = 0.03828346\n",
      "Iteration 28654, loss = 0.03828232\n",
      "Iteration 28655, loss = 0.03827429\n",
      "Iteration 28656, loss = 0.03827944\n",
      "Iteration 28657, loss = 0.03828282\n",
      "Iteration 28658, loss = 0.03827858\n",
      "Iteration 28659, loss = 0.03827100\n",
      "Iteration 28660, loss = 0.03827539\n",
      "Iteration 28661, loss = 0.03828171\n",
      "Iteration 28662, loss = 0.03828050\n",
      "Iteration 28663, loss = 0.03827714\n",
      "Iteration 28664, loss = 0.03827602\n",
      "Iteration 28665, loss = 0.03827310\n",
      "Iteration 28666, loss = 0.03826951\n",
      "Iteration 28667, loss = 0.03827453\n",
      "Iteration 28668, loss = 0.03827475\n",
      "Iteration 28669, loss = 0.03827052\n",
      "Iteration 28670, loss = 0.03826539\n",
      "Iteration 28671, loss = 0.03827155\n",
      "Iteration 28672, loss = 0.03827154\n",
      "Iteration 28673, loss = 0.03827419\n",
      "Iteration 28674, loss = 0.03827290\n",
      "Iteration 28675, loss = 0.03827514\n",
      "Iteration 28676, loss = 0.03826789\n",
      "Iteration 28677, loss = 0.03826701\n",
      "Iteration 28678, loss = 0.03826119\n",
      "Iteration 28679, loss = 0.03826541\n",
      "Iteration 28680, loss = 0.03827021\n",
      "Iteration 28681, loss = 0.03826704\n",
      "Iteration 28682, loss = 0.03826220\n",
      "Iteration 28683, loss = 0.03825235\n",
      "Iteration 28684, loss = 0.03825549\n",
      "Iteration 28685, loss = 0.03825667\n",
      "Iteration 28686, loss = 0.03825695\n",
      "Iteration 28687, loss = 0.03825129\n",
      "Iteration 28688, loss = 0.03825391\n",
      "Iteration 28689, loss = 0.03825360\n",
      "Iteration 28690, loss = 0.03824793\n",
      "Iteration 28691, loss = 0.03825106\n",
      "Iteration 28692, loss = 0.03825392\n",
      "Iteration 28693, loss = 0.03825611\n",
      "Iteration 28694, loss = 0.03825187\n",
      "Iteration 28695, loss = 0.03824901\n",
      "Iteration 28696, loss = 0.03824715\n",
      "Iteration 28697, loss = 0.03824966\n",
      "Iteration 28698, loss = 0.03824933\n",
      "Iteration 28699, loss = 0.03824799\n",
      "Iteration 28700, loss = 0.03824400\n",
      "Iteration 28701, loss = 0.03824533\n",
      "Iteration 28702, loss = 0.03824811\n",
      "Iteration 28703, loss = 0.03824489\n",
      "Iteration 28704, loss = 0.03823619\n",
      "Iteration 28705, loss = 0.03824255\n",
      "Iteration 28706, loss = 0.03824639\n",
      "Iteration 28707, loss = 0.03823497\n",
      "Iteration 28708, loss = 0.03823333\n",
      "Iteration 28709, loss = 0.03823752\n",
      "Iteration 28710, loss = 0.03823502\n",
      "Iteration 28711, loss = 0.03823141\n",
      "Iteration 28712, loss = 0.03822768\n",
      "Iteration 28713, loss = 0.03822622\n",
      "Iteration 28714, loss = 0.03823044\n",
      "Iteration 28715, loss = 0.03822556\n",
      "Iteration 28716, loss = 0.03822516\n",
      "Iteration 28717, loss = 0.03822498\n",
      "Iteration 28718, loss = 0.03822534\n",
      "Iteration 28719, loss = 0.03822394\n",
      "Iteration 28720, loss = 0.03822122\n",
      "Iteration 28721, loss = 0.03822345\n",
      "Iteration 28722, loss = 0.03822009\n",
      "Iteration 28723, loss = 0.03822220\n",
      "Iteration 28724, loss = 0.03822140\n",
      "Iteration 28725, loss = 0.03822216\n",
      "Iteration 28726, loss = 0.03822443\n",
      "Iteration 28727, loss = 0.03822565\n",
      "Iteration 28728, loss = 0.03821935\n",
      "Iteration 28729, loss = 0.03821314\n",
      "Iteration 28730, loss = 0.03822249\n",
      "Iteration 28731, loss = 0.03822374\n",
      "Iteration 28732, loss = 0.03822227\n",
      "Iteration 28733, loss = 0.03821406\n",
      "Iteration 28734, loss = 0.03821120\n",
      "Iteration 28735, loss = 0.03821325\n",
      "Iteration 28736, loss = 0.03821164\n",
      "Iteration 28737, loss = 0.03821084\n",
      "Iteration 28738, loss = 0.03821839\n",
      "Iteration 28739, loss = 0.03821504\n",
      "Iteration 28740, loss = 0.03821333\n",
      "Iteration 28741, loss = 0.03820779\n",
      "Iteration 28742, loss = 0.03820661\n",
      "Iteration 28743, loss = 0.03820618\n",
      "Iteration 28744, loss = 0.03819901\n",
      "Iteration 28745, loss = 0.03820201\n",
      "Iteration 28746, loss = 0.03820438\n",
      "Iteration 28747, loss = 0.03819830\n",
      "Iteration 28748, loss = 0.03819481\n",
      "Iteration 28749, loss = 0.03819789\n",
      "Iteration 28750, loss = 0.03819913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 28751, loss = 0.03819669\n",
      "Iteration 28752, loss = 0.03819332\n",
      "Iteration 28753, loss = 0.03819564\n",
      "Iteration 28754, loss = 0.03819913\n",
      "Iteration 28755, loss = 0.03819543\n",
      "Iteration 28756, loss = 0.03819211\n",
      "Iteration 28757, loss = 0.03819006\n",
      "Iteration 28758, loss = 0.03819032\n",
      "Iteration 28759, loss = 0.03818956\n",
      "Iteration 28760, loss = 0.03819265\n",
      "Iteration 28761, loss = 0.03818575\n",
      "Iteration 28762, loss = 0.03819101\n",
      "Iteration 28763, loss = 0.03819067\n",
      "Iteration 28764, loss = 0.03818642\n",
      "Iteration 28765, loss = 0.03818533\n",
      "Iteration 28766, loss = 0.03819195\n",
      "Iteration 28767, loss = 0.03819206\n",
      "Iteration 28768, loss = 0.03818343\n",
      "Iteration 28769, loss = 0.03818390\n",
      "Iteration 28770, loss = 0.03819055\n",
      "Iteration 28771, loss = 0.03818755\n",
      "Iteration 28772, loss = 0.03818308\n",
      "Iteration 28773, loss = 0.03818201\n",
      "Iteration 28774, loss = 0.03818646\n",
      "Iteration 28775, loss = 0.03818695\n",
      "Iteration 28776, loss = 0.03818533\n",
      "Iteration 28777, loss = 0.03818432\n",
      "Iteration 28778, loss = 0.03817239\n",
      "Iteration 28779, loss = 0.03817259\n",
      "Iteration 28780, loss = 0.03817160\n",
      "Iteration 28781, loss = 0.03817943\n",
      "Iteration 28782, loss = 0.03818070\n",
      "Iteration 28783, loss = 0.03816975\n",
      "Iteration 28784, loss = 0.03816334\n",
      "Iteration 28785, loss = 0.03816445\n",
      "Iteration 28786, loss = 0.03816602\n",
      "Iteration 28787, loss = 0.03816212\n",
      "Iteration 28788, loss = 0.03816442\n",
      "Iteration 28789, loss = 0.03816662\n",
      "Iteration 28790, loss = 0.03816682\n",
      "Iteration 28791, loss = 0.03816545\n",
      "Iteration 28792, loss = 0.03816127\n",
      "Iteration 28793, loss = 0.03816149\n",
      "Iteration 28794, loss = 0.03816362\n",
      "Iteration 28795, loss = 0.03816424\n",
      "Iteration 28796, loss = 0.03816113\n",
      "Iteration 28797, loss = 0.03815904\n",
      "Iteration 28798, loss = 0.03815055\n",
      "Iteration 28799, loss = 0.03814970\n",
      "Iteration 28800, loss = 0.03815214\n",
      "Iteration 28801, loss = 0.03815466\n",
      "Iteration 28802, loss = 0.03814731\n",
      "Iteration 28803, loss = 0.03814660\n",
      "Iteration 28804, loss = 0.03814873\n",
      "Iteration 28805, loss = 0.03815088\n",
      "Iteration 28806, loss = 0.03815020\n",
      "Iteration 28807, loss = 0.03815405\n",
      "Iteration 28808, loss = 0.03814961\n",
      "Iteration 28809, loss = 0.03815090\n",
      "Iteration 28810, loss = 0.03815269\n",
      "Iteration 28811, loss = 0.03814971\n",
      "Iteration 28812, loss = 0.03814486\n",
      "Iteration 28813, loss = 0.03814043\n",
      "Iteration 28814, loss = 0.03814424\n",
      "Iteration 28815, loss = 0.03814652\n",
      "Iteration 28816, loss = 0.03814409\n",
      "Iteration 28817, loss = 0.03813641\n",
      "Iteration 28818, loss = 0.03813867\n",
      "Iteration 28819, loss = 0.03814196\n",
      "Iteration 28820, loss = 0.03813579\n",
      "Iteration 28821, loss = 0.03812881\n",
      "Iteration 28822, loss = 0.03813070\n",
      "Iteration 28823, loss = 0.03812913\n",
      "Iteration 28824, loss = 0.03813048\n",
      "Iteration 28825, loss = 0.03813485\n",
      "Iteration 28826, loss = 0.03812988\n",
      "Iteration 28827, loss = 0.03812788\n",
      "Iteration 28828, loss = 0.03813106\n",
      "Iteration 28829, loss = 0.03813092\n",
      "Iteration 28830, loss = 0.03813062\n",
      "Iteration 28831, loss = 0.03812551\n",
      "Iteration 28832, loss = 0.03812602\n",
      "Iteration 28833, loss = 0.03813043\n",
      "Iteration 28834, loss = 0.03812900\n",
      "Iteration 28835, loss = 0.03812028\n",
      "Iteration 28836, loss = 0.03811799\n",
      "Iteration 28837, loss = 0.03811906\n",
      "Iteration 28838, loss = 0.03812038\n",
      "Iteration 28839, loss = 0.03811446\n",
      "Iteration 28840, loss = 0.03811637\n",
      "Iteration 28841, loss = 0.03811951\n",
      "Iteration 28842, loss = 0.03811809\n",
      "Iteration 28843, loss = 0.03811564\n",
      "Iteration 28844, loss = 0.03811477\n",
      "Iteration 28845, loss = 0.03811539\n",
      "Iteration 28846, loss = 0.03811162\n",
      "Iteration 28847, loss = 0.03811562\n",
      "Iteration 28848, loss = 0.03811563\n",
      "Iteration 28849, loss = 0.03810970\n",
      "Iteration 28850, loss = 0.03810873\n",
      "Iteration 28851, loss = 0.03811117\n",
      "Iteration 28852, loss = 0.03811125\n",
      "Iteration 28853, loss = 0.03810958\n",
      "Iteration 28854, loss = 0.03810839\n",
      "Iteration 28855, loss = 0.03810217\n",
      "Iteration 28856, loss = 0.03810821\n",
      "Iteration 28857, loss = 0.03811287\n",
      "Iteration 28858, loss = 0.03810278\n",
      "Iteration 28859, loss = 0.03810143\n",
      "Iteration 28860, loss = 0.03810744\n",
      "Iteration 28861, loss = 0.03810742\n",
      "Iteration 28862, loss = 0.03809859\n",
      "Iteration 28863, loss = 0.03809639\n",
      "Iteration 28864, loss = 0.03809928\n",
      "Iteration 28865, loss = 0.03809988\n",
      "Iteration 28866, loss = 0.03809530\n",
      "Iteration 28867, loss = 0.03809072\n",
      "Iteration 28868, loss = 0.03809260\n",
      "Iteration 28869, loss = 0.03809002\n",
      "Iteration 28870, loss = 0.03808842\n",
      "Iteration 28871, loss = 0.03808986\n",
      "Iteration 28872, loss = 0.03809020\n",
      "Iteration 28873, loss = 0.03808940\n",
      "Iteration 28874, loss = 0.03808649\n",
      "Iteration 28875, loss = 0.03808652\n",
      "Iteration 28876, loss = 0.03808481\n",
      "Iteration 28877, loss = 0.03808250\n",
      "Iteration 28878, loss = 0.03808043\n",
      "Iteration 28879, loss = 0.03807950\n",
      "Iteration 28880, loss = 0.03808310\n",
      "Iteration 28881, loss = 0.03808004\n",
      "Iteration 28882, loss = 0.03808251\n",
      "Iteration 28883, loss = 0.03808306\n",
      "Iteration 28884, loss = 0.03807922\n",
      "Iteration 28885, loss = 0.03808611\n",
      "Iteration 28886, loss = 0.03808832\n",
      "Iteration 28887, loss = 0.03808049\n",
      "Iteration 28888, loss = 0.03807739\n",
      "Iteration 28889, loss = 0.03808061\n",
      "Iteration 28890, loss = 0.03808325\n",
      "Iteration 28891, loss = 0.03807697\n",
      "Iteration 28892, loss = 0.03807293\n",
      "Iteration 28893, loss = 0.03807672\n",
      "Iteration 28894, loss = 0.03807743\n",
      "Iteration 28895, loss = 0.03807837\n",
      "Iteration 28896, loss = 0.03807808\n",
      "Iteration 28897, loss = 0.03807865\n",
      "Iteration 28898, loss = 0.03807592\n",
      "Iteration 28899, loss = 0.03806721\n",
      "Iteration 28900, loss = 0.03807541\n",
      "Iteration 28901, loss = 0.03807706\n",
      "Iteration 28902, loss = 0.03807154\n",
      "Iteration 28903, loss = 0.03806741\n",
      "Iteration 28904, loss = 0.03807363\n",
      "Iteration 28905, loss = 0.03807236\n",
      "Iteration 28906, loss = 0.03807143\n",
      "Iteration 28907, loss = 0.03807371\n",
      "Iteration 28908, loss = 0.03806990\n",
      "Iteration 28909, loss = 0.03807269\n",
      "Iteration 28910, loss = 0.03806915\n",
      "Iteration 28911, loss = 0.03805744\n",
      "Iteration 28912, loss = 0.03806296\n",
      "Iteration 28913, loss = 0.03806814\n",
      "Iteration 28914, loss = 0.03807355\n",
      "Iteration 28915, loss = 0.03806881\n",
      "Iteration 28916, loss = 0.03805325\n",
      "Iteration 28917, loss = 0.03805590\n",
      "Iteration 28918, loss = 0.03806165\n",
      "Iteration 28919, loss = 0.03806633\n",
      "Iteration 28920, loss = 0.03806793\n",
      "Iteration 28921, loss = 0.03806489\n",
      "Iteration 28922, loss = 0.03805639\n",
      "Iteration 28923, loss = 0.03805324\n",
      "Iteration 28924, loss = 0.03804420\n",
      "Iteration 28925, loss = 0.03805284\n",
      "Iteration 28926, loss = 0.03805813\n",
      "Iteration 28927, loss = 0.03805598\n",
      "Iteration 28928, loss = 0.03804500\n",
      "Iteration 28929, loss = 0.03804697\n",
      "Iteration 28930, loss = 0.03805012\n",
      "Iteration 28931, loss = 0.03804952\n",
      "Iteration 28932, loss = 0.03805234\n",
      "Iteration 28933, loss = 0.03804966\n",
      "Iteration 28934, loss = 0.03805004\n",
      "Iteration 28935, loss = 0.03804464\n",
      "Iteration 28936, loss = 0.03804664\n",
      "Iteration 28937, loss = 0.03803683\n",
      "Iteration 28938, loss = 0.03803934\n",
      "Iteration 28939, loss = 0.03804740\n",
      "Iteration 28940, loss = 0.03804646\n",
      "Iteration 28941, loss = 0.03804051\n",
      "Iteration 28942, loss = 0.03803994\n",
      "Iteration 28943, loss = 0.03803401\n",
      "Iteration 28944, loss = 0.03803093\n",
      "Iteration 28945, loss = 0.03802601\n",
      "Iteration 28946, loss = 0.03802837\n",
      "Iteration 28947, loss = 0.03803232\n",
      "Iteration 28948, loss = 0.03802675\n",
      "Iteration 28949, loss = 0.03801952\n",
      "Iteration 28950, loss = 0.03802450\n",
      "Iteration 28951, loss = 0.03802569\n",
      "Iteration 28952, loss = 0.03802419\n",
      "Iteration 28953, loss = 0.03802728\n",
      "Iteration 28954, loss = 0.03802408\n",
      "Iteration 28955, loss = 0.03801980\n",
      "Iteration 28956, loss = 0.03802340\n",
      "Iteration 28957, loss = 0.03802099\n",
      "Iteration 28958, loss = 0.03801694\n",
      "Iteration 28959, loss = 0.03801052\n",
      "Iteration 28960, loss = 0.03800999\n",
      "Iteration 28961, loss = 0.03801420\n",
      "Iteration 28962, loss = 0.03801223\n",
      "Iteration 28963, loss = 0.03801210\n",
      "Iteration 28964, loss = 0.03801617\n",
      "Iteration 28965, loss = 0.03801207\n",
      "Iteration 28966, loss = 0.03800916\n",
      "Iteration 28967, loss = 0.03801476\n",
      "Iteration 28968, loss = 0.03801800\n",
      "Iteration 28969, loss = 0.03801940\n",
      "Iteration 28970, loss = 0.03801904\n",
      "Iteration 28971, loss = 0.03801284\n",
      "Iteration 28972, loss = 0.03799896\n",
      "Iteration 28973, loss = 0.03800558\n",
      "Iteration 28974, loss = 0.03800757\n",
      "Iteration 28975, loss = 0.03800460\n",
      "Iteration 28976, loss = 0.03800469\n",
      "Iteration 28977, loss = 0.03800518\n",
      "Iteration 28978, loss = 0.03800831\n",
      "Iteration 28979, loss = 0.03800598\n",
      "Iteration 28980, loss = 0.03800102\n",
      "Iteration 28981, loss = 0.03799678\n",
      "Iteration 28982, loss = 0.03799883\n",
      "Iteration 28983, loss = 0.03799533\n",
      "Iteration 28984, loss = 0.03799277\n",
      "Iteration 28985, loss = 0.03799197\n",
      "Iteration 28986, loss = 0.03798814\n",
      "Iteration 28987, loss = 0.03798832\n",
      "Iteration 28988, loss = 0.03799189\n",
      "Iteration 28989, loss = 0.03798487\n",
      "Iteration 28990, loss = 0.03799651\n",
      "Iteration 28991, loss = 0.03799657\n",
      "Iteration 28992, loss = 0.03799747\n",
      "Iteration 28993, loss = 0.03799568\n",
      "Iteration 28994, loss = 0.03798870\n",
      "Iteration 28995, loss = 0.03798654\n",
      "Iteration 28996, loss = 0.03798660\n",
      "Iteration 28997, loss = 0.03798403\n",
      "Iteration 28998, loss = 0.03798211\n",
      "Iteration 28999, loss = 0.03798134\n",
      "Iteration 29000, loss = 0.03798357\n",
      "Iteration 29001, loss = 0.03798905\n",
      "Iteration 29002, loss = 0.03798699\n",
      "Iteration 29003, loss = 0.03798591\n",
      "Iteration 29004, loss = 0.03798224\n",
      "Iteration 29005, loss = 0.03798970\n",
      "Iteration 29006, loss = 0.03799026\n",
      "Iteration 29007, loss = 0.03798657\n",
      "Iteration 29008, loss = 0.03798403\n",
      "Iteration 29009, loss = 0.03797556\n",
      "Iteration 29010, loss = 0.03797255\n",
      "Iteration 29011, loss = 0.03797454\n",
      "Iteration 29012, loss = 0.03797591\n",
      "Iteration 29013, loss = 0.03797035\n",
      "Iteration 29014, loss = 0.03797542\n",
      "Iteration 29015, loss = 0.03797227\n",
      "Iteration 29016, loss = 0.03797250\n",
      "Iteration 29017, loss = 0.03797173\n",
      "Iteration 29018, loss = 0.03796650\n",
      "Iteration 29019, loss = 0.03796185\n",
      "Iteration 29020, loss = 0.03796885\n",
      "Iteration 29021, loss = 0.03796958\n",
      "Iteration 29022, loss = 0.03796276\n",
      "Iteration 29023, loss = 0.03795876\n",
      "Iteration 29024, loss = 0.03795985\n",
      "Iteration 29025, loss = 0.03796200\n",
      "Iteration 29026, loss = 0.03795959\n",
      "Iteration 29027, loss = 0.03795765\n",
      "Iteration 29028, loss = 0.03795685\n",
      "Iteration 29029, loss = 0.03795349\n",
      "Iteration 29030, loss = 0.03795580\n",
      "Iteration 29031, loss = 0.03795357\n",
      "Iteration 29032, loss = 0.03795410\n",
      "Iteration 29033, loss = 0.03795675\n",
      "Iteration 29034, loss = 0.03795626\n",
      "Iteration 29035, loss = 0.03795183\n",
      "Iteration 29036, loss = 0.03795280\n",
      "Iteration 29037, loss = 0.03795000\n",
      "Iteration 29038, loss = 0.03794591\n",
      "Iteration 29039, loss = 0.03795294\n",
      "Iteration 29040, loss = 0.03795089\n",
      "Iteration 29041, loss = 0.03794688\n",
      "Iteration 29042, loss = 0.03794333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 29043, loss = 0.03794300\n",
      "Iteration 29044, loss = 0.03794738\n",
      "Iteration 29045, loss = 0.03794666\n",
      "Iteration 29046, loss = 0.03794223\n",
      "Iteration 29047, loss = 0.03793781\n",
      "Iteration 29048, loss = 0.03793886\n",
      "Iteration 29049, loss = 0.03793697\n",
      "Iteration 29050, loss = 0.03793600\n",
      "Iteration 29051, loss = 0.03793592\n",
      "Iteration 29052, loss = 0.03793127\n",
      "Iteration 29053, loss = 0.03794122\n",
      "Iteration 29054, loss = 0.03794107\n",
      "Iteration 29055, loss = 0.03793770\n",
      "Iteration 29056, loss = 0.03793248\n",
      "Iteration 29057, loss = 0.03793235\n",
      "Iteration 29058, loss = 0.03793176\n",
      "Iteration 29059, loss = 0.03792871\n",
      "Iteration 29060, loss = 0.03792683\n",
      "Iteration 29061, loss = 0.03792698\n",
      "Iteration 29062, loss = 0.03793211\n",
      "Iteration 29063, loss = 0.03792555\n",
      "Iteration 29064, loss = 0.03792743\n",
      "Iteration 29065, loss = 0.03792718\n",
      "Iteration 29066, loss = 0.03793171\n",
      "Iteration 29067, loss = 0.03792747\n",
      "Iteration 29068, loss = 0.03793238\n",
      "Iteration 29069, loss = 0.03793748\n",
      "Iteration 29070, loss = 0.03793095\n",
      "Iteration 29071, loss = 0.03791945\n",
      "Iteration 29072, loss = 0.03791977\n",
      "Iteration 29073, loss = 0.03792471\n",
      "Iteration 29074, loss = 0.03792294\n",
      "Iteration 29075, loss = 0.03791948\n",
      "Iteration 29076, loss = 0.03791892\n",
      "Iteration 29077, loss = 0.03791645\n",
      "Iteration 29078, loss = 0.03791591\n",
      "Iteration 29079, loss = 0.03791987\n",
      "Iteration 29080, loss = 0.03791887\n",
      "Iteration 29081, loss = 0.03791472\n",
      "Iteration 29082, loss = 0.03790970\n",
      "Iteration 29083, loss = 0.03791988\n",
      "Iteration 29084, loss = 0.03792042\n",
      "Iteration 29085, loss = 0.03791846\n",
      "Iteration 29086, loss = 0.03791745\n",
      "Iteration 29087, loss = 0.03791328\n",
      "Iteration 29088, loss = 0.03791003\n",
      "Iteration 29089, loss = 0.03790447\n",
      "Iteration 29090, loss = 0.03791056\n",
      "Iteration 29091, loss = 0.03790807\n",
      "Iteration 29092, loss = 0.03790207\n",
      "Iteration 29093, loss = 0.03790408\n",
      "Iteration 29094, loss = 0.03790503\n",
      "Iteration 29095, loss = 0.03790446\n",
      "Iteration 29096, loss = 0.03789910\n",
      "Iteration 29097, loss = 0.03789735\n",
      "Iteration 29098, loss = 0.03790756\n",
      "Iteration 29099, loss = 0.03790497\n",
      "Iteration 29100, loss = 0.03789562\n",
      "Iteration 29101, loss = 0.03790003\n",
      "Iteration 29102, loss = 0.03790546\n",
      "Iteration 29103, loss = 0.03790208\n",
      "Iteration 29104, loss = 0.03789908\n",
      "Iteration 29105, loss = 0.03789834\n",
      "Iteration 29106, loss = 0.03789958\n",
      "Iteration 29107, loss = 0.03789411\n",
      "Iteration 29108, loss = 0.03788896\n",
      "Iteration 29109, loss = 0.03789303\n",
      "Iteration 29110, loss = 0.03789439\n",
      "Iteration 29111, loss = 0.03788852\n",
      "Iteration 29112, loss = 0.03788637\n",
      "Iteration 29113, loss = 0.03788440\n",
      "Iteration 29114, loss = 0.03788498\n",
      "Iteration 29115, loss = 0.03788652\n",
      "Iteration 29116, loss = 0.03788699\n",
      "Iteration 29117, loss = 0.03788486\n",
      "Iteration 29118, loss = 0.03788159\n",
      "Iteration 29119, loss = 0.03788304\n",
      "Iteration 29120, loss = 0.03788375\n",
      "Iteration 29121, loss = 0.03787894\n",
      "Iteration 29122, loss = 0.03787746\n",
      "Iteration 29123, loss = 0.03787927\n",
      "Iteration 29124, loss = 0.03787773\n",
      "Iteration 29125, loss = 0.03787738\n",
      "Iteration 29126, loss = 0.03787393\n",
      "Iteration 29127, loss = 0.03787158\n",
      "Iteration 29128, loss = 0.03786931\n",
      "Iteration 29129, loss = 0.03787225\n",
      "Iteration 29130, loss = 0.03787505\n",
      "Iteration 29131, loss = 0.03787643\n",
      "Iteration 29132, loss = 0.03787350\n",
      "Iteration 29133, loss = 0.03786569\n",
      "Iteration 29134, loss = 0.03787123\n",
      "Iteration 29135, loss = 0.03787193\n",
      "Iteration 29136, loss = 0.03786118\n",
      "Iteration 29137, loss = 0.03787108\n",
      "Iteration 29138, loss = 0.03787511\n",
      "Iteration 29139, loss = 0.03787558\n",
      "Iteration 29140, loss = 0.03787232\n",
      "Iteration 29141, loss = 0.03786602\n",
      "Iteration 29142, loss = 0.03786554\n",
      "Iteration 29143, loss = 0.03785983\n",
      "Iteration 29144, loss = 0.03786403\n",
      "Iteration 29145, loss = 0.03786739\n",
      "Iteration 29146, loss = 0.03786316\n",
      "Iteration 29147, loss = 0.03785852\n",
      "Iteration 29148, loss = 0.03785862\n",
      "Iteration 29149, loss = 0.03785809\n",
      "Iteration 29150, loss = 0.03786242\n",
      "Iteration 29151, loss = 0.03785753\n",
      "Iteration 29152, loss = 0.03785678\n",
      "Iteration 29153, loss = 0.03786004\n",
      "Iteration 29154, loss = 0.03786381\n",
      "Iteration 29155, loss = 0.03786296\n",
      "Iteration 29156, loss = 0.03785647\n",
      "Iteration 29157, loss = 0.03785262\n",
      "Iteration 29158, loss = 0.03784792\n",
      "Iteration 29159, loss = 0.03785374\n",
      "Iteration 29160, loss = 0.03785627\n",
      "Iteration 29161, loss = 0.03785177\n",
      "Iteration 29162, loss = 0.03784477\n",
      "Iteration 29163, loss = 0.03784901\n",
      "Iteration 29164, loss = 0.03785346\n",
      "Iteration 29165, loss = 0.03784919\n",
      "Iteration 29166, loss = 0.03784385\n",
      "Iteration 29167, loss = 0.03784421\n",
      "Iteration 29168, loss = 0.03785008\n",
      "Iteration 29169, loss = 0.03785141\n",
      "Iteration 29170, loss = 0.03784550\n",
      "Iteration 29171, loss = 0.03784133\n",
      "Iteration 29172, loss = 0.03783814\n",
      "Iteration 29173, loss = 0.03784595\n",
      "Iteration 29174, loss = 0.03784919\n",
      "Iteration 29175, loss = 0.03784647\n",
      "Iteration 29176, loss = 0.03783906\n",
      "Iteration 29177, loss = 0.03783851\n",
      "Iteration 29178, loss = 0.03783822\n",
      "Iteration 29179, loss = 0.03784068\n",
      "Iteration 29180, loss = 0.03783090\n",
      "Iteration 29181, loss = 0.03783325\n",
      "Iteration 29182, loss = 0.03783912\n",
      "Iteration 29183, loss = 0.03783881\n",
      "Iteration 29184, loss = 0.03783346\n",
      "Iteration 29185, loss = 0.03782929\n",
      "Iteration 29186, loss = 0.03782773\n",
      "Iteration 29187, loss = 0.03782859\n",
      "Iteration 29188, loss = 0.03782537\n",
      "Iteration 29189, loss = 0.03782283\n",
      "Iteration 29190, loss = 0.03782526\n",
      "Iteration 29191, loss = 0.03781934\n",
      "Iteration 29192, loss = 0.03782302\n",
      "Iteration 29193, loss = 0.03782584\n",
      "Iteration 29194, loss = 0.03782097\n",
      "Iteration 29195, loss = 0.03782379\n",
      "Iteration 29196, loss = 0.03782555\n",
      "Iteration 29197, loss = 0.03782664\n",
      "Iteration 29198, loss = 0.03782142\n",
      "Iteration 29199, loss = 0.03781713\n",
      "Iteration 29200, loss = 0.03782059\n",
      "Iteration 29201, loss = 0.03782452\n",
      "Iteration 29202, loss = 0.03782007\n",
      "Iteration 29203, loss = 0.03781873\n",
      "Iteration 29204, loss = 0.03781992\n",
      "Iteration 29205, loss = 0.03781546\n",
      "Iteration 29206, loss = 0.03781440\n",
      "Iteration 29207, loss = 0.03781530\n",
      "Iteration 29208, loss = 0.03781566\n",
      "Iteration 29209, loss = 0.03780618\n",
      "Iteration 29210, loss = 0.03781230\n",
      "Iteration 29211, loss = 0.03781779\n",
      "Iteration 29212, loss = 0.03781693\n",
      "Iteration 29213, loss = 0.03781330\n",
      "Iteration 29214, loss = 0.03780961\n",
      "Iteration 29215, loss = 0.03780206\n",
      "Iteration 29216, loss = 0.03780583\n",
      "Iteration 29217, loss = 0.03780971\n",
      "Iteration 29218, loss = 0.03780970\n",
      "Iteration 29219, loss = 0.03780805\n",
      "Iteration 29220, loss = 0.03780374\n",
      "Iteration 29221, loss = 0.03780551\n",
      "Iteration 29222, loss = 0.03780745\n",
      "Iteration 29223, loss = 0.03779776\n",
      "Iteration 29224, loss = 0.03779559\n",
      "Iteration 29225, loss = 0.03780014\n",
      "Iteration 29226, loss = 0.03779729\n",
      "Iteration 29227, loss = 0.03778974\n",
      "Iteration 29228, loss = 0.03779498\n",
      "Iteration 29229, loss = 0.03780262\n",
      "Iteration 29230, loss = 0.03780236\n",
      "Iteration 29231, loss = 0.03778825\n",
      "Iteration 29232, loss = 0.03779695\n",
      "Iteration 29233, loss = 0.03779836\n",
      "Iteration 29234, loss = 0.03779739\n",
      "Iteration 29235, loss = 0.03779319\n",
      "Iteration 29236, loss = 0.03779234\n",
      "Iteration 29237, loss = 0.03778792\n",
      "Iteration 29238, loss = 0.03779078\n",
      "Iteration 29239, loss = 0.03778782\n",
      "Iteration 29240, loss = 0.03778436\n",
      "Iteration 29241, loss = 0.03778468\n",
      "Iteration 29242, loss = 0.03778190\n",
      "Iteration 29243, loss = 0.03778270\n",
      "Iteration 29244, loss = 0.03778060\n",
      "Iteration 29245, loss = 0.03778038\n",
      "Iteration 29246, loss = 0.03778483\n",
      "Iteration 29247, loss = 0.03778152\n",
      "Iteration 29248, loss = 0.03777868\n",
      "Iteration 29249, loss = 0.03778094\n",
      "Iteration 29250, loss = 0.03777748\n",
      "Iteration 29251, loss = 0.03777417\n",
      "Iteration 29252, loss = 0.03777308\n",
      "Iteration 29253, loss = 0.03777325\n",
      "Iteration 29254, loss = 0.03777132\n",
      "Iteration 29255, loss = 0.03776959\n",
      "Iteration 29256, loss = 0.03777224\n",
      "Iteration 29257, loss = 0.03777396\n",
      "Iteration 29258, loss = 0.03777196\n",
      "Iteration 29259, loss = 0.03777227\n",
      "Iteration 29260, loss = 0.03776634\n",
      "Iteration 29261, loss = 0.03776706\n",
      "Iteration 29262, loss = 0.03776846\n",
      "Iteration 29263, loss = 0.03777242\n",
      "Iteration 29264, loss = 0.03777076\n",
      "Iteration 29265, loss = 0.03776305\n",
      "Iteration 29266, loss = 0.03776417\n",
      "Iteration 29267, loss = 0.03777145\n",
      "Iteration 29268, loss = 0.03776847\n",
      "Iteration 29269, loss = 0.03775665\n",
      "Iteration 29270, loss = 0.03776482\n",
      "Iteration 29271, loss = 0.03776904\n",
      "Iteration 29272, loss = 0.03776973\n",
      "Iteration 29273, loss = 0.03776572\n",
      "Iteration 29274, loss = 0.03776435\n",
      "Iteration 29275, loss = 0.03776640\n",
      "Iteration 29276, loss = 0.03776785\n",
      "Iteration 29277, loss = 0.03776104\n",
      "Iteration 29278, loss = 0.03775653\n",
      "Iteration 29279, loss = 0.03775186\n",
      "Iteration 29280, loss = 0.03775605\n",
      "Iteration 29281, loss = 0.03775640\n",
      "Iteration 29282, loss = 0.03775331\n",
      "Iteration 29283, loss = 0.03774902\n",
      "Iteration 29284, loss = 0.03775768\n",
      "Iteration 29285, loss = 0.03776028\n",
      "Iteration 29286, loss = 0.03775342\n",
      "Iteration 29287, loss = 0.03775384\n",
      "Iteration 29288, loss = 0.03774782\n",
      "Iteration 29289, loss = 0.03775260\n",
      "Iteration 29290, loss = 0.03775049\n",
      "Iteration 29291, loss = 0.03774550\n",
      "Iteration 29292, loss = 0.03773943\n",
      "Iteration 29293, loss = 0.03774682\n",
      "Iteration 29294, loss = 0.03774050\n",
      "Iteration 29295, loss = 0.03773990\n",
      "Iteration 29296, loss = 0.03774062\n",
      "Iteration 29297, loss = 0.03773808\n",
      "Iteration 29298, loss = 0.03773468\n",
      "Iteration 29299, loss = 0.03773316\n",
      "Iteration 29300, loss = 0.03773826\n",
      "Iteration 29301, loss = 0.03773929\n",
      "Iteration 29302, loss = 0.03773804\n",
      "Iteration 29303, loss = 0.03773908\n",
      "Iteration 29304, loss = 0.03773330\n",
      "Iteration 29305, loss = 0.03773145\n",
      "Iteration 29306, loss = 0.03773670\n",
      "Iteration 29307, loss = 0.03773436\n",
      "Iteration 29308, loss = 0.03772828\n",
      "Iteration 29309, loss = 0.03772751\n",
      "Iteration 29310, loss = 0.03773055\n",
      "Iteration 29311, loss = 0.03772968\n",
      "Iteration 29312, loss = 0.03772935\n",
      "Iteration 29313, loss = 0.03772431\n",
      "Iteration 29314, loss = 0.03772121\n",
      "Iteration 29315, loss = 0.03772756\n",
      "Iteration 29316, loss = 0.03772916\n",
      "Iteration 29317, loss = 0.03772388\n",
      "Iteration 29318, loss = 0.03771925\n",
      "Iteration 29319, loss = 0.03771953\n",
      "Iteration 29320, loss = 0.03772077\n",
      "Iteration 29321, loss = 0.03772243\n",
      "Iteration 29322, loss = 0.03771745\n",
      "Iteration 29323, loss = 0.03771899\n",
      "Iteration 29324, loss = 0.03772397\n",
      "Iteration 29325, loss = 0.03771539\n",
      "Iteration 29326, loss = 0.03771205\n",
      "Iteration 29327, loss = 0.03771983\n",
      "Iteration 29328, loss = 0.03771971\n",
      "Iteration 29329, loss = 0.03771794\n",
      "Iteration 29330, loss = 0.03772032\n",
      "Iteration 29331, loss = 0.03771868\n",
      "Iteration 29332, loss = 0.03771176\n",
      "Iteration 29333, loss = 0.03771229\n",
      "Iteration 29334, loss = 0.03770825\n",
      "Iteration 29335, loss = 0.03771185\n",
      "Iteration 29336, loss = 0.03771428\n",
      "Iteration 29337, loss = 0.03771707\n",
      "Iteration 29338, loss = 0.03771876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 29339, loss = 0.03771533\n",
      "Iteration 29340, loss = 0.03770759\n",
      "Iteration 29341, loss = 0.03771553\n",
      "Iteration 29342, loss = 0.03771755\n",
      "Iteration 29343, loss = 0.03770708\n",
      "Iteration 29344, loss = 0.03771279\n",
      "Iteration 29345, loss = 0.03771265\n",
      "Iteration 29346, loss = 0.03771077\n",
      "Iteration 29347, loss = 0.03771047\n",
      "Iteration 29348, loss = 0.03770672\n",
      "Iteration 29349, loss = 0.03769904\n",
      "Iteration 29350, loss = 0.03770726\n",
      "Iteration 29351, loss = 0.03770066\n",
      "Iteration 29352, loss = 0.03769487\n",
      "Iteration 29353, loss = 0.03769831\n",
      "Iteration 29354, loss = 0.03770391\n",
      "Iteration 29355, loss = 0.03770380\n",
      "Iteration 29356, loss = 0.03769864\n",
      "Iteration 29357, loss = 0.03769391\n",
      "Iteration 29358, loss = 0.03768798\n",
      "Iteration 29359, loss = 0.03768996\n",
      "Iteration 29360, loss = 0.03768706\n",
      "Iteration 29361, loss = 0.03768749\n",
      "Iteration 29362, loss = 0.03768829\n",
      "Iteration 29363, loss = 0.03769064\n",
      "Iteration 29364, loss = 0.03769148\n",
      "Iteration 29365, loss = 0.03768226\n",
      "Iteration 29366, loss = 0.03768987\n",
      "Iteration 29367, loss = 0.03769249\n",
      "Iteration 29368, loss = 0.03768600\n",
      "Iteration 29369, loss = 0.03768013\n",
      "Iteration 29370, loss = 0.03768209\n",
      "Iteration 29371, loss = 0.03768388\n",
      "Iteration 29372, loss = 0.03768256\n",
      "Iteration 29373, loss = 0.03767730\n",
      "Iteration 29374, loss = 0.03767900\n",
      "Iteration 29375, loss = 0.03768399\n",
      "Iteration 29376, loss = 0.03768215\n",
      "Iteration 29377, loss = 0.03767566\n",
      "Iteration 29378, loss = 0.03767556\n",
      "Iteration 29379, loss = 0.03767060\n",
      "Iteration 29380, loss = 0.03767367\n",
      "Iteration 29381, loss = 0.03767397\n",
      "Iteration 29382, loss = 0.03767947\n",
      "Iteration 29383, loss = 0.03768022\n",
      "Iteration 29384, loss = 0.03767180\n",
      "Iteration 29385, loss = 0.03767085\n",
      "Iteration 29386, loss = 0.03767347\n",
      "Iteration 29387, loss = 0.03767027\n",
      "Iteration 29388, loss = 0.03766663\n",
      "Iteration 29389, loss = 0.03766673\n",
      "Iteration 29390, loss = 0.03766638\n",
      "Iteration 29391, loss = 0.03766190\n",
      "Iteration 29392, loss = 0.03766157\n",
      "Iteration 29393, loss = 0.03766359\n",
      "Iteration 29394, loss = 0.03766887\n",
      "Iteration 29395, loss = 0.03767020\n",
      "Iteration 29396, loss = 0.03766366\n",
      "Iteration 29397, loss = 0.03765788\n",
      "Iteration 29398, loss = 0.03766512\n",
      "Iteration 29399, loss = 0.03766135\n",
      "Iteration 29400, loss = 0.03765454\n",
      "Iteration 29401, loss = 0.03765989\n",
      "Iteration 29402, loss = 0.03765469\n",
      "Iteration 29403, loss = 0.03764841\n",
      "Iteration 29404, loss = 0.03765259\n",
      "Iteration 29405, loss = 0.03765745\n",
      "Iteration 29406, loss = 0.03765768\n",
      "Iteration 29407, loss = 0.03765423\n",
      "Iteration 29408, loss = 0.03765851\n",
      "Iteration 29409, loss = 0.03766063\n",
      "Iteration 29410, loss = 0.03765764\n",
      "Iteration 29411, loss = 0.03764881\n",
      "Iteration 29412, loss = 0.03764975\n",
      "Iteration 29413, loss = 0.03765066\n",
      "Iteration 29414, loss = 0.03764999\n",
      "Iteration 29415, loss = 0.03764779\n",
      "Iteration 29416, loss = 0.03764377\n",
      "Iteration 29417, loss = 0.03764794\n",
      "Iteration 29418, loss = 0.03764552\n",
      "Iteration 29419, loss = 0.03763778\n",
      "Iteration 29420, loss = 0.03763846\n",
      "Iteration 29421, loss = 0.03764441\n",
      "Iteration 29422, loss = 0.03764558\n",
      "Iteration 29423, loss = 0.03764903\n",
      "Iteration 29424, loss = 0.03764367\n",
      "Iteration 29425, loss = 0.03763436\n",
      "Iteration 29426, loss = 0.03764555\n",
      "Iteration 29427, loss = 0.03764820\n",
      "Iteration 29428, loss = 0.03763598\n",
      "Iteration 29429, loss = 0.03763812\n",
      "Iteration 29430, loss = 0.03764689\n",
      "Iteration 29431, loss = 0.03764279\n",
      "Iteration 29432, loss = 0.03764155\n",
      "Iteration 29433, loss = 0.03763917\n",
      "Iteration 29434, loss = 0.03764212\n",
      "Iteration 29435, loss = 0.03764281\n",
      "Iteration 29436, loss = 0.03763848\n",
      "Iteration 29437, loss = 0.03763800\n",
      "Iteration 29438, loss = 0.03763611\n",
      "Iteration 29439, loss = 0.03763347\n",
      "Iteration 29440, loss = 0.03762360\n",
      "Iteration 29441, loss = 0.03762364\n",
      "Iteration 29442, loss = 0.03762459\n",
      "Iteration 29443, loss = 0.03762535\n",
      "Iteration 29444, loss = 0.03762378\n",
      "Iteration 29445, loss = 0.03762033\n",
      "Iteration 29446, loss = 0.03762289\n",
      "Iteration 29447, loss = 0.03762157\n",
      "Iteration 29448, loss = 0.03762008\n",
      "Iteration 29449, loss = 0.03762394\n",
      "Iteration 29450, loss = 0.03762089\n",
      "Iteration 29451, loss = 0.03761834\n",
      "Iteration 29452, loss = 0.03761326\n",
      "Iteration 29453, loss = 0.03761669\n",
      "Iteration 29454, loss = 0.03762271\n",
      "Iteration 29455, loss = 0.03761313\n",
      "Iteration 29456, loss = 0.03761474\n",
      "Iteration 29457, loss = 0.03761740\n",
      "Iteration 29458, loss = 0.03762013\n",
      "Iteration 29459, loss = 0.03761945\n",
      "Iteration 29460, loss = 0.03761257\n",
      "Iteration 29461, loss = 0.03760836\n",
      "Iteration 29462, loss = 0.03761514\n",
      "Iteration 29463, loss = 0.03761155\n",
      "Iteration 29464, loss = 0.03761743\n",
      "Iteration 29465, loss = 0.03761894\n",
      "Iteration 29466, loss = 0.03761789\n",
      "Iteration 29467, loss = 0.03761577\n",
      "Iteration 29468, loss = 0.03761197\n",
      "Iteration 29469, loss = 0.03760978\n",
      "Iteration 29470, loss = 0.03760546\n",
      "Iteration 29471, loss = 0.03760018\n",
      "Iteration 29472, loss = 0.03760695\n",
      "Iteration 29473, loss = 0.03761442\n",
      "Iteration 29474, loss = 0.03760433\n",
      "Iteration 29475, loss = 0.03760147\n",
      "Iteration 29476, loss = 0.03760877\n",
      "Iteration 29477, loss = 0.03761450\n",
      "Iteration 29478, loss = 0.03760760\n",
      "Iteration 29479, loss = 0.03760293\n",
      "Iteration 29480, loss = 0.03760874\n",
      "Iteration 29481, loss = 0.03760956\n",
      "Iteration 29482, loss = 0.03759931\n",
      "Iteration 29483, loss = 0.03759548\n",
      "Iteration 29484, loss = 0.03759530\n",
      "Iteration 29485, loss = 0.03760099\n",
      "Iteration 29486, loss = 0.03759779\n",
      "Iteration 29487, loss = 0.03758718\n",
      "Iteration 29488, loss = 0.03759123\n",
      "Iteration 29489, loss = 0.03759757\n",
      "Iteration 29490, loss = 0.03759348\n",
      "Iteration 29491, loss = 0.03759154\n",
      "Iteration 29492, loss = 0.03759126\n",
      "Iteration 29493, loss = 0.03759144\n",
      "Iteration 29494, loss = 0.03759199\n",
      "Iteration 29495, loss = 0.03759178\n",
      "Iteration 29496, loss = 0.03758748\n",
      "Iteration 29497, loss = 0.03758211\n",
      "Iteration 29498, loss = 0.03758623\n",
      "Iteration 29499, loss = 0.03758063\n",
      "Iteration 29500, loss = 0.03757776\n",
      "Iteration 29501, loss = 0.03758684\n",
      "Iteration 29502, loss = 0.03758588\n",
      "Iteration 29503, loss = 0.03758229\n",
      "Iteration 29504, loss = 0.03757842\n",
      "Iteration 29505, loss = 0.03757838\n",
      "Iteration 29506, loss = 0.03757778\n",
      "Iteration 29507, loss = 0.03757857\n",
      "Iteration 29508, loss = 0.03757833\n",
      "Iteration 29509, loss = 0.03757866\n",
      "Iteration 29510, loss = 0.03757764\n",
      "Iteration 29511, loss = 0.03757543\n",
      "Iteration 29512, loss = 0.03757539\n",
      "Iteration 29513, loss = 0.03757520\n",
      "Iteration 29514, loss = 0.03757001\n",
      "Iteration 29515, loss = 0.03756880\n",
      "Iteration 29516, loss = 0.03756215\n",
      "Iteration 29517, loss = 0.03757081\n",
      "Iteration 29518, loss = 0.03756872\n",
      "Iteration 29519, loss = 0.03756799\n",
      "Iteration 29520, loss = 0.03756763\n",
      "Iteration 29521, loss = 0.03756776\n",
      "Iteration 29522, loss = 0.03756310\n",
      "Iteration 29523, loss = 0.03755644\n",
      "Iteration 29524, loss = 0.03755728\n",
      "Iteration 29525, loss = 0.03755576\n",
      "Iteration 29526, loss = 0.03755699\n",
      "Iteration 29527, loss = 0.03755499\n",
      "Iteration 29528, loss = 0.03755846\n",
      "Iteration 29529, loss = 0.03755959\n",
      "Iteration 29530, loss = 0.03755716\n",
      "Iteration 29531, loss = 0.03755462\n",
      "Iteration 29532, loss = 0.03755162\n",
      "Iteration 29533, loss = 0.03755504\n",
      "Iteration 29534, loss = 0.03755803\n",
      "Iteration 29535, loss = 0.03755588\n",
      "Iteration 29536, loss = 0.03755453\n",
      "Iteration 29537, loss = 0.03755445\n",
      "Iteration 29538, loss = 0.03755569\n",
      "Iteration 29539, loss = 0.03755328\n",
      "Iteration 29540, loss = 0.03755143\n",
      "Iteration 29541, loss = 0.03755535\n",
      "Iteration 29542, loss = 0.03755415\n",
      "Iteration 29543, loss = 0.03755133\n",
      "Iteration 29544, loss = 0.03754509\n",
      "Iteration 29545, loss = 0.03754843\n",
      "Iteration 29546, loss = 0.03755046\n",
      "Iteration 29547, loss = 0.03754149\n",
      "Iteration 29548, loss = 0.03754776\n",
      "Iteration 29549, loss = 0.03755116\n",
      "Iteration 29550, loss = 0.03755082\n",
      "Iteration 29551, loss = 0.03754361\n",
      "Iteration 29552, loss = 0.03754097\n",
      "Iteration 29553, loss = 0.03754487\n",
      "Iteration 29554, loss = 0.03754379\n",
      "Iteration 29555, loss = 0.03753234\n",
      "Iteration 29556, loss = 0.03754100\n",
      "Iteration 29557, loss = 0.03754228\n",
      "Iteration 29558, loss = 0.03753766\n",
      "Iteration 29559, loss = 0.03753639\n",
      "Iteration 29560, loss = 0.03753843\n",
      "Iteration 29561, loss = 0.03753357\n",
      "Iteration 29562, loss = 0.03753580\n",
      "Iteration 29563, loss = 0.03753677\n",
      "Iteration 29564, loss = 0.03753350\n",
      "Iteration 29565, loss = 0.03752812\n",
      "Iteration 29566, loss = 0.03753634\n",
      "Iteration 29567, loss = 0.03753722\n",
      "Iteration 29568, loss = 0.03753412\n",
      "Iteration 29569, loss = 0.03753694\n",
      "Iteration 29570, loss = 0.03753290\n",
      "Iteration 29571, loss = 0.03752523\n",
      "Iteration 29572, loss = 0.03753292\n",
      "Iteration 29573, loss = 0.03753330\n",
      "Iteration 29574, loss = 0.03753319\n",
      "Iteration 29575, loss = 0.03754093\n",
      "Iteration 29576, loss = 0.03754056\n",
      "Iteration 29577, loss = 0.03753150\n",
      "Iteration 29578, loss = 0.03752624\n",
      "Iteration 29579, loss = 0.03752585\n",
      "Iteration 29580, loss = 0.03753476\n",
      "Iteration 29581, loss = 0.03753404\n",
      "Iteration 29582, loss = 0.03752907\n",
      "Iteration 29583, loss = 0.03752717\n",
      "Iteration 29584, loss = 0.03752219\n",
      "Iteration 29585, loss = 0.03752610\n",
      "Iteration 29586, loss = 0.03753192\n",
      "Iteration 29587, loss = 0.03753405\n",
      "Iteration 29588, loss = 0.03753028\n",
      "Iteration 29589, loss = 0.03752548\n",
      "Iteration 29590, loss = 0.03752263\n",
      "Iteration 29591, loss = 0.03751310\n",
      "Iteration 29592, loss = 0.03751455\n",
      "Iteration 29593, loss = 0.03751732\n",
      "Iteration 29594, loss = 0.03751229\n",
      "Iteration 29595, loss = 0.03751441\n",
      "Iteration 29596, loss = 0.03752231\n",
      "Iteration 29597, loss = 0.03752081\n",
      "Iteration 29598, loss = 0.03751561\n",
      "Iteration 29599, loss = 0.03750833\n",
      "Iteration 29600, loss = 0.03750871\n",
      "Iteration 29601, loss = 0.03750326\n",
      "Iteration 29602, loss = 0.03750498\n",
      "Iteration 29603, loss = 0.03750906\n",
      "Iteration 29604, loss = 0.03750243\n",
      "Iteration 29605, loss = 0.03750154\n",
      "Iteration 29606, loss = 0.03750419\n",
      "Iteration 29607, loss = 0.03750613\n",
      "Iteration 29608, loss = 0.03750664\n",
      "Iteration 29609, loss = 0.03749861\n",
      "Iteration 29610, loss = 0.03749945\n",
      "Iteration 29611, loss = 0.03749834\n",
      "Iteration 29612, loss = 0.03749688\n",
      "Iteration 29613, loss = 0.03749745\n",
      "Iteration 29614, loss = 0.03749689\n",
      "Iteration 29615, loss = 0.03749886\n",
      "Iteration 29616, loss = 0.03749483\n",
      "Iteration 29617, loss = 0.03749351\n",
      "Iteration 29618, loss = 0.03749157\n",
      "Iteration 29619, loss = 0.03749605\n",
      "Iteration 29620, loss = 0.03749467\n",
      "Iteration 29621, loss = 0.03749370\n",
      "Iteration 29622, loss = 0.03749053\n",
      "Iteration 29623, loss = 0.03748920\n",
      "Iteration 29624, loss = 0.03749097\n",
      "Iteration 29625, loss = 0.03749169\n",
      "Iteration 29626, loss = 0.03748961\n",
      "Iteration 29627, loss = 0.03748237\n",
      "Iteration 29628, loss = 0.03748145\n",
      "Iteration 29629, loss = 0.03747983\n",
      "Iteration 29630, loss = 0.03747577\n",
      "Iteration 29631, loss = 0.03747683\n",
      "Iteration 29632, loss = 0.03747793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 29633, loss = 0.03747991\n",
      "Iteration 29634, loss = 0.03748018\n",
      "Iteration 29635, loss = 0.03747892\n",
      "Iteration 29636, loss = 0.03747622\n",
      "Iteration 29637, loss = 0.03747185\n",
      "Iteration 29638, loss = 0.03747361\n",
      "Iteration 29639, loss = 0.03748474\n",
      "Iteration 29640, loss = 0.03747860\n",
      "Iteration 29641, loss = 0.03747366\n",
      "Iteration 29642, loss = 0.03747646\n",
      "Iteration 29643, loss = 0.03747735\n",
      "Iteration 29644, loss = 0.03747927\n",
      "Iteration 29645, loss = 0.03747453\n",
      "Iteration 29646, loss = 0.03747622\n",
      "Iteration 29647, loss = 0.03747578\n",
      "Iteration 29648, loss = 0.03747319\n",
      "Iteration 29649, loss = 0.03746771\n",
      "Iteration 29650, loss = 0.03746918\n",
      "Iteration 29651, loss = 0.03747363\n",
      "Iteration 29652, loss = 0.03747659\n",
      "Iteration 29653, loss = 0.03746737\n",
      "Iteration 29654, loss = 0.03746839\n",
      "Iteration 29655, loss = 0.03746592\n",
      "Iteration 29656, loss = 0.03747547\n",
      "Iteration 29657, loss = 0.03747674\n",
      "Iteration 29658, loss = 0.03747699\n",
      "Iteration 29659, loss = 0.03746417\n",
      "Iteration 29660, loss = 0.03746256\n",
      "Iteration 29661, loss = 0.03746297\n",
      "Iteration 29662, loss = 0.03746852\n",
      "Iteration 29663, loss = 0.03746730\n",
      "Iteration 29664, loss = 0.03746260\n",
      "Iteration 29665, loss = 0.03745955\n",
      "Iteration 29666, loss = 0.03745810\n",
      "Iteration 29667, loss = 0.03745468\n",
      "Iteration 29668, loss = 0.03745404\n",
      "Iteration 29669, loss = 0.03745779\n",
      "Iteration 29670, loss = 0.03745528\n",
      "Iteration 29671, loss = 0.03745081\n",
      "Iteration 29672, loss = 0.03744761\n",
      "Iteration 29673, loss = 0.03745702\n",
      "Iteration 29674, loss = 0.03745703\n",
      "Iteration 29675, loss = 0.03744903\n",
      "Iteration 29676, loss = 0.03744456\n",
      "Iteration 29677, loss = 0.03744605\n",
      "Iteration 29678, loss = 0.03744857\n",
      "Iteration 29679, loss = 0.03744412\n",
      "Iteration 29680, loss = 0.03744456\n",
      "Iteration 29681, loss = 0.03744451\n",
      "Iteration 29682, loss = 0.03744589\n",
      "Iteration 29683, loss = 0.03743824\n",
      "Iteration 29684, loss = 0.03744302\n",
      "Iteration 29685, loss = 0.03744254\n",
      "Iteration 29686, loss = 0.03744846\n",
      "Iteration 29687, loss = 0.03744442\n",
      "Iteration 29688, loss = 0.03743676\n",
      "Iteration 29689, loss = 0.03744398\n",
      "Iteration 29690, loss = 0.03744996\n",
      "Iteration 29691, loss = 0.03745174\n",
      "Iteration 29692, loss = 0.03744314\n",
      "Iteration 29693, loss = 0.03744413\n",
      "Iteration 29694, loss = 0.03744275\n",
      "Iteration 29695, loss = 0.03743920\n",
      "Iteration 29696, loss = 0.03743836\n",
      "Iteration 29697, loss = 0.03744317\n",
      "Iteration 29698, loss = 0.03744523\n",
      "Iteration 29699, loss = 0.03743953\n",
      "Iteration 29700, loss = 0.03743364\n",
      "Iteration 29701, loss = 0.03743498\n",
      "Iteration 29702, loss = 0.03743376\n",
      "Iteration 29703, loss = 0.03743943\n",
      "Iteration 29704, loss = 0.03743616\n",
      "Iteration 29705, loss = 0.03743131\n",
      "Iteration 29706, loss = 0.03743325\n",
      "Iteration 29707, loss = 0.03743304\n",
      "Iteration 29708, loss = 0.03743194\n",
      "Iteration 29709, loss = 0.03742335\n",
      "Iteration 29710, loss = 0.03742254\n",
      "Iteration 29711, loss = 0.03742521\n",
      "Iteration 29712, loss = 0.03743126\n",
      "Iteration 29713, loss = 0.03742712\n",
      "Iteration 29714, loss = 0.03742455\n",
      "Iteration 29715, loss = 0.03741952\n",
      "Iteration 29716, loss = 0.03742655\n",
      "Iteration 29717, loss = 0.03742597\n",
      "Iteration 29718, loss = 0.03741406\n",
      "Iteration 29719, loss = 0.03742085\n",
      "Iteration 29720, loss = 0.03742457\n",
      "Iteration 29721, loss = 0.03741906\n",
      "Iteration 29722, loss = 0.03742007\n",
      "Iteration 29723, loss = 0.03742060\n",
      "Iteration 29724, loss = 0.03741980\n",
      "Iteration 29725, loss = 0.03741459\n",
      "Iteration 29726, loss = 0.03741548\n",
      "Iteration 29727, loss = 0.03741505\n",
      "Iteration 29728, loss = 0.03741469\n",
      "Iteration 29729, loss = 0.03741387\n",
      "Iteration 29730, loss = 0.03741137\n",
      "Iteration 29731, loss = 0.03740328\n",
      "Iteration 29732, loss = 0.03740837\n",
      "Iteration 29733, loss = 0.03740745\n",
      "Iteration 29734, loss = 0.03740345\n",
      "Iteration 29735, loss = 0.03740926\n",
      "Iteration 29736, loss = 0.03740920\n",
      "Iteration 29737, loss = 0.03740576\n",
      "Iteration 29738, loss = 0.03740251\n",
      "Iteration 29739, loss = 0.03739937\n",
      "Iteration 29740, loss = 0.03740409\n",
      "Iteration 29741, loss = 0.03740703\n",
      "Iteration 29742, loss = 0.03740578\n",
      "Iteration 29743, loss = 0.03740839\n",
      "Iteration 29744, loss = 0.03740100\n",
      "Iteration 29745, loss = 0.03739831\n",
      "Iteration 29746, loss = 0.03740553\n",
      "Iteration 29747, loss = 0.03741183\n",
      "Iteration 29748, loss = 0.03741287\n",
      "Iteration 29749, loss = 0.03740984\n",
      "Iteration 29750, loss = 0.03739983\n",
      "Iteration 29751, loss = 0.03739038\n",
      "Iteration 29752, loss = 0.03739020\n",
      "Iteration 29753, loss = 0.03739497\n",
      "Iteration 29754, loss = 0.03738700\n",
      "Iteration 29755, loss = 0.03739108\n",
      "Iteration 29756, loss = 0.03739432\n",
      "Iteration 29757, loss = 0.03739203\n",
      "Iteration 29758, loss = 0.03738832\n",
      "Iteration 29759, loss = 0.03738215\n",
      "Iteration 29760, loss = 0.03739014\n",
      "Iteration 29761, loss = 0.03739085\n",
      "Iteration 29762, loss = 0.03738848\n",
      "Iteration 29763, loss = 0.03738682\n",
      "Iteration 29764, loss = 0.03738676\n",
      "Iteration 29765, loss = 0.03739101\n",
      "Iteration 29766, loss = 0.03739001\n",
      "Iteration 29767, loss = 0.03738766\n",
      "Iteration 29768, loss = 0.03738103\n",
      "Iteration 29769, loss = 0.03738219\n",
      "Iteration 29770, loss = 0.03738121\n",
      "Iteration 29771, loss = 0.03737867\n",
      "Iteration 29772, loss = 0.03737332\n",
      "Iteration 29773, loss = 0.03737692\n",
      "Iteration 29774, loss = 0.03738217\n",
      "Iteration 29775, loss = 0.03738128\n",
      "Iteration 29776, loss = 0.03737543\n",
      "Iteration 29777, loss = 0.03737318\n",
      "Iteration 29778, loss = 0.03737793\n",
      "Iteration 29779, loss = 0.03737454\n",
      "Iteration 29780, loss = 0.03736641\n",
      "Iteration 29781, loss = 0.03737054\n",
      "Iteration 29782, loss = 0.03736908\n",
      "Iteration 29783, loss = 0.03737103\n",
      "Iteration 29784, loss = 0.03736854\n",
      "Iteration 29785, loss = 0.03736234\n",
      "Iteration 29786, loss = 0.03737093\n",
      "Iteration 29787, loss = 0.03736870\n",
      "Iteration 29788, loss = 0.03736490\n",
      "Iteration 29789, loss = 0.03736852\n",
      "Iteration 29790, loss = 0.03736848\n",
      "Iteration 29791, loss = 0.03736158\n",
      "Iteration 29792, loss = 0.03735883\n",
      "Iteration 29793, loss = 0.03736039\n",
      "Iteration 29794, loss = 0.03736038\n",
      "Iteration 29795, loss = 0.03735964\n",
      "Iteration 29796, loss = 0.03735743\n",
      "Iteration 29797, loss = 0.03735506\n",
      "Iteration 29798, loss = 0.03735745\n",
      "Iteration 29799, loss = 0.03736086\n",
      "Iteration 29800, loss = 0.03735279\n",
      "Iteration 29801, loss = 0.03735345\n",
      "Iteration 29802, loss = 0.03735661\n",
      "Iteration 29803, loss = 0.03735096\n",
      "Iteration 29804, loss = 0.03735191\n",
      "Iteration 29805, loss = 0.03736062\n",
      "Iteration 29806, loss = 0.03735906\n",
      "Iteration 29807, loss = 0.03735149\n",
      "Iteration 29808, loss = 0.03735342\n",
      "Iteration 29809, loss = 0.03735079\n",
      "Iteration 29810, loss = 0.03734407\n",
      "Iteration 29811, loss = 0.03734358\n",
      "Iteration 29812, loss = 0.03734815\n",
      "Iteration 29813, loss = 0.03734317\n",
      "Iteration 29814, loss = 0.03733973\n",
      "Iteration 29815, loss = 0.03734344\n",
      "Iteration 29816, loss = 0.03734687\n",
      "Iteration 29817, loss = 0.03734405\n",
      "Iteration 29818, loss = 0.03734420\n",
      "Iteration 29819, loss = 0.03734281\n",
      "Iteration 29820, loss = 0.03734558\n",
      "Iteration 29821, loss = 0.03734702\n",
      "Iteration 29822, loss = 0.03734196\n",
      "Iteration 29823, loss = 0.03734263\n",
      "Iteration 29824, loss = 0.03733995\n",
      "Iteration 29825, loss = 0.03733661\n",
      "Iteration 29826, loss = 0.03733998\n",
      "Iteration 29827, loss = 0.03734527\n",
      "Iteration 29828, loss = 0.03734212\n",
      "Iteration 29829, loss = 0.03733242\n",
      "Iteration 29830, loss = 0.03733888\n",
      "Iteration 29831, loss = 0.03734436\n",
      "Iteration 29832, loss = 0.03734471\n",
      "Iteration 29833, loss = 0.03734203\n",
      "Iteration 29834, loss = 0.03733518\n",
      "Iteration 29835, loss = 0.03733214\n",
      "Iteration 29836, loss = 0.03733457\n",
      "Iteration 29837, loss = 0.03733110\n",
      "Iteration 29838, loss = 0.03733074\n",
      "Iteration 29839, loss = 0.03732832\n",
      "Iteration 29840, loss = 0.03732253\n",
      "Iteration 29841, loss = 0.03732289\n",
      "Iteration 29842, loss = 0.03732941\n",
      "Iteration 29843, loss = 0.03733063\n",
      "Iteration 29844, loss = 0.03732529\n",
      "Iteration 29845, loss = 0.03732151\n",
      "Iteration 29846, loss = 0.03732958\n",
      "Iteration 29847, loss = 0.03732996\n",
      "Iteration 29848, loss = 0.03732427\n",
      "Iteration 29849, loss = 0.03732449\n",
      "Iteration 29850, loss = 0.03732329\n",
      "Iteration 29851, loss = 0.03732466\n",
      "Iteration 29852, loss = 0.03732083\n",
      "Iteration 29853, loss = 0.03731948\n",
      "Iteration 29854, loss = 0.03731448\n",
      "Iteration 29855, loss = 0.03732411\n",
      "Iteration 29856, loss = 0.03732787\n",
      "Iteration 29857, loss = 0.03731935\n",
      "Iteration 29858, loss = 0.03731344\n",
      "Iteration 29859, loss = 0.03732134\n",
      "Iteration 29860, loss = 0.03731983\n",
      "Iteration 29861, loss = 0.03731951\n",
      "Iteration 29862, loss = 0.03731529\n",
      "Iteration 29863, loss = 0.03731420\n",
      "Iteration 29864, loss = 0.03731699\n",
      "Iteration 29865, loss = 0.03731533\n",
      "Iteration 29866, loss = 0.03731037\n",
      "Iteration 29867, loss = 0.03731255\n",
      "Iteration 29868, loss = 0.03731903\n",
      "Iteration 29869, loss = 0.03732062\n",
      "Iteration 29870, loss = 0.03731122\n",
      "Iteration 29871, loss = 0.03731037\n",
      "Iteration 29872, loss = 0.03731398\n",
      "Iteration 29873, loss = 0.03731076\n",
      "Iteration 29874, loss = 0.03730390\n",
      "Iteration 29875, loss = 0.03730480\n",
      "Iteration 29876, loss = 0.03730500\n",
      "Iteration 29877, loss = 0.03730096\n",
      "Iteration 29878, loss = 0.03730444\n",
      "Iteration 29879, loss = 0.03730367\n",
      "Iteration 29880, loss = 0.03729746\n",
      "Iteration 29881, loss = 0.03729709\n",
      "Iteration 29882, loss = 0.03729925\n",
      "Iteration 29883, loss = 0.03730421\n",
      "Iteration 29884, loss = 0.03730261\n",
      "Iteration 29885, loss = 0.03730370\n",
      "Iteration 29886, loss = 0.03729822\n",
      "Iteration 29887, loss = 0.03729658\n",
      "Iteration 29888, loss = 0.03729433\n",
      "Iteration 29889, loss = 0.03728984\n",
      "Iteration 29890, loss = 0.03729560\n",
      "Iteration 29891, loss = 0.03729951\n",
      "Iteration 29892, loss = 0.03729133\n",
      "Iteration 29893, loss = 0.03728534\n",
      "Iteration 29894, loss = 0.03729142\n",
      "Iteration 29895, loss = 0.03729135\n",
      "Iteration 29896, loss = 0.03728996\n",
      "Iteration 29897, loss = 0.03728566\n",
      "Iteration 29898, loss = 0.03728994\n",
      "Iteration 29899, loss = 0.03729283\n",
      "Iteration 29900, loss = 0.03728516\n",
      "Iteration 29901, loss = 0.03728616\n",
      "Iteration 29902, loss = 0.03729276\n",
      "Iteration 29903, loss = 0.03728341\n",
      "Iteration 29904, loss = 0.03728168\n",
      "Iteration 29905, loss = 0.03728894\n",
      "Iteration 29906, loss = 0.03729019\n",
      "Iteration 29907, loss = 0.03728246\n",
      "Iteration 29908, loss = 0.03728168\n",
      "Iteration 29909, loss = 0.03728757\n",
      "Iteration 29910, loss = 0.03728323\n",
      "Iteration 29911, loss = 0.03726967\n",
      "Iteration 29912, loss = 0.03727280\n",
      "Iteration 29913, loss = 0.03727743\n",
      "Iteration 29914, loss = 0.03728025\n",
      "Iteration 29915, loss = 0.03727648\n",
      "Iteration 29916, loss = 0.03727406\n",
      "Iteration 29917, loss = 0.03726674\n",
      "Iteration 29918, loss = 0.03726971\n",
      "Iteration 29919, loss = 0.03726829\n",
      "Iteration 29920, loss = 0.03726799\n",
      "Iteration 29921, loss = 0.03726784\n",
      "Iteration 29922, loss = 0.03726589\n",
      "Iteration 29923, loss = 0.03726587\n",
      "Iteration 29924, loss = 0.03726680\n",
      "Iteration 29925, loss = 0.03726485\n",
      "Iteration 29926, loss = 0.03726218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 29927, loss = 0.03726226\n",
      "Iteration 29928, loss = 0.03726466\n",
      "Iteration 29929, loss = 0.03726471\n",
      "Iteration 29930, loss = 0.03726464\n",
      "Iteration 29931, loss = 0.03725975\n",
      "Iteration 29932, loss = 0.03726079\n",
      "Iteration 29933, loss = 0.03726571\n",
      "Iteration 29934, loss = 0.03725873\n",
      "Iteration 29935, loss = 0.03725856\n",
      "Iteration 29936, loss = 0.03726183\n",
      "Iteration 29937, loss = 0.03725678\n",
      "Iteration 29938, loss = 0.03725846\n",
      "Iteration 29939, loss = 0.03725613\n",
      "Iteration 29940, loss = 0.03725486\n",
      "Iteration 29941, loss = 0.03725128\n",
      "Iteration 29942, loss = 0.03725449\n",
      "Iteration 29943, loss = 0.03725586\n",
      "Iteration 29944, loss = 0.03725947\n",
      "Iteration 29945, loss = 0.03725401\n",
      "Iteration 29946, loss = 0.03724569\n",
      "Iteration 29947, loss = 0.03725271\n",
      "Iteration 29948, loss = 0.03725407\n",
      "Iteration 29949, loss = 0.03724816\n",
      "Iteration 29950, loss = 0.03725082\n",
      "Iteration 29951, loss = 0.03725482\n",
      "Iteration 29952, loss = 0.03725323\n",
      "Iteration 29953, loss = 0.03725055\n",
      "Iteration 29954, loss = 0.03725080\n",
      "Iteration 29955, loss = 0.03724413\n",
      "Iteration 29956, loss = 0.03724266\n",
      "Iteration 29957, loss = 0.03724931\n",
      "Iteration 29958, loss = 0.03724860\n",
      "Iteration 29959, loss = 0.03724686\n",
      "Iteration 29960, loss = 0.03724047\n",
      "Iteration 29961, loss = 0.03723983\n",
      "Iteration 29962, loss = 0.03724901\n",
      "Iteration 29963, loss = 0.03724707\n",
      "Iteration 29964, loss = 0.03724017\n",
      "Iteration 29965, loss = 0.03723855\n",
      "Iteration 29966, loss = 0.03723333\n",
      "Iteration 29967, loss = 0.03723705\n",
      "Iteration 29968, loss = 0.03723946\n",
      "Iteration 29969, loss = 0.03723724\n",
      "Iteration 29970, loss = 0.03723335\n",
      "Iteration 29971, loss = 0.03723481\n",
      "Iteration 29972, loss = 0.03723548\n",
      "Iteration 29973, loss = 0.03723619\n",
      "Iteration 29974, loss = 0.03724628\n",
      "Iteration 29975, loss = 0.03724276\n",
      "Iteration 29976, loss = 0.03722725\n",
      "Iteration 29977, loss = 0.03723643\n",
      "Iteration 29978, loss = 0.03724397\n",
      "Iteration 29979, loss = 0.03724430\n",
      "Iteration 29980, loss = 0.03723434\n",
      "Iteration 29981, loss = 0.03723019\n",
      "Iteration 29982, loss = 0.03723790\n",
      "Iteration 29983, loss = 0.03724108\n",
      "Iteration 29984, loss = 0.03723376\n",
      "Iteration 29985, loss = 0.03723756\n",
      "Iteration 29986, loss = 0.03723100\n",
      "Iteration 29987, loss = 0.03723216\n",
      "Iteration 29988, loss = 0.03723735\n",
      "Iteration 29989, loss = 0.03723902\n",
      "Iteration 29990, loss = 0.03723719\n",
      "Iteration 29991, loss = 0.03722968\n",
      "Iteration 29992, loss = 0.03722967\n",
      "Iteration 29993, loss = 0.03722425\n",
      "Iteration 29994, loss = 0.03722477\n",
      "Iteration 29995, loss = 0.03722455\n",
      "Iteration 29996, loss = 0.03722112\n",
      "Iteration 29997, loss = 0.03722691\n",
      "Iteration 29998, loss = 0.03722493\n",
      "Iteration 29999, loss = 0.03722193\n",
      "Iteration 30000, loss = 0.03722160\n",
      "Iteration 30001, loss = 0.03721273\n",
      "Iteration 30002, loss = 0.03720822\n",
      "Iteration 30003, loss = 0.03721508\n",
      "Iteration 30004, loss = 0.03722025\n",
      "Iteration 30005, loss = 0.03721664\n",
      "Iteration 30006, loss = 0.03721946\n",
      "Iteration 30007, loss = 0.03721576\n",
      "Iteration 30008, loss = 0.03721629\n",
      "Iteration 30009, loss = 0.03721527\n",
      "Iteration 30010, loss = 0.03721838\n",
      "Iteration 30011, loss = 0.03721153\n",
      "Iteration 30012, loss = 0.03721313\n",
      "Iteration 30013, loss = 0.03721461\n",
      "Iteration 30014, loss = 0.03721984\n",
      "Iteration 30015, loss = 0.03721282\n",
      "Iteration 30016, loss = 0.03720439\n",
      "Iteration 30017, loss = 0.03720051\n",
      "Iteration 30018, loss = 0.03720081\n",
      "Iteration 30019, loss = 0.03720292\n",
      "Iteration 30020, loss = 0.03719893\n",
      "Iteration 30021, loss = 0.03719708\n",
      "Iteration 30022, loss = 0.03719950\n",
      "Iteration 30023, loss = 0.03720410\n",
      "Iteration 30024, loss = 0.03720139\n",
      "Iteration 30025, loss = 0.03719870\n",
      "Iteration 30026, loss = 0.03719642\n",
      "Iteration 30027, loss = 0.03719849\n",
      "Iteration 30028, loss = 0.03719750\n",
      "Iteration 30029, loss = 0.03719268\n",
      "Iteration 30030, loss = 0.03719655\n",
      "Iteration 30031, loss = 0.03719611\n",
      "Iteration 30032, loss = 0.03718759\n",
      "Iteration 30033, loss = 0.03719514\n",
      "Iteration 30034, loss = 0.03719947\n",
      "Iteration 30035, loss = 0.03718492\n",
      "Iteration 30036, loss = 0.03719181\n",
      "Iteration 30037, loss = 0.03719832\n",
      "Iteration 30038, loss = 0.03719859\n",
      "Iteration 30039, loss = 0.03719715\n",
      "Iteration 30040, loss = 0.03719230\n",
      "Iteration 30041, loss = 0.03718460\n",
      "Iteration 30042, loss = 0.03719330\n",
      "Iteration 30043, loss = 0.03719633\n",
      "Iteration 30044, loss = 0.03718537\n",
      "Iteration 30045, loss = 0.03717821\n",
      "Iteration 30046, loss = 0.03718220\n",
      "Iteration 30047, loss = 0.03718672\n",
      "Iteration 30048, loss = 0.03718214\n",
      "Iteration 30049, loss = 0.03718564\n",
      "Iteration 30050, loss = 0.03718689\n",
      "Iteration 30051, loss = 0.03717714\n",
      "Iteration 30052, loss = 0.03717761\n",
      "Iteration 30053, loss = 0.03718217\n",
      "Iteration 30054, loss = 0.03719166\n",
      "Iteration 30055, loss = 0.03719129\n",
      "Iteration 30056, loss = 0.03718284\n",
      "Iteration 30057, loss = 0.03717180\n",
      "Iteration 30058, loss = 0.03717315\n",
      "Iteration 30059, loss = 0.03716934\n",
      "Iteration 30060, loss = 0.03717826\n",
      "Iteration 30061, loss = 0.03717935\n",
      "Iteration 30062, loss = 0.03717292\n",
      "Iteration 30063, loss = 0.03717938\n",
      "Iteration 30064, loss = 0.03717622\n",
      "Iteration 30065, loss = 0.03716354\n",
      "Iteration 30066, loss = 0.03717138\n",
      "Iteration 30067, loss = 0.03718186\n",
      "Iteration 30068, loss = 0.03717749\n",
      "Iteration 30069, loss = 0.03717192\n",
      "Iteration 30070, loss = 0.03717480\n",
      "Iteration 30071, loss = 0.03717218\n",
      "Iteration 30072, loss = 0.03716943\n",
      "Iteration 30073, loss = 0.03716726\n",
      "Iteration 30074, loss = 0.03716663\n",
      "Iteration 30075, loss = 0.03716379\n",
      "Iteration 30076, loss = 0.03716160\n",
      "Iteration 30077, loss = 0.03715691\n",
      "Iteration 30078, loss = 0.03715739\n",
      "Iteration 30079, loss = 0.03715776\n",
      "Iteration 30080, loss = 0.03715408\n",
      "Iteration 30081, loss = 0.03715183\n",
      "Iteration 30082, loss = 0.03715324\n",
      "Iteration 30083, loss = 0.03715303\n",
      "Iteration 30084, loss = 0.03714852\n",
      "Iteration 30085, loss = 0.03714716\n",
      "Iteration 30086, loss = 0.03715077\n",
      "Iteration 30087, loss = 0.03714659\n",
      "Iteration 30088, loss = 0.03714604\n",
      "Iteration 30089, loss = 0.03714853\n",
      "Iteration 30090, loss = 0.03714977\n",
      "Iteration 30091, loss = 0.03714697\n",
      "Iteration 30092, loss = 0.03714884\n",
      "Iteration 30093, loss = 0.03714884\n",
      "Iteration 30094, loss = 0.03714667\n",
      "Iteration 30095, loss = 0.03714565\n",
      "Iteration 30096, loss = 0.03714432\n",
      "Iteration 30097, loss = 0.03714298\n",
      "Iteration 30098, loss = 0.03714237\n",
      "Iteration 30099, loss = 0.03714229\n",
      "Iteration 30100, loss = 0.03714358\n",
      "Iteration 30101, loss = 0.03714131\n",
      "Iteration 30102, loss = 0.03713824\n",
      "Iteration 30103, loss = 0.03714073\n",
      "Iteration 30104, loss = 0.03715006\n",
      "Iteration 30105, loss = 0.03714619\n",
      "Iteration 30106, loss = 0.03714116\n",
      "Iteration 30107, loss = 0.03714294\n",
      "Iteration 30108, loss = 0.03714473\n",
      "Iteration 30109, loss = 0.03714364\n",
      "Iteration 30110, loss = 0.03714626\n",
      "Iteration 30111, loss = 0.03714324\n",
      "Iteration 30112, loss = 0.03714478\n",
      "Iteration 30113, loss = 0.03713990\n",
      "Iteration 30114, loss = 0.03714210\n",
      "Iteration 30115, loss = 0.03714988\n",
      "Iteration 30116, loss = 0.03715058\n",
      "Iteration 30117, loss = 0.03713937\n",
      "Iteration 30118, loss = 0.03713499\n",
      "Iteration 30119, loss = 0.03714084\n",
      "Iteration 30120, loss = 0.03714363\n",
      "Iteration 30121, loss = 0.03713242\n",
      "Iteration 30122, loss = 0.03713252\n",
      "Iteration 30123, loss = 0.03713239\n",
      "Iteration 30124, loss = 0.03712971\n",
      "Iteration 30125, loss = 0.03712962\n",
      "Iteration 30126, loss = 0.03713739\n",
      "Iteration 30127, loss = 0.03713109\n",
      "Iteration 30128, loss = 0.03713056\n",
      "Iteration 30129, loss = 0.03712926\n",
      "Iteration 30130, loss = 0.03713314\n",
      "Iteration 30131, loss = 0.03712618\n",
      "Iteration 30132, loss = 0.03712140\n",
      "Iteration 30133, loss = 0.03712524\n",
      "Iteration 30134, loss = 0.03712016\n",
      "Iteration 30135, loss = 0.03711835\n",
      "Iteration 30136, loss = 0.03712079\n",
      "Iteration 30137, loss = 0.03711710\n",
      "Iteration 30138, loss = 0.03710954\n",
      "Iteration 30139, loss = 0.03710855\n",
      "Iteration 30140, loss = 0.03711178\n",
      "Iteration 30141, loss = 0.03710935\n",
      "Iteration 30142, loss = 0.03711018\n",
      "Iteration 30143, loss = 0.03711540\n",
      "Iteration 30144, loss = 0.03711550\n",
      "Iteration 30145, loss = 0.03711122\n",
      "Iteration 30146, loss = 0.03710953\n",
      "Iteration 30147, loss = 0.03710917\n",
      "Iteration 30148, loss = 0.03710742\n",
      "Iteration 30149, loss = 0.03710845\n",
      "Iteration 30150, loss = 0.03710596\n",
      "Iteration 30151, loss = 0.03710203\n",
      "Iteration 30152, loss = 0.03710826\n",
      "Iteration 30153, loss = 0.03710775\n",
      "Iteration 30154, loss = 0.03710530\n",
      "Iteration 30155, loss = 0.03710908\n",
      "Iteration 30156, loss = 0.03710940\n",
      "Iteration 30157, loss = 0.03711184\n",
      "Iteration 30158, loss = 0.03710821\n",
      "Iteration 30159, loss = 0.03710063\n",
      "Iteration 30160, loss = 0.03710453\n",
      "Iteration 30161, loss = 0.03711222\n",
      "Iteration 30162, loss = 0.03710347\n",
      "Iteration 30163, loss = 0.03710444\n",
      "Iteration 30164, loss = 0.03710536\n",
      "Iteration 30165, loss = 0.03711823\n",
      "Iteration 30166, loss = 0.03711727\n",
      "Iteration 30167, loss = 0.03710587\n",
      "Iteration 30168, loss = 0.03710420\n",
      "Iteration 30169, loss = 0.03710905\n",
      "Iteration 30170, loss = 0.03710738\n",
      "Iteration 30171, loss = 0.03710789\n",
      "Iteration 30172, loss = 0.03709783\n",
      "Iteration 30173, loss = 0.03710211\n",
      "Iteration 30174, loss = 0.03710344\n",
      "Iteration 30175, loss = 0.03710081\n",
      "Iteration 30176, loss = 0.03709894\n",
      "Iteration 30177, loss = 0.03710342\n",
      "Iteration 30178, loss = 0.03709666\n",
      "Iteration 30179, loss = 0.03709429\n",
      "Iteration 30180, loss = 0.03710011\n",
      "Iteration 30181, loss = 0.03709955\n",
      "Iteration 30182, loss = 0.03709540\n",
      "Iteration 30183, loss = 0.03709258\n",
      "Iteration 30184, loss = 0.03709691\n",
      "Iteration 30185, loss = 0.03709407\n",
      "Iteration 30186, loss = 0.03709167\n",
      "Iteration 30187, loss = 0.03709675\n",
      "Iteration 30188, loss = 0.03709331\n",
      "Iteration 30189, loss = 0.03708335\n",
      "Iteration 30190, loss = 0.03708021\n",
      "Iteration 30191, loss = 0.03709039\n",
      "Iteration 30192, loss = 0.03709209\n",
      "Iteration 30193, loss = 0.03708704\n",
      "Iteration 30194, loss = 0.03708772\n",
      "Iteration 30195, loss = 0.03708947\n",
      "Iteration 30196, loss = 0.03708749\n",
      "Iteration 30197, loss = 0.03708029\n",
      "Iteration 30198, loss = 0.03707888\n",
      "Iteration 30199, loss = 0.03708076\n",
      "Iteration 30200, loss = 0.03707372\n",
      "Iteration 30201, loss = 0.03707368\n",
      "Iteration 30202, loss = 0.03707433\n",
      "Iteration 30203, loss = 0.03707438\n",
      "Iteration 30204, loss = 0.03706825\n",
      "Iteration 30205, loss = 0.03707093\n",
      "Iteration 30206, loss = 0.03707083\n",
      "Iteration 30207, loss = 0.03706734\n",
      "Iteration 30208, loss = 0.03706732\n",
      "Iteration 30209, loss = 0.03706805\n",
      "Iteration 30210, loss = 0.03706604\n",
      "Iteration 30211, loss = 0.03706415\n",
      "Iteration 30212, loss = 0.03706562\n",
      "Iteration 30213, loss = 0.03706891\n",
      "Iteration 30214, loss = 0.03706221\n",
      "Iteration 30215, loss = 0.03706394\n",
      "Iteration 30216, loss = 0.03707326\n",
      "Iteration 30217, loss = 0.03707446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 30218, loss = 0.03707234\n",
      "Iteration 30219, loss = 0.03706720\n",
      "Iteration 30220, loss = 0.03706493\n",
      "Iteration 30221, loss = 0.03706111\n",
      "Iteration 30222, loss = 0.03706074\n",
      "Iteration 30223, loss = 0.03705426\n",
      "Iteration 30224, loss = 0.03705509\n",
      "Iteration 30225, loss = 0.03705551\n",
      "Iteration 30226, loss = 0.03705548\n",
      "Iteration 30227, loss = 0.03705569\n",
      "Iteration 30228, loss = 0.03705897\n",
      "Iteration 30229, loss = 0.03705526\n",
      "Iteration 30230, loss = 0.03706232\n",
      "Iteration 30231, loss = 0.03705797\n",
      "Iteration 30232, loss = 0.03705700\n",
      "Iteration 30233, loss = 0.03706351\n",
      "Iteration 30234, loss = 0.03706741\n",
      "Iteration 30235, loss = 0.03706614\n",
      "Iteration 30236, loss = 0.03706232\n",
      "Iteration 30237, loss = 0.03706094\n",
      "Iteration 30238, loss = 0.03705273\n",
      "Iteration 30239, loss = 0.03705530\n",
      "Iteration 30240, loss = 0.03706039\n",
      "Iteration 30241, loss = 0.03705243\n",
      "Iteration 30242, loss = 0.03704333\n",
      "Iteration 30243, loss = 0.03704674\n",
      "Iteration 30244, loss = 0.03704553\n",
      "Iteration 30245, loss = 0.03704613\n",
      "Iteration 30246, loss = 0.03704266\n",
      "Iteration 30247, loss = 0.03704263\n",
      "Iteration 30248, loss = 0.03704176\n",
      "Iteration 30249, loss = 0.03704056\n",
      "Iteration 30250, loss = 0.03704020\n",
      "Iteration 30251, loss = 0.03704040\n",
      "Iteration 30252, loss = 0.03703812\n",
      "Iteration 30253, loss = 0.03703668\n",
      "Iteration 30254, loss = 0.03703640\n",
      "Iteration 30255, loss = 0.03704194\n",
      "Iteration 30256, loss = 0.03704547\n",
      "Iteration 30257, loss = 0.03704453\n",
      "Iteration 30258, loss = 0.03703378\n",
      "Iteration 30259, loss = 0.03703428\n",
      "Iteration 30260, loss = 0.03704126\n",
      "Iteration 30261, loss = 0.03704352\n",
      "Iteration 30262, loss = 0.03703975\n",
      "Iteration 30263, loss = 0.03704183\n",
      "Iteration 30264, loss = 0.03703926\n",
      "Iteration 30265, loss = 0.03703001\n",
      "Iteration 30266, loss = 0.03702874\n",
      "Iteration 30267, loss = 0.03703935\n",
      "Iteration 30268, loss = 0.03703459\n",
      "Iteration 30269, loss = 0.03703164\n",
      "Iteration 30270, loss = 0.03703111\n",
      "Iteration 30271, loss = 0.03702772\n",
      "Iteration 30272, loss = 0.03702045\n",
      "Iteration 30273, loss = 0.03702180\n",
      "Iteration 30274, loss = 0.03702026\n",
      "Iteration 30275, loss = 0.03701780\n",
      "Iteration 30276, loss = 0.03702206\n",
      "Iteration 30277, loss = 0.03701562\n",
      "Iteration 30278, loss = 0.03702099\n",
      "Iteration 30279, loss = 0.03702071\n",
      "Iteration 30280, loss = 0.03701901\n",
      "Iteration 30281, loss = 0.03701961\n",
      "Iteration 30282, loss = 0.03701840\n",
      "Iteration 30283, loss = 0.03701980\n",
      "Iteration 30284, loss = 0.03701379\n",
      "Iteration 30285, loss = 0.03701279\n",
      "Iteration 30286, loss = 0.03701487\n",
      "Iteration 30287, loss = 0.03701303\n",
      "Iteration 30288, loss = 0.03701411\n",
      "Iteration 30289, loss = 0.03701862\n",
      "Iteration 30290, loss = 0.03701297\n",
      "Iteration 30291, loss = 0.03700777\n",
      "Iteration 30292, loss = 0.03701140\n",
      "Iteration 30293, loss = 0.03701293\n",
      "Iteration 30294, loss = 0.03700840\n",
      "Iteration 30295, loss = 0.03701198\n",
      "Iteration 30296, loss = 0.03701649\n",
      "Iteration 30297, loss = 0.03701301\n",
      "Iteration 30298, loss = 0.03701024\n",
      "Iteration 30299, loss = 0.03700856\n",
      "Iteration 30300, loss = 0.03700905\n",
      "Iteration 30301, loss = 0.03700722\n",
      "Iteration 30302, loss = 0.03700383\n",
      "Iteration 30303, loss = 0.03700300\n",
      "Iteration 30304, loss = 0.03700336\n",
      "Iteration 30305, loss = 0.03700051\n",
      "Iteration 30306, loss = 0.03700517\n",
      "Iteration 30307, loss = 0.03700971\n",
      "Iteration 30308, loss = 0.03700957\n",
      "Iteration 30309, loss = 0.03700789\n",
      "Iteration 30310, loss = 0.03700854\n",
      "Iteration 30311, loss = 0.03700852\n",
      "Iteration 30312, loss = 0.03700545\n",
      "Iteration 30313, loss = 0.03699466\n",
      "Iteration 30314, loss = 0.03699679\n",
      "Iteration 30315, loss = 0.03699941\n",
      "Iteration 30316, loss = 0.03699373\n",
      "Iteration 30317, loss = 0.03698872\n",
      "Iteration 30318, loss = 0.03699891\n",
      "Iteration 30319, loss = 0.03699867\n",
      "Iteration 30320, loss = 0.03700342\n",
      "Iteration 30321, loss = 0.03700136\n",
      "Iteration 30322, loss = 0.03699546\n",
      "Iteration 30323, loss = 0.03699213\n",
      "Iteration 30324, loss = 0.03699068\n",
      "Iteration 30325, loss = 0.03699263\n",
      "Iteration 30326, loss = 0.03699397\n",
      "Iteration 30327, loss = 0.03699080\n",
      "Iteration 30328, loss = 0.03698902\n",
      "Iteration 30329, loss = 0.03699434\n",
      "Iteration 30330, loss = 0.03699627\n",
      "Iteration 30331, loss = 0.03699436\n",
      "Iteration 30332, loss = 0.03699117\n",
      "Iteration 30333, loss = 0.03699000\n",
      "Iteration 30334, loss = 0.03698675\n",
      "Iteration 30335, loss = 0.03698266\n",
      "Iteration 30336, loss = 0.03698597\n",
      "Iteration 30337, loss = 0.03698499\n",
      "Iteration 30338, loss = 0.03698707\n",
      "Iteration 30339, loss = 0.03698636\n",
      "Iteration 30340, loss = 0.03698618\n",
      "Iteration 30341, loss = 0.03698482\n",
      "Iteration 30342, loss = 0.03698024\n",
      "Iteration 30343, loss = 0.03698311\n",
      "Iteration 30344, loss = 0.03698042\n",
      "Iteration 30345, loss = 0.03697449\n",
      "Iteration 30346, loss = 0.03697276\n",
      "Iteration 30347, loss = 0.03698790\n",
      "Iteration 30348, loss = 0.03698827\n",
      "Iteration 30349, loss = 0.03698439\n",
      "Iteration 30350, loss = 0.03697333\n",
      "Iteration 30351, loss = 0.03697900\n",
      "Iteration 30352, loss = 0.03697980\n",
      "Iteration 30353, loss = 0.03697802\n",
      "Iteration 30354, loss = 0.03697937\n",
      "Iteration 30355, loss = 0.03697969\n",
      "Iteration 30356, loss = 0.03697596\n",
      "Iteration 30357, loss = 0.03697461\n",
      "Iteration 30358, loss = 0.03697614\n",
      "Iteration 30359, loss = 0.03697005\n",
      "Iteration 30360, loss = 0.03696775\n",
      "Iteration 30361, loss = 0.03696849\n",
      "Iteration 30362, loss = 0.03697970\n",
      "Iteration 30363, loss = 0.03698065\n",
      "Iteration 30364, loss = 0.03696875\n",
      "Iteration 30365, loss = 0.03696796\n",
      "Iteration 30366, loss = 0.03697326\n",
      "Iteration 30367, loss = 0.03696550\n",
      "Iteration 30368, loss = 0.03697237\n",
      "Iteration 30369, loss = 0.03698671\n",
      "Iteration 30370, loss = 0.03699162\n",
      "Iteration 30371, loss = 0.03698735\n",
      "Iteration 30372, loss = 0.03697581\n",
      "Iteration 30373, loss = 0.03696311\n",
      "Iteration 30374, loss = 0.03695813\n",
      "Iteration 30375, loss = 0.03695911\n",
      "Iteration 30376, loss = 0.03696563\n",
      "Iteration 30377, loss = 0.03696184\n",
      "Iteration 30378, loss = 0.03695902\n",
      "Iteration 30379, loss = 0.03694712\n",
      "Iteration 30380, loss = 0.03695426\n",
      "Iteration 30381, loss = 0.03695654\n",
      "Iteration 30382, loss = 0.03695672\n",
      "Iteration 30383, loss = 0.03696344\n",
      "Iteration 30384, loss = 0.03696607\n",
      "Iteration 30385, loss = 0.03696262\n",
      "Iteration 30386, loss = 0.03695357\n",
      "Iteration 30387, loss = 0.03695648\n",
      "Iteration 30388, loss = 0.03695720\n",
      "Iteration 30389, loss = 0.03694636\n",
      "Iteration 30390, loss = 0.03694311\n",
      "Iteration 30391, loss = 0.03695218\n",
      "Iteration 30392, loss = 0.03694703\n",
      "Iteration 30393, loss = 0.03694623\n",
      "Iteration 30394, loss = 0.03694693\n",
      "Iteration 30395, loss = 0.03694370\n",
      "Iteration 30396, loss = 0.03694375\n",
      "Iteration 30397, loss = 0.03694005\n",
      "Iteration 30398, loss = 0.03693663\n",
      "Iteration 30399, loss = 0.03693958\n",
      "Iteration 30400, loss = 0.03694454\n",
      "Iteration 30401, loss = 0.03694544\n",
      "Iteration 30402, loss = 0.03693934\n",
      "Iteration 30403, loss = 0.03694065\n",
      "Iteration 30404, loss = 0.03694352\n",
      "Iteration 30405, loss = 0.03694252\n",
      "Iteration 30406, loss = 0.03694341\n",
      "Iteration 30407, loss = 0.03694119\n",
      "Iteration 30408, loss = 0.03694030\n",
      "Iteration 30409, loss = 0.03693656\n",
      "Iteration 30410, loss = 0.03693248\n",
      "Iteration 30411, loss = 0.03693197\n",
      "Iteration 30412, loss = 0.03693649\n",
      "Iteration 30413, loss = 0.03693052\n",
      "Iteration 30414, loss = 0.03693194\n",
      "Iteration 30415, loss = 0.03693496\n",
      "Iteration 30416, loss = 0.03693659\n",
      "Iteration 30417, loss = 0.03693637\n",
      "Iteration 30418, loss = 0.03692967\n",
      "Iteration 30419, loss = 0.03692646\n",
      "Iteration 30420, loss = 0.03693138\n",
      "Iteration 30421, loss = 0.03692442\n",
      "Iteration 30422, loss = 0.03693035\n",
      "Iteration 30423, loss = 0.03693251\n",
      "Iteration 30424, loss = 0.03692991\n",
      "Iteration 30425, loss = 0.03692486\n",
      "Iteration 30426, loss = 0.03692248\n",
      "Iteration 30427, loss = 0.03692485\n",
      "Iteration 30428, loss = 0.03692674\n",
      "Iteration 30429, loss = 0.03692534\n",
      "Iteration 30430, loss = 0.03692302\n",
      "Iteration 30431, loss = 0.03692077\n",
      "Iteration 30432, loss = 0.03692204\n",
      "Iteration 30433, loss = 0.03692346\n",
      "Iteration 30434, loss = 0.03691716\n",
      "Iteration 30435, loss = 0.03691193\n",
      "Iteration 30436, loss = 0.03691350\n",
      "Iteration 30437, loss = 0.03691121\n",
      "Iteration 30438, loss = 0.03690911\n",
      "Iteration 30439, loss = 0.03690787\n",
      "Iteration 30440, loss = 0.03691309\n",
      "Iteration 30441, loss = 0.03690909\n",
      "Iteration 30442, loss = 0.03690806\n",
      "Iteration 30443, loss = 0.03690888\n",
      "Iteration 30444, loss = 0.03690544\n",
      "Iteration 30445, loss = 0.03690453\n",
      "Iteration 30446, loss = 0.03690711\n",
      "Iteration 30447, loss = 0.03690386\n",
      "Iteration 30448, loss = 0.03690425\n",
      "Iteration 30449, loss = 0.03690365\n",
      "Iteration 30450, loss = 0.03690428\n",
      "Iteration 30451, loss = 0.03690807\n",
      "Iteration 30452, loss = 0.03691002\n",
      "Iteration 30453, loss = 0.03690559\n",
      "Iteration 30454, loss = 0.03690176\n",
      "Iteration 30455, loss = 0.03690237\n",
      "Iteration 30456, loss = 0.03690657\n",
      "Iteration 30457, loss = 0.03690078\n",
      "Iteration 30458, loss = 0.03690220\n",
      "Iteration 30459, loss = 0.03690921\n",
      "Iteration 30460, loss = 0.03690647\n",
      "Iteration 30461, loss = 0.03689682\n",
      "Iteration 30462, loss = 0.03690080\n",
      "Iteration 30463, loss = 0.03689668\n",
      "Iteration 30464, loss = 0.03689673\n",
      "Iteration 30465, loss = 0.03690473\n",
      "Iteration 30466, loss = 0.03689992\n",
      "Iteration 30467, loss = 0.03689193\n",
      "Iteration 30468, loss = 0.03689570\n",
      "Iteration 30469, loss = 0.03690129\n",
      "Iteration 30470, loss = 0.03689910\n",
      "Iteration 30471, loss = 0.03690183\n",
      "Iteration 30472, loss = 0.03690259\n",
      "Iteration 30473, loss = 0.03690232\n",
      "Iteration 30474, loss = 0.03689774\n",
      "Iteration 30475, loss = 0.03688756\n",
      "Iteration 30476, loss = 0.03689316\n",
      "Iteration 30477, loss = 0.03690107\n",
      "Iteration 30478, loss = 0.03689486\n",
      "Iteration 30479, loss = 0.03688876\n",
      "Iteration 30480, loss = 0.03688976\n",
      "Iteration 30481, loss = 0.03689007\n",
      "Iteration 30482, loss = 0.03688989\n",
      "Iteration 30483, loss = 0.03688708\n",
      "Iteration 30484, loss = 0.03688789\n",
      "Iteration 30485, loss = 0.03688765\n",
      "Iteration 30486, loss = 0.03688715\n",
      "Iteration 30487, loss = 0.03688570\n",
      "Iteration 30488, loss = 0.03687921\n",
      "Iteration 30489, loss = 0.03687881\n",
      "Iteration 30490, loss = 0.03688191\n",
      "Iteration 30491, loss = 0.03687750\n",
      "Iteration 30492, loss = 0.03687603\n",
      "Iteration 30493, loss = 0.03688252\n",
      "Iteration 30494, loss = 0.03688201\n",
      "Iteration 30495, loss = 0.03688011\n",
      "Iteration 30496, loss = 0.03687570\n",
      "Iteration 30497, loss = 0.03687096\n",
      "Iteration 30498, loss = 0.03687075\n",
      "Iteration 30499, loss = 0.03686883\n",
      "Iteration 30500, loss = 0.03686938\n",
      "Iteration 30501, loss = 0.03686823\n",
      "Iteration 30502, loss = 0.03686797\n",
      "Iteration 30503, loss = 0.03686693\n",
      "Iteration 30504, loss = 0.03686783\n",
      "Iteration 30505, loss = 0.03686800\n",
      "Iteration 30506, loss = 0.03686842\n",
      "Iteration 30507, loss = 0.03687014\n",
      "Iteration 30508, loss = 0.03686981\n",
      "Iteration 30509, loss = 0.03686924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 30510, loss = 0.03686424\n",
      "Iteration 30511, loss = 0.03686509\n",
      "Iteration 30512, loss = 0.03686938\n",
      "Iteration 30513, loss = 0.03686517\n",
      "Iteration 30514, loss = 0.03686130\n",
      "Iteration 30515, loss = 0.03686736\n",
      "Iteration 30516, loss = 0.03686730\n",
      "Iteration 30517, loss = 0.03686055\n",
      "Iteration 30518, loss = 0.03686092\n",
      "Iteration 30519, loss = 0.03686828\n",
      "Iteration 30520, loss = 0.03687310\n",
      "Iteration 30521, loss = 0.03687181\n",
      "Iteration 30522, loss = 0.03687341\n",
      "Iteration 30523, loss = 0.03686680\n",
      "Iteration 30524, loss = 0.03686727\n",
      "Iteration 30525, loss = 0.03686546\n",
      "Iteration 30526, loss = 0.03685484\n",
      "Iteration 30527, loss = 0.03686386\n",
      "Iteration 30528, loss = 0.03686531\n",
      "Iteration 30529, loss = 0.03687372\n",
      "Iteration 30530, loss = 0.03687341\n",
      "Iteration 30531, loss = 0.03686456\n",
      "Iteration 30532, loss = 0.03685540\n",
      "Iteration 30533, loss = 0.03685722\n",
      "Iteration 30534, loss = 0.03685865\n",
      "Iteration 30535, loss = 0.03685894\n",
      "Iteration 30536, loss = 0.03685767\n",
      "Iteration 30537, loss = 0.03685654\n",
      "Iteration 30538, loss = 0.03685565\n",
      "Iteration 30539, loss = 0.03685426\n",
      "Iteration 30540, loss = 0.03685648\n",
      "Iteration 30541, loss = 0.03686029\n",
      "Iteration 30542, loss = 0.03684974\n",
      "Iteration 30543, loss = 0.03685854\n",
      "Iteration 30544, loss = 0.03685814\n",
      "Iteration 30545, loss = 0.03685735\n",
      "Iteration 30546, loss = 0.03685082\n",
      "Iteration 30547, loss = 0.03683883\n",
      "Iteration 30548, loss = 0.03684580\n",
      "Iteration 30549, loss = 0.03685361\n",
      "Iteration 30550, loss = 0.03685074\n",
      "Iteration 30551, loss = 0.03685425\n",
      "Iteration 30552, loss = 0.03684974\n",
      "Iteration 30553, loss = 0.03684477\n",
      "Iteration 30554, loss = 0.03684077\n",
      "Iteration 30555, loss = 0.03684464\n",
      "Iteration 30556, loss = 0.03684157\n",
      "Iteration 30557, loss = 0.03683476\n",
      "Iteration 30558, loss = 0.03683219\n",
      "Iteration 30559, loss = 0.03683926\n",
      "Iteration 30560, loss = 0.03684084\n",
      "Iteration 30561, loss = 0.03683470\n",
      "Iteration 30562, loss = 0.03683075\n",
      "Iteration 30563, loss = 0.03683218\n",
      "Iteration 30564, loss = 0.03683371\n",
      "Iteration 30565, loss = 0.03683614\n",
      "Iteration 30566, loss = 0.03683759\n",
      "Iteration 30567, loss = 0.03682827\n",
      "Iteration 30568, loss = 0.03682934\n",
      "Iteration 30569, loss = 0.03684075\n",
      "Iteration 30570, loss = 0.03684040\n",
      "Iteration 30571, loss = 0.03683416\n",
      "Iteration 30572, loss = 0.03682667\n",
      "Iteration 30573, loss = 0.03683638\n",
      "Iteration 30574, loss = 0.03683960\n",
      "Iteration 30575, loss = 0.03683150\n",
      "Iteration 30576, loss = 0.03682595\n",
      "Iteration 30577, loss = 0.03682452\n",
      "Iteration 30578, loss = 0.03682853\n",
      "Iteration 30579, loss = 0.03683073\n",
      "Iteration 30580, loss = 0.03682712\n",
      "Iteration 30581, loss = 0.03681919\n",
      "Iteration 30582, loss = 0.03681932\n",
      "Iteration 30583, loss = 0.03682724\n",
      "Iteration 30584, loss = 0.03682944\n",
      "Iteration 30585, loss = 0.03682628\n",
      "Iteration 30586, loss = 0.03681986\n",
      "Iteration 30587, loss = 0.03682196\n",
      "Iteration 30588, loss = 0.03682339\n",
      "Iteration 30589, loss = 0.03682358\n",
      "Iteration 30590, loss = 0.03681980\n",
      "Iteration 30591, loss = 0.03681691\n",
      "Iteration 30592, loss = 0.03682018\n",
      "Iteration 30593, loss = 0.03682083\n",
      "Iteration 30594, loss = 0.03681738\n",
      "Iteration 30595, loss = 0.03681319\n",
      "Iteration 30596, loss = 0.03681014\n",
      "Iteration 30597, loss = 0.03681903\n",
      "Iteration 30598, loss = 0.03681805\n",
      "Iteration 30599, loss = 0.03680883\n",
      "Iteration 30600, loss = 0.03681485\n",
      "Iteration 30601, loss = 0.03682020\n",
      "Iteration 30602, loss = 0.03681954\n",
      "Iteration 30603, loss = 0.03681770\n",
      "Iteration 30604, loss = 0.03681021\n",
      "Iteration 30605, loss = 0.03680487\n",
      "Iteration 30606, loss = 0.03681043\n",
      "Iteration 30607, loss = 0.03681058\n",
      "Iteration 30608, loss = 0.03680832\n",
      "Iteration 30609, loss = 0.03680706\n",
      "Iteration 30610, loss = 0.03681404\n",
      "Iteration 30611, loss = 0.03681613\n",
      "Iteration 30612, loss = 0.03680877\n",
      "Iteration 30613, loss = 0.03680980\n",
      "Iteration 30614, loss = 0.03681401\n",
      "Iteration 30615, loss = 0.03680759\n",
      "Iteration 30616, loss = 0.03679992\n",
      "Iteration 30617, loss = 0.03680332\n",
      "Iteration 30618, loss = 0.03680748\n",
      "Iteration 30619, loss = 0.03680334\n",
      "Iteration 30620, loss = 0.03679754\n",
      "Iteration 30621, loss = 0.03679936\n",
      "Iteration 30622, loss = 0.03680128\n",
      "Iteration 30623, loss = 0.03680129\n",
      "Iteration 30624, loss = 0.03680118\n",
      "Iteration 30625, loss = 0.03679886\n",
      "Iteration 30626, loss = 0.03679482\n",
      "Iteration 30627, loss = 0.03679131\n",
      "Iteration 30628, loss = 0.03679661\n",
      "Iteration 30629, loss = 0.03680130\n",
      "Iteration 30630, loss = 0.03680164\n",
      "Iteration 30631, loss = 0.03679738\n",
      "Iteration 30632, loss = 0.03678715\n",
      "Iteration 30633, loss = 0.03679216\n",
      "Iteration 30634, loss = 0.03679148\n",
      "Iteration 30635, loss = 0.03678826\n",
      "Iteration 30636, loss = 0.03678390\n",
      "Iteration 30637, loss = 0.03678981\n",
      "Iteration 30638, loss = 0.03679002\n",
      "Iteration 30639, loss = 0.03678109\n",
      "Iteration 30640, loss = 0.03678390\n",
      "Iteration 30641, loss = 0.03678430\n",
      "Iteration 30642, loss = 0.03679271\n",
      "Iteration 30643, loss = 0.03679090\n",
      "Iteration 30644, loss = 0.03679076\n",
      "Iteration 30645, loss = 0.03678330\n",
      "Iteration 30646, loss = 0.03678654\n",
      "Iteration 30647, loss = 0.03679314\n",
      "Iteration 30648, loss = 0.03680248\n",
      "Iteration 30649, loss = 0.03679490\n",
      "Iteration 30650, loss = 0.03678448\n",
      "Iteration 30651, loss = 0.03678022\n",
      "Iteration 30652, loss = 0.03678708\n",
      "Iteration 30653, loss = 0.03678574\n",
      "Iteration 30654, loss = 0.03678335\n",
      "Iteration 30655, loss = 0.03678418\n",
      "Iteration 30656, loss = 0.03678549\n",
      "Iteration 30657, loss = 0.03678335\n",
      "Iteration 30658, loss = 0.03677432\n",
      "Iteration 30659, loss = 0.03677646\n",
      "Iteration 30660, loss = 0.03677862\n",
      "Iteration 30661, loss = 0.03677159\n",
      "Iteration 30662, loss = 0.03677651\n",
      "Iteration 30663, loss = 0.03677676\n",
      "Iteration 30664, loss = 0.03676975\n",
      "Iteration 30665, loss = 0.03676779\n",
      "Iteration 30666, loss = 0.03677925\n",
      "Iteration 30667, loss = 0.03677802\n",
      "Iteration 30668, loss = 0.03677312\n",
      "Iteration 30669, loss = 0.03677304\n",
      "Iteration 30670, loss = 0.03677438\n",
      "Iteration 30671, loss = 0.03676930\n",
      "Iteration 30672, loss = 0.03675770\n",
      "Iteration 30673, loss = 0.03677044\n",
      "Iteration 30674, loss = 0.03677775\n",
      "Iteration 30675, loss = 0.03676961\n",
      "Iteration 30676, loss = 0.03675661\n",
      "Iteration 30677, loss = 0.03676930\n",
      "Iteration 30678, loss = 0.03677662\n",
      "Iteration 30679, loss = 0.03678020\n",
      "Iteration 30680, loss = 0.03677319\n",
      "Iteration 30681, loss = 0.03676901\n",
      "Iteration 30682, loss = 0.03676285\n",
      "Iteration 30683, loss = 0.03676296\n",
      "Iteration 30684, loss = 0.03675687\n",
      "Iteration 30685, loss = 0.03675428\n",
      "Iteration 30686, loss = 0.03675218\n",
      "Iteration 30687, loss = 0.03675048\n",
      "Iteration 30688, loss = 0.03674661\n",
      "Iteration 30689, loss = 0.03674976\n",
      "Iteration 30690, loss = 0.03675657\n",
      "Iteration 30691, loss = 0.03675443\n",
      "Iteration 30692, loss = 0.03675126\n",
      "Iteration 30693, loss = 0.03674506\n",
      "Iteration 30694, loss = 0.03674920\n",
      "Iteration 30695, loss = 0.03675128\n",
      "Iteration 30696, loss = 0.03674702\n",
      "Iteration 30697, loss = 0.03674854\n",
      "Iteration 30698, loss = 0.03675174\n",
      "Iteration 30699, loss = 0.03675085\n",
      "Iteration 30700, loss = 0.03675025\n",
      "Iteration 30701, loss = 0.03674745\n",
      "Iteration 30702, loss = 0.03674937\n",
      "Iteration 30703, loss = 0.03674842\n",
      "Iteration 30704, loss = 0.03675526\n",
      "Iteration 30705, loss = 0.03675722\n",
      "Iteration 30706, loss = 0.03675222\n",
      "Iteration 30707, loss = 0.03673976\n",
      "Iteration 30708, loss = 0.03673933\n",
      "Iteration 30709, loss = 0.03675866\n",
      "Iteration 30710, loss = 0.03675607\n",
      "Iteration 30711, loss = 0.03674333\n",
      "Iteration 30712, loss = 0.03674251\n",
      "Iteration 30713, loss = 0.03674284\n",
      "Iteration 30714, loss = 0.03674338\n",
      "Iteration 30715, loss = 0.03674733\n",
      "Iteration 30716, loss = 0.03674335\n",
      "Iteration 30717, loss = 0.03673711\n",
      "Iteration 30718, loss = 0.03673743\n",
      "Iteration 30719, loss = 0.03673713\n",
      "Iteration 30720, loss = 0.03673952\n",
      "Iteration 30721, loss = 0.03674250\n",
      "Iteration 30722, loss = 0.03673358\n",
      "Iteration 30723, loss = 0.03672550\n",
      "Iteration 30724, loss = 0.03673736\n",
      "Iteration 30725, loss = 0.03674298\n",
      "Iteration 30726, loss = 0.03673756\n",
      "Iteration 30727, loss = 0.03672943\n",
      "Iteration 30728, loss = 0.03672517\n",
      "Iteration 30729, loss = 0.03673551\n",
      "Iteration 30730, loss = 0.03673699\n",
      "Iteration 30731, loss = 0.03673399\n",
      "Iteration 30732, loss = 0.03673467\n",
      "Iteration 30733, loss = 0.03672926\n",
      "Iteration 30734, loss = 0.03673232\n",
      "Iteration 30735, loss = 0.03673648\n",
      "Iteration 30736, loss = 0.03674016\n",
      "Iteration 30737, loss = 0.03673788\n",
      "Iteration 30738, loss = 0.03673919\n",
      "Iteration 30739, loss = 0.03673091\n",
      "Iteration 30740, loss = 0.03672529\n",
      "Iteration 30741, loss = 0.03672729\n",
      "Iteration 30742, loss = 0.03673000\n",
      "Iteration 30743, loss = 0.03672467\n",
      "Iteration 30744, loss = 0.03672643\n",
      "Iteration 30745, loss = 0.03672816\n",
      "Iteration 30746, loss = 0.03672318\n",
      "Iteration 30747, loss = 0.03671770\n",
      "Iteration 30748, loss = 0.03671795\n",
      "Iteration 30749, loss = 0.03672342\n",
      "Iteration 30750, loss = 0.03672408\n",
      "Iteration 30751, loss = 0.03672269\n",
      "Iteration 30752, loss = 0.03672305\n",
      "Iteration 30753, loss = 0.03672413\n",
      "Iteration 30754, loss = 0.03671691\n",
      "Iteration 30755, loss = 0.03671558\n",
      "Iteration 30756, loss = 0.03672070\n",
      "Iteration 30757, loss = 0.03672018\n",
      "Iteration 30758, loss = 0.03671155\n",
      "Iteration 30759, loss = 0.03671097\n",
      "Iteration 30760, loss = 0.03671345\n",
      "Iteration 30761, loss = 0.03671401\n",
      "Iteration 30762, loss = 0.03671042\n",
      "Iteration 30763, loss = 0.03670908\n",
      "Iteration 30764, loss = 0.03670300\n",
      "Iteration 30765, loss = 0.03670768\n",
      "Iteration 30766, loss = 0.03670772\n",
      "Iteration 30767, loss = 0.03670324\n",
      "Iteration 30768, loss = 0.03670583\n",
      "Iteration 30769, loss = 0.03670195\n",
      "Iteration 30770, loss = 0.03670299\n",
      "Iteration 30771, loss = 0.03670228\n",
      "Iteration 30772, loss = 0.03670272\n",
      "Iteration 30773, loss = 0.03669587\n",
      "Iteration 30774, loss = 0.03670129\n",
      "Iteration 30775, loss = 0.03670324\n",
      "Iteration 30776, loss = 0.03670118\n",
      "Iteration 30777, loss = 0.03670224\n",
      "Iteration 30778, loss = 0.03670030\n",
      "Iteration 30779, loss = 0.03669248\n",
      "Iteration 30780, loss = 0.03670102\n",
      "Iteration 30781, loss = 0.03670477\n",
      "Iteration 30782, loss = 0.03669830\n",
      "Iteration 30783, loss = 0.03669852\n",
      "Iteration 30784, loss = 0.03669105\n",
      "Iteration 30785, loss = 0.03669237\n",
      "Iteration 30786, loss = 0.03668866\n",
      "Iteration 30787, loss = 0.03669143\n",
      "Iteration 30788, loss = 0.03668992\n",
      "Iteration 30789, loss = 0.03669421\n",
      "Iteration 30790, loss = 0.03669568\n",
      "Iteration 30791, loss = 0.03669224\n",
      "Iteration 30792, loss = 0.03668890\n",
      "Iteration 30793, loss = 0.03668615\n",
      "Iteration 30794, loss = 0.03668982\n",
      "Iteration 30795, loss = 0.03668591\n",
      "Iteration 30796, loss = 0.03668957\n",
      "Iteration 30797, loss = 0.03669106\n",
      "Iteration 30798, loss = 0.03669179\n",
      "Iteration 30799, loss = 0.03669253\n",
      "Iteration 30800, loss = 0.03668711\n",
      "Iteration 30801, loss = 0.03667707\n",
      "Iteration 30802, loss = 0.03669128\n",
      "Iteration 30803, loss = 0.03669736\n",
      "Iteration 30804, loss = 0.03668625\n",
      "Iteration 30805, loss = 0.03667657\n",
      "Iteration 30806, loss = 0.03668776\n",
      "Iteration 30807, loss = 0.03669417\n",
      "Iteration 30808, loss = 0.03669018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 30809, loss = 0.03668127\n",
      "Iteration 30810, loss = 0.03668648\n",
      "Iteration 30811, loss = 0.03668908\n",
      "Iteration 30812, loss = 0.03668314\n",
      "Iteration 30813, loss = 0.03668175\n",
      "Iteration 30814, loss = 0.03668061\n",
      "Iteration 30815, loss = 0.03667565\n",
      "Iteration 30816, loss = 0.03667511\n",
      "Iteration 30817, loss = 0.03667089\n",
      "Iteration 30818, loss = 0.03666982\n",
      "Iteration 30819, loss = 0.03667731\n",
      "Iteration 30820, loss = 0.03667186\n",
      "Iteration 30821, loss = 0.03666963\n",
      "Iteration 30822, loss = 0.03667370\n",
      "Iteration 30823, loss = 0.03666709\n",
      "Iteration 30824, loss = 0.03667024\n",
      "Iteration 30825, loss = 0.03667509\n",
      "Iteration 30826, loss = 0.03667344\n",
      "Iteration 30827, loss = 0.03666588\n",
      "Iteration 30828, loss = 0.03666904\n",
      "Iteration 30829, loss = 0.03666893\n",
      "Iteration 30830, loss = 0.03666870\n",
      "Iteration 30831, loss = 0.03666437\n",
      "Iteration 30832, loss = 0.03666340\n",
      "Iteration 30833, loss = 0.03666441\n",
      "Iteration 30834, loss = 0.03666262\n",
      "Iteration 30835, loss = 0.03665602\n",
      "Iteration 30836, loss = 0.03665653\n",
      "Iteration 30837, loss = 0.03665678\n",
      "Iteration 30838, loss = 0.03665027\n",
      "Iteration 30839, loss = 0.03664992\n",
      "Iteration 30840, loss = 0.03665487\n",
      "Iteration 30841, loss = 0.03665361\n",
      "Iteration 30842, loss = 0.03665642\n",
      "Iteration 30843, loss = 0.03665581\n",
      "Iteration 30844, loss = 0.03665965\n",
      "Iteration 30845, loss = 0.03665919\n",
      "Iteration 30846, loss = 0.03665237\n",
      "Iteration 30847, loss = 0.03665027\n",
      "Iteration 30848, loss = 0.03665122\n",
      "Iteration 30849, loss = 0.03664793\n",
      "Iteration 30850, loss = 0.03664484\n",
      "Iteration 30851, loss = 0.03664632\n",
      "Iteration 30852, loss = 0.03665471\n",
      "Iteration 30853, loss = 0.03665158\n",
      "Iteration 30854, loss = 0.03664745\n",
      "Iteration 30855, loss = 0.03665251\n",
      "Iteration 30856, loss = 0.03665523\n",
      "Iteration 30857, loss = 0.03665317\n",
      "Iteration 30858, loss = 0.03665637\n",
      "Iteration 30859, loss = 0.03665426\n",
      "Iteration 30860, loss = 0.03665329\n",
      "Iteration 30861, loss = 0.03665588\n",
      "Iteration 30862, loss = 0.03664954\n",
      "Iteration 30863, loss = 0.03664113\n",
      "Iteration 30864, loss = 0.03664471\n",
      "Iteration 30865, loss = 0.03663853\n",
      "Iteration 30866, loss = 0.03664341\n",
      "Iteration 30867, loss = 0.03664750\n",
      "Iteration 30868, loss = 0.03664328\n",
      "Iteration 30869, loss = 0.03664225\n",
      "Iteration 30870, loss = 0.03664385\n",
      "Iteration 30871, loss = 0.03663691\n",
      "Iteration 30872, loss = 0.03664220\n",
      "Iteration 30873, loss = 0.03664287\n",
      "Iteration 30874, loss = 0.03664552\n",
      "Iteration 30875, loss = 0.03664819\n",
      "Iteration 30876, loss = 0.03663908\n",
      "Iteration 30877, loss = 0.03664415\n",
      "Iteration 30878, loss = 0.03664656\n",
      "Iteration 30879, loss = 0.03663716\n",
      "Iteration 30880, loss = 0.03663604\n",
      "Iteration 30881, loss = 0.03663723\n",
      "Iteration 30882, loss = 0.03663639\n",
      "Iteration 30883, loss = 0.03663270\n",
      "Iteration 30884, loss = 0.03663467\n",
      "Iteration 30885, loss = 0.03663090\n",
      "Iteration 30886, loss = 0.03662331\n",
      "Iteration 30887, loss = 0.03662706\n",
      "Iteration 30888, loss = 0.03662909\n",
      "Iteration 30889, loss = 0.03662897\n",
      "Iteration 30890, loss = 0.03663627\n",
      "Iteration 30891, loss = 0.03663353\n",
      "Iteration 30892, loss = 0.03662844\n",
      "Iteration 30893, loss = 0.03662942\n",
      "Iteration 30894, loss = 0.03663275\n",
      "Iteration 30895, loss = 0.03662618\n",
      "Iteration 30896, loss = 0.03662241\n",
      "Iteration 30897, loss = 0.03662240\n",
      "Iteration 30898, loss = 0.03663114\n",
      "Iteration 30899, loss = 0.03663239\n",
      "Iteration 30900, loss = 0.03662301\n",
      "Iteration 30901, loss = 0.03662294\n",
      "Iteration 30902, loss = 0.03662641\n",
      "Iteration 30903, loss = 0.03662591\n",
      "Iteration 30904, loss = 0.03662281\n",
      "Iteration 30905, loss = 0.03661505\n",
      "Iteration 30906, loss = 0.03661618\n",
      "Iteration 30907, loss = 0.03661552\n",
      "Iteration 30908, loss = 0.03662276\n",
      "Iteration 30909, loss = 0.03662140\n",
      "Iteration 30910, loss = 0.03661894\n",
      "Iteration 30911, loss = 0.03661168\n",
      "Iteration 30912, loss = 0.03661957\n",
      "Iteration 30913, loss = 0.03662187\n",
      "Iteration 30914, loss = 0.03662045\n",
      "Iteration 30915, loss = 0.03661511\n",
      "Iteration 30916, loss = 0.03661500\n",
      "Iteration 30917, loss = 0.03661319\n",
      "Iteration 30918, loss = 0.03661514\n",
      "Iteration 30919, loss = 0.03661159\n",
      "Iteration 30920, loss = 0.03660725\n",
      "Iteration 30921, loss = 0.03660528\n",
      "Iteration 30922, loss = 0.03660377\n",
      "Iteration 30923, loss = 0.03660717\n",
      "Iteration 30924, loss = 0.03660690\n",
      "Iteration 30925, loss = 0.03660610\n",
      "Iteration 30926, loss = 0.03660323\n",
      "Iteration 30927, loss = 0.03661125\n",
      "Iteration 30928, loss = 0.03660760\n",
      "Iteration 30929, loss = 0.03660166\n",
      "Iteration 30930, loss = 0.03660528\n",
      "Iteration 30931, loss = 0.03660596\n",
      "Iteration 30932, loss = 0.03659738\n",
      "Iteration 30933, loss = 0.03660061\n",
      "Iteration 30934, loss = 0.03660371\n",
      "Iteration 30935, loss = 0.03659712\n",
      "Iteration 30936, loss = 0.03659345\n",
      "Iteration 30937, loss = 0.03660017\n",
      "Iteration 30938, loss = 0.03659969\n",
      "Iteration 30939, loss = 0.03659278\n",
      "Iteration 30940, loss = 0.03659083\n",
      "Iteration 30941, loss = 0.03660241\n",
      "Iteration 30942, loss = 0.03660529\n",
      "Iteration 30943, loss = 0.03659496\n",
      "Iteration 30944, loss = 0.03659447\n",
      "Iteration 30945, loss = 0.03659710\n",
      "Iteration 30946, loss = 0.03659758\n",
      "Iteration 30947, loss = 0.03659567\n",
      "Iteration 30948, loss = 0.03658825\n",
      "Iteration 30949, loss = 0.03658999\n",
      "Iteration 30950, loss = 0.03659165\n",
      "Iteration 30951, loss = 0.03659153\n",
      "Iteration 30952, loss = 0.03659157\n",
      "Iteration 30953, loss = 0.03658985\n",
      "Iteration 30954, loss = 0.03658181\n",
      "Iteration 30955, loss = 0.03659235\n",
      "Iteration 30956, loss = 0.03659903\n",
      "Iteration 30957, loss = 0.03659270\n",
      "Iteration 30958, loss = 0.03658042\n",
      "Iteration 30959, loss = 0.03658812\n",
      "Iteration 30960, loss = 0.03659059\n",
      "Iteration 30961, loss = 0.03658655\n",
      "Iteration 30962, loss = 0.03658046\n",
      "Iteration 30963, loss = 0.03658706\n",
      "Iteration 30964, loss = 0.03658892\n",
      "Iteration 30965, loss = 0.03658511\n",
      "Iteration 30966, loss = 0.03659204\n",
      "Iteration 30967, loss = 0.03658701\n",
      "Iteration 30968, loss = 0.03657991\n",
      "Iteration 30969, loss = 0.03658440\n",
      "Iteration 30970, loss = 0.03658246\n",
      "Iteration 30971, loss = 0.03658574\n",
      "Iteration 30972, loss = 0.03658536\n",
      "Iteration 30973, loss = 0.03658302\n",
      "Iteration 30974, loss = 0.03657824\n",
      "Iteration 30975, loss = 0.03657420\n",
      "Iteration 30976, loss = 0.03656904\n",
      "Iteration 30977, loss = 0.03657047\n",
      "Iteration 30978, loss = 0.03658138\n",
      "Iteration 30979, loss = 0.03657546\n",
      "Iteration 30980, loss = 0.03656921\n",
      "Iteration 30981, loss = 0.03657310\n",
      "Iteration 30982, loss = 0.03657796\n",
      "Iteration 30983, loss = 0.03657516\n",
      "Iteration 30984, loss = 0.03657218\n",
      "Iteration 30985, loss = 0.03657041\n",
      "Iteration 30986, loss = 0.03657002\n",
      "Iteration 30987, loss = 0.03656389\n",
      "Iteration 30988, loss = 0.03656441\n",
      "Iteration 30989, loss = 0.03657659\n",
      "Iteration 30990, loss = 0.03657126\n",
      "Iteration 30991, loss = 0.03655558\n",
      "Iteration 30992, loss = 0.03657213\n",
      "Iteration 30993, loss = 0.03657775\n",
      "Iteration 30994, loss = 0.03657271\n",
      "Iteration 30995, loss = 0.03656980\n",
      "Iteration 30996, loss = 0.03656479\n",
      "Iteration 30997, loss = 0.03656001\n",
      "Iteration 30998, loss = 0.03655974\n",
      "Iteration 30999, loss = 0.03656250\n",
      "Iteration 31000, loss = 0.03655950\n",
      "Iteration 31001, loss = 0.03655428\n",
      "Iteration 31002, loss = 0.03655056\n",
      "Iteration 31003, loss = 0.03655380\n",
      "Iteration 31004, loss = 0.03656064\n",
      "Iteration 31005, loss = 0.03655399\n",
      "Iteration 31006, loss = 0.03655463\n",
      "Iteration 31007, loss = 0.03655787\n",
      "Iteration 31008, loss = 0.03656191\n",
      "Iteration 31009, loss = 0.03655595\n",
      "Iteration 31010, loss = 0.03655867\n",
      "Iteration 31011, loss = 0.03655472\n",
      "Iteration 31012, loss = 0.03655860\n",
      "Iteration 31013, loss = 0.03655481\n",
      "Iteration 31014, loss = 0.03655261\n",
      "Iteration 31015, loss = 0.03654982\n",
      "Iteration 31016, loss = 0.03655951\n",
      "Iteration 31017, loss = 0.03656013\n",
      "Iteration 31018, loss = 0.03655178\n",
      "Iteration 31019, loss = 0.03654945\n",
      "Iteration 31020, loss = 0.03655419\n",
      "Iteration 31021, loss = 0.03655980\n",
      "Iteration 31022, loss = 0.03655575\n",
      "Iteration 31023, loss = 0.03655157\n",
      "Iteration 31024, loss = 0.03654537\n",
      "Iteration 31025, loss = 0.03654428\n",
      "Iteration 31026, loss = 0.03654383\n",
      "Iteration 31027, loss = 0.03653844\n",
      "Iteration 31028, loss = 0.03654075\n",
      "Iteration 31029, loss = 0.03654345\n",
      "Iteration 31030, loss = 0.03654306\n",
      "Iteration 31031, loss = 0.03653838\n",
      "Iteration 31032, loss = 0.03653772\n",
      "Iteration 31033, loss = 0.03654575\n",
      "Iteration 31034, loss = 0.03654264\n",
      "Iteration 31035, loss = 0.03653866\n",
      "Iteration 31036, loss = 0.03654038\n",
      "Iteration 31037, loss = 0.03654036\n",
      "Iteration 31038, loss = 0.03653411\n",
      "Iteration 31039, loss = 0.03652777\n",
      "Iteration 31040, loss = 0.03654052\n",
      "Iteration 31041, loss = 0.03653917\n",
      "Iteration 31042, loss = 0.03652540\n",
      "Iteration 31043, loss = 0.03653336\n",
      "Iteration 31044, loss = 0.03653902\n",
      "Iteration 31045, loss = 0.03653414\n",
      "Iteration 31046, loss = 0.03652975\n",
      "Iteration 31047, loss = 0.03653348\n",
      "Iteration 31048, loss = 0.03652916\n",
      "Iteration 31049, loss = 0.03653676\n",
      "Iteration 31050, loss = 0.03653565\n",
      "Iteration 31051, loss = 0.03653308\n",
      "Iteration 31052, loss = 0.03652908\n",
      "Iteration 31053, loss = 0.03652352\n",
      "Iteration 31054, loss = 0.03652458\n",
      "Iteration 31055, loss = 0.03652448\n",
      "Iteration 31056, loss = 0.03652767\n",
      "Iteration 31057, loss = 0.03652698\n",
      "Iteration 31058, loss = 0.03652693\n",
      "Iteration 31059, loss = 0.03652029\n",
      "Iteration 31060, loss = 0.03652087\n",
      "Iteration 31061, loss = 0.03652925\n",
      "Iteration 31062, loss = 0.03652613\n",
      "Iteration 31063, loss = 0.03651974\n",
      "Iteration 31064, loss = 0.03652143\n",
      "Iteration 31065, loss = 0.03653003\n",
      "Iteration 31066, loss = 0.03653092\n",
      "Iteration 31067, loss = 0.03652753\n",
      "Iteration 31068, loss = 0.03652734\n",
      "Iteration 31069, loss = 0.03653098\n",
      "Iteration 31070, loss = 0.03652180\n",
      "Iteration 31071, loss = 0.03651254\n",
      "Iteration 31072, loss = 0.03653212\n",
      "Iteration 31073, loss = 0.03653768\n",
      "Iteration 31074, loss = 0.03652459\n",
      "Iteration 31075, loss = 0.03652069\n",
      "Iteration 31076, loss = 0.03653284\n",
      "Iteration 31077, loss = 0.03653824\n",
      "Iteration 31078, loss = 0.03653532\n",
      "Iteration 31079, loss = 0.03652738\n",
      "Iteration 31080, loss = 0.03651355\n",
      "Iteration 31081, loss = 0.03650646\n",
      "Iteration 31082, loss = 0.03652025\n",
      "Iteration 31083, loss = 0.03652093\n",
      "Iteration 31084, loss = 0.03651361\n",
      "Iteration 31085, loss = 0.03651323\n",
      "Iteration 31086, loss = 0.03650282\n",
      "Iteration 31087, loss = 0.03650385\n",
      "Iteration 31088, loss = 0.03650631\n",
      "Iteration 31089, loss = 0.03650505\n",
      "Iteration 31090, loss = 0.03649925\n",
      "Iteration 31091, loss = 0.03650810\n",
      "Iteration 31092, loss = 0.03651195\n",
      "Iteration 31093, loss = 0.03650375\n",
      "Iteration 31094, loss = 0.03650188\n",
      "Iteration 31095, loss = 0.03650712\n",
      "Iteration 31096, loss = 0.03650169\n",
      "Iteration 31097, loss = 0.03650428\n",
      "Iteration 31098, loss = 0.03650401\n",
      "Iteration 31099, loss = 0.03650005\n",
      "Iteration 31100, loss = 0.03649649\n",
      "Iteration 31101, loss = 0.03649104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 31102, loss = 0.03650131\n",
      "Iteration 31103, loss = 0.03649937\n",
      "Iteration 31104, loss = 0.03649518\n",
      "Iteration 31105, loss = 0.03649796\n",
      "Iteration 31106, loss = 0.03650167\n",
      "Iteration 31107, loss = 0.03649912\n",
      "Iteration 31108, loss = 0.03649715\n",
      "Iteration 31109, loss = 0.03649673\n",
      "Iteration 31110, loss = 0.03649086\n",
      "Iteration 31111, loss = 0.03649270\n",
      "Iteration 31112, loss = 0.03649797\n",
      "Iteration 31113, loss = 0.03649388\n",
      "Iteration 31114, loss = 0.03648587\n",
      "Iteration 31115, loss = 0.03648836\n",
      "Iteration 31116, loss = 0.03649300\n",
      "Iteration 31117, loss = 0.03649448\n",
      "Iteration 31118, loss = 0.03649215\n",
      "Iteration 31119, loss = 0.03648876\n",
      "Iteration 31120, loss = 0.03648637\n",
      "Iteration 31121, loss = 0.03648315\n",
      "Iteration 31122, loss = 0.03648353\n",
      "Iteration 31123, loss = 0.03648422\n",
      "Iteration 31124, loss = 0.03648647\n",
      "Iteration 31125, loss = 0.03648731\n",
      "Iteration 31126, loss = 0.03647779\n",
      "Iteration 31127, loss = 0.03648016\n",
      "Iteration 31128, loss = 0.03649161\n",
      "Iteration 31129, loss = 0.03649199\n",
      "Iteration 31130, loss = 0.03648185\n",
      "Iteration 31131, loss = 0.03647974\n",
      "Iteration 31132, loss = 0.03647121\n",
      "Iteration 31133, loss = 0.03647870\n",
      "Iteration 31134, loss = 0.03648074\n",
      "Iteration 31135, loss = 0.03647574\n",
      "Iteration 31136, loss = 0.03647610\n",
      "Iteration 31137, loss = 0.03647980\n",
      "Iteration 31138, loss = 0.03648413\n",
      "Iteration 31139, loss = 0.03648733\n",
      "Iteration 31140, loss = 0.03648318\n",
      "Iteration 31141, loss = 0.03647652\n",
      "Iteration 31142, loss = 0.03647554\n",
      "Iteration 31143, loss = 0.03647884\n",
      "Iteration 31144, loss = 0.03648191\n",
      "Iteration 31145, loss = 0.03647893\n",
      "Iteration 31146, loss = 0.03646789\n",
      "Iteration 31147, loss = 0.03647428\n",
      "Iteration 31148, loss = 0.03647978\n",
      "Iteration 31149, loss = 0.03647760\n",
      "Iteration 31150, loss = 0.03648304\n",
      "Iteration 31151, loss = 0.03648235\n",
      "Iteration 31152, loss = 0.03647673\n",
      "Iteration 31153, loss = 0.03647058\n",
      "Iteration 31154, loss = 0.03647498\n",
      "Iteration 31155, loss = 0.03648192\n",
      "Iteration 31156, loss = 0.03647385\n",
      "Iteration 31157, loss = 0.03647267\n",
      "Iteration 31158, loss = 0.03647398\n",
      "Iteration 31159, loss = 0.03648031\n",
      "Iteration 31160, loss = 0.03647837\n",
      "Iteration 31161, loss = 0.03647342\n",
      "Iteration 31162, loss = 0.03647554\n",
      "Iteration 31163, loss = 0.03647387\n",
      "Iteration 31164, loss = 0.03646671\n",
      "Iteration 31165, loss = 0.03646602\n",
      "Iteration 31166, loss = 0.03646613\n",
      "Iteration 31167, loss = 0.03645912\n",
      "Iteration 31168, loss = 0.03645975\n",
      "Iteration 31169, loss = 0.03646521\n",
      "Iteration 31170, loss = 0.03646759\n",
      "Iteration 31171, loss = 0.03647218\n",
      "Iteration 31172, loss = 0.03646660\n",
      "Iteration 31173, loss = 0.03645856\n",
      "Iteration 31174, loss = 0.03645460\n",
      "Iteration 31175, loss = 0.03645665\n",
      "Iteration 31176, loss = 0.03646025\n",
      "Iteration 31177, loss = 0.03646167\n",
      "Iteration 31178, loss = 0.03645685\n",
      "Iteration 31179, loss = 0.03645894\n",
      "Iteration 31180, loss = 0.03645996\n",
      "Iteration 31181, loss = 0.03646190\n",
      "Iteration 31182, loss = 0.03646267\n",
      "Iteration 31183, loss = 0.03645873\n",
      "Iteration 31184, loss = 0.03644533\n",
      "Iteration 31185, loss = 0.03644859\n",
      "Iteration 31186, loss = 0.03645485\n",
      "Iteration 31187, loss = 0.03645082\n",
      "Iteration 31188, loss = 0.03644413\n",
      "Iteration 31189, loss = 0.03644767\n",
      "Iteration 31190, loss = 0.03645818\n",
      "Iteration 31191, loss = 0.03645926\n",
      "Iteration 31192, loss = 0.03645127\n",
      "Iteration 31193, loss = 0.03644543\n",
      "Iteration 31194, loss = 0.03644605\n",
      "Iteration 31195, loss = 0.03644930\n",
      "Iteration 31196, loss = 0.03644836\n",
      "Iteration 31197, loss = 0.03644734\n",
      "Iteration 31198, loss = 0.03644249\n",
      "Iteration 31199, loss = 0.03643643\n",
      "Iteration 31200, loss = 0.03643928\n",
      "Iteration 31201, loss = 0.03644831\n",
      "Iteration 31202, loss = 0.03644007\n",
      "Iteration 31203, loss = 0.03644131\n",
      "Iteration 31204, loss = 0.03644604\n",
      "Iteration 31205, loss = 0.03644864\n",
      "Iteration 31206, loss = 0.03644334\n",
      "Iteration 31207, loss = 0.03644054\n",
      "Iteration 31208, loss = 0.03643503\n",
      "Iteration 31209, loss = 0.03642893\n",
      "Iteration 31210, loss = 0.03643234\n",
      "Iteration 31211, loss = 0.03643339\n",
      "Iteration 31212, loss = 0.03643179\n",
      "Iteration 31213, loss = 0.03643168\n",
      "Iteration 31214, loss = 0.03643234\n",
      "Iteration 31215, loss = 0.03643649\n",
      "Iteration 31216, loss = 0.03643673\n",
      "Iteration 31217, loss = 0.03643289\n",
      "Iteration 31218, loss = 0.03643186\n",
      "Iteration 31219, loss = 0.03642595\n",
      "Iteration 31220, loss = 0.03642538\n",
      "Iteration 31221, loss = 0.03642594\n",
      "Iteration 31222, loss = 0.03643127\n",
      "Iteration 31223, loss = 0.03642962\n",
      "Iteration 31224, loss = 0.03642176\n",
      "Iteration 31225, loss = 0.03642350\n",
      "Iteration 31226, loss = 0.03642619\n",
      "Iteration 31227, loss = 0.03642562\n",
      "Iteration 31228, loss = 0.03642662\n",
      "Iteration 31229, loss = 0.03642406\n",
      "Iteration 31230, loss = 0.03642269\n",
      "Iteration 31231, loss = 0.03642155\n",
      "Iteration 31232, loss = 0.03641650\n",
      "Iteration 31233, loss = 0.03641051\n",
      "Iteration 31234, loss = 0.03641577\n",
      "Iteration 31235, loss = 0.03641714\n",
      "Iteration 31236, loss = 0.03641417\n",
      "Iteration 31237, loss = 0.03641103\n",
      "Iteration 31238, loss = 0.03640829\n",
      "Iteration 31239, loss = 0.03641290\n",
      "Iteration 31240, loss = 0.03641163\n",
      "Iteration 31241, loss = 0.03640977\n",
      "Iteration 31242, loss = 0.03640689\n",
      "Iteration 31243, loss = 0.03640771\n",
      "Iteration 31244, loss = 0.03640558\n",
      "Iteration 31245, loss = 0.03640535\n",
      "Iteration 31246, loss = 0.03640638\n",
      "Iteration 31247, loss = 0.03640566\n",
      "Iteration 31248, loss = 0.03640150\n",
      "Iteration 31249, loss = 0.03640548\n",
      "Iteration 31250, loss = 0.03640871\n",
      "Iteration 31251, loss = 0.03640024\n",
      "Iteration 31252, loss = 0.03640608\n",
      "Iteration 31253, loss = 0.03640994\n",
      "Iteration 31254, loss = 0.03640923\n",
      "Iteration 31255, loss = 0.03640807\n",
      "Iteration 31256, loss = 0.03640492\n",
      "Iteration 31257, loss = 0.03639766\n",
      "Iteration 31258, loss = 0.03640076\n",
      "Iteration 31259, loss = 0.03640919\n",
      "Iteration 31260, loss = 0.03640311\n",
      "Iteration 31261, loss = 0.03639881\n",
      "Iteration 31262, loss = 0.03640278\n",
      "Iteration 31263, loss = 0.03640673\n",
      "Iteration 31264, loss = 0.03640545\n",
      "Iteration 31265, loss = 0.03640386\n",
      "Iteration 31266, loss = 0.03640865\n",
      "Iteration 31267, loss = 0.03640632\n",
      "Iteration 31268, loss = 0.03639749\n",
      "Iteration 31269, loss = 0.03639923\n",
      "Iteration 31270, loss = 0.03640081\n",
      "Iteration 31271, loss = 0.03640167\n",
      "Iteration 31272, loss = 0.03639277\n",
      "Iteration 31273, loss = 0.03639467\n",
      "Iteration 31274, loss = 0.03640133\n",
      "Iteration 31275, loss = 0.03640714\n",
      "Iteration 31276, loss = 0.03640934\n",
      "Iteration 31277, loss = 0.03640442\n",
      "Iteration 31278, loss = 0.03640313\n",
      "Iteration 31279, loss = 0.03639751\n",
      "Iteration 31280, loss = 0.03639802\n",
      "Iteration 31281, loss = 0.03640061\n",
      "Iteration 31282, loss = 0.03640223\n",
      "Iteration 31283, loss = 0.03639105\n",
      "Iteration 31284, loss = 0.03638642\n",
      "Iteration 31285, loss = 0.03639394\n",
      "Iteration 31286, loss = 0.03639136\n",
      "Iteration 31287, loss = 0.03639009\n",
      "Iteration 31288, loss = 0.03638577\n",
      "Iteration 31289, loss = 0.03637860\n",
      "Iteration 31290, loss = 0.03638572\n",
      "Iteration 31291, loss = 0.03638943\n",
      "Iteration 31292, loss = 0.03638622\n",
      "Iteration 31293, loss = 0.03638169\n",
      "Iteration 31294, loss = 0.03638341\n",
      "Iteration 31295, loss = 0.03638444\n",
      "Iteration 31296, loss = 0.03638719\n",
      "Iteration 31297, loss = 0.03639144\n",
      "Iteration 31298, loss = 0.03638857\n",
      "Iteration 31299, loss = 0.03637766\n",
      "Iteration 31300, loss = 0.03638227\n",
      "Iteration 31301, loss = 0.03638751\n",
      "Iteration 31302, loss = 0.03637651\n",
      "Iteration 31303, loss = 0.03637406\n",
      "Iteration 31304, loss = 0.03638223\n",
      "Iteration 31305, loss = 0.03638200\n",
      "Iteration 31306, loss = 0.03637924\n",
      "Iteration 31307, loss = 0.03637156\n",
      "Iteration 31308, loss = 0.03636615\n",
      "Iteration 31309, loss = 0.03637202\n",
      "Iteration 31310, loss = 0.03636507\n",
      "Iteration 31311, loss = 0.03636978\n",
      "Iteration 31312, loss = 0.03637800\n",
      "Iteration 31313, loss = 0.03637941\n",
      "Iteration 31314, loss = 0.03637212\n",
      "Iteration 31315, loss = 0.03636505\n",
      "Iteration 31316, loss = 0.03637540\n",
      "Iteration 31317, loss = 0.03638042\n",
      "Iteration 31318, loss = 0.03636941\n",
      "Iteration 31319, loss = 0.03636936\n",
      "Iteration 31320, loss = 0.03637656\n",
      "Iteration 31321, loss = 0.03638284\n",
      "Iteration 31322, loss = 0.03638474\n",
      "Iteration 31323, loss = 0.03637803\n",
      "Iteration 31324, loss = 0.03637349\n",
      "Iteration 31325, loss = 0.03637256\n",
      "Iteration 31326, loss = 0.03637398\n",
      "Iteration 31327, loss = 0.03636778\n",
      "Iteration 31328, loss = 0.03635985\n",
      "Iteration 31329, loss = 0.03636360\n",
      "Iteration 31330, loss = 0.03635507\n",
      "Iteration 31331, loss = 0.03635374\n",
      "Iteration 31332, loss = 0.03635659\n",
      "Iteration 31333, loss = 0.03635788\n",
      "Iteration 31334, loss = 0.03635091\n",
      "Iteration 31335, loss = 0.03635165\n",
      "Iteration 31336, loss = 0.03635233\n",
      "Iteration 31337, loss = 0.03635077\n",
      "Iteration 31338, loss = 0.03635400\n",
      "Iteration 31339, loss = 0.03635424\n",
      "Iteration 31340, loss = 0.03635063\n",
      "Iteration 31341, loss = 0.03635411\n",
      "Iteration 31342, loss = 0.03635165\n",
      "Iteration 31343, loss = 0.03635076\n",
      "Iteration 31344, loss = 0.03635337\n",
      "Iteration 31345, loss = 0.03635579\n",
      "Iteration 31346, loss = 0.03635158\n",
      "Iteration 31347, loss = 0.03634534\n",
      "Iteration 31348, loss = 0.03634734\n",
      "Iteration 31349, loss = 0.03634579\n",
      "Iteration 31350, loss = 0.03634498\n",
      "Iteration 31351, loss = 0.03634694\n",
      "Iteration 31352, loss = 0.03634481\n",
      "Iteration 31353, loss = 0.03634543\n",
      "Iteration 31354, loss = 0.03634134\n",
      "Iteration 31355, loss = 0.03634378\n",
      "Iteration 31356, loss = 0.03635084\n",
      "Iteration 31357, loss = 0.03634497\n",
      "Iteration 31358, loss = 0.03634151\n",
      "Iteration 31359, loss = 0.03634414\n",
      "Iteration 31360, loss = 0.03634987\n",
      "Iteration 31361, loss = 0.03634825\n",
      "Iteration 31362, loss = 0.03634842\n",
      "Iteration 31363, loss = 0.03634606\n",
      "Iteration 31364, loss = 0.03634404\n",
      "Iteration 31365, loss = 0.03634133\n",
      "Iteration 31366, loss = 0.03633585\n",
      "Iteration 31367, loss = 0.03634052\n",
      "Iteration 31368, loss = 0.03634189\n",
      "Iteration 31369, loss = 0.03634189\n",
      "Iteration 31370, loss = 0.03634146\n",
      "Iteration 31371, loss = 0.03633777\n",
      "Iteration 31372, loss = 0.03633888\n",
      "Iteration 31373, loss = 0.03634097\n",
      "Iteration 31374, loss = 0.03633827\n",
      "Iteration 31375, loss = 0.03634292\n",
      "Iteration 31376, loss = 0.03634174\n",
      "Iteration 31377, loss = 0.03633419\n",
      "Iteration 31378, loss = 0.03633444\n",
      "Iteration 31379, loss = 0.03633291\n",
      "Iteration 31380, loss = 0.03633584\n",
      "Iteration 31381, loss = 0.03633578\n",
      "Iteration 31382, loss = 0.03632400\n",
      "Iteration 31383, loss = 0.03632394\n",
      "Iteration 31384, loss = 0.03632461\n",
      "Iteration 31385, loss = 0.03632111\n",
      "Iteration 31386, loss = 0.03632876\n",
      "Iteration 31387, loss = 0.03632720\n",
      "Iteration 31388, loss = 0.03632406\n",
      "Iteration 31389, loss = 0.03632366\n",
      "Iteration 31390, loss = 0.03632244\n",
      "Iteration 31391, loss = 0.03632092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 31392, loss = 0.03631943\n",
      "Iteration 31393, loss = 0.03632229\n",
      "Iteration 31394, loss = 0.03632750\n",
      "Iteration 31395, loss = 0.03632265\n",
      "Iteration 31396, loss = 0.03632046\n",
      "Iteration 31397, loss = 0.03632246\n",
      "Iteration 31398, loss = 0.03631780\n",
      "Iteration 31399, loss = 0.03632090\n",
      "Iteration 31400, loss = 0.03632270\n",
      "Iteration 31401, loss = 0.03632274\n",
      "Iteration 31402, loss = 0.03632152\n",
      "Iteration 31403, loss = 0.03631781\n",
      "Iteration 31404, loss = 0.03631597\n",
      "Iteration 31405, loss = 0.03632154\n",
      "Iteration 31406, loss = 0.03632082\n",
      "Iteration 31407, loss = 0.03631496\n",
      "Iteration 31408, loss = 0.03630971\n",
      "Iteration 31409, loss = 0.03631712\n",
      "Iteration 31410, loss = 0.03631451\n",
      "Iteration 31411, loss = 0.03630965\n",
      "Iteration 31412, loss = 0.03631317\n",
      "Iteration 31413, loss = 0.03631688\n",
      "Iteration 31414, loss = 0.03631750\n",
      "Iteration 31415, loss = 0.03631753\n",
      "Iteration 31416, loss = 0.03631095\n",
      "Iteration 31417, loss = 0.03631854\n",
      "Iteration 31418, loss = 0.03631666\n",
      "Iteration 31419, loss = 0.03630613\n",
      "Iteration 31420, loss = 0.03630887\n",
      "Iteration 31421, loss = 0.03631657\n",
      "Iteration 31422, loss = 0.03631265\n",
      "Iteration 31423, loss = 0.03630973\n",
      "Iteration 31424, loss = 0.03630929\n",
      "Iteration 31425, loss = 0.03631360\n",
      "Iteration 31426, loss = 0.03630774\n",
      "Iteration 31427, loss = 0.03630632\n",
      "Iteration 31428, loss = 0.03630854\n",
      "Iteration 31429, loss = 0.03630658\n",
      "Iteration 31430, loss = 0.03630105\n",
      "Iteration 31431, loss = 0.03629607\n",
      "Iteration 31432, loss = 0.03629614\n",
      "Iteration 31433, loss = 0.03629909\n",
      "Iteration 31434, loss = 0.03629835\n",
      "Iteration 31435, loss = 0.03629442\n",
      "Iteration 31436, loss = 0.03629907\n",
      "Iteration 31437, loss = 0.03629510\n",
      "Iteration 31438, loss = 0.03629513\n",
      "Iteration 31439, loss = 0.03629683\n",
      "Iteration 31440, loss = 0.03629890\n",
      "Iteration 31441, loss = 0.03630029\n",
      "Iteration 31442, loss = 0.03629584\n",
      "Iteration 31443, loss = 0.03629159\n",
      "Iteration 31444, loss = 0.03629336\n",
      "Iteration 31445, loss = 0.03629786\n",
      "Iteration 31446, loss = 0.03629434\n",
      "Iteration 31447, loss = 0.03629330\n",
      "Iteration 31448, loss = 0.03629100\n",
      "Iteration 31449, loss = 0.03629352\n",
      "Iteration 31450, loss = 0.03629143\n",
      "Iteration 31451, loss = 0.03628739\n",
      "Iteration 31452, loss = 0.03629256\n",
      "Iteration 31453, loss = 0.03629379\n",
      "Iteration 31454, loss = 0.03629021\n",
      "Iteration 31455, loss = 0.03629216\n",
      "Iteration 31456, loss = 0.03629158\n",
      "Iteration 31457, loss = 0.03628519\n",
      "Iteration 31458, loss = 0.03628482\n",
      "Iteration 31459, loss = 0.03628472\n",
      "Iteration 31460, loss = 0.03627770\n",
      "Iteration 31461, loss = 0.03627830\n",
      "Iteration 31462, loss = 0.03628144\n",
      "Iteration 31463, loss = 0.03627762\n",
      "Iteration 31464, loss = 0.03627600\n",
      "Iteration 31465, loss = 0.03627482\n",
      "Iteration 31466, loss = 0.03627206\n",
      "Iteration 31467, loss = 0.03628235\n",
      "Iteration 31468, loss = 0.03628312\n",
      "Iteration 31469, loss = 0.03627829\n",
      "Iteration 31470, loss = 0.03627128\n",
      "Iteration 31471, loss = 0.03628451\n",
      "Iteration 31472, loss = 0.03628002\n",
      "Iteration 31473, loss = 0.03627085\n",
      "Iteration 31474, loss = 0.03628217\n",
      "Iteration 31475, loss = 0.03628872\n",
      "Iteration 31476, loss = 0.03628681\n",
      "Iteration 31477, loss = 0.03628755\n",
      "Iteration 31478, loss = 0.03628106\n",
      "Iteration 31479, loss = 0.03627082\n",
      "Iteration 31480, loss = 0.03627999\n",
      "Iteration 31481, loss = 0.03628268\n",
      "Iteration 31482, loss = 0.03627705\n",
      "Iteration 31483, loss = 0.03627122\n",
      "Iteration 31484, loss = 0.03627705\n",
      "Iteration 31485, loss = 0.03627735\n",
      "Iteration 31486, loss = 0.03627137\n",
      "Iteration 31487, loss = 0.03627026\n",
      "Iteration 31488, loss = 0.03627141\n",
      "Iteration 31489, loss = 0.03626210\n",
      "Iteration 31490, loss = 0.03626369\n",
      "Iteration 31491, loss = 0.03626194\n",
      "Iteration 31492, loss = 0.03626129\n",
      "Iteration 31493, loss = 0.03626181\n",
      "Iteration 31494, loss = 0.03626011\n",
      "Iteration 31495, loss = 0.03626253\n",
      "Iteration 31496, loss = 0.03626875\n",
      "Iteration 31497, loss = 0.03626723\n",
      "Iteration 31498, loss = 0.03626267\n",
      "Iteration 31499, loss = 0.03625809\n",
      "Iteration 31500, loss = 0.03627311\n",
      "Iteration 31501, loss = 0.03627518\n",
      "Iteration 31502, loss = 0.03626572\n",
      "Iteration 31503, loss = 0.03626491\n",
      "Iteration 31504, loss = 0.03626122\n",
      "Iteration 31505, loss = 0.03626447\n",
      "Iteration 31506, loss = 0.03626733\n",
      "Iteration 31507, loss = 0.03627637\n",
      "Iteration 31508, loss = 0.03627680\n",
      "Iteration 31509, loss = 0.03626953\n",
      "Iteration 31510, loss = 0.03626376\n",
      "Iteration 31511, loss = 0.03625716\n",
      "Iteration 31512, loss = 0.03626123\n",
      "Iteration 31513, loss = 0.03626157\n",
      "Iteration 31514, loss = 0.03625627\n",
      "Iteration 31515, loss = 0.03625373\n",
      "Iteration 31516, loss = 0.03625752\n",
      "Iteration 31517, loss = 0.03625336\n",
      "Iteration 31518, loss = 0.03625227\n",
      "Iteration 31519, loss = 0.03625264\n",
      "Iteration 31520, loss = 0.03625071\n",
      "Iteration 31521, loss = 0.03624330\n",
      "Iteration 31522, loss = 0.03625149\n",
      "Iteration 31523, loss = 0.03625406\n",
      "Iteration 31524, loss = 0.03625007\n",
      "Iteration 31525, loss = 0.03624307\n",
      "Iteration 31526, loss = 0.03625259\n",
      "Iteration 31527, loss = 0.03625368\n",
      "Iteration 31528, loss = 0.03625406\n",
      "Iteration 31529, loss = 0.03624815\n",
      "Iteration 31530, loss = 0.03625553\n",
      "Iteration 31531, loss = 0.03625312\n",
      "Iteration 31532, loss = 0.03625618\n",
      "Iteration 31533, loss = 0.03625680\n",
      "Iteration 31534, loss = 0.03624840\n",
      "Iteration 31535, loss = 0.03624923\n",
      "Iteration 31536, loss = 0.03625052\n",
      "Iteration 31537, loss = 0.03624287\n",
      "Iteration 31538, loss = 0.03623462\n",
      "Iteration 31539, loss = 0.03623741\n",
      "Iteration 31540, loss = 0.03623318\n",
      "Iteration 31541, loss = 0.03623292\n",
      "Iteration 31542, loss = 0.03623194\n",
      "Iteration 31543, loss = 0.03623266\n",
      "Iteration 31544, loss = 0.03623332\n",
      "Iteration 31545, loss = 0.03624181\n",
      "Iteration 31546, loss = 0.03623671\n",
      "Iteration 31547, loss = 0.03623536\n",
      "Iteration 31548, loss = 0.03623899\n",
      "Iteration 31549, loss = 0.03623660\n",
      "Iteration 31550, loss = 0.03623451\n",
      "Iteration 31551, loss = 0.03623880\n",
      "Iteration 31552, loss = 0.03623562\n",
      "Iteration 31553, loss = 0.03622432\n",
      "Iteration 31554, loss = 0.03623443\n",
      "Iteration 31555, loss = 0.03623879\n",
      "Iteration 31556, loss = 0.03623637\n",
      "Iteration 31557, loss = 0.03623867\n",
      "Iteration 31558, loss = 0.03623735\n",
      "Iteration 31559, loss = 0.03623214\n",
      "Iteration 31560, loss = 0.03623944\n",
      "Iteration 31561, loss = 0.03624384\n",
      "Iteration 31562, loss = 0.03622895\n",
      "Iteration 31563, loss = 0.03622749\n",
      "Iteration 31564, loss = 0.03623458\n",
      "Iteration 31565, loss = 0.03623321\n",
      "Iteration 31566, loss = 0.03622439\n",
      "Iteration 31567, loss = 0.03622378\n",
      "Iteration 31568, loss = 0.03623323\n",
      "Iteration 31569, loss = 0.03623716\n",
      "Iteration 31570, loss = 0.03623084\n",
      "Iteration 31571, loss = 0.03622793\n",
      "Iteration 31572, loss = 0.03623117\n",
      "Iteration 31573, loss = 0.03623003\n",
      "Iteration 31574, loss = 0.03623268\n",
      "Iteration 31575, loss = 0.03623178\n",
      "Iteration 31576, loss = 0.03622729\n",
      "Iteration 31577, loss = 0.03621912\n",
      "Iteration 31578, loss = 0.03622404\n",
      "Iteration 31579, loss = 0.03622191\n",
      "Iteration 31580, loss = 0.03622091\n",
      "Iteration 31581, loss = 0.03622207\n",
      "Iteration 31582, loss = 0.03621652\n",
      "Iteration 31583, loss = 0.03621917\n",
      "Iteration 31584, loss = 0.03621661\n",
      "Iteration 31585, loss = 0.03622016\n",
      "Iteration 31586, loss = 0.03622793\n",
      "Iteration 31587, loss = 0.03622892\n",
      "Iteration 31588, loss = 0.03622146\n",
      "Iteration 31589, loss = 0.03621179\n",
      "Iteration 31590, loss = 0.03621327\n",
      "Iteration 31591, loss = 0.03621728\n",
      "Iteration 31592, loss = 0.03621630\n",
      "Iteration 31593, loss = 0.03620674\n",
      "Iteration 31594, loss = 0.03620487\n",
      "Iteration 31595, loss = 0.03620959\n",
      "Iteration 31596, loss = 0.03621329\n",
      "Iteration 31597, loss = 0.03620882\n",
      "Iteration 31598, loss = 0.03620743\n",
      "Iteration 31599, loss = 0.03620783\n",
      "Iteration 31600, loss = 0.03620894\n",
      "Iteration 31601, loss = 0.03620702\n",
      "Iteration 31602, loss = 0.03620649\n",
      "Iteration 31603, loss = 0.03620269\n",
      "Iteration 31604, loss = 0.03619713\n",
      "Iteration 31605, loss = 0.03620427\n",
      "Iteration 31606, loss = 0.03620387\n",
      "Iteration 31607, loss = 0.03620017\n",
      "Iteration 31608, loss = 0.03620025\n",
      "Iteration 31609, loss = 0.03620392\n",
      "Iteration 31610, loss = 0.03620465\n",
      "Iteration 31611, loss = 0.03620023\n",
      "Iteration 31612, loss = 0.03619462\n",
      "Iteration 31613, loss = 0.03620311\n",
      "Iteration 31614, loss = 0.03620562\n",
      "Iteration 31615, loss = 0.03619876\n",
      "Iteration 31616, loss = 0.03619671\n",
      "Iteration 31617, loss = 0.03619664\n",
      "Iteration 31618, loss = 0.03619542\n",
      "Iteration 31619, loss = 0.03619522\n",
      "Iteration 31620, loss = 0.03618746\n",
      "Iteration 31621, loss = 0.03618973\n",
      "Iteration 31622, loss = 0.03619520\n",
      "Iteration 31623, loss = 0.03619370\n",
      "Iteration 31624, loss = 0.03619043\n",
      "Iteration 31625, loss = 0.03619339\n",
      "Iteration 31626, loss = 0.03618990\n",
      "Iteration 31627, loss = 0.03619102\n",
      "Iteration 31628, loss = 0.03619250\n",
      "Iteration 31629, loss = 0.03618737\n",
      "Iteration 31630, loss = 0.03618325\n",
      "Iteration 31631, loss = 0.03619107\n",
      "Iteration 31632, loss = 0.03619104\n",
      "Iteration 31633, loss = 0.03618335\n",
      "Iteration 31634, loss = 0.03618865\n",
      "Iteration 31635, loss = 0.03618923\n",
      "Iteration 31636, loss = 0.03618635\n",
      "Iteration 31637, loss = 0.03618959\n",
      "Iteration 31638, loss = 0.03619156\n",
      "Iteration 31639, loss = 0.03618704\n",
      "Iteration 31640, loss = 0.03618458\n",
      "Iteration 31641, loss = 0.03618538\n",
      "Iteration 31642, loss = 0.03618334\n",
      "Iteration 31643, loss = 0.03618615\n",
      "Iteration 31644, loss = 0.03618841\n",
      "Iteration 31645, loss = 0.03618350\n",
      "Iteration 31646, loss = 0.03617450\n",
      "Iteration 31647, loss = 0.03618249\n",
      "Iteration 31648, loss = 0.03618224\n",
      "Iteration 31649, loss = 0.03617617\n",
      "Iteration 31650, loss = 0.03617505\n",
      "Iteration 31651, loss = 0.03617924\n",
      "Iteration 31652, loss = 0.03617558\n",
      "Iteration 31653, loss = 0.03617767\n",
      "Iteration 31654, loss = 0.03617981\n",
      "Iteration 31655, loss = 0.03618076\n",
      "Iteration 31656, loss = 0.03617909\n",
      "Iteration 31657, loss = 0.03617459\n",
      "Iteration 31658, loss = 0.03616759\n",
      "Iteration 31659, loss = 0.03617754\n",
      "Iteration 31660, loss = 0.03618483\n",
      "Iteration 31661, loss = 0.03618203\n",
      "Iteration 31662, loss = 0.03617174\n",
      "Iteration 31663, loss = 0.03617152\n",
      "Iteration 31664, loss = 0.03617116\n",
      "Iteration 31665, loss = 0.03616565\n",
      "Iteration 31666, loss = 0.03617308\n",
      "Iteration 31667, loss = 0.03617445\n",
      "Iteration 31668, loss = 0.03617176\n",
      "Iteration 31669, loss = 0.03616820\n",
      "Iteration 31670, loss = 0.03617430\n",
      "Iteration 31671, loss = 0.03617927\n",
      "Iteration 31672, loss = 0.03617584\n",
      "Iteration 31673, loss = 0.03617096\n",
      "Iteration 31674, loss = 0.03616927\n",
      "Iteration 31675, loss = 0.03617665\n",
      "Iteration 31676, loss = 0.03617798\n",
      "Iteration 31677, loss = 0.03617707\n",
      "Iteration 31678, loss = 0.03616716\n",
      "Iteration 31679, loss = 0.03616652\n",
      "Iteration 31680, loss = 0.03617034\n",
      "Iteration 31681, loss = 0.03617229\n",
      "Iteration 31682, loss = 0.03617050\n",
      "Iteration 31683, loss = 0.03617179\n",
      "Iteration 31684, loss = 0.03616852\n",
      "Iteration 31685, loss = 0.03616658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 31686, loss = 0.03617313\n",
      "Iteration 31687, loss = 0.03616987\n",
      "Iteration 31688, loss = 0.03616925\n",
      "Iteration 31689, loss = 0.03616154\n",
      "Iteration 31690, loss = 0.03616396\n",
      "Iteration 31691, loss = 0.03616031\n",
      "Iteration 31692, loss = 0.03616024\n",
      "Iteration 31693, loss = 0.03617324\n",
      "Iteration 31694, loss = 0.03617068\n",
      "Iteration 31695, loss = 0.03615754\n",
      "Iteration 31696, loss = 0.03615221\n",
      "Iteration 31697, loss = 0.03616365\n",
      "Iteration 31698, loss = 0.03617277\n",
      "Iteration 31699, loss = 0.03616797\n",
      "Iteration 31700, loss = 0.03615286\n",
      "Iteration 31701, loss = 0.03615108\n",
      "Iteration 31702, loss = 0.03616345\n",
      "Iteration 31703, loss = 0.03616130\n",
      "Iteration 31704, loss = 0.03615869\n",
      "Iteration 31705, loss = 0.03615256\n",
      "Iteration 31706, loss = 0.03615463\n",
      "Iteration 31707, loss = 0.03616218\n",
      "Iteration 31708, loss = 0.03616918\n",
      "Iteration 31709, loss = 0.03616586\n",
      "Iteration 31710, loss = 0.03615245\n",
      "Iteration 31711, loss = 0.03614593\n",
      "Iteration 31712, loss = 0.03615179\n",
      "Iteration 31713, loss = 0.03616727\n",
      "Iteration 31714, loss = 0.03616507\n",
      "Iteration 31715, loss = 0.03615565\n",
      "Iteration 31716, loss = 0.03615320\n",
      "Iteration 31717, loss = 0.03615122\n",
      "Iteration 31718, loss = 0.03616312\n",
      "Iteration 31719, loss = 0.03616531\n",
      "Iteration 31720, loss = 0.03615449\n",
      "Iteration 31721, loss = 0.03613916\n",
      "Iteration 31722, loss = 0.03614401\n",
      "Iteration 31723, loss = 0.03614695\n",
      "Iteration 31724, loss = 0.03614532\n",
      "Iteration 31725, loss = 0.03614468\n",
      "Iteration 31726, loss = 0.03614180\n",
      "Iteration 31727, loss = 0.03614097\n",
      "Iteration 31728, loss = 0.03613601\n",
      "Iteration 31729, loss = 0.03612834\n",
      "Iteration 31730, loss = 0.03613259\n",
      "Iteration 31731, loss = 0.03613615\n",
      "Iteration 31732, loss = 0.03613022\n",
      "Iteration 31733, loss = 0.03612868\n",
      "Iteration 31734, loss = 0.03612819\n",
      "Iteration 31735, loss = 0.03612542\n",
      "Iteration 31736, loss = 0.03612683\n",
      "Iteration 31737, loss = 0.03612824\n",
      "Iteration 31738, loss = 0.03612460\n",
      "Iteration 31739, loss = 0.03612239\n",
      "Iteration 31740, loss = 0.03612572\n",
      "Iteration 31741, loss = 0.03612156\n",
      "Iteration 31742, loss = 0.03612487\n",
      "Iteration 31743, loss = 0.03612583\n",
      "Iteration 31744, loss = 0.03612650\n",
      "Iteration 31745, loss = 0.03612516\n",
      "Iteration 31746, loss = 0.03612755\n",
      "Iteration 31747, loss = 0.03613178\n",
      "Iteration 31748, loss = 0.03612247\n",
      "Iteration 31749, loss = 0.03612374\n",
      "Iteration 31750, loss = 0.03612879\n",
      "Iteration 31751, loss = 0.03613197\n",
      "Iteration 31752, loss = 0.03612338\n",
      "Iteration 31753, loss = 0.03612313\n",
      "Iteration 31754, loss = 0.03612236\n",
      "Iteration 31755, loss = 0.03612706\n",
      "Iteration 31756, loss = 0.03613353\n",
      "Iteration 31757, loss = 0.03612139\n",
      "Iteration 31758, loss = 0.03612282\n",
      "Iteration 31759, loss = 0.03612509\n",
      "Iteration 31760, loss = 0.03613569\n",
      "Iteration 31761, loss = 0.03613473\n",
      "Iteration 31762, loss = 0.03612277\n",
      "Iteration 31763, loss = 0.03611287\n",
      "Iteration 31764, loss = 0.03612046\n",
      "Iteration 31765, loss = 0.03612177\n",
      "Iteration 31766, loss = 0.03611418\n",
      "Iteration 31767, loss = 0.03611906\n",
      "Iteration 31768, loss = 0.03611224\n",
      "Iteration 31769, loss = 0.03611596\n",
      "Iteration 31770, loss = 0.03611994\n",
      "Iteration 31771, loss = 0.03611255\n",
      "Iteration 31772, loss = 0.03610927\n",
      "Iteration 31773, loss = 0.03611593\n",
      "Iteration 31774, loss = 0.03612104\n",
      "Iteration 31775, loss = 0.03611700\n",
      "Iteration 31776, loss = 0.03611372\n",
      "Iteration 31777, loss = 0.03611261\n",
      "Iteration 31778, loss = 0.03610689\n",
      "Iteration 31779, loss = 0.03611029\n",
      "Iteration 31780, loss = 0.03611171\n",
      "Iteration 31781, loss = 0.03610916\n",
      "Iteration 31782, loss = 0.03610292\n",
      "Iteration 31783, loss = 0.03609907\n",
      "Iteration 31784, loss = 0.03610531\n",
      "Iteration 31785, loss = 0.03610186\n",
      "Iteration 31786, loss = 0.03610023\n",
      "Iteration 31787, loss = 0.03610246\n",
      "Iteration 31788, loss = 0.03610419\n",
      "Iteration 31789, loss = 0.03610254\n",
      "Iteration 31790, loss = 0.03610340\n",
      "Iteration 31791, loss = 0.03609554\n",
      "Iteration 31792, loss = 0.03609308\n",
      "Iteration 31793, loss = 0.03610038\n",
      "Iteration 31794, loss = 0.03609331\n",
      "Iteration 31795, loss = 0.03609400\n",
      "Iteration 31796, loss = 0.03609636\n",
      "Iteration 31797, loss = 0.03609555\n",
      "Iteration 31798, loss = 0.03609263\n",
      "Iteration 31799, loss = 0.03609175\n",
      "Iteration 31800, loss = 0.03609615\n",
      "Iteration 31801, loss = 0.03609543\n",
      "Iteration 31802, loss = 0.03609159\n",
      "Iteration 31803, loss = 0.03609303\n",
      "Iteration 31804, loss = 0.03609185\n",
      "Iteration 31805, loss = 0.03609552\n",
      "Iteration 31806, loss = 0.03609949\n",
      "Iteration 31807, loss = 0.03609144\n",
      "Iteration 31808, loss = 0.03608927\n",
      "Iteration 31809, loss = 0.03609708\n",
      "Iteration 31810, loss = 0.03609704\n",
      "Iteration 31811, loss = 0.03609754\n",
      "Iteration 31812, loss = 0.03609736\n",
      "Iteration 31813, loss = 0.03608991\n",
      "Iteration 31814, loss = 0.03609834\n",
      "Iteration 31815, loss = 0.03610223\n",
      "Iteration 31816, loss = 0.03609284\n",
      "Iteration 31817, loss = 0.03607678\n",
      "Iteration 31818, loss = 0.03609581\n",
      "Iteration 31819, loss = 0.03610120\n",
      "Iteration 31820, loss = 0.03608854\n",
      "Iteration 31821, loss = 0.03607770\n",
      "Iteration 31822, loss = 0.03608215\n",
      "Iteration 31823, loss = 0.03608633\n",
      "Iteration 31824, loss = 0.03608732\n",
      "Iteration 31825, loss = 0.03608861\n",
      "Iteration 31826, loss = 0.03609290\n",
      "Iteration 31827, loss = 0.03609009\n",
      "Iteration 31828, loss = 0.03608256\n",
      "Iteration 31829, loss = 0.03608046\n",
      "Iteration 31830, loss = 0.03607867\n",
      "Iteration 31831, loss = 0.03607348\n",
      "Iteration 31832, loss = 0.03608249\n",
      "Iteration 31833, loss = 0.03608613\n",
      "Iteration 31834, loss = 0.03608247\n",
      "Iteration 31835, loss = 0.03607422\n",
      "Iteration 31836, loss = 0.03607292\n",
      "Iteration 31837, loss = 0.03607440\n",
      "Iteration 31838, loss = 0.03607603\n",
      "Iteration 31839, loss = 0.03606916\n",
      "Iteration 31840, loss = 0.03607168\n",
      "Iteration 31841, loss = 0.03607403\n",
      "Iteration 31842, loss = 0.03607207\n",
      "Iteration 31843, loss = 0.03606741\n",
      "Iteration 31844, loss = 0.03607223\n",
      "Iteration 31845, loss = 0.03607122\n",
      "Iteration 31846, loss = 0.03606148\n",
      "Iteration 31847, loss = 0.03606779\n",
      "Iteration 31848, loss = 0.03606873\n",
      "Iteration 31849, loss = 0.03607046\n",
      "Iteration 31850, loss = 0.03606878\n",
      "Iteration 31851, loss = 0.03607042\n",
      "Iteration 31852, loss = 0.03606726\n",
      "Iteration 31853, loss = 0.03607318\n",
      "Iteration 31854, loss = 0.03607629\n",
      "Iteration 31855, loss = 0.03606706\n",
      "Iteration 31856, loss = 0.03606465\n",
      "Iteration 31857, loss = 0.03606585\n",
      "Iteration 31858, loss = 0.03607010\n",
      "Iteration 31859, loss = 0.03606537\n",
      "Iteration 31860, loss = 0.03606627\n",
      "Iteration 31861, loss = 0.03605882\n",
      "Iteration 31862, loss = 0.03606961\n",
      "Iteration 31863, loss = 0.03607382\n",
      "Iteration 31864, loss = 0.03606589\n",
      "Iteration 31865, loss = 0.03605919\n",
      "Iteration 31866, loss = 0.03605900\n",
      "Iteration 31867, loss = 0.03606008\n",
      "Iteration 31868, loss = 0.03605830\n",
      "Iteration 31869, loss = 0.03606180\n",
      "Iteration 31870, loss = 0.03606452\n",
      "Iteration 31871, loss = 0.03605714\n",
      "Iteration 31872, loss = 0.03605587\n",
      "Iteration 31873, loss = 0.03605759\n",
      "Iteration 31874, loss = 0.03605581\n",
      "Iteration 31875, loss = 0.03605640\n",
      "Iteration 31876, loss = 0.03605413\n",
      "Iteration 31877, loss = 0.03605084\n",
      "Iteration 31878, loss = 0.03604814\n",
      "Iteration 31879, loss = 0.03604652\n",
      "Iteration 31880, loss = 0.03604827\n",
      "Iteration 31881, loss = 0.03605279\n",
      "Iteration 31882, loss = 0.03605399\n",
      "Iteration 31883, loss = 0.03604998\n",
      "Iteration 31884, loss = 0.03604382\n",
      "Iteration 31885, loss = 0.03604960\n",
      "Iteration 31886, loss = 0.03605214\n",
      "Iteration 31887, loss = 0.03604151\n",
      "Iteration 31888, loss = 0.03604774\n",
      "Iteration 31889, loss = 0.03605109\n",
      "Iteration 31890, loss = 0.03605182\n",
      "Iteration 31891, loss = 0.03605063\n",
      "Iteration 31892, loss = 0.03604609\n",
      "Iteration 31893, loss = 0.03604463\n",
      "Iteration 31894, loss = 0.03604656\n",
      "Iteration 31895, loss = 0.03604693\n",
      "Iteration 31896, loss = 0.03604911\n",
      "Iteration 31897, loss = 0.03604357\n",
      "Iteration 31898, loss = 0.03604649\n",
      "Iteration 31899, loss = 0.03604181\n",
      "Iteration 31900, loss = 0.03603716\n",
      "Iteration 31901, loss = 0.03603694\n",
      "Iteration 31902, loss = 0.03603748\n",
      "Iteration 31903, loss = 0.03603872\n",
      "Iteration 31904, loss = 0.03604150\n",
      "Iteration 31905, loss = 0.03603831\n",
      "Iteration 31906, loss = 0.03603743\n",
      "Iteration 31907, loss = 0.03603496\n",
      "Iteration 31908, loss = 0.03602853\n",
      "Iteration 31909, loss = 0.03603502\n",
      "Iteration 31910, loss = 0.03603290\n",
      "Iteration 31911, loss = 0.03602535\n",
      "Iteration 31912, loss = 0.03602839\n",
      "Iteration 31913, loss = 0.03602955\n",
      "Iteration 31914, loss = 0.03603052\n",
      "Iteration 31915, loss = 0.03602760\n",
      "Iteration 31916, loss = 0.03603035\n",
      "Iteration 31917, loss = 0.03603038\n",
      "Iteration 31918, loss = 0.03603066\n",
      "Iteration 31919, loss = 0.03602905\n",
      "Iteration 31920, loss = 0.03602931\n",
      "Iteration 31921, loss = 0.03602853\n",
      "Iteration 31922, loss = 0.03602682\n",
      "Iteration 31923, loss = 0.03602623\n",
      "Iteration 31924, loss = 0.03602981\n",
      "Iteration 31925, loss = 0.03603058\n",
      "Iteration 31926, loss = 0.03602510\n",
      "Iteration 31927, loss = 0.03602205\n",
      "Iteration 31928, loss = 0.03602842\n",
      "Iteration 31929, loss = 0.03602613\n",
      "Iteration 31930, loss = 0.03601972\n",
      "Iteration 31931, loss = 0.03602182\n",
      "Iteration 31932, loss = 0.03602853\n",
      "Iteration 31933, loss = 0.03602311\n",
      "Iteration 31934, loss = 0.03602086\n",
      "Iteration 31935, loss = 0.03602427\n",
      "Iteration 31936, loss = 0.03601710\n",
      "Iteration 31937, loss = 0.03601607\n",
      "Iteration 31938, loss = 0.03601466\n",
      "Iteration 31939, loss = 0.03601273\n",
      "Iteration 31940, loss = 0.03601347\n",
      "Iteration 31941, loss = 0.03601138\n",
      "Iteration 31942, loss = 0.03601259\n",
      "Iteration 31943, loss = 0.03601655\n",
      "Iteration 31944, loss = 0.03601764\n",
      "Iteration 31945, loss = 0.03601614\n",
      "Iteration 31946, loss = 0.03601408\n",
      "Iteration 31947, loss = 0.03601172\n",
      "Iteration 31948, loss = 0.03600842\n",
      "Iteration 31949, loss = 0.03600652\n",
      "Iteration 31950, loss = 0.03600880\n",
      "Iteration 31951, loss = 0.03601079\n",
      "Iteration 31952, loss = 0.03600900\n",
      "Iteration 31953, loss = 0.03601086\n",
      "Iteration 31954, loss = 0.03600994\n",
      "Iteration 31955, loss = 0.03601079\n",
      "Iteration 31956, loss = 0.03600886\n",
      "Iteration 31957, loss = 0.03600473\n",
      "Iteration 31958, loss = 0.03600527\n",
      "Iteration 31959, loss = 0.03600626\n",
      "Iteration 31960, loss = 0.03600794\n",
      "Iteration 31961, loss = 0.03600402\n",
      "Iteration 31962, loss = 0.03600458\n",
      "Iteration 31963, loss = 0.03600900\n",
      "Iteration 31964, loss = 0.03600394\n",
      "Iteration 31965, loss = 0.03599990\n",
      "Iteration 31966, loss = 0.03600026\n",
      "Iteration 31967, loss = 0.03600296\n",
      "Iteration 31968, loss = 0.03599901\n",
      "Iteration 31969, loss = 0.03599900\n",
      "Iteration 31970, loss = 0.03600090\n",
      "Iteration 31971, loss = 0.03599695\n",
      "Iteration 31972, loss = 0.03599434\n",
      "Iteration 31973, loss = 0.03599743\n",
      "Iteration 31974, loss = 0.03600015\n",
      "Iteration 31975, loss = 0.03600006\n",
      "Iteration 31976, loss = 0.03599457\n",
      "Iteration 31977, loss = 0.03599568\n",
      "Iteration 31978, loss = 0.03599371\n",
      "Iteration 31979, loss = 0.03599456\n",
      "Iteration 31980, loss = 0.03599584\n",
      "Iteration 31981, loss = 0.03599736\n",
      "Iteration 31982, loss = 0.03599473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 31983, loss = 0.03599865\n",
      "Iteration 31984, loss = 0.03599341\n",
      "Iteration 31985, loss = 0.03598681\n",
      "Iteration 31986, loss = 0.03599420\n",
      "Iteration 31987, loss = 0.03599433\n",
      "Iteration 31988, loss = 0.03598803\n",
      "Iteration 31989, loss = 0.03599127\n",
      "Iteration 31990, loss = 0.03599311\n",
      "Iteration 31991, loss = 0.03599146\n",
      "Iteration 31992, loss = 0.03598641\n",
      "Iteration 31993, loss = 0.03598441\n",
      "Iteration 31994, loss = 0.03598523\n",
      "Iteration 31995, loss = 0.03598169\n",
      "Iteration 31996, loss = 0.03598495\n",
      "Iteration 31997, loss = 0.03598447\n",
      "Iteration 31998, loss = 0.03598619\n",
      "Iteration 31999, loss = 0.03598753\n",
      "Iteration 32000, loss = 0.03598508\n",
      "Iteration 32001, loss = 0.03598291\n",
      "Iteration 32002, loss = 0.03598188\n",
      "Iteration 32003, loss = 0.03598044\n",
      "Iteration 32004, loss = 0.03597738\n",
      "Iteration 32005, loss = 0.03598083\n",
      "Iteration 32006, loss = 0.03597744\n",
      "Iteration 32007, loss = 0.03597775\n",
      "Iteration 32008, loss = 0.03597758\n",
      "Iteration 32009, loss = 0.03597561\n",
      "Iteration 32010, loss = 0.03597928\n",
      "Iteration 32011, loss = 0.03597600\n",
      "Iteration 32012, loss = 0.03597687\n",
      "Iteration 32013, loss = 0.03597465\n",
      "Iteration 32014, loss = 0.03597451\n",
      "Iteration 32015, loss = 0.03597329\n",
      "Iteration 32016, loss = 0.03597645\n",
      "Iteration 32017, loss = 0.03597248\n",
      "Iteration 32018, loss = 0.03597694\n",
      "Iteration 32019, loss = 0.03597475\n",
      "Iteration 32020, loss = 0.03596837\n",
      "Iteration 32021, loss = 0.03597231\n",
      "Iteration 32022, loss = 0.03597697\n",
      "Iteration 32023, loss = 0.03597976\n",
      "Iteration 32024, loss = 0.03597821\n",
      "Iteration 32025, loss = 0.03597517\n",
      "Iteration 32026, loss = 0.03597444\n",
      "Iteration 32027, loss = 0.03597122\n",
      "Iteration 32028, loss = 0.03597402\n",
      "Iteration 32029, loss = 0.03597771\n",
      "Iteration 32030, loss = 0.03597312\n",
      "Iteration 32031, loss = 0.03596612\n",
      "Iteration 32032, loss = 0.03596737\n",
      "Iteration 32033, loss = 0.03597312\n",
      "Iteration 32034, loss = 0.03597094\n",
      "Iteration 32035, loss = 0.03596820\n",
      "Iteration 32036, loss = 0.03596929\n",
      "Iteration 32037, loss = 0.03596111\n",
      "Iteration 32038, loss = 0.03596473\n",
      "Iteration 32039, loss = 0.03596506\n",
      "Iteration 32040, loss = 0.03596513\n",
      "Iteration 32041, loss = 0.03596073\n",
      "Iteration 32042, loss = 0.03596270\n",
      "Iteration 32043, loss = 0.03595856\n",
      "Iteration 32044, loss = 0.03595665\n",
      "Iteration 32045, loss = 0.03595708\n",
      "Iteration 32046, loss = 0.03595622\n",
      "Iteration 32047, loss = 0.03596142\n",
      "Iteration 32048, loss = 0.03596154\n",
      "Iteration 32049, loss = 0.03595480\n",
      "Iteration 32050, loss = 0.03595538\n",
      "Iteration 32051, loss = 0.03595502\n",
      "Iteration 32052, loss = 0.03595144\n",
      "Iteration 32053, loss = 0.03595138\n",
      "Iteration 32054, loss = 0.03595450\n",
      "Iteration 32055, loss = 0.03595131\n",
      "Iteration 32056, loss = 0.03594967\n",
      "Iteration 32057, loss = 0.03595353\n",
      "Iteration 32058, loss = 0.03595312\n",
      "Iteration 32059, loss = 0.03595300\n",
      "Iteration 32060, loss = 0.03594989\n",
      "Iteration 32061, loss = 0.03594915\n",
      "Iteration 32062, loss = 0.03595194\n",
      "Iteration 32063, loss = 0.03595241\n",
      "Iteration 32064, loss = 0.03595438\n",
      "Iteration 32065, loss = 0.03594720\n",
      "Iteration 32066, loss = 0.03595028\n",
      "Iteration 32067, loss = 0.03594943\n",
      "Iteration 32068, loss = 0.03595027\n",
      "Iteration 32069, loss = 0.03594635\n",
      "Iteration 32070, loss = 0.03594496\n",
      "Iteration 32071, loss = 0.03594198\n",
      "Iteration 32072, loss = 0.03594352\n",
      "Iteration 32073, loss = 0.03594550\n",
      "Iteration 32074, loss = 0.03594115\n",
      "Iteration 32075, loss = 0.03594702\n",
      "Iteration 32076, loss = 0.03594612\n",
      "Iteration 32077, loss = 0.03594034\n",
      "Iteration 32078, loss = 0.03594099\n",
      "Iteration 32079, loss = 0.03593783\n",
      "Iteration 32080, loss = 0.03594142\n",
      "Iteration 32081, loss = 0.03594057\n",
      "Iteration 32082, loss = 0.03593911\n",
      "Iteration 32083, loss = 0.03594005\n",
      "Iteration 32084, loss = 0.03594218\n",
      "Iteration 32085, loss = 0.03594254\n",
      "Iteration 32086, loss = 0.03594356\n",
      "Iteration 32087, loss = 0.03593780\n",
      "Iteration 32088, loss = 0.03593528\n",
      "Iteration 32089, loss = 0.03594148\n",
      "Iteration 32090, loss = 0.03594048\n",
      "Iteration 32091, loss = 0.03594140\n",
      "Iteration 32092, loss = 0.03594036\n",
      "Iteration 32093, loss = 0.03593718\n",
      "Iteration 32094, loss = 0.03593997\n",
      "Iteration 32095, loss = 0.03594429\n",
      "Iteration 32096, loss = 0.03594276\n",
      "Iteration 32097, loss = 0.03593987\n",
      "Iteration 32098, loss = 0.03593894\n",
      "Iteration 32099, loss = 0.03592918\n",
      "Iteration 32100, loss = 0.03594064\n",
      "Iteration 32101, loss = 0.03594928\n",
      "Iteration 32102, loss = 0.03595119\n",
      "Iteration 32103, loss = 0.03593622\n",
      "Iteration 32104, loss = 0.03593015\n",
      "Iteration 32105, loss = 0.03593754\n",
      "Iteration 32106, loss = 0.03593863\n",
      "Iteration 32107, loss = 0.03593936\n",
      "Iteration 32108, loss = 0.03593772\n",
      "Iteration 32109, loss = 0.03592689\n",
      "Iteration 32110, loss = 0.03593543\n",
      "Iteration 32111, loss = 0.03593965\n",
      "Iteration 32112, loss = 0.03592502\n",
      "Iteration 32113, loss = 0.03592450\n",
      "Iteration 32114, loss = 0.03593122\n",
      "Iteration 32115, loss = 0.03592963\n",
      "Iteration 32116, loss = 0.03592348\n",
      "Iteration 32117, loss = 0.03591855\n",
      "Iteration 32118, loss = 0.03592216\n",
      "Iteration 32119, loss = 0.03591941\n",
      "Iteration 32120, loss = 0.03592338\n",
      "Iteration 32121, loss = 0.03592574\n",
      "Iteration 32122, loss = 0.03592307\n",
      "Iteration 32123, loss = 0.03591597\n",
      "Iteration 32124, loss = 0.03591926\n",
      "Iteration 32125, loss = 0.03592065\n",
      "Iteration 32126, loss = 0.03591462\n",
      "Iteration 32127, loss = 0.03590680\n",
      "Iteration 32128, loss = 0.03591272\n",
      "Iteration 32129, loss = 0.03591561\n",
      "Iteration 32130, loss = 0.03591516\n",
      "Iteration 32131, loss = 0.03591547\n",
      "Iteration 32132, loss = 0.03591632\n",
      "Iteration 32133, loss = 0.03591623\n",
      "Iteration 32134, loss = 0.03591679\n",
      "Iteration 32135, loss = 0.03590933\n",
      "Iteration 32136, loss = 0.03591687\n",
      "Iteration 32137, loss = 0.03591420\n",
      "Iteration 32138, loss = 0.03590618\n",
      "Iteration 32139, loss = 0.03590874\n",
      "Iteration 32140, loss = 0.03591039\n",
      "Iteration 32141, loss = 0.03591366\n",
      "Iteration 32142, loss = 0.03591076\n",
      "Iteration 32143, loss = 0.03590905\n",
      "Iteration 32144, loss = 0.03590358\n",
      "Iteration 32145, loss = 0.03590337\n",
      "Iteration 32146, loss = 0.03591311\n",
      "Iteration 32147, loss = 0.03591633\n",
      "Iteration 32148, loss = 0.03590839\n",
      "Iteration 32149, loss = 0.03591381\n",
      "Iteration 32150, loss = 0.03591059\n",
      "Iteration 32151, loss = 0.03590933\n",
      "Iteration 32152, loss = 0.03591799\n",
      "Iteration 32153, loss = 0.03591976\n",
      "Iteration 32154, loss = 0.03591705\n",
      "Iteration 32155, loss = 0.03590695\n",
      "Iteration 32156, loss = 0.03590967\n",
      "Iteration 32157, loss = 0.03590060\n",
      "Iteration 32158, loss = 0.03589619\n",
      "Iteration 32159, loss = 0.03590901\n",
      "Iteration 32160, loss = 0.03590892\n",
      "Iteration 32161, loss = 0.03589654\n",
      "Iteration 32162, loss = 0.03589692\n",
      "Iteration 32163, loss = 0.03590495\n",
      "Iteration 32164, loss = 0.03590551\n",
      "Iteration 32165, loss = 0.03590463\n",
      "Iteration 32166, loss = 0.03589751\n",
      "Iteration 32167, loss = 0.03589642\n",
      "Iteration 32168, loss = 0.03589636\n",
      "Iteration 32169, loss = 0.03589590\n",
      "Iteration 32170, loss = 0.03589609\n",
      "Iteration 32171, loss = 0.03589639\n",
      "Iteration 32172, loss = 0.03589435\n",
      "Iteration 32173, loss = 0.03589720\n",
      "Iteration 32174, loss = 0.03589528\n",
      "Iteration 32175, loss = 0.03589018\n",
      "Iteration 32176, loss = 0.03588785\n",
      "Iteration 32177, loss = 0.03589438\n",
      "Iteration 32178, loss = 0.03589245\n",
      "Iteration 32179, loss = 0.03589544\n",
      "Iteration 32180, loss = 0.03588889\n",
      "Iteration 32181, loss = 0.03589450\n",
      "Iteration 32182, loss = 0.03589812\n",
      "Iteration 32183, loss = 0.03589342\n",
      "Iteration 32184, loss = 0.03589842\n",
      "Iteration 32185, loss = 0.03589256\n",
      "Iteration 32186, loss = 0.03588396\n",
      "Iteration 32187, loss = 0.03589125\n",
      "Iteration 32188, loss = 0.03588923\n",
      "Iteration 32189, loss = 0.03588519\n",
      "Iteration 32190, loss = 0.03589206\n",
      "Iteration 32191, loss = 0.03589352\n",
      "Iteration 32192, loss = 0.03589015\n",
      "Iteration 32193, loss = 0.03589139\n",
      "Iteration 32194, loss = 0.03589481\n",
      "Iteration 32195, loss = 0.03589855\n",
      "Iteration 32196, loss = 0.03588629\n",
      "Iteration 32197, loss = 0.03588311\n",
      "Iteration 32198, loss = 0.03589078\n",
      "Iteration 32199, loss = 0.03589662\n",
      "Iteration 32200, loss = 0.03589024\n",
      "Iteration 32201, loss = 0.03588455\n",
      "Iteration 32202, loss = 0.03588437\n",
      "Iteration 32203, loss = 0.03587818\n",
      "Iteration 32204, loss = 0.03588830\n",
      "Iteration 32205, loss = 0.03589001\n",
      "Iteration 32206, loss = 0.03588121\n",
      "Iteration 32207, loss = 0.03587101\n",
      "Iteration 32208, loss = 0.03587561\n",
      "Iteration 32209, loss = 0.03588146\n",
      "Iteration 32210, loss = 0.03587903\n",
      "Iteration 32211, loss = 0.03587323\n",
      "Iteration 32212, loss = 0.03587669\n",
      "Iteration 32213, loss = 0.03587549\n",
      "Iteration 32214, loss = 0.03587983\n",
      "Iteration 32215, loss = 0.03587647\n",
      "Iteration 32216, loss = 0.03586850\n",
      "Iteration 32217, loss = 0.03586620\n",
      "Iteration 32218, loss = 0.03586955\n",
      "Iteration 32219, loss = 0.03586606\n",
      "Iteration 32220, loss = 0.03585904\n",
      "Iteration 32221, loss = 0.03586519\n",
      "Iteration 32222, loss = 0.03586246\n",
      "Iteration 32223, loss = 0.03586401\n",
      "Iteration 32224, loss = 0.03586273\n",
      "Iteration 32225, loss = 0.03586877\n",
      "Iteration 32226, loss = 0.03586646\n",
      "Iteration 32227, loss = 0.03586240\n",
      "Iteration 32228, loss = 0.03586092\n",
      "Iteration 32229, loss = 0.03586928\n",
      "Iteration 32230, loss = 0.03586703\n",
      "Iteration 32231, loss = 0.03585506\n",
      "Iteration 32232, loss = 0.03586525\n",
      "Iteration 32233, loss = 0.03587146\n",
      "Iteration 32234, loss = 0.03587021\n",
      "Iteration 32235, loss = 0.03586921\n",
      "Iteration 32236, loss = 0.03586389\n",
      "Iteration 32237, loss = 0.03585388\n",
      "Iteration 32238, loss = 0.03586224\n",
      "Iteration 32239, loss = 0.03586796\n",
      "Iteration 32240, loss = 0.03586145\n",
      "Iteration 32241, loss = 0.03585091\n",
      "Iteration 32242, loss = 0.03585571\n",
      "Iteration 32243, loss = 0.03585930\n",
      "Iteration 32244, loss = 0.03586107\n",
      "Iteration 32245, loss = 0.03586797\n",
      "Iteration 32246, loss = 0.03586786\n",
      "Iteration 32247, loss = 0.03586182\n",
      "Iteration 32248, loss = 0.03585411\n",
      "Iteration 32249, loss = 0.03585323\n",
      "Iteration 32250, loss = 0.03585711\n",
      "Iteration 32251, loss = 0.03585730\n",
      "Iteration 32252, loss = 0.03584798\n",
      "Iteration 32253, loss = 0.03585385\n",
      "Iteration 32254, loss = 0.03585690\n",
      "Iteration 32255, loss = 0.03585025\n",
      "Iteration 32256, loss = 0.03585424\n",
      "Iteration 32257, loss = 0.03585288\n",
      "Iteration 32258, loss = 0.03584766\n",
      "Iteration 32259, loss = 0.03585469\n",
      "Iteration 32260, loss = 0.03585621\n",
      "Iteration 32261, loss = 0.03584537\n",
      "Iteration 32262, loss = 0.03584440\n",
      "Iteration 32263, loss = 0.03585374\n",
      "Iteration 32264, loss = 0.03584573\n",
      "Iteration 32265, loss = 0.03583952\n",
      "Iteration 32266, loss = 0.03584545\n",
      "Iteration 32267, loss = 0.03584324\n",
      "Iteration 32268, loss = 0.03584129\n",
      "Iteration 32269, loss = 0.03584090\n",
      "Iteration 32270, loss = 0.03585008\n",
      "Iteration 32271, loss = 0.03584933\n",
      "Iteration 32272, loss = 0.03584180\n",
      "Iteration 32273, loss = 0.03583744\n",
      "Iteration 32274, loss = 0.03584074\n",
      "Iteration 32275, loss = 0.03584307\n",
      "Iteration 32276, loss = 0.03584432\n",
      "Iteration 32277, loss = 0.03584760\n",
      "Iteration 32278, loss = 0.03584331\n",
      "Iteration 32279, loss = 0.03583487\n",
      "Iteration 32280, loss = 0.03583052\n",
      "Iteration 32281, loss = 0.03583597\n",
      "Iteration 32282, loss = 0.03583722\n",
      "Iteration 32283, loss = 0.03583501\n",
      "Iteration 32284, loss = 0.03583151\n",
      "Iteration 32285, loss = 0.03583197\n",
      "Iteration 32286, loss = 0.03583472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 32287, loss = 0.03583566\n",
      "Iteration 32288, loss = 0.03583290\n",
      "Iteration 32289, loss = 0.03583216\n",
      "Iteration 32290, loss = 0.03583181\n",
      "Iteration 32291, loss = 0.03582139\n",
      "Iteration 32292, loss = 0.03582929\n",
      "Iteration 32293, loss = 0.03583487\n",
      "Iteration 32294, loss = 0.03583473\n",
      "Iteration 32295, loss = 0.03582989\n",
      "Iteration 32296, loss = 0.03582360\n",
      "Iteration 32297, loss = 0.03582367\n",
      "Iteration 32298, loss = 0.03583832\n",
      "Iteration 32299, loss = 0.03583420\n",
      "Iteration 32300, loss = 0.03582555\n",
      "Iteration 32301, loss = 0.03582480\n",
      "Iteration 32302, loss = 0.03581930\n",
      "Iteration 32303, loss = 0.03581928\n",
      "Iteration 32304, loss = 0.03582054\n",
      "Iteration 32305, loss = 0.03582157\n",
      "Iteration 32306, loss = 0.03582223\n",
      "Iteration 32307, loss = 0.03582572\n",
      "Iteration 32308, loss = 0.03582605\n",
      "Iteration 32309, loss = 0.03582615\n",
      "Iteration 32310, loss = 0.03582322\n",
      "Iteration 32311, loss = 0.03581551\n",
      "Iteration 32312, loss = 0.03581930\n",
      "Iteration 32313, loss = 0.03581929\n",
      "Iteration 32314, loss = 0.03581361\n",
      "Iteration 32315, loss = 0.03581666\n",
      "Iteration 32316, loss = 0.03582022\n",
      "Iteration 32317, loss = 0.03581535\n",
      "Iteration 32318, loss = 0.03581339\n",
      "Iteration 32319, loss = 0.03581188\n",
      "Iteration 32320, loss = 0.03581299\n",
      "Iteration 32321, loss = 0.03581055\n",
      "Iteration 32322, loss = 0.03581579\n",
      "Iteration 32323, loss = 0.03581382\n",
      "Iteration 32324, loss = 0.03581494\n",
      "Iteration 32325, loss = 0.03581414\n",
      "Iteration 32326, loss = 0.03581171\n",
      "Iteration 32327, loss = 0.03581290\n",
      "Iteration 32328, loss = 0.03580902\n",
      "Iteration 32329, loss = 0.03580566\n",
      "Iteration 32330, loss = 0.03581323\n",
      "Iteration 32331, loss = 0.03581346\n",
      "Iteration 32332, loss = 0.03580970\n",
      "Iteration 32333, loss = 0.03580810\n",
      "Iteration 32334, loss = 0.03580625\n",
      "Iteration 32335, loss = 0.03580475\n",
      "Iteration 32336, loss = 0.03580273\n",
      "Iteration 32337, loss = 0.03580538\n",
      "Iteration 32338, loss = 0.03580426\n",
      "Iteration 32339, loss = 0.03580250\n",
      "Iteration 32340, loss = 0.03580768\n",
      "Iteration 32341, loss = 0.03580828\n",
      "Iteration 32342, loss = 0.03580256\n",
      "Iteration 32343, loss = 0.03579888\n",
      "Iteration 32344, loss = 0.03581318\n",
      "Iteration 32345, loss = 0.03582312\n",
      "Iteration 32346, loss = 0.03581469\n",
      "Iteration 32347, loss = 0.03580693\n",
      "Iteration 32348, loss = 0.03581054\n",
      "Iteration 32349, loss = 0.03581131\n",
      "Iteration 32350, loss = 0.03580264\n",
      "Iteration 32351, loss = 0.03579883\n",
      "Iteration 32352, loss = 0.03580490\n",
      "Iteration 32353, loss = 0.03580796\n",
      "Iteration 32354, loss = 0.03580543\n",
      "Iteration 32355, loss = 0.03580335\n",
      "Iteration 32356, loss = 0.03580434\n",
      "Iteration 32357, loss = 0.03580930\n",
      "Iteration 32358, loss = 0.03580667\n",
      "Iteration 32359, loss = 0.03580363\n",
      "Iteration 32360, loss = 0.03580186\n",
      "Iteration 32361, loss = 0.03580446\n",
      "Iteration 32362, loss = 0.03579417\n",
      "Iteration 32363, loss = 0.03579077\n",
      "Iteration 32364, loss = 0.03580126\n",
      "Iteration 32365, loss = 0.03580273\n",
      "Iteration 32366, loss = 0.03579358\n",
      "Iteration 32367, loss = 0.03580300\n",
      "Iteration 32368, loss = 0.03580502\n",
      "Iteration 32369, loss = 0.03580007\n",
      "Iteration 32370, loss = 0.03579446\n",
      "Iteration 32371, loss = 0.03578458\n",
      "Iteration 32372, loss = 0.03579348\n",
      "Iteration 32373, loss = 0.03579720\n",
      "Iteration 32374, loss = 0.03579110\n",
      "Iteration 32375, loss = 0.03578135\n",
      "Iteration 32376, loss = 0.03578672\n",
      "Iteration 32377, loss = 0.03578447\n",
      "Iteration 32378, loss = 0.03578847\n",
      "Iteration 32379, loss = 0.03578580\n",
      "Iteration 32380, loss = 0.03578273\n",
      "Iteration 32381, loss = 0.03578230\n",
      "Iteration 32382, loss = 0.03578536\n",
      "Iteration 32383, loss = 0.03578526\n",
      "Iteration 32384, loss = 0.03578787\n",
      "Iteration 32385, loss = 0.03578664\n",
      "Iteration 32386, loss = 0.03578009\n",
      "Iteration 32387, loss = 0.03578184\n",
      "Iteration 32388, loss = 0.03578698\n",
      "Iteration 32389, loss = 0.03578371\n",
      "Iteration 32390, loss = 0.03577924\n",
      "Iteration 32391, loss = 0.03577663\n",
      "Iteration 32392, loss = 0.03578166\n",
      "Iteration 32393, loss = 0.03578370\n",
      "Iteration 32394, loss = 0.03577460\n",
      "Iteration 32395, loss = 0.03577209\n",
      "Iteration 32396, loss = 0.03577610\n",
      "Iteration 32397, loss = 0.03577027\n",
      "Iteration 32398, loss = 0.03577344\n",
      "Iteration 32399, loss = 0.03577181\n",
      "Iteration 32400, loss = 0.03577538\n",
      "Iteration 32401, loss = 0.03577223\n",
      "Iteration 32402, loss = 0.03576795\n",
      "Iteration 32403, loss = 0.03577219\n",
      "Iteration 32404, loss = 0.03577625\n",
      "Iteration 32405, loss = 0.03577683\n",
      "Iteration 32406, loss = 0.03576791\n",
      "Iteration 32407, loss = 0.03576606\n",
      "Iteration 32408, loss = 0.03576518\n",
      "Iteration 32409, loss = 0.03576674\n",
      "Iteration 32410, loss = 0.03577292\n",
      "Iteration 32411, loss = 0.03577492\n",
      "Iteration 32412, loss = 0.03577417\n",
      "Iteration 32413, loss = 0.03577071\n",
      "Iteration 32414, loss = 0.03576835\n",
      "Iteration 32415, loss = 0.03576744\n",
      "Iteration 32416, loss = 0.03576693\n",
      "Iteration 32417, loss = 0.03576336\n",
      "Iteration 32418, loss = 0.03576147\n",
      "Iteration 32419, loss = 0.03576964\n",
      "Iteration 32420, loss = 0.03576847\n",
      "Iteration 32421, loss = 0.03576422\n",
      "Iteration 32422, loss = 0.03576235\n",
      "Iteration 32423, loss = 0.03576361\n",
      "Iteration 32424, loss = 0.03575839\n",
      "Iteration 32425, loss = 0.03576362\n",
      "Iteration 32426, loss = 0.03575921\n",
      "Iteration 32427, loss = 0.03576289\n",
      "Iteration 32428, loss = 0.03576009\n",
      "Iteration 32429, loss = 0.03576148\n",
      "Iteration 32430, loss = 0.03575512\n",
      "Iteration 32431, loss = 0.03575264\n",
      "Iteration 32432, loss = 0.03575753\n",
      "Iteration 32433, loss = 0.03575434\n",
      "Iteration 32434, loss = 0.03575602\n",
      "Iteration 32435, loss = 0.03575872\n",
      "Iteration 32436, loss = 0.03575197\n",
      "Iteration 32437, loss = 0.03574753\n",
      "Iteration 32438, loss = 0.03575426\n",
      "Iteration 32439, loss = 0.03575013\n",
      "Iteration 32440, loss = 0.03575022\n",
      "Iteration 32441, loss = 0.03575524\n",
      "Iteration 32442, loss = 0.03575365\n",
      "Iteration 32443, loss = 0.03575308\n",
      "Iteration 32444, loss = 0.03574815\n",
      "Iteration 32445, loss = 0.03575250\n",
      "Iteration 32446, loss = 0.03575526\n",
      "Iteration 32447, loss = 0.03574463\n",
      "Iteration 32448, loss = 0.03574913\n",
      "Iteration 32449, loss = 0.03575664\n",
      "Iteration 32450, loss = 0.03575593\n",
      "Iteration 32451, loss = 0.03575772\n",
      "Iteration 32452, loss = 0.03575551\n",
      "Iteration 32453, loss = 0.03575388\n",
      "Iteration 32454, loss = 0.03574972\n",
      "Iteration 32455, loss = 0.03574451\n",
      "Iteration 32456, loss = 0.03574588\n",
      "Iteration 32457, loss = 0.03574648\n",
      "Iteration 32458, loss = 0.03574861\n",
      "Iteration 32459, loss = 0.03574506\n",
      "Iteration 32460, loss = 0.03574111\n",
      "Iteration 32461, loss = 0.03574075\n",
      "Iteration 32462, loss = 0.03575200\n",
      "Iteration 32463, loss = 0.03575028\n",
      "Iteration 32464, loss = 0.03574668\n",
      "Iteration 32465, loss = 0.03575201\n",
      "Iteration 32466, loss = 0.03574793\n",
      "Iteration 32467, loss = 0.03574222\n",
      "Iteration 32468, loss = 0.03573932\n",
      "Iteration 32469, loss = 0.03573299\n",
      "Iteration 32470, loss = 0.03573243\n",
      "Iteration 32471, loss = 0.03573384\n",
      "Iteration 32472, loss = 0.03573214\n",
      "Iteration 32473, loss = 0.03572772\n",
      "Iteration 32474, loss = 0.03573298\n",
      "Iteration 32475, loss = 0.03573459\n",
      "Iteration 32476, loss = 0.03572496\n",
      "Iteration 32477, loss = 0.03573224\n",
      "Iteration 32478, loss = 0.03573436\n",
      "Iteration 32479, loss = 0.03573070\n",
      "Iteration 32480, loss = 0.03573494\n",
      "Iteration 32481, loss = 0.03573377\n",
      "Iteration 32482, loss = 0.03572739\n",
      "Iteration 32483, loss = 0.03572911\n",
      "Iteration 32484, loss = 0.03573129\n",
      "Iteration 32485, loss = 0.03573330\n",
      "Iteration 32486, loss = 0.03572498\n",
      "Iteration 32487, loss = 0.03572487\n",
      "Iteration 32488, loss = 0.03573255\n",
      "Iteration 32489, loss = 0.03573051\n",
      "Iteration 32490, loss = 0.03572832\n",
      "Iteration 32491, loss = 0.03572787\n",
      "Iteration 32492, loss = 0.03572068\n",
      "Iteration 32493, loss = 0.03572213\n",
      "Iteration 32494, loss = 0.03572590\n",
      "Iteration 32495, loss = 0.03572433\n",
      "Iteration 32496, loss = 0.03572263\n",
      "Iteration 32497, loss = 0.03572160\n",
      "Iteration 32498, loss = 0.03571414\n",
      "Iteration 32499, loss = 0.03571873\n",
      "Iteration 32500, loss = 0.03571742\n",
      "Iteration 32501, loss = 0.03571617\n",
      "Iteration 32502, loss = 0.03571598\n",
      "Iteration 32503, loss = 0.03571800\n",
      "Iteration 32504, loss = 0.03571709\n",
      "Iteration 32505, loss = 0.03572242\n",
      "Iteration 32506, loss = 0.03572353\n",
      "Iteration 32507, loss = 0.03571914\n",
      "Iteration 32508, loss = 0.03571226\n",
      "Iteration 32509, loss = 0.03572314\n",
      "Iteration 32510, loss = 0.03572673\n",
      "Iteration 32511, loss = 0.03571164\n",
      "Iteration 32512, loss = 0.03571720\n",
      "Iteration 32513, loss = 0.03572951\n",
      "Iteration 32514, loss = 0.03573218\n",
      "Iteration 32515, loss = 0.03572187\n",
      "Iteration 32516, loss = 0.03572375\n",
      "Iteration 32517, loss = 0.03571910\n",
      "Iteration 32518, loss = 0.03571971\n",
      "Iteration 32519, loss = 0.03572194\n",
      "Iteration 32520, loss = 0.03571761\n",
      "Iteration 32521, loss = 0.03571504\n",
      "Iteration 32522, loss = 0.03572293\n",
      "Iteration 32523, loss = 0.03572446\n",
      "Iteration 32524, loss = 0.03571714\n",
      "Iteration 32525, loss = 0.03571598\n",
      "Iteration 32526, loss = 0.03571447\n",
      "Iteration 32527, loss = 0.03571112\n",
      "Iteration 32528, loss = 0.03570652\n",
      "Iteration 32529, loss = 0.03570989\n",
      "Iteration 32530, loss = 0.03570972\n",
      "Iteration 32531, loss = 0.03571088\n",
      "Iteration 32532, loss = 0.03570933\n",
      "Iteration 32533, loss = 0.03570744\n",
      "Iteration 32534, loss = 0.03570528\n",
      "Iteration 32535, loss = 0.03571413\n",
      "Iteration 32536, loss = 0.03571176\n",
      "Iteration 32537, loss = 0.03570216\n",
      "Iteration 32538, loss = 0.03570270\n",
      "Iteration 32539, loss = 0.03570207\n",
      "Iteration 32540, loss = 0.03570207\n",
      "Iteration 32541, loss = 0.03570045\n",
      "Iteration 32542, loss = 0.03570220\n",
      "Iteration 32543, loss = 0.03570019\n",
      "Iteration 32544, loss = 0.03569919\n",
      "Iteration 32545, loss = 0.03569875\n",
      "Iteration 32546, loss = 0.03570691\n",
      "Iteration 32547, loss = 0.03570559\n",
      "Iteration 32548, loss = 0.03569730\n",
      "Iteration 32549, loss = 0.03569686\n",
      "Iteration 32550, loss = 0.03569791\n",
      "Iteration 32551, loss = 0.03569913\n",
      "Iteration 32552, loss = 0.03569981\n",
      "Iteration 32553, loss = 0.03569738\n",
      "Iteration 32554, loss = 0.03569871\n",
      "Iteration 32555, loss = 0.03569346\n",
      "Iteration 32556, loss = 0.03569739\n",
      "Iteration 32557, loss = 0.03570279\n",
      "Iteration 32558, loss = 0.03570354\n",
      "Iteration 32559, loss = 0.03570072\n",
      "Iteration 32560, loss = 0.03569447\n",
      "Iteration 32561, loss = 0.03569270\n",
      "Iteration 32562, loss = 0.03568919\n",
      "Iteration 32563, loss = 0.03569830\n",
      "Iteration 32564, loss = 0.03569833\n",
      "Iteration 32565, loss = 0.03569446\n",
      "Iteration 32566, loss = 0.03568538\n",
      "Iteration 32567, loss = 0.03569656\n",
      "Iteration 32568, loss = 0.03569969\n",
      "Iteration 32569, loss = 0.03569606\n",
      "Iteration 32570, loss = 0.03568875\n",
      "Iteration 32571, loss = 0.03568486\n",
      "Iteration 32572, loss = 0.03568464\n",
      "Iteration 32573, loss = 0.03569503\n",
      "Iteration 32574, loss = 0.03569681\n",
      "Iteration 32575, loss = 0.03569043\n",
      "Iteration 32576, loss = 0.03568329\n",
      "Iteration 32577, loss = 0.03568469\n",
      "Iteration 32578, loss = 0.03569226\n",
      "Iteration 32579, loss = 0.03568609\n",
      "Iteration 32580, loss = 0.03568149\n",
      "Iteration 32581, loss = 0.03568622\n",
      "Iteration 32582, loss = 0.03569276\n",
      "Iteration 32583, loss = 0.03569349\n",
      "Iteration 32584, loss = 0.03568428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 32585, loss = 0.03568684\n",
      "Iteration 32586, loss = 0.03568577\n",
      "Iteration 32587, loss = 0.03567994\n",
      "Iteration 32588, loss = 0.03568478\n",
      "Iteration 32589, loss = 0.03569253\n",
      "Iteration 32590, loss = 0.03568963\n",
      "Iteration 32591, loss = 0.03568609\n",
      "Iteration 32592, loss = 0.03568101\n",
      "Iteration 32593, loss = 0.03567380\n",
      "Iteration 32594, loss = 0.03567961\n",
      "Iteration 32595, loss = 0.03568167\n",
      "Iteration 32596, loss = 0.03567996\n",
      "Iteration 32597, loss = 0.03567501\n",
      "Iteration 32598, loss = 0.03568198\n",
      "Iteration 32599, loss = 0.03567875\n",
      "Iteration 32600, loss = 0.03567228\n",
      "Iteration 32601, loss = 0.03567760\n",
      "Iteration 32602, loss = 0.03568450\n",
      "Iteration 32603, loss = 0.03568286\n",
      "Iteration 32604, loss = 0.03567652\n",
      "Iteration 32605, loss = 0.03567267\n",
      "Iteration 32606, loss = 0.03567444\n",
      "Iteration 32607, loss = 0.03567685\n",
      "Iteration 32608, loss = 0.03567937\n",
      "Iteration 32609, loss = 0.03568114\n",
      "Iteration 32610, loss = 0.03567007\n",
      "Iteration 32611, loss = 0.03566011\n",
      "Iteration 32612, loss = 0.03566561\n",
      "Iteration 32613, loss = 0.03566425\n",
      "Iteration 32614, loss = 0.03566134\n",
      "Iteration 32615, loss = 0.03566310\n",
      "Iteration 32616, loss = 0.03566373\n",
      "Iteration 32617, loss = 0.03565652\n",
      "Iteration 32618, loss = 0.03565722\n",
      "Iteration 32619, loss = 0.03565663\n",
      "Iteration 32620, loss = 0.03565806\n",
      "Iteration 32621, loss = 0.03565763\n",
      "Iteration 32622, loss = 0.03566488\n",
      "Iteration 32623, loss = 0.03566601\n",
      "Iteration 32624, loss = 0.03566092\n",
      "Iteration 32625, loss = 0.03565696\n",
      "Iteration 32626, loss = 0.03565544\n",
      "Iteration 32627, loss = 0.03566029\n",
      "Iteration 32628, loss = 0.03565071\n",
      "Iteration 32629, loss = 0.03565387\n",
      "Iteration 32630, loss = 0.03565940\n",
      "Iteration 32631, loss = 0.03565543\n",
      "Iteration 32632, loss = 0.03565311\n",
      "Iteration 32633, loss = 0.03565633\n",
      "Iteration 32634, loss = 0.03566597\n",
      "Iteration 32635, loss = 0.03566307\n",
      "Iteration 32636, loss = 0.03565661\n",
      "Iteration 32637, loss = 0.03565155\n",
      "Iteration 32638, loss = 0.03565589\n",
      "Iteration 32639, loss = 0.03565610\n",
      "Iteration 32640, loss = 0.03566244\n",
      "Iteration 32641, loss = 0.03565520\n",
      "Iteration 32642, loss = 0.03565462\n",
      "Iteration 32643, loss = 0.03565802\n",
      "Iteration 32644, loss = 0.03566931\n",
      "Iteration 32645, loss = 0.03567029\n",
      "Iteration 32646, loss = 0.03566189\n",
      "Iteration 32647, loss = 0.03565645\n",
      "Iteration 32648, loss = 0.03565605\n",
      "Iteration 32649, loss = 0.03565744\n",
      "Iteration 32650, loss = 0.03565501\n",
      "Iteration 32651, loss = 0.03564684\n",
      "Iteration 32652, loss = 0.03564297\n",
      "Iteration 32653, loss = 0.03564100\n",
      "Iteration 32654, loss = 0.03564154\n",
      "Iteration 32655, loss = 0.03564002\n",
      "Iteration 32656, loss = 0.03564517\n",
      "Iteration 32657, loss = 0.03564584\n",
      "Iteration 32658, loss = 0.03564477\n",
      "Iteration 32659, loss = 0.03564371\n",
      "Iteration 32660, loss = 0.03565034\n",
      "Iteration 32661, loss = 0.03564285\n",
      "Iteration 32662, loss = 0.03563677\n",
      "Iteration 32663, loss = 0.03564262\n",
      "Iteration 32664, loss = 0.03563348\n",
      "Iteration 32665, loss = 0.03563047\n",
      "Iteration 32666, loss = 0.03563039\n",
      "Iteration 32667, loss = 0.03563275\n",
      "Iteration 32668, loss = 0.03563578\n",
      "Iteration 32669, loss = 0.03563351\n",
      "Iteration 32670, loss = 0.03563056\n",
      "Iteration 32671, loss = 0.03563012\n",
      "Iteration 32672, loss = 0.03562941\n",
      "Iteration 32673, loss = 0.03562666\n",
      "Iteration 32674, loss = 0.03563467\n",
      "Iteration 32675, loss = 0.03562780\n",
      "Iteration 32676, loss = 0.03562701\n",
      "Iteration 32677, loss = 0.03563026\n",
      "Iteration 32678, loss = 0.03563375\n",
      "Iteration 32679, loss = 0.03563523\n",
      "Iteration 32680, loss = 0.03563444\n",
      "Iteration 32681, loss = 0.03563070\n",
      "Iteration 32682, loss = 0.03563330\n",
      "Iteration 32683, loss = 0.03563021\n",
      "Iteration 32684, loss = 0.03562499\n",
      "Iteration 32685, loss = 0.03562652\n",
      "Iteration 32686, loss = 0.03563000\n",
      "Iteration 32687, loss = 0.03563317\n",
      "Iteration 32688, loss = 0.03562860\n",
      "Iteration 32689, loss = 0.03562536\n",
      "Iteration 32690, loss = 0.03562003\n",
      "Iteration 32691, loss = 0.03561656\n",
      "Iteration 32692, loss = 0.03562398\n",
      "Iteration 32693, loss = 0.03562131\n",
      "Iteration 32694, loss = 0.03561724\n",
      "Iteration 32695, loss = 0.03562330\n",
      "Iteration 32696, loss = 0.03562659\n",
      "Iteration 32697, loss = 0.03562213\n",
      "Iteration 32698, loss = 0.03562151\n",
      "Iteration 32699, loss = 0.03561716\n",
      "Iteration 32700, loss = 0.03562317\n",
      "Iteration 32701, loss = 0.03562011\n",
      "Iteration 32702, loss = 0.03561579\n",
      "Iteration 32703, loss = 0.03562228\n",
      "Iteration 32704, loss = 0.03562482\n",
      "Iteration 32705, loss = 0.03562044\n",
      "Iteration 32706, loss = 0.03562442\n",
      "Iteration 32707, loss = 0.03561862\n",
      "Iteration 32708, loss = 0.03561595\n",
      "Iteration 32709, loss = 0.03561956\n",
      "Iteration 32710, loss = 0.03561955\n",
      "Iteration 32711, loss = 0.03562581\n",
      "Iteration 32712, loss = 0.03562430\n",
      "Iteration 32713, loss = 0.03561770\n",
      "Iteration 32714, loss = 0.03561021\n",
      "Iteration 32715, loss = 0.03561803\n",
      "Iteration 32716, loss = 0.03562534\n",
      "Iteration 32717, loss = 0.03562096\n",
      "Iteration 32718, loss = 0.03561027\n",
      "Iteration 32719, loss = 0.03561141\n",
      "Iteration 32720, loss = 0.03561068\n",
      "Iteration 32721, loss = 0.03562252\n",
      "Iteration 32722, loss = 0.03562462\n",
      "Iteration 32723, loss = 0.03561703\n",
      "Iteration 32724, loss = 0.03560978\n",
      "Iteration 32725, loss = 0.03561034\n",
      "Iteration 32726, loss = 0.03561552\n",
      "Iteration 32727, loss = 0.03561754\n",
      "Iteration 32728, loss = 0.03561224\n",
      "Iteration 32729, loss = 0.03561054\n",
      "Iteration 32730, loss = 0.03561099\n",
      "Iteration 32731, loss = 0.03561290\n",
      "Iteration 32732, loss = 0.03560982\n",
      "Iteration 32733, loss = 0.03560867\n",
      "Iteration 32734, loss = 0.03560959\n",
      "Iteration 32735, loss = 0.03560058\n",
      "Iteration 32736, loss = 0.03560217\n",
      "Iteration 32737, loss = 0.03560998\n",
      "Iteration 32738, loss = 0.03560675\n",
      "Iteration 32739, loss = 0.03559636\n",
      "Iteration 32740, loss = 0.03560272\n",
      "Iteration 32741, loss = 0.03560385\n",
      "Iteration 32742, loss = 0.03560389\n",
      "Iteration 32743, loss = 0.03560424\n",
      "Iteration 32744, loss = 0.03560460\n",
      "Iteration 32745, loss = 0.03559720\n",
      "Iteration 32746, loss = 0.03558830\n",
      "Iteration 32747, loss = 0.03560203\n",
      "Iteration 32748, loss = 0.03560519\n",
      "Iteration 32749, loss = 0.03560465\n",
      "Iteration 32750, loss = 0.03559660\n",
      "Iteration 32751, loss = 0.03559904\n",
      "Iteration 32752, loss = 0.03559998\n",
      "Iteration 32753, loss = 0.03559961\n",
      "Iteration 32754, loss = 0.03560166\n",
      "Iteration 32755, loss = 0.03559865\n",
      "Iteration 32756, loss = 0.03559094\n",
      "Iteration 32757, loss = 0.03559160\n",
      "Iteration 32758, loss = 0.03559616\n",
      "Iteration 32759, loss = 0.03560260\n",
      "Iteration 32760, loss = 0.03559849\n",
      "Iteration 32761, loss = 0.03558408\n",
      "Iteration 32762, loss = 0.03559665\n",
      "Iteration 32763, loss = 0.03560443\n",
      "Iteration 32764, loss = 0.03559529\n",
      "Iteration 32765, loss = 0.03559505\n",
      "Iteration 32766, loss = 0.03559285\n",
      "Iteration 32767, loss = 0.03559663\n",
      "Iteration 32768, loss = 0.03560547\n",
      "Iteration 32769, loss = 0.03560499\n",
      "Iteration 32770, loss = 0.03559853\n",
      "Iteration 32771, loss = 0.03558493\n",
      "Iteration 32772, loss = 0.03558350\n",
      "Iteration 32773, loss = 0.03559096\n",
      "Iteration 32774, loss = 0.03559058\n",
      "Iteration 32775, loss = 0.03559435\n",
      "Iteration 32776, loss = 0.03559019\n",
      "Iteration 32777, loss = 0.03558054\n",
      "Iteration 32778, loss = 0.03558895\n",
      "Iteration 32779, loss = 0.03559690\n",
      "Iteration 32780, loss = 0.03559037\n",
      "Iteration 32781, loss = 0.03558296\n",
      "Iteration 32782, loss = 0.03558828\n",
      "Iteration 32783, loss = 0.03559368\n",
      "Iteration 32784, loss = 0.03559207\n",
      "Iteration 32785, loss = 0.03558413\n",
      "Iteration 32786, loss = 0.03558231\n",
      "Iteration 32787, loss = 0.03558982\n",
      "Iteration 32788, loss = 0.03560010\n",
      "Iteration 32789, loss = 0.03560072\n",
      "Iteration 32790, loss = 0.03559007\n",
      "Iteration 32791, loss = 0.03558566\n",
      "Iteration 32792, loss = 0.03558738\n",
      "Iteration 32793, loss = 0.03558447\n",
      "Iteration 32794, loss = 0.03557897\n",
      "Iteration 32795, loss = 0.03557669\n",
      "Iteration 32796, loss = 0.03556960\n",
      "Iteration 32797, loss = 0.03556572\n",
      "Iteration 32798, loss = 0.03556773\n",
      "Iteration 32799, loss = 0.03557357\n",
      "Iteration 32800, loss = 0.03557165\n",
      "Iteration 32801, loss = 0.03556540\n",
      "Iteration 32802, loss = 0.03557211\n",
      "Iteration 32803, loss = 0.03557317\n",
      "Iteration 32804, loss = 0.03557149\n",
      "Iteration 32805, loss = 0.03556666\n",
      "Iteration 32806, loss = 0.03556230\n",
      "Iteration 32807, loss = 0.03556807\n",
      "Iteration 32808, loss = 0.03557333\n",
      "Iteration 32809, loss = 0.03557515\n",
      "Iteration 32810, loss = 0.03557021\n",
      "Iteration 32811, loss = 0.03556212\n",
      "Iteration 32812, loss = 0.03556489\n",
      "Iteration 32813, loss = 0.03556815\n",
      "Iteration 32814, loss = 0.03557414\n",
      "Iteration 32815, loss = 0.03557436\n",
      "Iteration 32816, loss = 0.03556881\n",
      "Iteration 32817, loss = 0.03556731\n",
      "Iteration 32818, loss = 0.03556098\n",
      "Iteration 32819, loss = 0.03555917\n",
      "Iteration 32820, loss = 0.03556108\n",
      "Iteration 32821, loss = 0.03555578\n",
      "Iteration 32822, loss = 0.03555480\n",
      "Iteration 32823, loss = 0.03556052\n",
      "Iteration 32824, loss = 0.03556189\n",
      "Iteration 32825, loss = 0.03555707\n",
      "Iteration 32826, loss = 0.03554730\n",
      "Iteration 32827, loss = 0.03555596\n",
      "Iteration 32828, loss = 0.03555415\n",
      "Iteration 32829, loss = 0.03554992\n",
      "Iteration 32830, loss = 0.03555319\n",
      "Iteration 32831, loss = 0.03555236\n",
      "Iteration 32832, loss = 0.03555150\n",
      "Iteration 32833, loss = 0.03555080\n",
      "Iteration 32834, loss = 0.03555080\n",
      "Iteration 32835, loss = 0.03554722\n",
      "Iteration 32836, loss = 0.03555343\n",
      "Iteration 32837, loss = 0.03555375\n",
      "Iteration 32838, loss = 0.03554602\n",
      "Iteration 32839, loss = 0.03555232\n",
      "Iteration 32840, loss = 0.03556064\n",
      "Iteration 32841, loss = 0.03555469\n",
      "Iteration 32842, loss = 0.03555991\n",
      "Iteration 32843, loss = 0.03555985\n",
      "Iteration 32844, loss = 0.03555879\n",
      "Iteration 32845, loss = 0.03556183\n",
      "Iteration 32846, loss = 0.03555898\n",
      "Iteration 32847, loss = 0.03554495\n",
      "Iteration 32848, loss = 0.03554535\n",
      "Iteration 32849, loss = 0.03555031\n",
      "Iteration 32850, loss = 0.03554509\n",
      "Iteration 32851, loss = 0.03553972\n",
      "Iteration 32852, loss = 0.03554149\n",
      "Iteration 32853, loss = 0.03553946\n",
      "Iteration 32854, loss = 0.03553861\n",
      "Iteration 32855, loss = 0.03553877\n",
      "Iteration 32856, loss = 0.03554707\n",
      "Iteration 32857, loss = 0.03555026\n",
      "Iteration 32858, loss = 0.03554670\n",
      "Iteration 32859, loss = 0.03554028\n",
      "Iteration 32860, loss = 0.03553481\n",
      "Iteration 32861, loss = 0.03553433\n",
      "Iteration 32862, loss = 0.03553729\n",
      "Iteration 32863, loss = 0.03553811\n",
      "Iteration 32864, loss = 0.03553219\n",
      "Iteration 32865, loss = 0.03553631\n",
      "Iteration 32866, loss = 0.03553925\n",
      "Iteration 32867, loss = 0.03554087\n",
      "Iteration 32868, loss = 0.03553969\n",
      "Iteration 32869, loss = 0.03554214\n",
      "Iteration 32870, loss = 0.03554081\n",
      "Iteration 32871, loss = 0.03554010\n",
      "Iteration 32872, loss = 0.03553455\n",
      "Iteration 32873, loss = 0.03553891\n",
      "Iteration 32874, loss = 0.03553811\n",
      "Iteration 32875, loss = 0.03553296\n",
      "Iteration 32876, loss = 0.03552895\n",
      "Iteration 32877, loss = 0.03552733\n",
      "Iteration 32878, loss = 0.03553203\n",
      "Iteration 32879, loss = 0.03553256\n",
      "Iteration 32880, loss = 0.03553181\n",
      "Iteration 32881, loss = 0.03552769\n",
      "Iteration 32882, loss = 0.03552342\n",
      "Iteration 32883, loss = 0.03553411\n",
      "Iteration 32884, loss = 0.03553377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 32885, loss = 0.03552607\n",
      "Iteration 32886, loss = 0.03552860\n",
      "Iteration 32887, loss = 0.03553178\n",
      "Iteration 32888, loss = 0.03552777\n",
      "Iteration 32889, loss = 0.03552779\n",
      "Iteration 32890, loss = 0.03552789\n",
      "Iteration 32891, loss = 0.03552225\n",
      "Iteration 32892, loss = 0.03551703\n",
      "Iteration 32893, loss = 0.03552108\n",
      "Iteration 32894, loss = 0.03552503\n",
      "Iteration 32895, loss = 0.03551939\n",
      "Iteration 32896, loss = 0.03551553\n",
      "Iteration 32897, loss = 0.03551670\n",
      "Iteration 32898, loss = 0.03552166\n",
      "Iteration 32899, loss = 0.03551738\n",
      "Iteration 32900, loss = 0.03551787\n",
      "Iteration 32901, loss = 0.03551630\n",
      "Iteration 32902, loss = 0.03551921\n",
      "Iteration 32903, loss = 0.03551364\n",
      "Iteration 32904, loss = 0.03551648\n",
      "Iteration 32905, loss = 0.03552164\n",
      "Iteration 32906, loss = 0.03551774\n",
      "Iteration 32907, loss = 0.03551136\n",
      "Iteration 32908, loss = 0.03551685\n",
      "Iteration 32909, loss = 0.03551190\n",
      "Iteration 32910, loss = 0.03551762\n",
      "Iteration 32911, loss = 0.03552059\n",
      "Iteration 32912, loss = 0.03551630\n",
      "Iteration 32913, loss = 0.03552486\n",
      "Iteration 32914, loss = 0.03552933\n",
      "Iteration 32915, loss = 0.03552399\n",
      "Iteration 32916, loss = 0.03551667\n",
      "Iteration 32917, loss = 0.03551030\n",
      "Iteration 32918, loss = 0.03551906\n",
      "Iteration 32919, loss = 0.03551607\n",
      "Iteration 32920, loss = 0.03550914\n",
      "Iteration 32921, loss = 0.03551368\n",
      "Iteration 32922, loss = 0.03550999\n",
      "Iteration 32923, loss = 0.03550285\n",
      "Iteration 32924, loss = 0.03551384\n",
      "Iteration 32925, loss = 0.03551666\n",
      "Iteration 32926, loss = 0.03550765\n",
      "Iteration 32927, loss = 0.03550650\n",
      "Iteration 32928, loss = 0.03551455\n",
      "Iteration 32929, loss = 0.03551745\n",
      "Iteration 32930, loss = 0.03551458\n",
      "Iteration 32931, loss = 0.03551331\n",
      "Iteration 32932, loss = 0.03551417\n",
      "Iteration 32933, loss = 0.03551040\n",
      "Iteration 32934, loss = 0.03549800\n",
      "Iteration 32935, loss = 0.03550847\n",
      "Iteration 32936, loss = 0.03551799\n",
      "Iteration 32937, loss = 0.03551210\n",
      "Iteration 32938, loss = 0.03549823\n",
      "Iteration 32939, loss = 0.03550462\n",
      "Iteration 32940, loss = 0.03549956\n",
      "Iteration 32941, loss = 0.03549917\n",
      "Iteration 32942, loss = 0.03550329\n",
      "Iteration 32943, loss = 0.03550142\n",
      "Iteration 32944, loss = 0.03549619\n",
      "Iteration 32945, loss = 0.03549313\n",
      "Iteration 32946, loss = 0.03549757\n",
      "Iteration 32947, loss = 0.03550395\n",
      "Iteration 32948, loss = 0.03550164\n",
      "Iteration 32949, loss = 0.03549683\n",
      "Iteration 32950, loss = 0.03549240\n",
      "Iteration 32951, loss = 0.03549988\n",
      "Iteration 32952, loss = 0.03550036\n",
      "Iteration 32953, loss = 0.03548961\n",
      "Iteration 32954, loss = 0.03548805\n",
      "Iteration 32955, loss = 0.03549196\n",
      "Iteration 32956, loss = 0.03549190\n",
      "Iteration 32957, loss = 0.03549350\n",
      "Iteration 32958, loss = 0.03549539\n",
      "Iteration 32959, loss = 0.03549568\n",
      "Iteration 32960, loss = 0.03549151\n",
      "Iteration 32961, loss = 0.03548522\n",
      "Iteration 32962, loss = 0.03549564\n",
      "Iteration 32963, loss = 0.03550023\n",
      "Iteration 32964, loss = 0.03549743\n",
      "Iteration 32965, loss = 0.03548773\n",
      "Iteration 32966, loss = 0.03548302\n",
      "Iteration 32967, loss = 0.03549091\n",
      "Iteration 32968, loss = 0.03549453\n",
      "Iteration 32969, loss = 0.03549278\n",
      "Iteration 32970, loss = 0.03548792\n",
      "Iteration 32971, loss = 0.03548989\n",
      "Iteration 32972, loss = 0.03548898\n",
      "Iteration 32973, loss = 0.03548874\n",
      "Iteration 32974, loss = 0.03548717\n",
      "Iteration 32975, loss = 0.03548711\n",
      "Iteration 32976, loss = 0.03548723\n",
      "Iteration 32977, loss = 0.03549210\n",
      "Iteration 32978, loss = 0.03549454\n",
      "Iteration 32979, loss = 0.03548190\n",
      "Iteration 32980, loss = 0.03548267\n",
      "Iteration 32981, loss = 0.03548631\n",
      "Iteration 32982, loss = 0.03549038\n",
      "Iteration 32983, loss = 0.03548680\n",
      "Iteration 32984, loss = 0.03548201\n",
      "Iteration 32985, loss = 0.03547800\n",
      "Iteration 32986, loss = 0.03547658\n",
      "Iteration 32987, loss = 0.03547686\n",
      "Iteration 32988, loss = 0.03548006\n",
      "Iteration 32989, loss = 0.03546833\n",
      "Iteration 32990, loss = 0.03547410\n",
      "Iteration 32991, loss = 0.03547623\n",
      "Iteration 32992, loss = 0.03547279\n",
      "Iteration 32993, loss = 0.03547137\n",
      "Iteration 32994, loss = 0.03547275\n",
      "Iteration 32995, loss = 0.03546779\n",
      "Iteration 32996, loss = 0.03546860\n",
      "Iteration 32997, loss = 0.03547274\n",
      "Iteration 32998, loss = 0.03546702\n",
      "Iteration 32999, loss = 0.03547235\n",
      "Iteration 33000, loss = 0.03547183\n",
      "Iteration 33001, loss = 0.03547281\n",
      "Iteration 33002, loss = 0.03547542\n",
      "Iteration 33003, loss = 0.03547157\n",
      "Iteration 33004, loss = 0.03547580\n",
      "Iteration 33005, loss = 0.03547567\n",
      "Iteration 33006, loss = 0.03547865\n",
      "Iteration 33007, loss = 0.03547765\n",
      "Iteration 33008, loss = 0.03546891\n",
      "Iteration 33009, loss = 0.03546900\n",
      "Iteration 33010, loss = 0.03547747\n",
      "Iteration 33011, loss = 0.03547315\n",
      "Iteration 33012, loss = 0.03546504\n",
      "Iteration 33013, loss = 0.03546093\n",
      "Iteration 33014, loss = 0.03547049\n",
      "Iteration 33015, loss = 0.03547292\n",
      "Iteration 33016, loss = 0.03547414\n",
      "Iteration 33017, loss = 0.03548035\n",
      "Iteration 33018, loss = 0.03547608\n",
      "Iteration 33019, loss = 0.03546279\n",
      "Iteration 33020, loss = 0.03545921\n",
      "Iteration 33021, loss = 0.03547248\n",
      "Iteration 33022, loss = 0.03546623\n",
      "Iteration 33023, loss = 0.03546196\n",
      "Iteration 33024, loss = 0.03546004\n",
      "Iteration 33025, loss = 0.03546649\n",
      "Iteration 33026, loss = 0.03546436\n",
      "Iteration 33027, loss = 0.03545498\n",
      "Iteration 33028, loss = 0.03546318\n",
      "Iteration 33029, loss = 0.03546442\n",
      "Iteration 33030, loss = 0.03545416\n",
      "Iteration 33031, loss = 0.03545555\n",
      "Iteration 33032, loss = 0.03546191\n",
      "Iteration 33033, loss = 0.03546214\n",
      "Iteration 33034, loss = 0.03545712\n",
      "Iteration 33035, loss = 0.03545518\n",
      "Iteration 33036, loss = 0.03545178\n",
      "Iteration 33037, loss = 0.03545791\n",
      "Iteration 33038, loss = 0.03544935\n",
      "Iteration 33039, loss = 0.03545415\n",
      "Iteration 33040, loss = 0.03546396\n",
      "Iteration 33041, loss = 0.03546289\n",
      "Iteration 33042, loss = 0.03545868\n",
      "Iteration 33043, loss = 0.03545385\n",
      "Iteration 33044, loss = 0.03544626\n",
      "Iteration 33045, loss = 0.03545416\n",
      "Iteration 33046, loss = 0.03544960\n",
      "Iteration 33047, loss = 0.03544924\n",
      "Iteration 33048, loss = 0.03545124\n",
      "Iteration 33049, loss = 0.03544679\n",
      "Iteration 33050, loss = 0.03544388\n",
      "Iteration 33051, loss = 0.03544938\n",
      "Iteration 33052, loss = 0.03545041\n",
      "Iteration 33053, loss = 0.03544869\n",
      "Iteration 33054, loss = 0.03545044\n",
      "Iteration 33055, loss = 0.03544908\n",
      "Iteration 33056, loss = 0.03544513\n",
      "Iteration 33057, loss = 0.03544772\n",
      "Iteration 33058, loss = 0.03544644\n",
      "Iteration 33059, loss = 0.03543688\n",
      "Iteration 33060, loss = 0.03543977\n",
      "Iteration 33061, loss = 0.03544337\n",
      "Iteration 33062, loss = 0.03543699\n",
      "Iteration 33063, loss = 0.03543513\n",
      "Iteration 33064, loss = 0.03544727\n",
      "Iteration 33065, loss = 0.03544752\n",
      "Iteration 33066, loss = 0.03544500\n",
      "Iteration 33067, loss = 0.03543682\n",
      "Iteration 33068, loss = 0.03544210\n",
      "Iteration 33069, loss = 0.03544659\n",
      "Iteration 33070, loss = 0.03544719\n",
      "Iteration 33071, loss = 0.03544629\n",
      "Iteration 33072, loss = 0.03543661\n",
      "Iteration 33073, loss = 0.03543439\n",
      "Iteration 33074, loss = 0.03544103\n",
      "Iteration 33075, loss = 0.03544293\n",
      "Iteration 33076, loss = 0.03543477\n",
      "Iteration 33077, loss = 0.03543370\n",
      "Iteration 33078, loss = 0.03544011\n",
      "Iteration 33079, loss = 0.03544671\n",
      "Iteration 33080, loss = 0.03544522\n",
      "Iteration 33081, loss = 0.03543607\n",
      "Iteration 33082, loss = 0.03542887\n",
      "Iteration 33083, loss = 0.03543744\n",
      "Iteration 33084, loss = 0.03543506\n",
      "Iteration 33085, loss = 0.03542613\n",
      "Iteration 33086, loss = 0.03542715\n",
      "Iteration 33087, loss = 0.03543745\n",
      "Iteration 33088, loss = 0.03543451\n",
      "Iteration 33089, loss = 0.03543500\n",
      "Iteration 33090, loss = 0.03542874\n",
      "Iteration 33091, loss = 0.03543732\n",
      "Iteration 33092, loss = 0.03543669\n",
      "Iteration 33093, loss = 0.03543257\n",
      "Iteration 33094, loss = 0.03543077\n",
      "Iteration 33095, loss = 0.03543068\n",
      "Iteration 33096, loss = 0.03542777\n",
      "Iteration 33097, loss = 0.03542710\n",
      "Iteration 33098, loss = 0.03541918\n",
      "Iteration 33099, loss = 0.03542246\n",
      "Iteration 33100, loss = 0.03542578\n",
      "Iteration 33101, loss = 0.03541472\n",
      "Iteration 33102, loss = 0.03541700\n",
      "Iteration 33103, loss = 0.03541509\n",
      "Iteration 33104, loss = 0.03541748\n",
      "Iteration 33105, loss = 0.03541872\n",
      "Iteration 33106, loss = 0.03541419\n",
      "Iteration 33107, loss = 0.03541766\n",
      "Iteration 33108, loss = 0.03541471\n",
      "Iteration 33109, loss = 0.03542000\n",
      "Iteration 33110, loss = 0.03541794\n",
      "Iteration 33111, loss = 0.03541819\n",
      "Iteration 33112, loss = 0.03541082\n",
      "Iteration 33113, loss = 0.03542035\n",
      "Iteration 33114, loss = 0.03542079\n",
      "Iteration 33115, loss = 0.03541470\n",
      "Iteration 33116, loss = 0.03540998\n",
      "Iteration 33117, loss = 0.03542239\n",
      "Iteration 33118, loss = 0.03542432\n",
      "Iteration 33119, loss = 0.03542287\n",
      "Iteration 33120, loss = 0.03541755\n",
      "Iteration 33121, loss = 0.03542268\n",
      "Iteration 33122, loss = 0.03541774\n",
      "Iteration 33123, loss = 0.03540664\n",
      "Iteration 33124, loss = 0.03541663\n",
      "Iteration 33125, loss = 0.03542442\n",
      "Iteration 33126, loss = 0.03541748\n",
      "Iteration 33127, loss = 0.03540394\n",
      "Iteration 33128, loss = 0.03541052\n",
      "Iteration 33129, loss = 0.03541739\n",
      "Iteration 33130, loss = 0.03541272\n",
      "Iteration 33131, loss = 0.03540652\n",
      "Iteration 33132, loss = 0.03540844\n",
      "Iteration 33133, loss = 0.03540782\n",
      "Iteration 33134, loss = 0.03540697\n",
      "Iteration 33135, loss = 0.03541194\n",
      "Iteration 33136, loss = 0.03541182\n",
      "Iteration 33137, loss = 0.03540316\n",
      "Iteration 33138, loss = 0.03539898\n",
      "Iteration 33139, loss = 0.03540010\n",
      "Iteration 33140, loss = 0.03540919\n",
      "Iteration 33141, loss = 0.03541026\n",
      "Iteration 33142, loss = 0.03540892\n",
      "Iteration 33143, loss = 0.03540481\n",
      "Iteration 33144, loss = 0.03539621\n",
      "Iteration 33145, loss = 0.03540650\n",
      "Iteration 33146, loss = 0.03540590\n",
      "Iteration 33147, loss = 0.03540112\n",
      "Iteration 33148, loss = 0.03539979\n",
      "Iteration 33149, loss = 0.03539573\n",
      "Iteration 33150, loss = 0.03540635\n",
      "Iteration 33151, loss = 0.03540721\n",
      "Iteration 33152, loss = 0.03539394\n",
      "Iteration 33153, loss = 0.03539391\n",
      "Iteration 33154, loss = 0.03540447\n",
      "Iteration 33155, loss = 0.03540484\n",
      "Iteration 33156, loss = 0.03539642\n",
      "Iteration 33157, loss = 0.03539895\n",
      "Iteration 33158, loss = 0.03539853\n",
      "Iteration 33159, loss = 0.03539582\n",
      "Iteration 33160, loss = 0.03540071\n",
      "Iteration 33161, loss = 0.03540344\n",
      "Iteration 33162, loss = 0.03539791\n",
      "Iteration 33163, loss = 0.03538615\n",
      "Iteration 33164, loss = 0.03538759\n",
      "Iteration 33165, loss = 0.03539716\n",
      "Iteration 33166, loss = 0.03539598\n",
      "Iteration 33167, loss = 0.03537948\n",
      "Iteration 33168, loss = 0.03538770\n",
      "Iteration 33169, loss = 0.03538880\n",
      "Iteration 33170, loss = 0.03538470\n",
      "Iteration 33171, loss = 0.03538551\n",
      "Iteration 33172, loss = 0.03538732\n",
      "Iteration 33173, loss = 0.03538022\n",
      "Iteration 33174, loss = 0.03538801\n",
      "Iteration 33175, loss = 0.03538983\n",
      "Iteration 33176, loss = 0.03538648\n",
      "Iteration 33177, loss = 0.03538536\n",
      "Iteration 33178, loss = 0.03538732\n",
      "Iteration 33179, loss = 0.03538131\n",
      "Iteration 33180, loss = 0.03538094\n",
      "Iteration 33181, loss = 0.03538797\n",
      "Iteration 33182, loss = 0.03538906\n",
      "Iteration 33183, loss = 0.03537776\n",
      "Iteration 33184, loss = 0.03537499\n",
      "Iteration 33185, loss = 0.03538433\n",
      "Iteration 33186, loss = 0.03538395\n",
      "Iteration 33187, loss = 0.03537806\n",
      "Iteration 33188, loss = 0.03537719\n",
      "Iteration 33189, loss = 0.03537648\n",
      "Iteration 33190, loss = 0.03538385\n",
      "Iteration 33191, loss = 0.03538363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 33192, loss = 0.03537433\n",
      "Iteration 33193, loss = 0.03537248\n",
      "Iteration 33194, loss = 0.03537960\n",
      "Iteration 33195, loss = 0.03537679\n",
      "Iteration 33196, loss = 0.03537222\n",
      "Iteration 33197, loss = 0.03537573\n",
      "Iteration 33198, loss = 0.03538293\n",
      "Iteration 33199, loss = 0.03537756\n",
      "Iteration 33200, loss = 0.03537126\n",
      "Iteration 33201, loss = 0.03537880\n",
      "Iteration 33202, loss = 0.03537807\n",
      "Iteration 33203, loss = 0.03536961\n",
      "Iteration 33204, loss = 0.03536467\n",
      "Iteration 33205, loss = 0.03537291\n",
      "Iteration 33206, loss = 0.03537237\n",
      "Iteration 33207, loss = 0.03536261\n",
      "Iteration 33208, loss = 0.03536563\n",
      "Iteration 33209, loss = 0.03536548\n",
      "Iteration 33210, loss = 0.03536404\n",
      "Iteration 33211, loss = 0.03536257\n",
      "Iteration 33212, loss = 0.03536138\n",
      "Iteration 33213, loss = 0.03535906\n",
      "Iteration 33214, loss = 0.03536178\n",
      "Iteration 33215, loss = 0.03536213\n",
      "Iteration 33216, loss = 0.03536301\n",
      "Iteration 33217, loss = 0.03536112\n",
      "Iteration 33218, loss = 0.03535766\n",
      "Iteration 33219, loss = 0.03536473\n",
      "Iteration 33220, loss = 0.03537116\n",
      "Iteration 33221, loss = 0.03536241\n",
      "Iteration 33222, loss = 0.03536579\n",
      "Iteration 33223, loss = 0.03536336\n",
      "Iteration 33224, loss = 0.03536105\n",
      "Iteration 33225, loss = 0.03535878\n",
      "Iteration 33226, loss = 0.03536369\n",
      "Iteration 33227, loss = 0.03536571\n",
      "Iteration 33228, loss = 0.03536029\n",
      "Iteration 33229, loss = 0.03535587\n",
      "Iteration 33230, loss = 0.03536467\n",
      "Iteration 33231, loss = 0.03536133\n",
      "Iteration 33232, loss = 0.03536118\n",
      "Iteration 33233, loss = 0.03535933\n",
      "Iteration 33234, loss = 0.03535699\n",
      "Iteration 33235, loss = 0.03536201\n",
      "Iteration 33236, loss = 0.03536354\n",
      "Iteration 33237, loss = 0.03535653\n",
      "Iteration 33238, loss = 0.03535770\n",
      "Iteration 33239, loss = 0.03535228\n",
      "Iteration 33240, loss = 0.03535746\n",
      "Iteration 33241, loss = 0.03536073\n",
      "Iteration 33242, loss = 0.03536502\n",
      "Iteration 33243, loss = 0.03536087\n",
      "Iteration 33244, loss = 0.03535461\n",
      "Iteration 33245, loss = 0.03535338\n",
      "Iteration 33246, loss = 0.03535489\n",
      "Iteration 33247, loss = 0.03535712\n",
      "Iteration 33248, loss = 0.03535927\n",
      "Iteration 33249, loss = 0.03536515\n",
      "Iteration 33250, loss = 0.03535639\n",
      "Iteration 33251, loss = 0.03534997\n",
      "Iteration 33252, loss = 0.03535287\n",
      "Iteration 33253, loss = 0.03536205\n",
      "Iteration 33254, loss = 0.03535851\n",
      "Iteration 33255, loss = 0.03535044\n",
      "Iteration 33256, loss = 0.03534801\n",
      "Iteration 33257, loss = 0.03534493\n",
      "Iteration 33258, loss = 0.03534534\n",
      "Iteration 33259, loss = 0.03534271\n",
      "Iteration 33260, loss = 0.03534444\n",
      "Iteration 33261, loss = 0.03534399\n",
      "Iteration 33262, loss = 0.03534664\n",
      "Iteration 33263, loss = 0.03534288\n",
      "Iteration 33264, loss = 0.03534255\n",
      "Iteration 33265, loss = 0.03534559\n",
      "Iteration 33266, loss = 0.03534843\n",
      "Iteration 33267, loss = 0.03535196\n",
      "Iteration 33268, loss = 0.03534372\n",
      "Iteration 33269, loss = 0.03534310\n",
      "Iteration 33270, loss = 0.03535510\n",
      "Iteration 33271, loss = 0.03534994\n",
      "Iteration 33272, loss = 0.03534539\n",
      "Iteration 33273, loss = 0.03534580\n",
      "Iteration 33274, loss = 0.03533521\n",
      "Iteration 33275, loss = 0.03533630\n",
      "Iteration 33276, loss = 0.03534613\n",
      "Iteration 33277, loss = 0.03535729\n",
      "Iteration 33278, loss = 0.03534877\n",
      "Iteration 33279, loss = 0.03532783\n",
      "Iteration 33280, loss = 0.03534070\n",
      "Iteration 33281, loss = 0.03534965\n",
      "Iteration 33282, loss = 0.03535084\n",
      "Iteration 33283, loss = 0.03534542\n",
      "Iteration 33284, loss = 0.03533547\n",
      "Iteration 33285, loss = 0.03533458\n",
      "Iteration 33286, loss = 0.03533044\n",
      "Iteration 33287, loss = 0.03532513\n",
      "Iteration 33288, loss = 0.03533518\n",
      "Iteration 33289, loss = 0.03533697\n",
      "Iteration 33290, loss = 0.03532836\n",
      "Iteration 33291, loss = 0.03532738\n",
      "Iteration 33292, loss = 0.03533092\n",
      "Iteration 33293, loss = 0.03532959\n",
      "Iteration 33294, loss = 0.03532721\n",
      "Iteration 33295, loss = 0.03532371\n",
      "Iteration 33296, loss = 0.03532082\n",
      "Iteration 33297, loss = 0.03532934\n",
      "Iteration 33298, loss = 0.03533198\n",
      "Iteration 33299, loss = 0.03532831\n",
      "Iteration 33300, loss = 0.03532506\n",
      "Iteration 33301, loss = 0.03532521\n",
      "Iteration 33302, loss = 0.03532387\n",
      "Iteration 33303, loss = 0.03532336\n",
      "Iteration 33304, loss = 0.03532049\n",
      "Iteration 33305, loss = 0.03531880\n",
      "Iteration 33306, loss = 0.03531901\n",
      "Iteration 33307, loss = 0.03532624\n",
      "Iteration 33308, loss = 0.03531999\n",
      "Iteration 33309, loss = 0.03531762\n",
      "Iteration 33310, loss = 0.03532126\n",
      "Iteration 33311, loss = 0.03531861\n",
      "Iteration 33312, loss = 0.03532217\n",
      "Iteration 33313, loss = 0.03531884\n",
      "Iteration 33314, loss = 0.03531796\n",
      "Iteration 33315, loss = 0.03532501\n",
      "Iteration 33316, loss = 0.03532520\n",
      "Iteration 33317, loss = 0.03532168\n",
      "Iteration 33318, loss = 0.03531947\n",
      "Iteration 33319, loss = 0.03531009\n",
      "Iteration 33320, loss = 0.03531978\n",
      "Iteration 33321, loss = 0.03532830\n",
      "Iteration 33322, loss = 0.03532297\n",
      "Iteration 33323, loss = 0.03531644\n",
      "Iteration 33324, loss = 0.03530661\n",
      "Iteration 33325, loss = 0.03531336\n",
      "Iteration 33326, loss = 0.03531408\n",
      "Iteration 33327, loss = 0.03531225\n",
      "Iteration 33328, loss = 0.03531618\n",
      "Iteration 33329, loss = 0.03531416\n",
      "Iteration 33330, loss = 0.03530764\n",
      "Iteration 33331, loss = 0.03530985\n",
      "Iteration 33332, loss = 0.03531503\n",
      "Iteration 33333, loss = 0.03531113\n",
      "Iteration 33334, loss = 0.03530580\n",
      "Iteration 33335, loss = 0.03531088\n",
      "Iteration 33336, loss = 0.03531797\n",
      "Iteration 33337, loss = 0.03531234\n",
      "Iteration 33338, loss = 0.03530261\n",
      "Iteration 33339, loss = 0.03531361\n",
      "Iteration 33340, loss = 0.03531862\n",
      "Iteration 33341, loss = 0.03530936\n",
      "Iteration 33342, loss = 0.03531534\n",
      "Iteration 33343, loss = 0.03531863\n",
      "Iteration 33344, loss = 0.03530971\n",
      "Iteration 33345, loss = 0.03530646\n",
      "Iteration 33346, loss = 0.03531163\n",
      "Iteration 33347, loss = 0.03531337\n",
      "Iteration 33348, loss = 0.03530478\n",
      "Iteration 33349, loss = 0.03530114\n",
      "Iteration 33350, loss = 0.03530497\n",
      "Iteration 33351, loss = 0.03530958\n",
      "Iteration 33352, loss = 0.03530969\n",
      "Iteration 33353, loss = 0.03530477\n",
      "Iteration 33354, loss = 0.03529555\n",
      "Iteration 33355, loss = 0.03530120\n",
      "Iteration 33356, loss = 0.03530739\n",
      "Iteration 33357, loss = 0.03530445\n",
      "Iteration 33358, loss = 0.03529582\n",
      "Iteration 33359, loss = 0.03529797\n",
      "Iteration 33360, loss = 0.03530496\n",
      "Iteration 33361, loss = 0.03530252\n",
      "Iteration 33362, loss = 0.03530770\n",
      "Iteration 33363, loss = 0.03530409\n",
      "Iteration 33364, loss = 0.03528760\n",
      "Iteration 33365, loss = 0.03529474\n",
      "Iteration 33366, loss = 0.03530658\n",
      "Iteration 33367, loss = 0.03531004\n",
      "Iteration 33368, loss = 0.03529848\n",
      "Iteration 33369, loss = 0.03530278\n",
      "Iteration 33370, loss = 0.03529939\n",
      "Iteration 33371, loss = 0.03529103\n",
      "Iteration 33372, loss = 0.03529218\n",
      "Iteration 33373, loss = 0.03529065\n",
      "Iteration 33374, loss = 0.03528794\n",
      "Iteration 33375, loss = 0.03528777\n",
      "Iteration 33376, loss = 0.03529242\n",
      "Iteration 33377, loss = 0.03529043\n",
      "Iteration 33378, loss = 0.03529086\n",
      "Iteration 33379, loss = 0.03528167\n",
      "Iteration 33380, loss = 0.03528314\n",
      "Iteration 33381, loss = 0.03528287\n",
      "Iteration 33382, loss = 0.03528555\n",
      "Iteration 33383, loss = 0.03528505\n",
      "Iteration 33384, loss = 0.03528566\n",
      "Iteration 33385, loss = 0.03528654\n",
      "Iteration 33386, loss = 0.03528425\n",
      "Iteration 33387, loss = 0.03528246\n",
      "Iteration 33388, loss = 0.03528657\n",
      "Iteration 33389, loss = 0.03528995\n",
      "Iteration 33390, loss = 0.03529088\n",
      "Iteration 33391, loss = 0.03528181\n",
      "Iteration 33392, loss = 0.03528356\n",
      "Iteration 33393, loss = 0.03528475\n",
      "Iteration 33394, loss = 0.03528332\n",
      "Iteration 33395, loss = 0.03528209\n",
      "Iteration 33396, loss = 0.03528218\n",
      "Iteration 33397, loss = 0.03528502\n",
      "Iteration 33398, loss = 0.03528402\n",
      "Iteration 33399, loss = 0.03527539\n",
      "Iteration 33400, loss = 0.03527703\n",
      "Iteration 33401, loss = 0.03528190\n",
      "Iteration 33402, loss = 0.03527352\n",
      "Iteration 33403, loss = 0.03527127\n",
      "Iteration 33404, loss = 0.03527714\n",
      "Iteration 33405, loss = 0.03527396\n",
      "Iteration 33406, loss = 0.03527615\n",
      "Iteration 33407, loss = 0.03527100\n",
      "Iteration 33408, loss = 0.03527029\n",
      "Iteration 33409, loss = 0.03527097\n",
      "Iteration 33410, loss = 0.03526886\n",
      "Iteration 33411, loss = 0.03527483\n",
      "Iteration 33412, loss = 0.03527357\n",
      "Iteration 33413, loss = 0.03526954\n",
      "Iteration 33414, loss = 0.03527422\n",
      "Iteration 33415, loss = 0.03527511\n",
      "Iteration 33416, loss = 0.03527116\n",
      "Iteration 33417, loss = 0.03527653\n",
      "Iteration 33418, loss = 0.03527195\n",
      "Iteration 33419, loss = 0.03528079\n",
      "Iteration 33420, loss = 0.03528384\n",
      "Iteration 33421, loss = 0.03528341\n",
      "Iteration 33422, loss = 0.03528077\n",
      "Iteration 33423, loss = 0.03527293\n",
      "Iteration 33424, loss = 0.03526941\n",
      "Iteration 33425, loss = 0.03527210\n",
      "Iteration 33426, loss = 0.03527712\n",
      "Iteration 33427, loss = 0.03527525\n",
      "Iteration 33428, loss = 0.03527349\n",
      "Iteration 33429, loss = 0.03526592\n",
      "Iteration 33430, loss = 0.03527212\n",
      "Iteration 33431, loss = 0.03527215\n",
      "Iteration 33432, loss = 0.03526920\n",
      "Iteration 33433, loss = 0.03525971\n",
      "Iteration 33434, loss = 0.03525701\n",
      "Iteration 33435, loss = 0.03526425\n",
      "Iteration 33436, loss = 0.03526639\n",
      "Iteration 33437, loss = 0.03525750\n",
      "Iteration 33438, loss = 0.03525804\n",
      "Iteration 33439, loss = 0.03526102\n",
      "Iteration 33440, loss = 0.03526161\n",
      "Iteration 33441, loss = 0.03526048\n",
      "Iteration 33442, loss = 0.03526419\n",
      "Iteration 33443, loss = 0.03525744\n",
      "Iteration 33444, loss = 0.03524818\n",
      "Iteration 33445, loss = 0.03525710\n",
      "Iteration 33446, loss = 0.03525487\n",
      "Iteration 33447, loss = 0.03525283\n",
      "Iteration 33448, loss = 0.03525389\n",
      "Iteration 33449, loss = 0.03524871\n",
      "Iteration 33450, loss = 0.03525369\n",
      "Iteration 33451, loss = 0.03525001\n",
      "Iteration 33452, loss = 0.03524794\n",
      "Iteration 33453, loss = 0.03524947\n",
      "Iteration 33454, loss = 0.03525396\n",
      "Iteration 33455, loss = 0.03525332\n",
      "Iteration 33456, loss = 0.03524769\n",
      "Iteration 33457, loss = 0.03524378\n",
      "Iteration 33458, loss = 0.03524685\n",
      "Iteration 33459, loss = 0.03524628\n",
      "Iteration 33460, loss = 0.03524851\n",
      "Iteration 33461, loss = 0.03525105\n",
      "Iteration 33462, loss = 0.03524609\n",
      "Iteration 33463, loss = 0.03525054\n",
      "Iteration 33464, loss = 0.03525541\n",
      "Iteration 33465, loss = 0.03524950\n",
      "Iteration 33466, loss = 0.03524962\n",
      "Iteration 33467, loss = 0.03524432\n",
      "Iteration 33468, loss = 0.03524193\n",
      "Iteration 33469, loss = 0.03525469\n",
      "Iteration 33470, loss = 0.03525230\n",
      "Iteration 33471, loss = 0.03524506\n",
      "Iteration 33472, loss = 0.03524650\n",
      "Iteration 33473, loss = 0.03524757\n",
      "Iteration 33474, loss = 0.03525246\n",
      "Iteration 33475, loss = 0.03525518\n",
      "Iteration 33476, loss = 0.03524505\n",
      "Iteration 33477, loss = 0.03524173\n",
      "Iteration 33478, loss = 0.03525055\n",
      "Iteration 33479, loss = 0.03525439\n",
      "Iteration 33480, loss = 0.03525039\n",
      "Iteration 33481, loss = 0.03523870\n",
      "Iteration 33482, loss = 0.03524813\n",
      "Iteration 33483, loss = 0.03525218\n",
      "Iteration 33484, loss = 0.03524495\n",
      "Iteration 33485, loss = 0.03524143\n",
      "Iteration 33486, loss = 0.03524228\n",
      "Iteration 33487, loss = 0.03524221\n",
      "Iteration 33488, loss = 0.03524629\n",
      "Iteration 33489, loss = 0.03524362\n",
      "Iteration 33490, loss = 0.03523912\n",
      "Iteration 33491, loss = 0.03524128\n",
      "Iteration 33492, loss = 0.03523799\n",
      "Iteration 33493, loss = 0.03524403\n",
      "Iteration 33494, loss = 0.03524024\n",
      "Iteration 33495, loss = 0.03523556\n",
      "Iteration 33496, loss = 0.03523993\n",
      "Iteration 33497, loss = 0.03523981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 33498, loss = 0.03522953\n",
      "Iteration 33499, loss = 0.03522869\n",
      "Iteration 33500, loss = 0.03523635\n",
      "Iteration 33501, loss = 0.03523220\n",
      "Iteration 33502, loss = 0.03522506\n",
      "Iteration 33503, loss = 0.03522858\n",
      "Iteration 33504, loss = 0.03522854\n",
      "Iteration 33505, loss = 0.03522457\n",
      "Iteration 33506, loss = 0.03521998\n",
      "Iteration 33507, loss = 0.03522301\n",
      "Iteration 33508, loss = 0.03522810\n",
      "Iteration 33509, loss = 0.03522553\n",
      "Iteration 33510, loss = 0.03522365\n",
      "Iteration 33511, loss = 0.03521919\n",
      "Iteration 33512, loss = 0.03522404\n",
      "Iteration 33513, loss = 0.03523182\n",
      "Iteration 33514, loss = 0.03522970\n",
      "Iteration 33515, loss = 0.03522224\n",
      "Iteration 33516, loss = 0.03521853\n",
      "Iteration 33517, loss = 0.03522618\n",
      "Iteration 33518, loss = 0.03523663\n",
      "Iteration 33519, loss = 0.03524012\n",
      "Iteration 33520, loss = 0.03523444\n",
      "Iteration 33521, loss = 0.03522445\n",
      "Iteration 33522, loss = 0.03521662\n",
      "Iteration 33523, loss = 0.03521819\n",
      "Iteration 33524, loss = 0.03521797\n",
      "Iteration 33525, loss = 0.03521877\n",
      "Iteration 33526, loss = 0.03521725\n",
      "Iteration 33527, loss = 0.03521437\n",
      "Iteration 33528, loss = 0.03521898\n",
      "Iteration 33529, loss = 0.03522555\n",
      "Iteration 33530, loss = 0.03522389\n",
      "Iteration 33531, loss = 0.03521664\n",
      "Iteration 33532, loss = 0.03521701\n",
      "Iteration 33533, loss = 0.03521801\n",
      "Iteration 33534, loss = 0.03522648\n",
      "Iteration 33535, loss = 0.03522317\n",
      "Iteration 33536, loss = 0.03521840\n",
      "Iteration 33537, loss = 0.03521939\n",
      "Iteration 33538, loss = 0.03521972\n",
      "Iteration 33539, loss = 0.03522430\n",
      "Iteration 33540, loss = 0.03522338\n",
      "Iteration 33541, loss = 0.03521238\n",
      "Iteration 33542, loss = 0.03521592\n",
      "Iteration 33543, loss = 0.03522643\n",
      "Iteration 33544, loss = 0.03522799\n",
      "Iteration 33545, loss = 0.03521182\n",
      "Iteration 33546, loss = 0.03521233\n",
      "Iteration 33547, loss = 0.03522320\n",
      "Iteration 33548, loss = 0.03522608\n",
      "Iteration 33549, loss = 0.03522884\n",
      "Iteration 33550, loss = 0.03522576\n",
      "Iteration 33551, loss = 0.03521512\n",
      "Iteration 33552, loss = 0.03521019\n",
      "Iteration 33553, loss = 0.03521203\n",
      "Iteration 33554, loss = 0.03520914\n",
      "Iteration 33555, loss = 0.03520180\n",
      "Iteration 33556, loss = 0.03520420\n",
      "Iteration 33557, loss = 0.03521246\n",
      "Iteration 33558, loss = 0.03521257\n",
      "Iteration 33559, loss = 0.03520622\n",
      "Iteration 33560, loss = 0.03520764\n",
      "Iteration 33561, loss = 0.03520436\n",
      "Iteration 33562, loss = 0.03519734\n",
      "Iteration 33563, loss = 0.03520116\n",
      "Iteration 33564, loss = 0.03519716\n",
      "Iteration 33565, loss = 0.03519943\n",
      "Iteration 33566, loss = 0.03520018\n",
      "Iteration 33567, loss = 0.03520282\n",
      "Iteration 33568, loss = 0.03520105\n",
      "Iteration 33569, loss = 0.03519496\n",
      "Iteration 33570, loss = 0.03519833\n",
      "Iteration 33571, loss = 0.03519899\n",
      "Iteration 33572, loss = 0.03519464\n",
      "Iteration 33573, loss = 0.03519084\n",
      "Iteration 33574, loss = 0.03520085\n",
      "Iteration 33575, loss = 0.03520115\n",
      "Iteration 33576, loss = 0.03519221\n",
      "Iteration 33577, loss = 0.03518945\n",
      "Iteration 33578, loss = 0.03519145\n",
      "Iteration 33579, loss = 0.03519448\n",
      "Iteration 33580, loss = 0.03519284\n",
      "Iteration 33581, loss = 0.03518915\n",
      "Iteration 33582, loss = 0.03519853\n",
      "Iteration 33583, loss = 0.03520274\n",
      "Iteration 33584, loss = 0.03520370\n",
      "Iteration 33585, loss = 0.03519706\n",
      "Iteration 33586, loss = 0.03518883\n",
      "Iteration 33587, loss = 0.03519272\n",
      "Iteration 33588, loss = 0.03518602\n",
      "Iteration 33589, loss = 0.03518929\n",
      "Iteration 33590, loss = 0.03519299\n",
      "Iteration 33591, loss = 0.03519036\n",
      "Iteration 33592, loss = 0.03518583\n",
      "Iteration 33593, loss = 0.03518907\n",
      "Iteration 33594, loss = 0.03518877\n",
      "Iteration 33595, loss = 0.03519121\n",
      "Iteration 33596, loss = 0.03519093\n",
      "Iteration 33597, loss = 0.03518731\n",
      "Iteration 33598, loss = 0.03518721\n",
      "Iteration 33599, loss = 0.03518896\n",
      "Iteration 33600, loss = 0.03519041\n",
      "Iteration 33601, loss = 0.03518922\n",
      "Iteration 33602, loss = 0.03519011\n",
      "Iteration 33603, loss = 0.03518769\n",
      "Iteration 33604, loss = 0.03518702\n",
      "Iteration 33605, loss = 0.03519288\n",
      "Iteration 33606, loss = 0.03519530\n",
      "Iteration 33607, loss = 0.03518569\n",
      "Iteration 33608, loss = 0.03519056\n",
      "Iteration 33609, loss = 0.03518858\n",
      "Iteration 33610, loss = 0.03518926\n",
      "Iteration 33611, loss = 0.03519473\n",
      "Iteration 33612, loss = 0.03518477\n",
      "Iteration 33613, loss = 0.03517545\n",
      "Iteration 33614, loss = 0.03518354\n",
      "Iteration 33615, loss = 0.03517959\n",
      "Iteration 33616, loss = 0.03517966\n",
      "Iteration 33617, loss = 0.03517719\n",
      "Iteration 33618, loss = 0.03517646\n",
      "Iteration 33619, loss = 0.03517855\n",
      "Iteration 33620, loss = 0.03517635\n",
      "Iteration 33621, loss = 0.03517463\n",
      "Iteration 33622, loss = 0.03517694\n",
      "Iteration 33623, loss = 0.03517754\n",
      "Iteration 33624, loss = 0.03517591\n",
      "Iteration 33625, loss = 0.03517227\n",
      "Iteration 33626, loss = 0.03517026\n",
      "Iteration 33627, loss = 0.03517070\n",
      "Iteration 33628, loss = 0.03516826\n",
      "Iteration 33629, loss = 0.03516673\n",
      "Iteration 33630, loss = 0.03517192\n",
      "Iteration 33631, loss = 0.03516931\n",
      "Iteration 33632, loss = 0.03516391\n",
      "Iteration 33633, loss = 0.03517200\n",
      "Iteration 33634, loss = 0.03517581\n",
      "Iteration 33635, loss = 0.03517529\n",
      "Iteration 33636, loss = 0.03516602\n",
      "Iteration 33637, loss = 0.03517031\n",
      "Iteration 33638, loss = 0.03517354\n",
      "Iteration 33639, loss = 0.03517341\n",
      "Iteration 33640, loss = 0.03516697\n",
      "Iteration 33641, loss = 0.03516818\n",
      "Iteration 33642, loss = 0.03516760\n",
      "Iteration 33643, loss = 0.03516553\n",
      "Iteration 33644, loss = 0.03516336\n",
      "Iteration 33645, loss = 0.03515957\n",
      "Iteration 33646, loss = 0.03516468\n",
      "Iteration 33647, loss = 0.03516613\n",
      "Iteration 33648, loss = 0.03517139\n",
      "Iteration 33649, loss = 0.03517005\n",
      "Iteration 33650, loss = 0.03516216\n",
      "Iteration 33651, loss = 0.03515930\n",
      "Iteration 33652, loss = 0.03516122\n",
      "Iteration 33653, loss = 0.03516139\n",
      "Iteration 33654, loss = 0.03516065\n",
      "Iteration 33655, loss = 0.03516148\n",
      "Iteration 33656, loss = 0.03516735\n",
      "Iteration 33657, loss = 0.03517039\n",
      "Iteration 33658, loss = 0.03516695\n",
      "Iteration 33659, loss = 0.03515894\n",
      "Iteration 33660, loss = 0.03516158\n",
      "Iteration 33661, loss = 0.03516726\n",
      "Iteration 33662, loss = 0.03517063\n",
      "Iteration 33663, loss = 0.03516100\n",
      "Iteration 33664, loss = 0.03515152\n",
      "Iteration 33665, loss = 0.03515819\n",
      "Iteration 33666, loss = 0.03516003\n",
      "Iteration 33667, loss = 0.03514818\n",
      "Iteration 33668, loss = 0.03515338\n",
      "Iteration 33669, loss = 0.03516198\n",
      "Iteration 33670, loss = 0.03516266\n",
      "Iteration 33671, loss = 0.03516556\n",
      "Iteration 33672, loss = 0.03515868\n",
      "Iteration 33673, loss = 0.03515015\n",
      "Iteration 33674, loss = 0.03515850\n",
      "Iteration 33675, loss = 0.03515913\n",
      "Iteration 33676, loss = 0.03514983\n",
      "Iteration 33677, loss = 0.03514519\n",
      "Iteration 33678, loss = 0.03514637\n",
      "Iteration 33679, loss = 0.03514405\n",
      "Iteration 33680, loss = 0.03514877\n",
      "Iteration 33681, loss = 0.03514444\n",
      "Iteration 33682, loss = 0.03515320\n",
      "Iteration 33683, loss = 0.03515056\n",
      "Iteration 33684, loss = 0.03514883\n",
      "Iteration 33685, loss = 0.03515076\n",
      "Iteration 33686, loss = 0.03515504\n",
      "Iteration 33687, loss = 0.03515080\n",
      "Iteration 33688, loss = 0.03514780\n",
      "Iteration 33689, loss = 0.03514084\n",
      "Iteration 33690, loss = 0.03514408\n",
      "Iteration 33691, loss = 0.03514549\n",
      "Iteration 33692, loss = 0.03513809\n",
      "Iteration 33693, loss = 0.03513925\n",
      "Iteration 33694, loss = 0.03513631\n",
      "Iteration 33695, loss = 0.03513482\n",
      "Iteration 33696, loss = 0.03513635\n",
      "Iteration 33697, loss = 0.03513346\n",
      "Iteration 33698, loss = 0.03513304\n",
      "Iteration 33699, loss = 0.03514081\n",
      "Iteration 33700, loss = 0.03513390\n",
      "Iteration 33701, loss = 0.03514017\n",
      "Iteration 33702, loss = 0.03514323\n",
      "Iteration 33703, loss = 0.03514243\n",
      "Iteration 33704, loss = 0.03514101\n",
      "Iteration 33705, loss = 0.03513344\n",
      "Iteration 33706, loss = 0.03513083\n",
      "Iteration 33707, loss = 0.03513457\n",
      "Iteration 33708, loss = 0.03513528\n",
      "Iteration 33709, loss = 0.03512309\n",
      "Iteration 33710, loss = 0.03513176\n",
      "Iteration 33711, loss = 0.03513121\n",
      "Iteration 33712, loss = 0.03512430\n",
      "Iteration 33713, loss = 0.03512787\n",
      "Iteration 33714, loss = 0.03512857\n",
      "Iteration 33715, loss = 0.03512378\n",
      "Iteration 33716, loss = 0.03513141\n",
      "Iteration 33717, loss = 0.03512990\n",
      "Iteration 33718, loss = 0.03512880\n",
      "Iteration 33719, loss = 0.03512723\n",
      "Iteration 33720, loss = 0.03513288\n",
      "Iteration 33721, loss = 0.03512962\n",
      "Iteration 33722, loss = 0.03512735\n",
      "Iteration 33723, loss = 0.03512646\n",
      "Iteration 33724, loss = 0.03512747\n",
      "Iteration 33725, loss = 0.03513061\n",
      "Iteration 33726, loss = 0.03512790\n",
      "Iteration 33727, loss = 0.03512627\n",
      "Iteration 33728, loss = 0.03511860\n",
      "Iteration 33729, loss = 0.03512202\n",
      "Iteration 33730, loss = 0.03512684\n",
      "Iteration 33731, loss = 0.03511670\n",
      "Iteration 33732, loss = 0.03511852\n",
      "Iteration 33733, loss = 0.03512189\n",
      "Iteration 33734, loss = 0.03512479\n",
      "Iteration 33735, loss = 0.03512096\n",
      "Iteration 33736, loss = 0.03512061\n",
      "Iteration 33737, loss = 0.03512410\n",
      "Iteration 33738, loss = 0.03512190\n",
      "Iteration 33739, loss = 0.03511732\n",
      "Iteration 33740, loss = 0.03512685\n",
      "Iteration 33741, loss = 0.03512819\n",
      "Iteration 33742, loss = 0.03512544\n",
      "Iteration 33743, loss = 0.03511890\n",
      "Iteration 33744, loss = 0.03511942\n",
      "Iteration 33745, loss = 0.03512256\n",
      "Iteration 33746, loss = 0.03511682\n",
      "Iteration 33747, loss = 0.03511252\n",
      "Iteration 33748, loss = 0.03512147\n",
      "Iteration 33749, loss = 0.03512126\n",
      "Iteration 33750, loss = 0.03511142\n",
      "Iteration 33751, loss = 0.03510982\n",
      "Iteration 33752, loss = 0.03511647\n",
      "Iteration 33753, loss = 0.03511876\n",
      "Iteration 33754, loss = 0.03511303\n",
      "Iteration 33755, loss = 0.03511587\n",
      "Iteration 33756, loss = 0.03511353\n",
      "Iteration 33757, loss = 0.03510973\n",
      "Iteration 33758, loss = 0.03510694\n",
      "Iteration 33759, loss = 0.03511327\n",
      "Iteration 33760, loss = 0.03510971\n",
      "Iteration 33761, loss = 0.03510753\n",
      "Iteration 33762, loss = 0.03511358\n",
      "Iteration 33763, loss = 0.03511295\n",
      "Iteration 33764, loss = 0.03511083\n",
      "Iteration 33765, loss = 0.03510790\n",
      "Iteration 33766, loss = 0.03510689\n",
      "Iteration 33767, loss = 0.03511771\n",
      "Iteration 33768, loss = 0.03511980\n",
      "Iteration 33769, loss = 0.03512016\n",
      "Iteration 33770, loss = 0.03511435\n",
      "Iteration 33771, loss = 0.03511331\n",
      "Iteration 33772, loss = 0.03511129\n",
      "Iteration 33773, loss = 0.03510914\n",
      "Iteration 33774, loss = 0.03511351\n",
      "Iteration 33775, loss = 0.03511895\n",
      "Iteration 33776, loss = 0.03511225\n",
      "Iteration 33777, loss = 0.03511424\n",
      "Iteration 33778, loss = 0.03511638\n",
      "Iteration 33779, loss = 0.03510847\n",
      "Iteration 33780, loss = 0.03511129\n",
      "Iteration 33781, loss = 0.03511773\n",
      "Iteration 33782, loss = 0.03511284\n",
      "Iteration 33783, loss = 0.03510297\n",
      "Iteration 33784, loss = 0.03510650\n",
      "Iteration 33785, loss = 0.03510893\n",
      "Iteration 33786, loss = 0.03510359\n",
      "Iteration 33787, loss = 0.03510134\n",
      "Iteration 33788, loss = 0.03510667\n",
      "Iteration 33789, loss = 0.03510856\n",
      "Iteration 33790, loss = 0.03510277\n",
      "Iteration 33791, loss = 0.03510295\n",
      "Iteration 33792, loss = 0.03511081\n",
      "Iteration 33793, loss = 0.03510426\n",
      "Iteration 33794, loss = 0.03509859\n",
      "Iteration 33795, loss = 0.03511171\n",
      "Iteration 33796, loss = 0.03511650\n",
      "Iteration 33797, loss = 0.03511126\n",
      "Iteration 33798, loss = 0.03510830\n",
      "Iteration 33799, loss = 0.03510825\n",
      "Iteration 33800, loss = 0.03510738\n",
      "Iteration 33801, loss = 0.03510684\n",
      "Iteration 33802, loss = 0.03509944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 33803, loss = 0.03509295\n",
      "Iteration 33804, loss = 0.03509375\n",
      "Iteration 33805, loss = 0.03509514\n",
      "Iteration 33806, loss = 0.03508845\n",
      "Iteration 33807, loss = 0.03508884\n",
      "Iteration 33808, loss = 0.03509440\n",
      "Iteration 33809, loss = 0.03508977\n",
      "Iteration 33810, loss = 0.03508989\n",
      "Iteration 33811, loss = 0.03509005\n",
      "Iteration 33812, loss = 0.03509047\n",
      "Iteration 33813, loss = 0.03508442\n",
      "Iteration 33814, loss = 0.03508562\n",
      "Iteration 33815, loss = 0.03508823\n",
      "Iteration 33816, loss = 0.03508408\n",
      "Iteration 33817, loss = 0.03508247\n",
      "Iteration 33818, loss = 0.03508476\n",
      "Iteration 33819, loss = 0.03508691\n",
      "Iteration 33820, loss = 0.03508605\n",
      "Iteration 33821, loss = 0.03508041\n",
      "Iteration 33822, loss = 0.03507636\n",
      "Iteration 33823, loss = 0.03507901\n",
      "Iteration 33824, loss = 0.03507669\n",
      "Iteration 33825, loss = 0.03507947\n",
      "Iteration 33826, loss = 0.03507548\n",
      "Iteration 33827, loss = 0.03508051\n",
      "Iteration 33828, loss = 0.03508251\n",
      "Iteration 33829, loss = 0.03507955\n",
      "Iteration 33830, loss = 0.03508200\n",
      "Iteration 33831, loss = 0.03507681\n",
      "Iteration 33832, loss = 0.03507444\n",
      "Iteration 33833, loss = 0.03507698\n",
      "Iteration 33834, loss = 0.03507475\n",
      "Iteration 33835, loss = 0.03507984\n",
      "Iteration 33836, loss = 0.03508165\n",
      "Iteration 33837, loss = 0.03508193\n",
      "Iteration 33838, loss = 0.03507844\n",
      "Iteration 33839, loss = 0.03508113\n",
      "Iteration 33840, loss = 0.03507953\n",
      "Iteration 33841, loss = 0.03507325\n",
      "Iteration 33842, loss = 0.03507687\n",
      "Iteration 33843, loss = 0.03507921\n",
      "Iteration 33844, loss = 0.03507673\n",
      "Iteration 33845, loss = 0.03506875\n",
      "Iteration 33846, loss = 0.03507575\n",
      "Iteration 33847, loss = 0.03507586\n",
      "Iteration 33848, loss = 0.03507566\n",
      "Iteration 33849, loss = 0.03507447\n",
      "Iteration 33850, loss = 0.03507246\n",
      "Iteration 33851, loss = 0.03506756\n",
      "Iteration 33852, loss = 0.03507376\n",
      "Iteration 33853, loss = 0.03506996\n",
      "Iteration 33854, loss = 0.03507994\n",
      "Iteration 33855, loss = 0.03507575\n",
      "Iteration 33856, loss = 0.03506926\n",
      "Iteration 33857, loss = 0.03506982\n",
      "Iteration 33858, loss = 0.03507008\n",
      "Iteration 33859, loss = 0.03506857\n",
      "Iteration 33860, loss = 0.03507608\n",
      "Iteration 33861, loss = 0.03507332\n",
      "Iteration 33862, loss = 0.03506207\n",
      "Iteration 33863, loss = 0.03506642\n",
      "Iteration 33864, loss = 0.03507700\n",
      "Iteration 33865, loss = 0.03507215\n",
      "Iteration 33866, loss = 0.03506512\n",
      "Iteration 33867, loss = 0.03506771\n",
      "Iteration 33868, loss = 0.03506121\n",
      "Iteration 33869, loss = 0.03506438\n",
      "Iteration 33870, loss = 0.03507026\n",
      "Iteration 33871, loss = 0.03506856\n",
      "Iteration 33872, loss = 0.03505988\n",
      "Iteration 33873, loss = 0.03506025\n",
      "Iteration 33874, loss = 0.03506372\n",
      "Iteration 33875, loss = 0.03506026\n",
      "Iteration 33876, loss = 0.03506108\n",
      "Iteration 33877, loss = 0.03506247\n",
      "Iteration 33878, loss = 0.03505877\n",
      "Iteration 33879, loss = 0.03505294\n",
      "Iteration 33880, loss = 0.03505725\n",
      "Iteration 33881, loss = 0.03506317\n",
      "Iteration 33882, loss = 0.03506079\n",
      "Iteration 33883, loss = 0.03505400\n",
      "Iteration 33884, loss = 0.03505584\n",
      "Iteration 33885, loss = 0.03505918\n",
      "Iteration 33886, loss = 0.03506488\n",
      "Iteration 33887, loss = 0.03506131\n",
      "Iteration 33888, loss = 0.03505275\n",
      "Iteration 33889, loss = 0.03505424\n",
      "Iteration 33890, loss = 0.03506101\n",
      "Iteration 33891, loss = 0.03505962\n",
      "Iteration 33892, loss = 0.03505775\n",
      "Iteration 33893, loss = 0.03506386\n",
      "Iteration 33894, loss = 0.03506184\n",
      "Iteration 33895, loss = 0.03506246\n",
      "Iteration 33896, loss = 0.03505962\n",
      "Iteration 33897, loss = 0.03505513\n",
      "Iteration 33898, loss = 0.03504960\n",
      "Iteration 33899, loss = 0.03505083\n",
      "Iteration 33900, loss = 0.03505594\n",
      "Iteration 33901, loss = 0.03505016\n",
      "Iteration 33902, loss = 0.03505144\n",
      "Iteration 33903, loss = 0.03505590\n",
      "Iteration 33904, loss = 0.03505237\n",
      "Iteration 33905, loss = 0.03504491\n",
      "Iteration 33906, loss = 0.03504809\n",
      "Iteration 33907, loss = 0.03504857\n",
      "Iteration 33908, loss = 0.03505167\n",
      "Iteration 33909, loss = 0.03505593\n",
      "Iteration 33910, loss = 0.03505105\n",
      "Iteration 33911, loss = 0.03504291\n",
      "Iteration 33912, loss = 0.03505140\n",
      "Iteration 33913, loss = 0.03505096\n",
      "Iteration 33914, loss = 0.03504263\n",
      "Iteration 33915, loss = 0.03504484\n",
      "Iteration 33916, loss = 0.03504015\n",
      "Iteration 33917, loss = 0.03504370\n",
      "Iteration 33918, loss = 0.03504622\n",
      "Iteration 33919, loss = 0.03505036\n",
      "Iteration 33920, loss = 0.03504748\n",
      "Iteration 33921, loss = 0.03504596\n",
      "Iteration 33922, loss = 0.03503787\n",
      "Iteration 33923, loss = 0.03503856\n",
      "Iteration 33924, loss = 0.03503510\n",
      "Iteration 33925, loss = 0.03503748\n",
      "Iteration 33926, loss = 0.03504130\n",
      "Iteration 33927, loss = 0.03503575\n",
      "Iteration 33928, loss = 0.03503437\n",
      "Iteration 33929, loss = 0.03503437\n",
      "Iteration 33930, loss = 0.03503487\n",
      "Iteration 33931, loss = 0.03503437\n",
      "Iteration 33932, loss = 0.03503396\n",
      "Iteration 33933, loss = 0.03502887\n",
      "Iteration 33934, loss = 0.03504131\n",
      "Iteration 33935, loss = 0.03504199\n",
      "Iteration 33936, loss = 0.03503155\n",
      "Iteration 33937, loss = 0.03502940\n",
      "Iteration 33938, loss = 0.03503072\n",
      "Iteration 33939, loss = 0.03503071\n",
      "Iteration 33940, loss = 0.03503178\n",
      "Iteration 33941, loss = 0.03502217\n",
      "Iteration 33942, loss = 0.03502762\n",
      "Iteration 33943, loss = 0.03503917\n",
      "Iteration 33944, loss = 0.03503321\n",
      "Iteration 33945, loss = 0.03503529\n",
      "Iteration 33946, loss = 0.03503644\n",
      "Iteration 33947, loss = 0.03503151\n",
      "Iteration 33948, loss = 0.03503235\n",
      "Iteration 33949, loss = 0.03504361\n",
      "Iteration 33950, loss = 0.03503747\n",
      "Iteration 33951, loss = 0.03503072\n",
      "Iteration 33952, loss = 0.03502999\n",
      "Iteration 33953, loss = 0.03504264\n",
      "Iteration 33954, loss = 0.03505099\n",
      "Iteration 33955, loss = 0.03504906\n",
      "Iteration 33956, loss = 0.03504092\n",
      "Iteration 33957, loss = 0.03502580\n",
      "Iteration 33958, loss = 0.03502535\n",
      "Iteration 33959, loss = 0.03503431\n",
      "Iteration 33960, loss = 0.03502822\n",
      "Iteration 33961, loss = 0.03501689\n",
      "Iteration 33962, loss = 0.03502400\n",
      "Iteration 33963, loss = 0.03502789\n",
      "Iteration 33964, loss = 0.03503188\n",
      "Iteration 33965, loss = 0.03502295\n",
      "Iteration 33966, loss = 0.03502171\n",
      "Iteration 33967, loss = 0.03502503\n",
      "Iteration 33968, loss = 0.03502390\n",
      "Iteration 33969, loss = 0.03501953\n",
      "Iteration 33970, loss = 0.03501173\n",
      "Iteration 33971, loss = 0.03501906\n",
      "Iteration 33972, loss = 0.03502436\n",
      "Iteration 33973, loss = 0.03501497\n",
      "Iteration 33974, loss = 0.03501804\n",
      "Iteration 33975, loss = 0.03501793\n",
      "Iteration 33976, loss = 0.03502239\n",
      "Iteration 33977, loss = 0.03501994\n",
      "Iteration 33978, loss = 0.03501501\n",
      "Iteration 33979, loss = 0.03501648\n",
      "Iteration 33980, loss = 0.03501916\n",
      "Iteration 33981, loss = 0.03501729\n",
      "Iteration 33982, loss = 0.03501364\n",
      "Iteration 33983, loss = 0.03501157\n",
      "Iteration 33984, loss = 0.03500831\n",
      "Iteration 33985, loss = 0.03501818\n",
      "Iteration 33986, loss = 0.03501649\n",
      "Iteration 33987, loss = 0.03501632\n",
      "Iteration 33988, loss = 0.03501347\n",
      "Iteration 33989, loss = 0.03500365\n",
      "Iteration 33990, loss = 0.03500701\n",
      "Iteration 33991, loss = 0.03500968\n",
      "Iteration 33992, loss = 0.03500635\n",
      "Iteration 33993, loss = 0.03500241\n",
      "Iteration 33994, loss = 0.03500607\n",
      "Iteration 33995, loss = 0.03500838\n",
      "Iteration 33996, loss = 0.03501451\n",
      "Iteration 33997, loss = 0.03501594\n",
      "Iteration 33998, loss = 0.03500893\n",
      "Iteration 33999, loss = 0.03501310\n",
      "Iteration 34000, loss = 0.03501259\n",
      "Iteration 34001, loss = 0.03501522\n",
      "Iteration 34002, loss = 0.03502130\n",
      "Iteration 34003, loss = 0.03501837\n",
      "Iteration 34004, loss = 0.03500620\n",
      "Iteration 34005, loss = 0.03499941\n",
      "Iteration 34006, loss = 0.03501660\n",
      "Iteration 34007, loss = 0.03502554\n",
      "Iteration 34008, loss = 0.03501763\n",
      "Iteration 34009, loss = 0.03500526\n",
      "Iteration 34010, loss = 0.03500810\n",
      "Iteration 34011, loss = 0.03500854\n",
      "Iteration 34012, loss = 0.03500742\n",
      "Iteration 34013, loss = 0.03501155\n",
      "Iteration 34014, loss = 0.03501942\n",
      "Iteration 34015, loss = 0.03501207\n",
      "Iteration 34016, loss = 0.03499615\n",
      "Iteration 34017, loss = 0.03499781\n",
      "Iteration 34018, loss = 0.03500344\n",
      "Iteration 34019, loss = 0.03500075\n",
      "Iteration 34020, loss = 0.03499283\n",
      "Iteration 34021, loss = 0.03499312\n",
      "Iteration 34022, loss = 0.03500115\n",
      "Iteration 34023, loss = 0.03499907\n",
      "Iteration 34024, loss = 0.03499468\n",
      "Iteration 34025, loss = 0.03499531\n",
      "Iteration 34026, loss = 0.03499611\n",
      "Iteration 34027, loss = 0.03499852\n",
      "Iteration 34028, loss = 0.03499992\n",
      "Iteration 34029, loss = 0.03499267\n",
      "Iteration 34030, loss = 0.03498984\n",
      "Iteration 34031, loss = 0.03499169\n",
      "Iteration 34032, loss = 0.03499922\n",
      "Iteration 34033, loss = 0.03500349\n",
      "Iteration 34034, loss = 0.03499992\n",
      "Iteration 34035, loss = 0.03499219\n",
      "Iteration 34036, loss = 0.03498933\n",
      "Iteration 34037, loss = 0.03498575\n",
      "Iteration 34038, loss = 0.03498128\n",
      "Iteration 34039, loss = 0.03498737\n",
      "Iteration 34040, loss = 0.03498330\n",
      "Iteration 34041, loss = 0.03498103\n",
      "Iteration 34042, loss = 0.03499103\n",
      "Iteration 34043, loss = 0.03499022\n",
      "Iteration 34044, loss = 0.03498661\n",
      "Iteration 34045, loss = 0.03498949\n",
      "Iteration 34046, loss = 0.03498332\n",
      "Iteration 34047, loss = 0.03497476\n",
      "Iteration 34048, loss = 0.03498673\n",
      "Iteration 34049, loss = 0.03498872\n",
      "Iteration 34050, loss = 0.03498151\n",
      "Iteration 34051, loss = 0.03498280\n",
      "Iteration 34052, loss = 0.03498568\n",
      "Iteration 34053, loss = 0.03499065\n",
      "Iteration 34054, loss = 0.03498507\n",
      "Iteration 34055, loss = 0.03497323\n",
      "Iteration 34056, loss = 0.03497676\n",
      "Iteration 34057, loss = 0.03498433\n",
      "Iteration 34058, loss = 0.03498112\n",
      "Iteration 34059, loss = 0.03497955\n",
      "Iteration 34060, loss = 0.03498213\n",
      "Iteration 34061, loss = 0.03498218\n",
      "Iteration 34062, loss = 0.03498041\n",
      "Iteration 34063, loss = 0.03497167\n",
      "Iteration 34064, loss = 0.03497435\n",
      "Iteration 34065, loss = 0.03497461\n",
      "Iteration 34066, loss = 0.03497170\n",
      "Iteration 34067, loss = 0.03496833\n",
      "Iteration 34068, loss = 0.03497202\n",
      "Iteration 34069, loss = 0.03497319\n",
      "Iteration 34070, loss = 0.03497062\n",
      "Iteration 34071, loss = 0.03497256\n",
      "Iteration 34072, loss = 0.03498087\n",
      "Iteration 34073, loss = 0.03497797\n",
      "Iteration 34074, loss = 0.03497148\n",
      "Iteration 34075, loss = 0.03496807\n",
      "Iteration 34076, loss = 0.03497191\n",
      "Iteration 34077, loss = 0.03497772\n",
      "Iteration 34078, loss = 0.03496945\n",
      "Iteration 34079, loss = 0.03497573\n",
      "Iteration 34080, loss = 0.03497560\n",
      "Iteration 34081, loss = 0.03497753\n",
      "Iteration 34082, loss = 0.03498325\n",
      "Iteration 34083, loss = 0.03498179\n",
      "Iteration 34084, loss = 0.03497251\n",
      "Iteration 34085, loss = 0.03496737\n",
      "Iteration 34086, loss = 0.03497421\n",
      "Iteration 34087, loss = 0.03497696\n",
      "Iteration 34088, loss = 0.03497474\n",
      "Iteration 34089, loss = 0.03496541\n",
      "Iteration 34090, loss = 0.03496600\n",
      "Iteration 34091, loss = 0.03497116\n",
      "Iteration 34092, loss = 0.03497340\n",
      "Iteration 34093, loss = 0.03497239\n",
      "Iteration 34094, loss = 0.03496671\n",
      "Iteration 34095, loss = 0.03496252\n",
      "Iteration 34096, loss = 0.03496326\n",
      "Iteration 34097, loss = 0.03497043\n",
      "Iteration 34098, loss = 0.03497507\n",
      "Iteration 34099, loss = 0.03496338\n",
      "Iteration 34100, loss = 0.03495794\n",
      "Iteration 34101, loss = 0.03497080\n",
      "Iteration 34102, loss = 0.03497399\n",
      "Iteration 34103, loss = 0.03497600\n",
      "Iteration 34104, loss = 0.03497169\n",
      "Iteration 34105, loss = 0.03495959\n",
      "Iteration 34106, loss = 0.03495080\n",
      "Iteration 34107, loss = 0.03497356\n",
      "Iteration 34108, loss = 0.03497246\n",
      "Iteration 34109, loss = 0.03496418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 34110, loss = 0.03495367\n",
      "Iteration 34111, loss = 0.03495971\n",
      "Iteration 34112, loss = 0.03496165\n",
      "Iteration 34113, loss = 0.03495886\n",
      "Iteration 34114, loss = 0.03495249\n",
      "Iteration 34115, loss = 0.03495485\n",
      "Iteration 34116, loss = 0.03494896\n",
      "Iteration 34117, loss = 0.03495084\n",
      "Iteration 34118, loss = 0.03495761\n",
      "Iteration 34119, loss = 0.03495434\n",
      "Iteration 34120, loss = 0.03495051\n",
      "Iteration 34121, loss = 0.03495129\n",
      "Iteration 34122, loss = 0.03496653\n",
      "Iteration 34123, loss = 0.03496651\n",
      "Iteration 34124, loss = 0.03495262\n",
      "Iteration 34125, loss = 0.03495388\n",
      "Iteration 34126, loss = 0.03495509\n",
      "Iteration 34127, loss = 0.03495498\n",
      "Iteration 34128, loss = 0.03495535\n",
      "Iteration 34129, loss = 0.03494588\n",
      "Iteration 34130, loss = 0.03494504\n",
      "Iteration 34131, loss = 0.03494982\n",
      "Iteration 34132, loss = 0.03495370\n",
      "Iteration 34133, loss = 0.03495358\n",
      "Iteration 34134, loss = 0.03494722\n",
      "Iteration 34135, loss = 0.03494508\n",
      "Iteration 34136, loss = 0.03495505\n",
      "Iteration 34137, loss = 0.03495581\n",
      "Iteration 34138, loss = 0.03496022\n",
      "Iteration 34139, loss = 0.03495591\n",
      "Iteration 34140, loss = 0.03495128\n",
      "Iteration 34141, loss = 0.03494779\n",
      "Iteration 34142, loss = 0.03495755\n",
      "Iteration 34143, loss = 0.03495728\n",
      "Iteration 34144, loss = 0.03494878\n",
      "Iteration 34145, loss = 0.03494192\n",
      "Iteration 34146, loss = 0.03494667\n",
      "Iteration 34147, loss = 0.03494967\n",
      "Iteration 34148, loss = 0.03493868\n",
      "Iteration 34149, loss = 0.03494528\n",
      "Iteration 34150, loss = 0.03495064\n",
      "Iteration 34151, loss = 0.03494706\n",
      "Iteration 34152, loss = 0.03494307\n",
      "Iteration 34153, loss = 0.03493250\n",
      "Iteration 34154, loss = 0.03494132\n",
      "Iteration 34155, loss = 0.03494750\n",
      "Iteration 34156, loss = 0.03494446\n",
      "Iteration 34157, loss = 0.03492857\n",
      "Iteration 34158, loss = 0.03493556\n",
      "Iteration 34159, loss = 0.03494476\n",
      "Iteration 34160, loss = 0.03494385\n",
      "Iteration 34161, loss = 0.03493868\n",
      "Iteration 34162, loss = 0.03492998\n",
      "Iteration 34163, loss = 0.03493097\n",
      "Iteration 34164, loss = 0.03494041\n",
      "Iteration 34165, loss = 0.03493885\n",
      "Iteration 34166, loss = 0.03492960\n",
      "Iteration 34167, loss = 0.03493170\n",
      "Iteration 34168, loss = 0.03493636\n",
      "Iteration 34169, loss = 0.03494074\n",
      "Iteration 34170, loss = 0.03493616\n",
      "Iteration 34171, loss = 0.03493273\n",
      "Iteration 34172, loss = 0.03493139\n",
      "Iteration 34173, loss = 0.03492923\n",
      "Iteration 34174, loss = 0.03492208\n",
      "Iteration 34175, loss = 0.03493730\n",
      "Iteration 34176, loss = 0.03493891\n",
      "Iteration 34177, loss = 0.03493203\n",
      "Iteration 34178, loss = 0.03492281\n",
      "Iteration 34179, loss = 0.03492592\n",
      "Iteration 34180, loss = 0.03493390\n",
      "Iteration 34181, loss = 0.03493216\n",
      "Iteration 34182, loss = 0.03492065\n",
      "Iteration 34183, loss = 0.03492820\n",
      "Iteration 34184, loss = 0.03493873\n",
      "Iteration 34185, loss = 0.03492592\n",
      "Iteration 34186, loss = 0.03492101\n",
      "Iteration 34187, loss = 0.03493271\n",
      "Iteration 34188, loss = 0.03493549\n",
      "Iteration 34189, loss = 0.03492723\n",
      "Iteration 34190, loss = 0.03491690\n",
      "Iteration 34191, loss = 0.03492086\n",
      "Iteration 34192, loss = 0.03492720\n",
      "Iteration 34193, loss = 0.03492621\n",
      "Iteration 34194, loss = 0.03491885\n",
      "Iteration 34195, loss = 0.03491323\n",
      "Iteration 34196, loss = 0.03492095\n",
      "Iteration 34197, loss = 0.03491569\n",
      "Iteration 34198, loss = 0.03490854\n",
      "Iteration 34199, loss = 0.03491960\n",
      "Iteration 34200, loss = 0.03492388\n",
      "Iteration 34201, loss = 0.03491786\n",
      "Iteration 34202, loss = 0.03491174\n",
      "Iteration 34203, loss = 0.03491462\n",
      "Iteration 34204, loss = 0.03491589\n",
      "Iteration 34205, loss = 0.03491147\n",
      "Iteration 34206, loss = 0.03490645\n",
      "Iteration 34207, loss = 0.03491022\n",
      "Iteration 34208, loss = 0.03491615\n",
      "Iteration 34209, loss = 0.03491309\n",
      "Iteration 34210, loss = 0.03491302\n",
      "Iteration 34211, loss = 0.03491101\n",
      "Iteration 34212, loss = 0.03491397\n",
      "Iteration 34213, loss = 0.03491024\n",
      "Iteration 34214, loss = 0.03490957\n",
      "Iteration 34215, loss = 0.03491595\n",
      "Iteration 34216, loss = 0.03491316\n",
      "Iteration 34217, loss = 0.03490866\n",
      "Iteration 34218, loss = 0.03491269\n",
      "Iteration 34219, loss = 0.03491054\n",
      "Iteration 34220, loss = 0.03491300\n",
      "Iteration 34221, loss = 0.03491454\n",
      "Iteration 34222, loss = 0.03491129\n",
      "Iteration 34223, loss = 0.03491984\n",
      "Iteration 34224, loss = 0.03492212\n",
      "Iteration 34225, loss = 0.03492037\n",
      "Iteration 34226, loss = 0.03491328\n",
      "Iteration 34227, loss = 0.03490721\n",
      "Iteration 34228, loss = 0.03490024\n",
      "Iteration 34229, loss = 0.03491530\n",
      "Iteration 34230, loss = 0.03492002\n",
      "Iteration 34231, loss = 0.03491266\n",
      "Iteration 34232, loss = 0.03489551\n",
      "Iteration 34233, loss = 0.03489575\n",
      "Iteration 34234, loss = 0.03489690\n",
      "Iteration 34235, loss = 0.03489799\n",
      "Iteration 34236, loss = 0.03489978\n",
      "Iteration 34237, loss = 0.03489417\n",
      "Iteration 34238, loss = 0.03490490\n",
      "Iteration 34239, loss = 0.03491033\n",
      "Iteration 34240, loss = 0.03489951\n",
      "Iteration 34241, loss = 0.03490713\n",
      "Iteration 34242, loss = 0.03491334\n",
      "Iteration 34243, loss = 0.03490683\n",
      "Iteration 34244, loss = 0.03490310\n",
      "Iteration 34245, loss = 0.03491509\n",
      "Iteration 34246, loss = 0.03491316\n",
      "Iteration 34247, loss = 0.03490822\n",
      "Iteration 34248, loss = 0.03489453\n",
      "Iteration 34249, loss = 0.03490609\n",
      "Iteration 34250, loss = 0.03491563\n",
      "Iteration 34251, loss = 0.03490522\n",
      "Iteration 34252, loss = 0.03490678\n",
      "Iteration 34253, loss = 0.03490879\n",
      "Iteration 34254, loss = 0.03489916\n",
      "Iteration 34255, loss = 0.03489939\n",
      "Iteration 34256, loss = 0.03489420\n",
      "Iteration 34257, loss = 0.03490028\n",
      "Iteration 34258, loss = 0.03490377\n",
      "Iteration 34259, loss = 0.03489353\n",
      "Iteration 34260, loss = 0.03488825\n",
      "Iteration 34261, loss = 0.03489812\n",
      "Iteration 34262, loss = 0.03489839\n",
      "Iteration 34263, loss = 0.03489932\n",
      "Iteration 34264, loss = 0.03489397\n",
      "Iteration 34265, loss = 0.03489328\n",
      "Iteration 34266, loss = 0.03489957\n",
      "Iteration 34267, loss = 0.03490257\n",
      "Iteration 34268, loss = 0.03490306\n",
      "Iteration 34269, loss = 0.03489085\n",
      "Iteration 34270, loss = 0.03489045\n",
      "Iteration 34271, loss = 0.03488481\n",
      "Iteration 34272, loss = 0.03489580\n",
      "Iteration 34273, loss = 0.03490061\n",
      "Iteration 34274, loss = 0.03489989\n",
      "Iteration 34275, loss = 0.03489175\n",
      "Iteration 34276, loss = 0.03489054\n",
      "Iteration 34277, loss = 0.03489431\n",
      "Iteration 34278, loss = 0.03490292\n",
      "Iteration 34279, loss = 0.03490335\n",
      "Iteration 34280, loss = 0.03489451\n",
      "Iteration 34281, loss = 0.03488547\n",
      "Iteration 34282, loss = 0.03488334\n",
      "Iteration 34283, loss = 0.03487780\n",
      "Iteration 34284, loss = 0.03488170\n",
      "Iteration 34285, loss = 0.03488381\n",
      "Iteration 34286, loss = 0.03487988\n",
      "Iteration 34287, loss = 0.03487891\n",
      "Iteration 34288, loss = 0.03486646\n",
      "Iteration 34289, loss = 0.03488137\n",
      "Iteration 34290, loss = 0.03488432\n",
      "Iteration 34291, loss = 0.03487825\n",
      "Iteration 34292, loss = 0.03487803\n",
      "Iteration 34293, loss = 0.03487737\n",
      "Iteration 34294, loss = 0.03488777\n",
      "Iteration 34295, loss = 0.03488823\n",
      "Iteration 34296, loss = 0.03488495\n",
      "Iteration 34297, loss = 0.03487609\n",
      "Iteration 34298, loss = 0.03487422\n",
      "Iteration 34299, loss = 0.03488345\n",
      "Iteration 34300, loss = 0.03488723\n",
      "Iteration 34301, loss = 0.03488171\n",
      "Iteration 34302, loss = 0.03488335\n",
      "Iteration 34303, loss = 0.03487795\n",
      "Iteration 34304, loss = 0.03487827\n",
      "Iteration 34305, loss = 0.03488306\n",
      "Iteration 34306, loss = 0.03487993\n",
      "Iteration 34307, loss = 0.03487378\n",
      "Iteration 34308, loss = 0.03487475\n",
      "Iteration 34309, loss = 0.03487534\n",
      "Iteration 34310, loss = 0.03487180\n",
      "Iteration 34311, loss = 0.03487455\n",
      "Iteration 34312, loss = 0.03487102\n",
      "Iteration 34313, loss = 0.03486454\n",
      "Iteration 34314, loss = 0.03487113\n",
      "Iteration 34315, loss = 0.03487747\n",
      "Iteration 34316, loss = 0.03487400\n",
      "Iteration 34317, loss = 0.03486262\n",
      "Iteration 34318, loss = 0.03487132\n",
      "Iteration 34319, loss = 0.03487215\n",
      "Iteration 34320, loss = 0.03486817\n",
      "Iteration 34321, loss = 0.03487192\n",
      "Iteration 34322, loss = 0.03486727\n",
      "Iteration 34323, loss = 0.03485806\n",
      "Iteration 34324, loss = 0.03486140\n",
      "Iteration 34325, loss = 0.03485931\n",
      "Iteration 34326, loss = 0.03486204\n",
      "Iteration 34327, loss = 0.03486711\n",
      "Iteration 34328, loss = 0.03486137\n",
      "Iteration 34329, loss = 0.03486554\n",
      "Iteration 34330, loss = 0.03486555\n",
      "Iteration 34331, loss = 0.03485598\n",
      "Iteration 34332, loss = 0.03485529\n",
      "Iteration 34333, loss = 0.03487316\n",
      "Iteration 34334, loss = 0.03487566\n",
      "Iteration 34335, loss = 0.03486316\n",
      "Iteration 34336, loss = 0.03484931\n",
      "Iteration 34337, loss = 0.03485283\n",
      "Iteration 34338, loss = 0.03485600\n",
      "Iteration 34339, loss = 0.03485785\n",
      "Iteration 34340, loss = 0.03485498\n",
      "Iteration 34341, loss = 0.03485358\n",
      "Iteration 34342, loss = 0.03484840\n",
      "Iteration 34343, loss = 0.03485751\n",
      "Iteration 34344, loss = 0.03486021\n",
      "Iteration 34345, loss = 0.03485671\n",
      "Iteration 34346, loss = 0.03485077\n",
      "Iteration 34347, loss = 0.03485202\n",
      "Iteration 34348, loss = 0.03485446\n",
      "Iteration 34349, loss = 0.03485677\n",
      "Iteration 34350, loss = 0.03485567\n",
      "Iteration 34351, loss = 0.03484869\n",
      "Iteration 34352, loss = 0.03485026\n",
      "Iteration 34353, loss = 0.03485156\n",
      "Iteration 34354, loss = 0.03484464\n",
      "Iteration 34355, loss = 0.03484471\n",
      "Iteration 34356, loss = 0.03484228\n",
      "Iteration 34357, loss = 0.03483985\n",
      "Iteration 34358, loss = 0.03484081\n",
      "Iteration 34359, loss = 0.03484236\n",
      "Iteration 34360, loss = 0.03484449\n",
      "Iteration 34361, loss = 0.03484273\n",
      "Iteration 34362, loss = 0.03483939\n",
      "Iteration 34363, loss = 0.03484235\n",
      "Iteration 34364, loss = 0.03484682\n",
      "Iteration 34365, loss = 0.03485275\n",
      "Iteration 34366, loss = 0.03484897\n",
      "Iteration 34367, loss = 0.03483657\n",
      "Iteration 34368, loss = 0.03484896\n",
      "Iteration 34369, loss = 0.03485804\n",
      "Iteration 34370, loss = 0.03484944\n",
      "Iteration 34371, loss = 0.03484401\n",
      "Iteration 34372, loss = 0.03484184\n",
      "Iteration 34373, loss = 0.03484296\n",
      "Iteration 34374, loss = 0.03484471\n",
      "Iteration 34375, loss = 0.03484188\n",
      "Iteration 34376, loss = 0.03483683\n",
      "Iteration 34377, loss = 0.03484510\n",
      "Iteration 34378, loss = 0.03484126\n",
      "Iteration 34379, loss = 0.03484634\n",
      "Iteration 34380, loss = 0.03484614\n",
      "Iteration 34381, loss = 0.03484390\n",
      "Iteration 34382, loss = 0.03483037\n",
      "Iteration 34383, loss = 0.03484861\n",
      "Iteration 34384, loss = 0.03485807\n",
      "Iteration 34385, loss = 0.03484576\n",
      "Iteration 34386, loss = 0.03482764\n",
      "Iteration 34387, loss = 0.03484260\n",
      "Iteration 34388, loss = 0.03484833\n",
      "Iteration 34389, loss = 0.03484977\n",
      "Iteration 34390, loss = 0.03484915\n",
      "Iteration 34391, loss = 0.03484321\n",
      "Iteration 34392, loss = 0.03484241\n",
      "Iteration 34393, loss = 0.03483843\n",
      "Iteration 34394, loss = 0.03484366\n",
      "Iteration 34395, loss = 0.03484278\n",
      "Iteration 34396, loss = 0.03483960\n",
      "Iteration 34397, loss = 0.03483703\n",
      "Iteration 34398, loss = 0.03484186\n",
      "Iteration 34399, loss = 0.03484301\n",
      "Iteration 34400, loss = 0.03484089\n",
      "Iteration 34401, loss = 0.03483469\n",
      "Iteration 34402, loss = 0.03482942\n",
      "Iteration 34403, loss = 0.03483051\n",
      "Iteration 34404, loss = 0.03482814\n",
      "Iteration 34405, loss = 0.03483280\n",
      "Iteration 34406, loss = 0.03482802\n",
      "Iteration 34407, loss = 0.03482836\n",
      "Iteration 34408, loss = 0.03483389\n",
      "Iteration 34409, loss = 0.03483585\n",
      "Iteration 34410, loss = 0.03482779\n",
      "Iteration 34411, loss = 0.03482519\n",
      "Iteration 34412, loss = 0.03483195\n",
      "Iteration 34413, loss = 0.03483224\n",
      "Iteration 34414, loss = 0.03482974\n",
      "Iteration 34415, loss = 0.03482001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 34416, loss = 0.03483107\n",
      "Iteration 34417, loss = 0.03483743\n",
      "Iteration 34418, loss = 0.03483717\n",
      "Iteration 34419, loss = 0.03483899\n",
      "Iteration 34420, loss = 0.03483383\n",
      "Iteration 34421, loss = 0.03482599\n",
      "Iteration 34422, loss = 0.03482173\n",
      "Iteration 34423, loss = 0.03481468\n",
      "Iteration 34424, loss = 0.03482412\n",
      "Iteration 34425, loss = 0.03482575\n",
      "Iteration 34426, loss = 0.03482102\n",
      "Iteration 34427, loss = 0.03481369\n",
      "Iteration 34428, loss = 0.03481544\n",
      "Iteration 34429, loss = 0.03481111\n",
      "Iteration 34430, loss = 0.03481586\n",
      "Iteration 34431, loss = 0.03481924\n",
      "Iteration 34432, loss = 0.03481543\n",
      "Iteration 34433, loss = 0.03481856\n",
      "Iteration 34434, loss = 0.03481218\n",
      "Iteration 34435, loss = 0.03481005\n",
      "Iteration 34436, loss = 0.03481210\n",
      "Iteration 34437, loss = 0.03482098\n",
      "Iteration 34438, loss = 0.03482072\n",
      "Iteration 34439, loss = 0.03482285\n",
      "Iteration 34440, loss = 0.03482076\n",
      "Iteration 34441, loss = 0.03482025\n",
      "Iteration 34442, loss = 0.03482116\n",
      "Iteration 34443, loss = 0.03481362\n",
      "Iteration 34444, loss = 0.03481105\n",
      "Iteration 34445, loss = 0.03482029\n",
      "Iteration 34446, loss = 0.03481998\n",
      "Iteration 34447, loss = 0.03481583\n",
      "Iteration 34448, loss = 0.03481705\n",
      "Iteration 34449, loss = 0.03481562\n",
      "Iteration 34450, loss = 0.03480922\n",
      "Iteration 34451, loss = 0.03481441\n",
      "Iteration 34452, loss = 0.03480984\n",
      "Iteration 34453, loss = 0.03481078\n",
      "Iteration 34454, loss = 0.03481119\n",
      "Iteration 34455, loss = 0.03479938\n",
      "Iteration 34456, loss = 0.03481001\n",
      "Iteration 34457, loss = 0.03481025\n",
      "Iteration 34458, loss = 0.03479912\n",
      "Iteration 34459, loss = 0.03480324\n",
      "Iteration 34460, loss = 0.03480834\n",
      "Iteration 34461, loss = 0.03480814\n",
      "Iteration 34462, loss = 0.03480406\n",
      "Iteration 34463, loss = 0.03480271\n",
      "Iteration 34464, loss = 0.03480349\n",
      "Iteration 34465, loss = 0.03480057\n",
      "Iteration 34466, loss = 0.03480245\n",
      "Iteration 34467, loss = 0.03480095\n",
      "Iteration 34468, loss = 0.03480182\n",
      "Iteration 34469, loss = 0.03479875\n",
      "Iteration 34470, loss = 0.03479668\n",
      "Iteration 34471, loss = 0.03479865\n",
      "Iteration 34472, loss = 0.03479618\n",
      "Iteration 34473, loss = 0.03479681\n",
      "Iteration 34474, loss = 0.03480023\n",
      "Iteration 34475, loss = 0.03479500\n",
      "Iteration 34476, loss = 0.03479111\n",
      "Iteration 34477, loss = 0.03479075\n",
      "Iteration 34478, loss = 0.03479106\n",
      "Iteration 34479, loss = 0.03479335\n",
      "Iteration 34480, loss = 0.03478497\n",
      "Iteration 34481, loss = 0.03479616\n",
      "Iteration 34482, loss = 0.03480125\n",
      "Iteration 34483, loss = 0.03479890\n",
      "Iteration 34484, loss = 0.03479156\n",
      "Iteration 34485, loss = 0.03479852\n",
      "Iteration 34486, loss = 0.03479840\n",
      "Iteration 34487, loss = 0.03480920\n",
      "Iteration 34488, loss = 0.03480869\n",
      "Iteration 34489, loss = 0.03479108\n",
      "Iteration 34490, loss = 0.03478587\n",
      "Iteration 34491, loss = 0.03479882\n",
      "Iteration 34492, loss = 0.03480564\n",
      "Iteration 34493, loss = 0.03480605\n",
      "Iteration 34494, loss = 0.03480333\n",
      "Iteration 34495, loss = 0.03479471\n",
      "Iteration 34496, loss = 0.03478957\n",
      "Iteration 34497, loss = 0.03479072\n",
      "Iteration 34498, loss = 0.03479776\n",
      "Iteration 34499, loss = 0.03479061\n",
      "Iteration 34500, loss = 0.03478369\n",
      "Iteration 34501, loss = 0.03477866\n",
      "Iteration 34502, loss = 0.03479656\n",
      "Iteration 34503, loss = 0.03480115\n",
      "Iteration 34504, loss = 0.03480536\n",
      "Iteration 34505, loss = 0.03480045\n",
      "Iteration 34506, loss = 0.03478888\n",
      "Iteration 34507, loss = 0.03479152\n",
      "Iteration 34508, loss = 0.03479793\n",
      "Iteration 34509, loss = 0.03479588\n",
      "Iteration 34510, loss = 0.03478628\n",
      "Iteration 34511, loss = 0.03478527\n",
      "Iteration 34512, loss = 0.03477728\n",
      "Iteration 34513, loss = 0.03478289\n",
      "Iteration 34514, loss = 0.03478629\n",
      "Iteration 34515, loss = 0.03478652\n",
      "Iteration 34516, loss = 0.03478467\n",
      "Iteration 34517, loss = 0.03477377\n",
      "Iteration 34518, loss = 0.03477681\n",
      "Iteration 34519, loss = 0.03478237\n",
      "Iteration 34520, loss = 0.03477949\n",
      "Iteration 34521, loss = 0.03477585\n",
      "Iteration 34522, loss = 0.03478056\n",
      "Iteration 34523, loss = 0.03478109\n",
      "Iteration 34524, loss = 0.03478297\n",
      "Iteration 34525, loss = 0.03478110\n",
      "Iteration 34526, loss = 0.03477960\n",
      "Iteration 34527, loss = 0.03477492\n",
      "Iteration 34528, loss = 0.03478129\n",
      "Iteration 34529, loss = 0.03478328\n",
      "Iteration 34530, loss = 0.03477977\n",
      "Iteration 34531, loss = 0.03477397\n",
      "Iteration 34532, loss = 0.03477667\n",
      "Iteration 34533, loss = 0.03477683\n",
      "Iteration 34534, loss = 0.03476976\n",
      "Iteration 34535, loss = 0.03477075\n",
      "Iteration 34536, loss = 0.03478144\n",
      "Iteration 34537, loss = 0.03477720\n",
      "Iteration 34538, loss = 0.03476773\n",
      "Iteration 34539, loss = 0.03477649\n",
      "Iteration 34540, loss = 0.03478368\n",
      "Iteration 34541, loss = 0.03477777\n",
      "Iteration 34542, loss = 0.03476390\n",
      "Iteration 34543, loss = 0.03476162\n",
      "Iteration 34544, loss = 0.03476908\n",
      "Iteration 34545, loss = 0.03477051\n",
      "Iteration 34546, loss = 0.03476886\n",
      "Iteration 34547, loss = 0.03476382\n",
      "Iteration 34548, loss = 0.03476526\n",
      "Iteration 34549, loss = 0.03477178\n",
      "Iteration 34550, loss = 0.03477128\n",
      "Iteration 34551, loss = 0.03477430\n",
      "Iteration 34552, loss = 0.03477091\n",
      "Iteration 34553, loss = 0.03476547\n",
      "Iteration 34554, loss = 0.03475947\n",
      "Iteration 34555, loss = 0.03476337\n",
      "Iteration 34556, loss = 0.03475570\n",
      "Iteration 34557, loss = 0.03475984\n",
      "Iteration 34558, loss = 0.03476312\n",
      "Iteration 34559, loss = 0.03476258\n",
      "Iteration 34560, loss = 0.03476303\n",
      "Iteration 34561, loss = 0.03476349\n",
      "Iteration 34562, loss = 0.03476047\n",
      "Iteration 34563, loss = 0.03476113\n",
      "Iteration 34564, loss = 0.03476002\n",
      "Iteration 34565, loss = 0.03475770\n",
      "Iteration 34566, loss = 0.03475895\n",
      "Iteration 34567, loss = 0.03475913\n",
      "Iteration 34568, loss = 0.03475716\n",
      "Iteration 34569, loss = 0.03475394\n",
      "Iteration 34570, loss = 0.03475463\n",
      "Iteration 34571, loss = 0.03475667\n",
      "Iteration 34572, loss = 0.03476477\n",
      "Iteration 34573, loss = 0.03475961\n",
      "Iteration 34574, loss = 0.03475307\n",
      "Iteration 34575, loss = 0.03475327\n",
      "Iteration 34576, loss = 0.03476228\n",
      "Iteration 34577, loss = 0.03475397\n",
      "Iteration 34578, loss = 0.03474864\n",
      "Iteration 34579, loss = 0.03475896\n",
      "Iteration 34580, loss = 0.03477033\n",
      "Iteration 34581, loss = 0.03476814\n",
      "Iteration 34582, loss = 0.03475434\n",
      "Iteration 34583, loss = 0.03474580\n",
      "Iteration 34584, loss = 0.03475186\n",
      "Iteration 34585, loss = 0.03475782\n",
      "Iteration 34586, loss = 0.03474871\n",
      "Iteration 34587, loss = 0.03474601\n",
      "Iteration 34588, loss = 0.03475447\n",
      "Iteration 34589, loss = 0.03476001\n",
      "Iteration 34590, loss = 0.03475975\n",
      "Iteration 34591, loss = 0.03475774\n",
      "Iteration 34592, loss = 0.03475286\n",
      "Iteration 34593, loss = 0.03474969\n",
      "Iteration 34594, loss = 0.03475088\n",
      "Iteration 34595, loss = 0.03474758\n",
      "Iteration 34596, loss = 0.03474127\n",
      "Iteration 34597, loss = 0.03474179\n",
      "Iteration 34598, loss = 0.03474360\n",
      "Iteration 34599, loss = 0.03474328\n",
      "Iteration 34600, loss = 0.03474240\n",
      "Iteration 34601, loss = 0.03473306\n",
      "Iteration 34602, loss = 0.03474412\n",
      "Iteration 34603, loss = 0.03475195\n",
      "Iteration 34604, loss = 0.03474847\n",
      "Iteration 34605, loss = 0.03474290\n",
      "Iteration 34606, loss = 0.03474920\n",
      "Iteration 34607, loss = 0.03475046\n",
      "Iteration 34608, loss = 0.03474348\n",
      "Iteration 34609, loss = 0.03474120\n",
      "Iteration 34610, loss = 0.03473579\n",
      "Iteration 34611, loss = 0.03473748\n",
      "Iteration 34612, loss = 0.03473209\n",
      "Iteration 34613, loss = 0.03473102\n",
      "Iteration 34614, loss = 0.03473099\n",
      "Iteration 34615, loss = 0.03473826\n",
      "Iteration 34616, loss = 0.03474467\n",
      "Iteration 34617, loss = 0.03474510\n",
      "Iteration 34618, loss = 0.03474076\n",
      "Iteration 34619, loss = 0.03473166\n",
      "Iteration 34620, loss = 0.03473152\n",
      "Iteration 34621, loss = 0.03473234\n",
      "Iteration 34622, loss = 0.03472882\n",
      "Iteration 34623, loss = 0.03473677\n",
      "Iteration 34624, loss = 0.03473737\n",
      "Iteration 34625, loss = 0.03473130\n",
      "Iteration 34626, loss = 0.03472758\n",
      "Iteration 34627, loss = 0.03472984\n",
      "Iteration 34628, loss = 0.03473394\n",
      "Iteration 34629, loss = 0.03473130\n",
      "Iteration 34630, loss = 0.03472741\n",
      "Iteration 34631, loss = 0.03472402\n",
      "Iteration 34632, loss = 0.03473073\n",
      "Iteration 34633, loss = 0.03473784\n",
      "Iteration 34634, loss = 0.03473350\n",
      "Iteration 34635, loss = 0.03472847\n",
      "Iteration 34636, loss = 0.03472899\n",
      "Iteration 34637, loss = 0.03473527\n",
      "Iteration 34638, loss = 0.03473374\n",
      "Iteration 34639, loss = 0.03473122\n",
      "Iteration 34640, loss = 0.03472403\n",
      "Iteration 34641, loss = 0.03472492\n",
      "Iteration 34642, loss = 0.03473416\n",
      "Iteration 34643, loss = 0.03474029\n",
      "Iteration 34644, loss = 0.03473863\n",
      "Iteration 34645, loss = 0.03473072\n",
      "Iteration 34646, loss = 0.03472632\n",
      "Iteration 34647, loss = 0.03472568\n",
      "Iteration 34648, loss = 0.03472865\n",
      "Iteration 34649, loss = 0.03472802\n",
      "Iteration 34650, loss = 0.03473091\n",
      "Iteration 34651, loss = 0.03472176\n",
      "Iteration 34652, loss = 0.03471673\n",
      "Iteration 34653, loss = 0.03472200\n",
      "Iteration 34654, loss = 0.03471677\n",
      "Iteration 34655, loss = 0.03471965\n",
      "Iteration 34656, loss = 0.03471616\n",
      "Iteration 34657, loss = 0.03472387\n",
      "Iteration 34658, loss = 0.03472440\n",
      "Iteration 34659, loss = 0.03471564\n",
      "Iteration 34660, loss = 0.03471287\n",
      "Iteration 34661, loss = 0.03472665\n",
      "Iteration 34662, loss = 0.03472776\n",
      "Iteration 34663, loss = 0.03471729\n",
      "Iteration 34664, loss = 0.03471951\n",
      "Iteration 34665, loss = 0.03472793\n",
      "Iteration 34666, loss = 0.03472669\n",
      "Iteration 34667, loss = 0.03471753\n",
      "Iteration 34668, loss = 0.03472823\n",
      "Iteration 34669, loss = 0.03473287\n",
      "Iteration 34670, loss = 0.03472918\n",
      "Iteration 34671, loss = 0.03472765\n",
      "Iteration 34672, loss = 0.03472482\n",
      "Iteration 34673, loss = 0.03470787\n",
      "Iteration 34674, loss = 0.03471349\n",
      "Iteration 34675, loss = 0.03471975\n",
      "Iteration 34676, loss = 0.03471663\n",
      "Iteration 34677, loss = 0.03471540\n",
      "Iteration 34678, loss = 0.03471638\n",
      "Iteration 34679, loss = 0.03471558\n",
      "Iteration 34680, loss = 0.03470881\n",
      "Iteration 34681, loss = 0.03470367\n",
      "Iteration 34682, loss = 0.03471429\n",
      "Iteration 34683, loss = 0.03471276\n",
      "Iteration 34684, loss = 0.03471289\n",
      "Iteration 34685, loss = 0.03470420\n",
      "Iteration 34686, loss = 0.03470426\n",
      "Iteration 34687, loss = 0.03470526\n",
      "Iteration 34688, loss = 0.03470401\n",
      "Iteration 34689, loss = 0.03471396\n",
      "Iteration 34690, loss = 0.03471670\n",
      "Iteration 34691, loss = 0.03471474\n",
      "Iteration 34692, loss = 0.03470779\n",
      "Iteration 34693, loss = 0.03470693\n",
      "Iteration 34694, loss = 0.03471271\n",
      "Iteration 34695, loss = 0.03471062\n",
      "Iteration 34696, loss = 0.03469288\n",
      "Iteration 34697, loss = 0.03470248\n",
      "Iteration 34698, loss = 0.03470470\n",
      "Iteration 34699, loss = 0.03471057\n",
      "Iteration 34700, loss = 0.03470931\n",
      "Iteration 34701, loss = 0.03470038\n",
      "Iteration 34702, loss = 0.03470298\n",
      "Iteration 34703, loss = 0.03470711\n",
      "Iteration 34704, loss = 0.03470977\n",
      "Iteration 34705, loss = 0.03470312\n",
      "Iteration 34706, loss = 0.03469269\n",
      "Iteration 34707, loss = 0.03469773\n",
      "Iteration 34708, loss = 0.03469361\n",
      "Iteration 34709, loss = 0.03469070\n",
      "Iteration 34710, loss = 0.03469966\n",
      "Iteration 34711, loss = 0.03469980\n",
      "Iteration 34712, loss = 0.03469242\n",
      "Iteration 34713, loss = 0.03469156\n",
      "Iteration 34714, loss = 0.03469631\n",
      "Iteration 34715, loss = 0.03469757\n",
      "Iteration 34716, loss = 0.03469594\n",
      "Iteration 34717, loss = 0.03469294\n",
      "Iteration 34718, loss = 0.03469928\n",
      "Iteration 34719, loss = 0.03469691\n",
      "Iteration 34720, loss = 0.03469762\n",
      "Iteration 34721, loss = 0.03470816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 34722, loss = 0.03470347\n",
      "Iteration 34723, loss = 0.03469058\n",
      "Iteration 34724, loss = 0.03468778\n",
      "Iteration 34725, loss = 0.03469606\n",
      "Iteration 34726, loss = 0.03469565\n",
      "Iteration 34727, loss = 0.03469601\n",
      "Iteration 34728, loss = 0.03469363\n",
      "Iteration 34729, loss = 0.03468544\n",
      "Iteration 34730, loss = 0.03468530\n",
      "Iteration 34731, loss = 0.03469743\n",
      "Iteration 34732, loss = 0.03469983\n",
      "Iteration 34733, loss = 0.03469543\n",
      "Iteration 34734, loss = 0.03469024\n",
      "Iteration 34735, loss = 0.03469646\n",
      "Iteration 34736, loss = 0.03470405\n",
      "Iteration 34737, loss = 0.03470631\n",
      "Iteration 34738, loss = 0.03469782\n",
      "Iteration 34739, loss = 0.03468597\n",
      "Iteration 34740, loss = 0.03468136\n",
      "Iteration 34741, loss = 0.03469502\n",
      "Iteration 34742, loss = 0.03469818\n",
      "Iteration 34743, loss = 0.03468271\n",
      "Iteration 34744, loss = 0.03468724\n",
      "Iteration 34745, loss = 0.03468841\n",
      "Iteration 34746, loss = 0.03468265\n",
      "Iteration 34747, loss = 0.03468848\n",
      "Iteration 34748, loss = 0.03469515\n",
      "Iteration 34749, loss = 0.03469490\n",
      "Iteration 34750, loss = 0.03469256\n",
      "Iteration 34751, loss = 0.03468627\n",
      "Iteration 34752, loss = 0.03469025\n",
      "Iteration 34753, loss = 0.03469321\n",
      "Iteration 34754, loss = 0.03469280\n",
      "Iteration 34755, loss = 0.03469224\n",
      "Iteration 34756, loss = 0.03468710\n",
      "Iteration 34757, loss = 0.03468102\n",
      "Iteration 34758, loss = 0.03467817\n",
      "Iteration 34759, loss = 0.03468260\n",
      "Iteration 34760, loss = 0.03467773\n",
      "Iteration 34761, loss = 0.03467537\n",
      "Iteration 34762, loss = 0.03467547\n",
      "Iteration 34763, loss = 0.03467627\n",
      "Iteration 34764, loss = 0.03467659\n",
      "Iteration 34765, loss = 0.03466925\n",
      "Iteration 34766, loss = 0.03467850\n",
      "Iteration 34767, loss = 0.03468174\n",
      "Iteration 34768, loss = 0.03467066\n",
      "Iteration 34769, loss = 0.03467040\n",
      "Iteration 34770, loss = 0.03468115\n",
      "Iteration 34771, loss = 0.03467962\n",
      "Iteration 34772, loss = 0.03467817\n",
      "Iteration 34773, loss = 0.03467653\n",
      "Iteration 34774, loss = 0.03466666\n",
      "Iteration 34775, loss = 0.03467344\n",
      "Iteration 34776, loss = 0.03467315\n",
      "Iteration 34777, loss = 0.03466897\n",
      "Iteration 34778, loss = 0.03467185\n",
      "Iteration 34779, loss = 0.03467340\n",
      "Iteration 34780, loss = 0.03467975\n",
      "Iteration 34781, loss = 0.03467848\n",
      "Iteration 34782, loss = 0.03467343\n",
      "Iteration 34783, loss = 0.03466865\n",
      "Iteration 34784, loss = 0.03467063\n",
      "Iteration 34785, loss = 0.03467409\n",
      "Iteration 34786, loss = 0.03466921\n",
      "Iteration 34787, loss = 0.03467306\n",
      "Iteration 34788, loss = 0.03466779\n",
      "Iteration 34789, loss = 0.03467464\n",
      "Iteration 34790, loss = 0.03468285\n",
      "Iteration 34791, loss = 0.03467491\n",
      "Iteration 34792, loss = 0.03466270\n",
      "Iteration 34793, loss = 0.03466976\n",
      "Iteration 34794, loss = 0.03467219\n",
      "Iteration 34795, loss = 0.03467460\n",
      "Iteration 34796, loss = 0.03467536\n",
      "Iteration 34797, loss = 0.03466829\n",
      "Iteration 34798, loss = 0.03465632\n",
      "Iteration 34799, loss = 0.03466462\n",
      "Iteration 34800, loss = 0.03466367\n",
      "Iteration 34801, loss = 0.03466223\n",
      "Iteration 34802, loss = 0.03466238\n",
      "Iteration 34803, loss = 0.03465231\n",
      "Iteration 34804, loss = 0.03466307\n",
      "Iteration 34805, loss = 0.03466751\n",
      "Iteration 34806, loss = 0.03465560\n",
      "Iteration 34807, loss = 0.03465984\n",
      "Iteration 34808, loss = 0.03466069\n",
      "Iteration 34809, loss = 0.03466517\n",
      "Iteration 34810, loss = 0.03466740\n",
      "Iteration 34811, loss = 0.03465532\n",
      "Iteration 34812, loss = 0.03465061\n",
      "Iteration 34813, loss = 0.03465205\n",
      "Iteration 34814, loss = 0.03465735\n",
      "Iteration 34815, loss = 0.03465559\n",
      "Iteration 34816, loss = 0.03465255\n",
      "Iteration 34817, loss = 0.03465128\n",
      "Iteration 34818, loss = 0.03465382\n",
      "Iteration 34819, loss = 0.03465474\n",
      "Iteration 34820, loss = 0.03465272\n",
      "Iteration 34821, loss = 0.03465184\n",
      "Iteration 34822, loss = 0.03465628\n",
      "Iteration 34823, loss = 0.03465321\n",
      "Iteration 34824, loss = 0.03466298\n",
      "Iteration 34825, loss = 0.03465725\n",
      "Iteration 34826, loss = 0.03464861\n",
      "Iteration 34827, loss = 0.03464675\n",
      "Iteration 34828, loss = 0.03465437\n",
      "Iteration 34829, loss = 0.03465682\n",
      "Iteration 34830, loss = 0.03465456\n",
      "Iteration 34831, loss = 0.03465352\n",
      "Iteration 34832, loss = 0.03464941\n",
      "Iteration 34833, loss = 0.03465082\n",
      "Iteration 34834, loss = 0.03464623\n",
      "Iteration 34835, loss = 0.03464754\n",
      "Iteration 34836, loss = 0.03465175\n",
      "Iteration 34837, loss = 0.03465388\n",
      "Iteration 34838, loss = 0.03465250\n",
      "Iteration 34839, loss = 0.03464745\n",
      "Iteration 34840, loss = 0.03464469\n",
      "Iteration 34841, loss = 0.03464664\n",
      "Iteration 34842, loss = 0.03464297\n",
      "Iteration 34843, loss = 0.03464355\n",
      "Iteration 34844, loss = 0.03464649\n",
      "Iteration 34845, loss = 0.03464581\n",
      "Iteration 34846, loss = 0.03463271\n",
      "Iteration 34847, loss = 0.03463976\n",
      "Iteration 34848, loss = 0.03464193\n",
      "Iteration 34849, loss = 0.03463729\n",
      "Iteration 34850, loss = 0.03463872\n",
      "Iteration 34851, loss = 0.03464161\n",
      "Iteration 34852, loss = 0.03464421\n",
      "Iteration 34853, loss = 0.03465148\n",
      "Iteration 34854, loss = 0.03464630\n",
      "Iteration 34855, loss = 0.03464057\n",
      "Iteration 34856, loss = 0.03463294\n",
      "Iteration 34857, loss = 0.03463850\n",
      "Iteration 34858, loss = 0.03463326\n",
      "Iteration 34859, loss = 0.03463168\n",
      "Iteration 34860, loss = 0.03463689\n",
      "Iteration 34861, loss = 0.03463496\n",
      "Iteration 34862, loss = 0.03463825\n",
      "Iteration 34863, loss = 0.03463006\n",
      "Iteration 34864, loss = 0.03463315\n",
      "Iteration 34865, loss = 0.03463619\n",
      "Iteration 34866, loss = 0.03463035\n",
      "Iteration 34867, loss = 0.03463652\n",
      "Iteration 34868, loss = 0.03463439\n",
      "Iteration 34869, loss = 0.03464102\n",
      "Iteration 34870, loss = 0.03464150\n",
      "Iteration 34871, loss = 0.03463223\n",
      "Iteration 34872, loss = 0.03463538\n",
      "Iteration 34873, loss = 0.03463305\n",
      "Iteration 34874, loss = 0.03462495\n",
      "Iteration 34875, loss = 0.03463095\n",
      "Iteration 34876, loss = 0.03464302\n",
      "Iteration 34877, loss = 0.03463845\n",
      "Iteration 34878, loss = 0.03462976\n",
      "Iteration 34879, loss = 0.03462542\n",
      "Iteration 34880, loss = 0.03463564\n",
      "Iteration 34881, loss = 0.03463968\n",
      "Iteration 34882, loss = 0.03463422\n",
      "Iteration 34883, loss = 0.03463020\n",
      "Iteration 34884, loss = 0.03462893\n",
      "Iteration 34885, loss = 0.03462362\n",
      "Iteration 34886, loss = 0.03461955\n",
      "Iteration 34887, loss = 0.03462401\n",
      "Iteration 34888, loss = 0.03461944\n",
      "Iteration 34889, loss = 0.03461925\n",
      "Iteration 34890, loss = 0.03462337\n",
      "Iteration 34891, loss = 0.03462373\n",
      "Iteration 34892, loss = 0.03462089\n",
      "Iteration 34893, loss = 0.03461829\n",
      "Iteration 34894, loss = 0.03462056\n",
      "Iteration 34895, loss = 0.03462421\n",
      "Iteration 34896, loss = 0.03462243\n",
      "Iteration 34897, loss = 0.03461655\n",
      "Iteration 34898, loss = 0.03461840\n",
      "Iteration 34899, loss = 0.03461759\n",
      "Iteration 34900, loss = 0.03461239\n",
      "Iteration 34901, loss = 0.03461642\n",
      "Iteration 34902, loss = 0.03461871\n",
      "Iteration 34903, loss = 0.03461076\n",
      "Iteration 34904, loss = 0.03460986\n",
      "Iteration 34905, loss = 0.03461674\n",
      "Iteration 34906, loss = 0.03461405\n",
      "Iteration 34907, loss = 0.03461804\n",
      "Iteration 34908, loss = 0.03461546\n",
      "Iteration 34909, loss = 0.03462718\n",
      "Iteration 34910, loss = 0.03462615\n",
      "Iteration 34911, loss = 0.03461967\n",
      "Iteration 34912, loss = 0.03462093\n",
      "Iteration 34913, loss = 0.03462396\n",
      "Iteration 34914, loss = 0.03460605\n",
      "Iteration 34915, loss = 0.03461295\n",
      "Iteration 34916, loss = 0.03461976\n",
      "Iteration 34917, loss = 0.03461937\n",
      "Iteration 34918, loss = 0.03461566\n",
      "Iteration 34919, loss = 0.03460451\n",
      "Iteration 34920, loss = 0.03461315\n",
      "Iteration 34921, loss = 0.03462372\n",
      "Iteration 34922, loss = 0.03461211\n",
      "Iteration 34923, loss = 0.03460949\n",
      "Iteration 34924, loss = 0.03461996\n",
      "Iteration 34925, loss = 0.03462462\n",
      "Iteration 34926, loss = 0.03462056\n",
      "Iteration 34927, loss = 0.03461108\n",
      "Iteration 34928, loss = 0.03461516\n",
      "Iteration 34929, loss = 0.03461666\n",
      "Iteration 34930, loss = 0.03461651\n",
      "Iteration 34931, loss = 0.03461232\n",
      "Iteration 34932, loss = 0.03460889\n",
      "Iteration 34933, loss = 0.03460372\n",
      "Iteration 34934, loss = 0.03461229\n",
      "Iteration 34935, loss = 0.03461185\n",
      "Iteration 34936, loss = 0.03461198\n",
      "Iteration 34937, loss = 0.03460273\n",
      "Iteration 34938, loss = 0.03460522\n",
      "Iteration 34939, loss = 0.03461141\n",
      "Iteration 34940, loss = 0.03461515\n",
      "Iteration 34941, loss = 0.03461127\n",
      "Iteration 34942, loss = 0.03459960\n",
      "Iteration 34943, loss = 0.03459714\n",
      "Iteration 34944, loss = 0.03460352\n",
      "Iteration 34945, loss = 0.03460275\n",
      "Iteration 34946, loss = 0.03460192\n",
      "Iteration 34947, loss = 0.03459792\n",
      "Iteration 34948, loss = 0.03460150\n",
      "Iteration 34949, loss = 0.03460505\n",
      "Iteration 34950, loss = 0.03460285\n",
      "Iteration 34951, loss = 0.03460818\n",
      "Iteration 34952, loss = 0.03460682\n",
      "Iteration 34953, loss = 0.03459951\n",
      "Iteration 34954, loss = 0.03459058\n",
      "Iteration 34955, loss = 0.03460318\n",
      "Iteration 34956, loss = 0.03461451\n",
      "Iteration 34957, loss = 0.03460553\n",
      "Iteration 34958, loss = 0.03460011\n",
      "Iteration 34959, loss = 0.03460427\n",
      "Iteration 34960, loss = 0.03460101\n",
      "Iteration 34961, loss = 0.03460616\n",
      "Iteration 34962, loss = 0.03460656\n",
      "Iteration 34963, loss = 0.03460692\n",
      "Iteration 34964, loss = 0.03460235\n",
      "Iteration 34965, loss = 0.03459034\n",
      "Iteration 34966, loss = 0.03459270\n",
      "Iteration 34967, loss = 0.03459211\n",
      "Iteration 34968, loss = 0.03459968\n",
      "Iteration 34969, loss = 0.03459948\n",
      "Iteration 34970, loss = 0.03459646\n",
      "Iteration 34971, loss = 0.03459760\n",
      "Iteration 34972, loss = 0.03460485\n",
      "Iteration 34973, loss = 0.03460163\n",
      "Iteration 34974, loss = 0.03459540\n",
      "Iteration 34975, loss = 0.03458730\n",
      "Iteration 34976, loss = 0.03459397\n",
      "Iteration 34977, loss = 0.03460175\n",
      "Iteration 34978, loss = 0.03459886\n",
      "Iteration 34979, loss = 0.03458767\n",
      "Iteration 34980, loss = 0.03459420\n",
      "Iteration 34981, loss = 0.03459327\n",
      "Iteration 34982, loss = 0.03459401\n",
      "Iteration 34983, loss = 0.03459857\n",
      "Iteration 34984, loss = 0.03459444\n",
      "Iteration 34985, loss = 0.03458618\n",
      "Iteration 34986, loss = 0.03457706\n",
      "Iteration 34987, loss = 0.03458311\n",
      "Iteration 34988, loss = 0.03459218\n",
      "Iteration 34989, loss = 0.03458187\n",
      "Iteration 34990, loss = 0.03457673\n",
      "Iteration 34991, loss = 0.03458486\n",
      "Iteration 34992, loss = 0.03458462\n",
      "Iteration 34993, loss = 0.03458787\n",
      "Iteration 34994, loss = 0.03458162\n",
      "Iteration 34995, loss = 0.03458264\n",
      "Iteration 34996, loss = 0.03457931\n",
      "Iteration 34997, loss = 0.03458246\n",
      "Iteration 34998, loss = 0.03458743\n",
      "Iteration 34999, loss = 0.03457925\n",
      "Iteration 35000, loss = 0.03456702\n",
      "Iteration 35001, loss = 0.03458753\n",
      "Iteration 35002, loss = 0.03459186\n",
      "Iteration 35003, loss = 0.03458002\n",
      "Iteration 35004, loss = 0.03456900\n",
      "Iteration 35005, loss = 0.03457764\n",
      "Iteration 35006, loss = 0.03458771\n",
      "Iteration 35007, loss = 0.03459071\n",
      "Iteration 35008, loss = 0.03458383\n",
      "Iteration 35009, loss = 0.03457318\n",
      "Iteration 35010, loss = 0.03456284\n",
      "Iteration 35011, loss = 0.03457714\n",
      "Iteration 35012, loss = 0.03458013\n",
      "Iteration 35013, loss = 0.03456801\n",
      "Iteration 35014, loss = 0.03456375\n",
      "Iteration 35015, loss = 0.03457417\n",
      "Iteration 35016, loss = 0.03458075\n",
      "Iteration 35017, loss = 0.03457739\n",
      "Iteration 35018, loss = 0.03457435\n",
      "Iteration 35019, loss = 0.03457812\n",
      "Iteration 35020, loss = 0.03457737\n",
      "Iteration 35021, loss = 0.03457815\n",
      "Iteration 35022, loss = 0.03457427\n",
      "Iteration 35023, loss = 0.03456476\n",
      "Iteration 35024, loss = 0.03457261\n",
      "Iteration 35025, loss = 0.03457495\n",
      "Iteration 35026, loss = 0.03457697\n",
      "Iteration 35027, loss = 0.03457503\n",
      "Iteration 35028, loss = 0.03457431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 35029, loss = 0.03456887\n",
      "Iteration 35030, loss = 0.03457036\n",
      "Iteration 35031, loss = 0.03457736\n",
      "Iteration 35032, loss = 0.03457487\n",
      "Iteration 35033, loss = 0.03455663\n",
      "Iteration 35034, loss = 0.03456668\n",
      "Iteration 35035, loss = 0.03457809\n",
      "Iteration 35036, loss = 0.03457702\n",
      "Iteration 35037, loss = 0.03457195\n",
      "Iteration 35038, loss = 0.03456688\n",
      "Iteration 35039, loss = 0.03455819\n",
      "Iteration 35040, loss = 0.03455610\n",
      "Iteration 35041, loss = 0.03456193\n",
      "Iteration 35042, loss = 0.03456421\n",
      "Iteration 35043, loss = 0.03455587\n",
      "Iteration 35044, loss = 0.03455729\n",
      "Iteration 35045, loss = 0.03456036\n",
      "Iteration 35046, loss = 0.03456376\n",
      "Iteration 35047, loss = 0.03455858\n",
      "Iteration 35048, loss = 0.03455994\n",
      "Iteration 35049, loss = 0.03456541\n",
      "Iteration 35050, loss = 0.03456100\n",
      "Iteration 35051, loss = 0.03456243\n",
      "Iteration 35052, loss = 0.03455918\n",
      "Iteration 35053, loss = 0.03455511\n",
      "Iteration 35054, loss = 0.03454665\n",
      "Iteration 35055, loss = 0.03455356\n",
      "Iteration 35056, loss = 0.03455958\n",
      "Iteration 35057, loss = 0.03455957\n",
      "Iteration 35058, loss = 0.03455404\n",
      "Iteration 35059, loss = 0.03455265\n",
      "Iteration 35060, loss = 0.03455399\n",
      "Iteration 35061, loss = 0.03456337\n",
      "Iteration 35062, loss = 0.03455458\n",
      "Iteration 35063, loss = 0.03455531\n",
      "Iteration 35064, loss = 0.03455008\n",
      "Iteration 35065, loss = 0.03456073\n",
      "Iteration 35066, loss = 0.03456271\n",
      "Iteration 35067, loss = 0.03455135\n",
      "Iteration 35068, loss = 0.03455166\n",
      "Iteration 35069, loss = 0.03455214\n",
      "Iteration 35070, loss = 0.03455381\n",
      "Iteration 35071, loss = 0.03455679\n",
      "Iteration 35072, loss = 0.03454847\n",
      "Iteration 35073, loss = 0.03455533\n",
      "Iteration 35074, loss = 0.03455464\n",
      "Iteration 35075, loss = 0.03455465\n",
      "Iteration 35076, loss = 0.03455349\n",
      "Iteration 35077, loss = 0.03454603\n",
      "Iteration 35078, loss = 0.03454386\n",
      "Iteration 35079, loss = 0.03454083\n",
      "Iteration 35080, loss = 0.03453488\n",
      "Iteration 35081, loss = 0.03454749\n",
      "Iteration 35082, loss = 0.03455184\n",
      "Iteration 35083, loss = 0.03454657\n",
      "Iteration 35084, loss = 0.03453837\n",
      "Iteration 35085, loss = 0.03454838\n",
      "Iteration 35086, loss = 0.03455062\n",
      "Iteration 35087, loss = 0.03454466\n",
      "Iteration 35088, loss = 0.03454237\n",
      "Iteration 35089, loss = 0.03454664\n",
      "Iteration 35090, loss = 0.03454422\n",
      "Iteration 35091, loss = 0.03452979\n",
      "Iteration 35092, loss = 0.03453944\n",
      "Iteration 35093, loss = 0.03454114\n",
      "Iteration 35094, loss = 0.03454233\n",
      "Iteration 35095, loss = 0.03454021\n",
      "Iteration 35096, loss = 0.03453465\n",
      "Iteration 35097, loss = 0.03453087\n",
      "Iteration 35098, loss = 0.03453611\n",
      "Iteration 35099, loss = 0.03453840\n",
      "Iteration 35100, loss = 0.03454042\n",
      "Iteration 35101, loss = 0.03453573\n",
      "Iteration 35102, loss = 0.03453132\n",
      "Iteration 35103, loss = 0.03453588\n",
      "Iteration 35104, loss = 0.03453487\n",
      "Iteration 35105, loss = 0.03453515\n",
      "Iteration 35106, loss = 0.03453580\n",
      "Iteration 35107, loss = 0.03453146\n",
      "Iteration 35108, loss = 0.03452498\n",
      "Iteration 35109, loss = 0.03453259\n",
      "Iteration 35110, loss = 0.03454545\n",
      "Iteration 35111, loss = 0.03453898\n",
      "Iteration 35112, loss = 0.03452009\n",
      "Iteration 35113, loss = 0.03453041\n",
      "Iteration 35114, loss = 0.03453202\n",
      "Iteration 35115, loss = 0.03453517\n",
      "Iteration 35116, loss = 0.03453964\n",
      "Iteration 35117, loss = 0.03453184\n",
      "Iteration 35118, loss = 0.03453280\n",
      "Iteration 35119, loss = 0.03453024\n",
      "Iteration 35120, loss = 0.03452937\n",
      "Iteration 35121, loss = 0.03453087\n",
      "Iteration 35122, loss = 0.03453142\n",
      "Iteration 35123, loss = 0.03452235\n",
      "Iteration 35124, loss = 0.03452585\n",
      "Iteration 35125, loss = 0.03453243\n",
      "Iteration 35126, loss = 0.03454153\n",
      "Iteration 35127, loss = 0.03453677\n",
      "Iteration 35128, loss = 0.03452243\n",
      "Iteration 35129, loss = 0.03452130\n",
      "Iteration 35130, loss = 0.03453539\n",
      "Iteration 35131, loss = 0.03454506\n",
      "Iteration 35132, loss = 0.03453584\n",
      "Iteration 35133, loss = 0.03452361\n",
      "Iteration 35134, loss = 0.03451890\n",
      "Iteration 35135, loss = 0.03453080\n",
      "Iteration 35136, loss = 0.03454544\n",
      "Iteration 35137, loss = 0.03453809\n",
      "Iteration 35138, loss = 0.03452310\n",
      "Iteration 35139, loss = 0.03453281\n",
      "Iteration 35140, loss = 0.03453398\n",
      "Iteration 35141, loss = 0.03452957\n",
      "Iteration 35142, loss = 0.03452625\n",
      "Iteration 35143, loss = 0.03452150\n",
      "Iteration 35144, loss = 0.03451406\n",
      "Iteration 35145, loss = 0.03451918\n",
      "Iteration 35146, loss = 0.03452064\n",
      "Iteration 35147, loss = 0.03451940\n",
      "Iteration 35148, loss = 0.03451711\n",
      "Iteration 35149, loss = 0.03451031\n",
      "Iteration 35150, loss = 0.03451237\n",
      "Iteration 35151, loss = 0.03451459\n",
      "Iteration 35152, loss = 0.03451649\n",
      "Iteration 35153, loss = 0.03451416\n",
      "Iteration 35154, loss = 0.03451233\n",
      "Iteration 35155, loss = 0.03451749\n",
      "Iteration 35156, loss = 0.03451108\n",
      "Iteration 35157, loss = 0.03450314\n",
      "Iteration 35158, loss = 0.03450924\n",
      "Iteration 35159, loss = 0.03451139\n",
      "Iteration 35160, loss = 0.03451269\n",
      "Iteration 35161, loss = 0.03451528\n",
      "Iteration 35162, loss = 0.03451416\n",
      "Iteration 35163, loss = 0.03450825\n",
      "Iteration 35164, loss = 0.03451051\n",
      "Iteration 35165, loss = 0.03451311\n",
      "Iteration 35166, loss = 0.03450789\n",
      "Iteration 35167, loss = 0.03450265\n",
      "Iteration 35168, loss = 0.03450678\n",
      "Iteration 35169, loss = 0.03451346\n",
      "Iteration 35170, loss = 0.03451770\n",
      "Iteration 35171, loss = 0.03451451\n",
      "Iteration 35172, loss = 0.03450744\n",
      "Iteration 35173, loss = 0.03449400\n",
      "Iteration 35174, loss = 0.03451383\n",
      "Iteration 35175, loss = 0.03451799\n",
      "Iteration 35176, loss = 0.03450129\n",
      "Iteration 35177, loss = 0.03450315\n",
      "Iteration 35178, loss = 0.03450810\n",
      "Iteration 35179, loss = 0.03451126\n",
      "Iteration 35180, loss = 0.03451120\n",
      "Iteration 35181, loss = 0.03450120\n",
      "Iteration 35182, loss = 0.03449966\n",
      "Iteration 35183, loss = 0.03450856\n",
      "Iteration 35184, loss = 0.03451556\n",
      "Iteration 35185, loss = 0.03450955\n",
      "Iteration 35186, loss = 0.03449269\n",
      "Iteration 35187, loss = 0.03449883\n",
      "Iteration 35188, loss = 0.03449930\n",
      "Iteration 35189, loss = 0.03450336\n",
      "Iteration 35190, loss = 0.03450790\n",
      "Iteration 35191, loss = 0.03450785\n",
      "Iteration 35192, loss = 0.03449891\n",
      "Iteration 35193, loss = 0.03450497\n",
      "Iteration 35194, loss = 0.03451064\n",
      "Iteration 35195, loss = 0.03450348\n",
      "Iteration 35196, loss = 0.03449336\n",
      "Iteration 35197, loss = 0.03449368\n",
      "Iteration 35198, loss = 0.03449067\n",
      "Iteration 35199, loss = 0.03449251\n",
      "Iteration 35200, loss = 0.03449790\n",
      "Iteration 35201, loss = 0.03449557\n",
      "Iteration 35202, loss = 0.03449427\n",
      "Iteration 35203, loss = 0.03451041\n",
      "Iteration 35204, loss = 0.03451209\n",
      "Iteration 35205, loss = 0.03449804\n",
      "Iteration 35206, loss = 0.03450273\n",
      "Iteration 35207, loss = 0.03451508\n",
      "Iteration 35208, loss = 0.03451226\n",
      "Iteration 35209, loss = 0.03450803\n",
      "Iteration 35210, loss = 0.03450856\n",
      "Iteration 35211, loss = 0.03449665\n",
      "Iteration 35212, loss = 0.03449402\n",
      "Iteration 35213, loss = 0.03448922\n",
      "Iteration 35214, loss = 0.03449015\n",
      "Iteration 35215, loss = 0.03449851\n",
      "Iteration 35216, loss = 0.03450467\n",
      "Iteration 35217, loss = 0.03448917\n",
      "Iteration 35218, loss = 0.03448149\n",
      "Iteration 35219, loss = 0.03449194\n",
      "Iteration 35220, loss = 0.03449504\n",
      "Iteration 35221, loss = 0.03449206\n",
      "Iteration 35222, loss = 0.03449083\n",
      "Iteration 35223, loss = 0.03450477\n",
      "Iteration 35224, loss = 0.03450098\n",
      "Iteration 35225, loss = 0.03448014\n",
      "Iteration 35226, loss = 0.03448526\n",
      "Iteration 35227, loss = 0.03449354\n",
      "Iteration 35228, loss = 0.03449971\n",
      "Iteration 35229, loss = 0.03449257\n",
      "Iteration 35230, loss = 0.03448402\n",
      "Iteration 35231, loss = 0.03449431\n",
      "Iteration 35232, loss = 0.03449548\n",
      "Iteration 35233, loss = 0.03448896\n",
      "Iteration 35234, loss = 0.03448081\n",
      "Iteration 35235, loss = 0.03447452\n",
      "Iteration 35236, loss = 0.03448654\n",
      "Iteration 35237, loss = 0.03448204\n",
      "Iteration 35238, loss = 0.03448477\n",
      "Iteration 35239, loss = 0.03448262\n",
      "Iteration 35240, loss = 0.03447909\n",
      "Iteration 35241, loss = 0.03448044\n",
      "Iteration 35242, loss = 0.03447254\n",
      "Iteration 35243, loss = 0.03447699\n",
      "Iteration 35244, loss = 0.03447728\n",
      "Iteration 35245, loss = 0.03447416\n",
      "Iteration 35246, loss = 0.03447653\n",
      "Iteration 35247, loss = 0.03448774\n",
      "Iteration 35248, loss = 0.03449385\n",
      "Iteration 35249, loss = 0.03448912\n",
      "Iteration 35250, loss = 0.03447805\n",
      "Iteration 35251, loss = 0.03446821\n",
      "Iteration 35252, loss = 0.03448521\n",
      "Iteration 35253, loss = 0.03449967\n",
      "Iteration 35254, loss = 0.03448996\n",
      "Iteration 35255, loss = 0.03447025\n",
      "Iteration 35256, loss = 0.03447439\n",
      "Iteration 35257, loss = 0.03448126\n",
      "Iteration 35258, loss = 0.03448360\n",
      "Iteration 35259, loss = 0.03448076\n",
      "Iteration 35260, loss = 0.03448394\n",
      "Iteration 35261, loss = 0.03447692\n",
      "Iteration 35262, loss = 0.03446177\n",
      "Iteration 35263, loss = 0.03447432\n",
      "Iteration 35264, loss = 0.03448158\n",
      "Iteration 35265, loss = 0.03447468\n",
      "Iteration 35266, loss = 0.03447143\n",
      "Iteration 35267, loss = 0.03446643\n",
      "Iteration 35268, loss = 0.03447242\n",
      "Iteration 35269, loss = 0.03447278\n",
      "Iteration 35270, loss = 0.03447287\n",
      "Iteration 35271, loss = 0.03446638\n",
      "Iteration 35272, loss = 0.03445712\n",
      "Iteration 35273, loss = 0.03446485\n",
      "Iteration 35274, loss = 0.03446600\n",
      "Iteration 35275, loss = 0.03445793\n",
      "Iteration 35276, loss = 0.03445775\n",
      "Iteration 35277, loss = 0.03446752\n",
      "Iteration 35278, loss = 0.03446706\n",
      "Iteration 35279, loss = 0.03446406\n",
      "Iteration 35280, loss = 0.03446289\n",
      "Iteration 35281, loss = 0.03446194\n",
      "Iteration 35282, loss = 0.03445672\n",
      "Iteration 35283, loss = 0.03446031\n",
      "Iteration 35284, loss = 0.03446352\n",
      "Iteration 35285, loss = 0.03446216\n",
      "Iteration 35286, loss = 0.03445697\n",
      "Iteration 35287, loss = 0.03445171\n",
      "Iteration 35288, loss = 0.03446814\n",
      "Iteration 35289, loss = 0.03447313\n",
      "Iteration 35290, loss = 0.03446435\n",
      "Iteration 35291, loss = 0.03446676\n",
      "Iteration 35292, loss = 0.03446479\n",
      "Iteration 35293, loss = 0.03446826\n",
      "Iteration 35294, loss = 0.03447039\n",
      "Iteration 35295, loss = 0.03446832\n",
      "Iteration 35296, loss = 0.03445405\n",
      "Iteration 35297, loss = 0.03445452\n",
      "Iteration 35298, loss = 0.03446616\n",
      "Iteration 35299, loss = 0.03446414\n",
      "Iteration 35300, loss = 0.03445254\n",
      "Iteration 35301, loss = 0.03444889\n",
      "Iteration 35302, loss = 0.03445571\n",
      "Iteration 35303, loss = 0.03445488\n",
      "Iteration 35304, loss = 0.03445599\n",
      "Iteration 35305, loss = 0.03445726\n",
      "Iteration 35306, loss = 0.03444934\n",
      "Iteration 35307, loss = 0.03444529\n",
      "Iteration 35308, loss = 0.03445153\n",
      "Iteration 35309, loss = 0.03445339\n",
      "Iteration 35310, loss = 0.03445620\n",
      "Iteration 35311, loss = 0.03445136\n",
      "Iteration 35312, loss = 0.03445102\n",
      "Iteration 35313, loss = 0.03445101\n",
      "Iteration 35314, loss = 0.03445086\n",
      "Iteration 35315, loss = 0.03444789\n",
      "Iteration 35316, loss = 0.03444242\n",
      "Iteration 35317, loss = 0.03444682\n",
      "Iteration 35318, loss = 0.03444187\n",
      "Iteration 35319, loss = 0.03444125\n",
      "Iteration 35320, loss = 0.03444064\n",
      "Iteration 35321, loss = 0.03444956\n",
      "Iteration 35322, loss = 0.03446174\n",
      "Iteration 35323, loss = 0.03445905\n",
      "Iteration 35324, loss = 0.03444037\n",
      "Iteration 35325, loss = 0.03444602\n",
      "Iteration 35326, loss = 0.03445340\n",
      "Iteration 35327, loss = 0.03445543\n",
      "Iteration 35328, loss = 0.03444914\n",
      "Iteration 35329, loss = 0.03444832\n",
      "Iteration 35330, loss = 0.03445516\n",
      "Iteration 35331, loss = 0.03445121\n",
      "Iteration 35332, loss = 0.03443740\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 35333, loss = 0.03444120\n",
      "Iteration 35334, loss = 0.03444378\n",
      "Iteration 35335, loss = 0.03444354\n",
      "Iteration 35336, loss = 0.03443106\n",
      "Iteration 35337, loss = 0.03443129\n",
      "Iteration 35338, loss = 0.03443048\n",
      "Iteration 35339, loss = 0.03442979\n",
      "Iteration 35340, loss = 0.03443507\n",
      "Iteration 35341, loss = 0.03443693\n",
      "Iteration 35342, loss = 0.03444052\n",
      "Iteration 35343, loss = 0.03444337\n",
      "Iteration 35344, loss = 0.03443292\n",
      "Iteration 35345, loss = 0.03443969\n",
      "Iteration 35346, loss = 0.03443993\n",
      "Iteration 35347, loss = 0.03444174\n",
      "Iteration 35348, loss = 0.03444738\n",
      "Iteration 35349, loss = 0.03444110\n",
      "Iteration 35350, loss = 0.03443842\n",
      "Iteration 35351, loss = 0.03443219\n",
      "Iteration 35352, loss = 0.03443204\n",
      "Iteration 35353, loss = 0.03442955\n",
      "Iteration 35354, loss = 0.03443261\n",
      "Iteration 35355, loss = 0.03443570\n",
      "Iteration 35356, loss = 0.03443204\n",
      "Iteration 35357, loss = 0.03442761\n",
      "Iteration 35358, loss = 0.03443109\n",
      "Iteration 35359, loss = 0.03443579\n",
      "Iteration 35360, loss = 0.03443422\n",
      "Iteration 35361, loss = 0.03443127\n",
      "Iteration 35362, loss = 0.03442519\n",
      "Iteration 35363, loss = 0.03442117\n",
      "Iteration 35364, loss = 0.03443665\n",
      "Iteration 35365, loss = 0.03443141\n",
      "Iteration 35366, loss = 0.03442759\n",
      "Iteration 35367, loss = 0.03442539\n",
      "Iteration 35368, loss = 0.03442872\n",
      "Iteration 35369, loss = 0.03443194\n",
      "Iteration 35370, loss = 0.03442918\n",
      "Iteration 35371, loss = 0.03441660\n",
      "Iteration 35372, loss = 0.03442223\n",
      "Iteration 35373, loss = 0.03442606\n",
      "Iteration 35374, loss = 0.03441978\n",
      "Iteration 35375, loss = 0.03442387\n",
      "Iteration 35376, loss = 0.03441877\n",
      "Iteration 35377, loss = 0.03441858\n",
      "Iteration 35378, loss = 0.03441578\n",
      "Iteration 35379, loss = 0.03441953\n",
      "Iteration 35380, loss = 0.03442530\n",
      "Iteration 35381, loss = 0.03442482\n",
      "Iteration 35382, loss = 0.03442138\n",
      "Iteration 35383, loss = 0.03441550\n",
      "Iteration 35384, loss = 0.03441355\n",
      "Iteration 35385, loss = 0.03441440\n",
      "Iteration 35386, loss = 0.03441268\n",
      "Iteration 35387, loss = 0.03441342\n",
      "Iteration 35388, loss = 0.03441645\n",
      "Iteration 35389, loss = 0.03441603\n",
      "Iteration 35390, loss = 0.03442416\n",
      "Iteration 35391, loss = 0.03442624\n",
      "Iteration 35392, loss = 0.03442044\n",
      "Iteration 35393, loss = 0.03441685\n",
      "Iteration 35394, loss = 0.03441077\n",
      "Iteration 35395, loss = 0.03441053\n",
      "Iteration 35396, loss = 0.03441262\n",
      "Iteration 35397, loss = 0.03440766\n",
      "Iteration 35398, loss = 0.03440638\n",
      "Iteration 35399, loss = 0.03441084\n",
      "Iteration 35400, loss = 0.03441008\n",
      "Iteration 35401, loss = 0.03440591\n",
      "Iteration 35402, loss = 0.03441472\n",
      "Iteration 35403, loss = 0.03441573\n",
      "Iteration 35404, loss = 0.03440799\n",
      "Iteration 35405, loss = 0.03441765\n",
      "Iteration 35406, loss = 0.03442156\n",
      "Iteration 35407, loss = 0.03441201\n",
      "Iteration 35408, loss = 0.03441722\n",
      "Iteration 35409, loss = 0.03441843\n",
      "Iteration 35410, loss = 0.03440712\n",
      "Iteration 35411, loss = 0.03441024\n",
      "Iteration 35412, loss = 0.03441468\n",
      "Iteration 35413, loss = 0.03441627\n",
      "Iteration 35414, loss = 0.03441525\n",
      "Iteration 35415, loss = 0.03440920\n",
      "Iteration 35416, loss = 0.03440909\n",
      "Iteration 35417, loss = 0.03440830\n",
      "Iteration 35418, loss = 0.03439857\n",
      "Iteration 35419, loss = 0.03440513\n",
      "Iteration 35420, loss = 0.03440457\n",
      "Iteration 35421, loss = 0.03440251\n",
      "Iteration 35422, loss = 0.03439650\n",
      "Iteration 35423, loss = 0.03439730\n",
      "Iteration 35424, loss = 0.03439729\n",
      "Iteration 35425, loss = 0.03439145\n",
      "Iteration 35426, loss = 0.03440118\n",
      "Iteration 35427, loss = 0.03440571\n",
      "Iteration 35428, loss = 0.03440749\n",
      "Iteration 35429, loss = 0.03439961\n",
      "Iteration 35430, loss = 0.03439860\n",
      "Iteration 35431, loss = 0.03439592\n",
      "Iteration 35432, loss = 0.03439498\n",
      "Iteration 35433, loss = 0.03438722\n",
      "Iteration 35434, loss = 0.03438841\n",
      "Iteration 35435, loss = 0.03438499\n",
      "Iteration 35436, loss = 0.03439347\n",
      "Iteration 35437, loss = 0.03439298\n",
      "Iteration 35438, loss = 0.03438895\n",
      "Iteration 35439, loss = 0.03439191\n",
      "Iteration 35440, loss = 0.03439200\n",
      "Iteration 35441, loss = 0.03438720\n",
      "Iteration 35442, loss = 0.03438634\n",
      "Iteration 35443, loss = 0.03439252\n",
      "Iteration 35444, loss = 0.03438951\n",
      "Iteration 35445, loss = 0.03439040\n",
      "Iteration 35446, loss = 0.03439164\n",
      "Iteration 35447, loss = 0.03439124\n",
      "Iteration 35448, loss = 0.03439329\n",
      "Iteration 35449, loss = 0.03438914\n",
      "Iteration 35450, loss = 0.03439303\n",
      "Iteration 35451, loss = 0.03439498\n",
      "Iteration 35452, loss = 0.03439915\n",
      "Iteration 35453, loss = 0.03439651\n",
      "Iteration 35454, loss = 0.03439022\n",
      "Iteration 35455, loss = 0.03438703\n",
      "Iteration 35456, loss = 0.03438999\n",
      "Iteration 35457, loss = 0.03439211\n",
      "Iteration 35458, loss = 0.03439329\n",
      "Iteration 35459, loss = 0.03438526\n",
      "Iteration 35460, loss = 0.03438851\n",
      "Iteration 35461, loss = 0.03439103\n",
      "Iteration 35462, loss = 0.03438896\n",
      "Iteration 35463, loss = 0.03438868\n",
      "Iteration 35464, loss = 0.03438923\n",
      "Iteration 35465, loss = 0.03438638\n",
      "Iteration 35466, loss = 0.03439062\n",
      "Iteration 35467, loss = 0.03439273\n",
      "Iteration 35468, loss = 0.03438804\n",
      "Iteration 35469, loss = 0.03438323\n",
      "Iteration 35470, loss = 0.03437453\n",
      "Iteration 35471, loss = 0.03437646\n",
      "Iteration 35472, loss = 0.03437280\n",
      "Iteration 35473, loss = 0.03438445\n",
      "Iteration 35474, loss = 0.03438514\n",
      "Iteration 35475, loss = 0.03437827\n",
      "Iteration 35476, loss = 0.03437372\n",
      "Iteration 35477, loss = 0.03438245\n",
      "Iteration 35478, loss = 0.03438230\n",
      "Iteration 35479, loss = 0.03437399\n",
      "Iteration 35480, loss = 0.03437966\n",
      "Iteration 35481, loss = 0.03438391\n",
      "Iteration 35482, loss = 0.03438592\n",
      "Iteration 35483, loss = 0.03439140\n",
      "Iteration 35484, loss = 0.03439091\n",
      "Iteration 35485, loss = 0.03438338\n",
      "Iteration 35486, loss = 0.03437819\n",
      "Iteration 35487, loss = 0.03437413\n",
      "Iteration 35488, loss = 0.03438042\n",
      "Iteration 35489, loss = 0.03438347\n",
      "Iteration 35490, loss = 0.03438201\n",
      "Iteration 35491, loss = 0.03437336\n",
      "Iteration 35492, loss = 0.03437448\n",
      "Iteration 35493, loss = 0.03437427\n",
      "Iteration 35494, loss = 0.03437069\n",
      "Iteration 35495, loss = 0.03436737\n",
      "Iteration 35496, loss = 0.03436763\n",
      "Iteration 35497, loss = 0.03436352\n",
      "Iteration 35498, loss = 0.03436794\n",
      "Iteration 35499, loss = 0.03436670\n",
      "Iteration 35500, loss = 0.03437276\n",
      "Iteration 35501, loss = 0.03437434\n",
      "Iteration 35502, loss = 0.03436570\n",
      "Iteration 35503, loss = 0.03436641\n",
      "Iteration 35504, loss = 0.03437611\n",
      "Iteration 35505, loss = 0.03436827\n",
      "Iteration 35506, loss = 0.03436598\n",
      "Iteration 35507, loss = 0.03436158\n",
      "Iteration 35508, loss = 0.03436407\n",
      "Iteration 35509, loss = 0.03437288\n",
      "Iteration 35510, loss = 0.03437413\n",
      "Iteration 35511, loss = 0.03437281\n",
      "Iteration 35512, loss = 0.03436724\n",
      "Iteration 35513, loss = 0.03436677\n",
      "Iteration 35514, loss = 0.03436938\n",
      "Iteration 35515, loss = 0.03435986\n",
      "Iteration 35516, loss = 0.03435490\n",
      "Iteration 35517, loss = 0.03436463\n",
      "Iteration 35518, loss = 0.03436543\n",
      "Iteration 35519, loss = 0.03436042\n",
      "Iteration 35520, loss = 0.03436289\n",
      "Iteration 35521, loss = 0.03436209\n",
      "Iteration 35522, loss = 0.03436895\n",
      "Iteration 35523, loss = 0.03437033\n",
      "Iteration 35524, loss = 0.03437216\n",
      "Iteration 35525, loss = 0.03436227\n",
      "Iteration 35526, loss = 0.03435218\n",
      "Iteration 35527, loss = 0.03436411\n",
      "Iteration 35528, loss = 0.03435954\n",
      "Iteration 35529, loss = 0.03435948\n",
      "Iteration 35530, loss = 0.03435818\n",
      "Iteration 35531, loss = 0.03436012\n",
      "Iteration 35532, loss = 0.03435762\n",
      "Iteration 35533, loss = 0.03436272\n",
      "Iteration 35534, loss = 0.03436332\n",
      "Iteration 35535, loss = 0.03435824\n",
      "Iteration 35536, loss = 0.03434764\n",
      "Iteration 35537, loss = 0.03436318\n",
      "Iteration 35538, loss = 0.03436333\n",
      "Iteration 35539, loss = 0.03435783\n",
      "Iteration 35540, loss = 0.03435081\n",
      "Iteration 35541, loss = 0.03435590\n",
      "Iteration 35542, loss = 0.03435973\n",
      "Iteration 35543, loss = 0.03435967\n",
      "Iteration 35544, loss = 0.03435084\n",
      "Iteration 35545, loss = 0.03435599\n",
      "Iteration 35546, loss = 0.03435801\n",
      "Iteration 35547, loss = 0.03434692\n",
      "Iteration 35548, loss = 0.03434633\n",
      "Iteration 35549, loss = 0.03435046\n",
      "Iteration 35550, loss = 0.03435380\n",
      "Iteration 35551, loss = 0.03434887\n",
      "Iteration 35552, loss = 0.03434678\n",
      "Iteration 35553, loss = 0.03434112\n",
      "Iteration 35554, loss = 0.03434712\n",
      "Iteration 35555, loss = 0.03434568\n",
      "Iteration 35556, loss = 0.03434745\n",
      "Iteration 35557, loss = 0.03434701\n",
      "Iteration 35558, loss = 0.03433963\n",
      "Iteration 35559, loss = 0.03434344\n",
      "Iteration 35560, loss = 0.03434813\n",
      "Iteration 35561, loss = 0.03434785\n",
      "Iteration 35562, loss = 0.03434290\n",
      "Iteration 35563, loss = 0.03434017\n",
      "Iteration 35564, loss = 0.03434352\n",
      "Iteration 35565, loss = 0.03434142\n",
      "Iteration 35566, loss = 0.03433509\n",
      "Iteration 35567, loss = 0.03433937\n",
      "Iteration 35568, loss = 0.03434091\n",
      "Iteration 35569, loss = 0.03434204\n",
      "Iteration 35570, loss = 0.03434359\n",
      "Iteration 35571, loss = 0.03433740\n",
      "Iteration 35572, loss = 0.03433915\n",
      "Iteration 35573, loss = 0.03434439\n",
      "Iteration 35574, loss = 0.03434195\n",
      "Iteration 35575, loss = 0.03434199\n",
      "Iteration 35576, loss = 0.03434005\n",
      "Iteration 35577, loss = 0.03433741\n",
      "Iteration 35578, loss = 0.03434109\n",
      "Iteration 35579, loss = 0.03435046\n",
      "Iteration 35580, loss = 0.03434916\n",
      "Iteration 35581, loss = 0.03434478\n",
      "Iteration 35582, loss = 0.03433428\n",
      "Iteration 35583, loss = 0.03433252\n",
      "Iteration 35584, loss = 0.03433257\n",
      "Iteration 35585, loss = 0.03434270\n",
      "Iteration 35586, loss = 0.03434912\n",
      "Iteration 35587, loss = 0.03434390\n",
      "Iteration 35588, loss = 0.03433571\n",
      "Iteration 35589, loss = 0.03433915\n",
      "Iteration 35590, loss = 0.03434194\n",
      "Iteration 35591, loss = 0.03433348\n",
      "Iteration 35592, loss = 0.03432451\n",
      "Iteration 35593, loss = 0.03432814\n",
      "Iteration 35594, loss = 0.03432653\n",
      "Iteration 35595, loss = 0.03433141\n",
      "Iteration 35596, loss = 0.03432911\n",
      "Iteration 35597, loss = 0.03432361\n",
      "Iteration 35598, loss = 0.03432753\n",
      "Iteration 35599, loss = 0.03432927\n",
      "Iteration 35600, loss = 0.03432113\n",
      "Iteration 35601, loss = 0.03432767\n",
      "Iteration 35602, loss = 0.03432737\n",
      "Iteration 35603, loss = 0.03432342\n",
      "Iteration 35604, loss = 0.03432870\n",
      "Iteration 35605, loss = 0.03433513\n",
      "Iteration 35606, loss = 0.03433034\n",
      "Iteration 35607, loss = 0.03432758\n",
      "Iteration 35608, loss = 0.03432494\n",
      "Iteration 35609, loss = 0.03432930\n",
      "Iteration 35610, loss = 0.03433354\n",
      "Iteration 35611, loss = 0.03432204\n",
      "Iteration 35612, loss = 0.03432470\n",
      "Iteration 35613, loss = 0.03433088\n",
      "Iteration 35614, loss = 0.03433194\n",
      "Iteration 35615, loss = 0.03432405\n",
      "Iteration 35616, loss = 0.03432311\n",
      "Iteration 35617, loss = 0.03432747\n",
      "Iteration 35618, loss = 0.03432149\n",
      "Iteration 35619, loss = 0.03432468\n",
      "Iteration 35620, loss = 0.03432922\n",
      "Iteration 35621, loss = 0.03432197\n",
      "Iteration 35622, loss = 0.03432540\n",
      "Iteration 35623, loss = 0.03433120\n",
      "Iteration 35624, loss = 0.03432531\n",
      "Iteration 35625, loss = 0.03431839\n",
      "Iteration 35626, loss = 0.03432432\n",
      "Iteration 35627, loss = 0.03432115\n",
      "Iteration 35628, loss = 0.03431845\n",
      "Iteration 35629, loss = 0.03432589\n",
      "Iteration 35630, loss = 0.03432947\n",
      "Iteration 35631, loss = 0.03432631\n",
      "Iteration 35632, loss = 0.03432427\n",
      "Iteration 35633, loss = 0.03432364\n",
      "Iteration 35634, loss = 0.03432553\n",
      "Iteration 35635, loss = 0.03432578\n",
      "Iteration 35636, loss = 0.03431973\n",
      "Iteration 35637, loss = 0.03431374\n",
      "Iteration 35638, loss = 0.03431473\n",
      "Iteration 35639, loss = 0.03431729\n",
      "Iteration 35640, loss = 0.03430929\n",
      "Iteration 35641, loss = 0.03431581\n",
      "Iteration 35642, loss = 0.03431846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 35643, loss = 0.03431894\n",
      "Iteration 35644, loss = 0.03431833\n",
      "Iteration 35645, loss = 0.03431349\n",
      "Iteration 35646, loss = 0.03431082\n",
      "Iteration 35647, loss = 0.03431982\n",
      "Iteration 35648, loss = 0.03431728\n",
      "Iteration 35649, loss = 0.03431324\n",
      "Iteration 35650, loss = 0.03430349\n",
      "Iteration 35651, loss = 0.03431057\n",
      "Iteration 35652, loss = 0.03431350\n",
      "Iteration 35653, loss = 0.03430872\n",
      "Iteration 35654, loss = 0.03430924\n",
      "Iteration 35655, loss = 0.03431011\n",
      "Iteration 35656, loss = 0.03430327\n",
      "Iteration 35657, loss = 0.03430404\n",
      "Iteration 35658, loss = 0.03430489\n",
      "Iteration 35659, loss = 0.03430174\n",
      "Iteration 35660, loss = 0.03430287\n",
      "Iteration 35661, loss = 0.03430776\n",
      "Iteration 35662, loss = 0.03430602\n",
      "Iteration 35663, loss = 0.03429925\n",
      "Iteration 35664, loss = 0.03430339\n",
      "Iteration 35665, loss = 0.03430012\n",
      "Iteration 35666, loss = 0.03429875\n",
      "Iteration 35667, loss = 0.03430376\n",
      "Iteration 35668, loss = 0.03430299\n",
      "Iteration 35669, loss = 0.03429595\n",
      "Iteration 35670, loss = 0.03429534\n",
      "Iteration 35671, loss = 0.03430527\n",
      "Iteration 35672, loss = 0.03429953\n",
      "Iteration 35673, loss = 0.03429588\n",
      "Iteration 35674, loss = 0.03429840\n",
      "Iteration 35675, loss = 0.03430735\n",
      "Iteration 35676, loss = 0.03430827\n",
      "Iteration 35677, loss = 0.03430164\n",
      "Iteration 35678, loss = 0.03430147\n",
      "Iteration 35679, loss = 0.03430026\n",
      "Iteration 35680, loss = 0.03429617\n",
      "Iteration 35681, loss = 0.03428914\n",
      "Iteration 35682, loss = 0.03429918\n",
      "Iteration 35683, loss = 0.03431557\n",
      "Iteration 35684, loss = 0.03431333\n",
      "Iteration 35685, loss = 0.03429147\n",
      "Iteration 35686, loss = 0.03430234\n",
      "Iteration 35687, loss = 0.03431535\n",
      "Iteration 35688, loss = 0.03431954\n",
      "Iteration 35689, loss = 0.03431591\n",
      "Iteration 35690, loss = 0.03431090\n",
      "Iteration 35691, loss = 0.03430420\n",
      "Iteration 35692, loss = 0.03430551\n",
      "Iteration 35693, loss = 0.03429977\n",
      "Iteration 35694, loss = 0.03429989\n",
      "Iteration 35695, loss = 0.03429347\n",
      "Iteration 35696, loss = 0.03430301\n",
      "Iteration 35697, loss = 0.03429849\n",
      "Iteration 35698, loss = 0.03429727\n",
      "Iteration 35699, loss = 0.03430148\n",
      "Iteration 35700, loss = 0.03429592\n",
      "Iteration 35701, loss = 0.03429439\n",
      "Iteration 35702, loss = 0.03429398\n",
      "Iteration 35703, loss = 0.03429398\n",
      "Iteration 35704, loss = 0.03429229\n",
      "Iteration 35705, loss = 0.03430475\n",
      "Iteration 35706, loss = 0.03430595\n",
      "Iteration 35707, loss = 0.03430512\n",
      "Iteration 35708, loss = 0.03430236\n",
      "Iteration 35709, loss = 0.03430117\n",
      "Iteration 35710, loss = 0.03429235\n",
      "Iteration 35711, loss = 0.03429823\n",
      "Iteration 35712, loss = 0.03430073\n",
      "Iteration 35713, loss = 0.03429605\n",
      "Iteration 35714, loss = 0.03429574\n",
      "Iteration 35715, loss = 0.03428863\n",
      "Iteration 35716, loss = 0.03428551\n",
      "Iteration 35717, loss = 0.03429556\n",
      "Iteration 35718, loss = 0.03429967\n",
      "Iteration 35719, loss = 0.03429672\n",
      "Iteration 35720, loss = 0.03429447\n",
      "Iteration 35721, loss = 0.03428452\n",
      "Iteration 35722, loss = 0.03428650\n",
      "Iteration 35723, loss = 0.03429147\n",
      "Iteration 35724, loss = 0.03430781\n",
      "Iteration 35725, loss = 0.03430757\n",
      "Iteration 35726, loss = 0.03428930\n",
      "Iteration 35727, loss = 0.03428628\n",
      "Iteration 35728, loss = 0.03428977\n",
      "Iteration 35729, loss = 0.03431042\n",
      "Iteration 35730, loss = 0.03431427\n",
      "Iteration 35731, loss = 0.03429824\n",
      "Iteration 35732, loss = 0.03427831\n",
      "Iteration 35733, loss = 0.03428506\n",
      "Iteration 35734, loss = 0.03429526\n",
      "Iteration 35735, loss = 0.03429468\n",
      "Iteration 35736, loss = 0.03428874\n",
      "Iteration 35737, loss = 0.03428369\n",
      "Iteration 35738, loss = 0.03428272\n",
      "Iteration 35739, loss = 0.03427782\n",
      "Iteration 35740, loss = 0.03427439\n",
      "Iteration 35741, loss = 0.03427894\n",
      "Iteration 35742, loss = 0.03428137\n",
      "Iteration 35743, loss = 0.03427160\n",
      "Iteration 35744, loss = 0.03427409\n",
      "Iteration 35745, loss = 0.03428323\n",
      "Iteration 35746, loss = 0.03428643\n",
      "Iteration 35747, loss = 0.03427992\n",
      "Iteration 35748, loss = 0.03426953\n",
      "Iteration 35749, loss = 0.03427453\n",
      "Iteration 35750, loss = 0.03428259\n",
      "Iteration 35751, loss = 0.03428429\n",
      "Iteration 35752, loss = 0.03428388\n",
      "Iteration 35753, loss = 0.03427048\n",
      "Iteration 35754, loss = 0.03427344\n",
      "Iteration 35755, loss = 0.03427536\n",
      "Iteration 35756, loss = 0.03428465\n",
      "Iteration 35757, loss = 0.03428810\n",
      "Iteration 35758, loss = 0.03427948\n",
      "Iteration 35759, loss = 0.03426688\n",
      "Iteration 35760, loss = 0.03426680\n",
      "Iteration 35761, loss = 0.03426856\n",
      "Iteration 35762, loss = 0.03427627\n",
      "Iteration 35763, loss = 0.03427581\n",
      "Iteration 35764, loss = 0.03426578\n",
      "Iteration 35765, loss = 0.03426784\n",
      "Iteration 35766, loss = 0.03426514\n",
      "Iteration 35767, loss = 0.03426625\n",
      "Iteration 35768, loss = 0.03427315\n",
      "Iteration 35769, loss = 0.03426790\n",
      "Iteration 35770, loss = 0.03426526\n",
      "Iteration 35771, loss = 0.03426748\n",
      "Iteration 35772, loss = 0.03427388\n",
      "Iteration 35773, loss = 0.03427121\n",
      "Iteration 35774, loss = 0.03426466\n",
      "Iteration 35775, loss = 0.03426378\n",
      "Iteration 35776, loss = 0.03425752\n",
      "Iteration 35777, loss = 0.03426196\n",
      "Iteration 35778, loss = 0.03426635\n",
      "Iteration 35779, loss = 0.03426114\n",
      "Iteration 35780, loss = 0.03424903\n",
      "Iteration 35781, loss = 0.03425823\n",
      "Iteration 35782, loss = 0.03425776\n",
      "Iteration 35783, loss = 0.03426011\n",
      "Iteration 35784, loss = 0.03426144\n",
      "Iteration 35785, loss = 0.03426399\n",
      "Iteration 35786, loss = 0.03425660\n",
      "Iteration 35787, loss = 0.03425738\n",
      "Iteration 35788, loss = 0.03426267\n",
      "Iteration 35789, loss = 0.03425911\n",
      "Iteration 35790, loss = 0.03425673\n",
      "Iteration 35791, loss = 0.03425069\n",
      "Iteration 35792, loss = 0.03426228\n",
      "Iteration 35793, loss = 0.03426036\n",
      "Iteration 35794, loss = 0.03424897\n",
      "Iteration 35795, loss = 0.03425227\n",
      "Iteration 35796, loss = 0.03426025\n",
      "Iteration 35797, loss = 0.03425698\n",
      "Iteration 35798, loss = 0.03424880\n",
      "Iteration 35799, loss = 0.03425099\n",
      "Iteration 35800, loss = 0.03424349\n",
      "Iteration 35801, loss = 0.03425778\n",
      "Iteration 35802, loss = 0.03425492\n",
      "Iteration 35803, loss = 0.03425031\n",
      "Iteration 35804, loss = 0.03424836\n",
      "Iteration 35805, loss = 0.03424927\n",
      "Iteration 35806, loss = 0.03425047\n",
      "Iteration 35807, loss = 0.03425276\n",
      "Iteration 35808, loss = 0.03424521\n",
      "Iteration 35809, loss = 0.03424419\n",
      "Iteration 35810, loss = 0.03424651\n",
      "Iteration 35811, loss = 0.03424729\n",
      "Iteration 35812, loss = 0.03424828\n",
      "Iteration 35813, loss = 0.03424248\n",
      "Iteration 35814, loss = 0.03423791\n",
      "Iteration 35815, loss = 0.03424624\n",
      "Iteration 35816, loss = 0.03424300\n",
      "Iteration 35817, loss = 0.03423435\n",
      "Iteration 35818, loss = 0.03425121\n",
      "Iteration 35819, loss = 0.03425304\n",
      "Iteration 35820, loss = 0.03425020\n",
      "Iteration 35821, loss = 0.03425631\n",
      "Iteration 35822, loss = 0.03425520\n",
      "Iteration 35823, loss = 0.03425325\n",
      "Iteration 35824, loss = 0.03424651\n",
      "Iteration 35825, loss = 0.03424088\n",
      "Iteration 35826, loss = 0.03424605\n",
      "Iteration 35827, loss = 0.03425504\n",
      "Iteration 35828, loss = 0.03425351\n",
      "Iteration 35829, loss = 0.03423998\n",
      "Iteration 35830, loss = 0.03423313\n",
      "Iteration 35831, loss = 0.03424119\n",
      "Iteration 35832, loss = 0.03424498\n",
      "Iteration 35833, loss = 0.03423788\n",
      "Iteration 35834, loss = 0.03422895\n",
      "Iteration 35835, loss = 0.03423491\n",
      "Iteration 35836, loss = 0.03424672\n",
      "Iteration 35837, loss = 0.03424803\n",
      "Iteration 35838, loss = 0.03424465\n",
      "Iteration 35839, loss = 0.03423682\n",
      "Iteration 35840, loss = 0.03423301\n",
      "Iteration 35841, loss = 0.03424023\n",
      "Iteration 35842, loss = 0.03423241\n",
      "Iteration 35843, loss = 0.03423264\n",
      "Iteration 35844, loss = 0.03423834\n",
      "Iteration 35845, loss = 0.03423408\n",
      "Iteration 35846, loss = 0.03422776\n",
      "Iteration 35847, loss = 0.03423264\n",
      "Iteration 35848, loss = 0.03422821\n",
      "Iteration 35849, loss = 0.03423508\n",
      "Iteration 35850, loss = 0.03424046\n",
      "Iteration 35851, loss = 0.03423924\n",
      "Iteration 35852, loss = 0.03423530\n",
      "Iteration 35853, loss = 0.03422997\n",
      "Iteration 35854, loss = 0.03424384\n",
      "Iteration 35855, loss = 0.03424451\n",
      "Iteration 35856, loss = 0.03424132\n",
      "Iteration 35857, loss = 0.03423781\n",
      "Iteration 35858, loss = 0.03422304\n",
      "Iteration 35859, loss = 0.03422484\n",
      "Iteration 35860, loss = 0.03422736\n",
      "Iteration 35861, loss = 0.03422771\n",
      "Iteration 35862, loss = 0.03423274\n",
      "Iteration 35863, loss = 0.03423258\n",
      "Iteration 35864, loss = 0.03423042\n",
      "Iteration 35865, loss = 0.03422998\n",
      "Iteration 35866, loss = 0.03423057\n",
      "Iteration 35867, loss = 0.03423184\n",
      "Iteration 35868, loss = 0.03422475\n",
      "Iteration 35869, loss = 0.03422927\n",
      "Iteration 35870, loss = 0.03423329\n",
      "Iteration 35871, loss = 0.03422840\n",
      "Iteration 35872, loss = 0.03422426\n",
      "Iteration 35873, loss = 0.03422379\n",
      "Iteration 35874, loss = 0.03421261\n",
      "Iteration 35875, loss = 0.03421953\n",
      "Iteration 35876, loss = 0.03422676\n",
      "Iteration 35877, loss = 0.03422445\n",
      "Iteration 35878, loss = 0.03421481\n",
      "Iteration 35879, loss = 0.03420798\n",
      "Iteration 35880, loss = 0.03422329\n",
      "Iteration 35881, loss = 0.03422500\n",
      "Iteration 35882, loss = 0.03421892\n",
      "Iteration 35883, loss = 0.03421874\n",
      "Iteration 35884, loss = 0.03421899\n",
      "Iteration 35885, loss = 0.03421998\n",
      "Iteration 35886, loss = 0.03422204\n",
      "Iteration 35887, loss = 0.03421730\n",
      "Iteration 35888, loss = 0.03421424\n",
      "Iteration 35889, loss = 0.03421888\n",
      "Iteration 35890, loss = 0.03421659\n",
      "Iteration 35891, loss = 0.03421909\n",
      "Iteration 35892, loss = 0.03420954\n",
      "Iteration 35893, loss = 0.03421192\n",
      "Iteration 35894, loss = 0.03422072\n",
      "Iteration 35895, loss = 0.03422037\n",
      "Iteration 35896, loss = 0.03422158\n",
      "Iteration 35897, loss = 0.03421752\n",
      "Iteration 35898, loss = 0.03421081\n",
      "Iteration 35899, loss = 0.03420584\n",
      "Iteration 35900, loss = 0.03421638\n",
      "Iteration 35901, loss = 0.03421561\n",
      "Iteration 35902, loss = 0.03420420\n",
      "Iteration 35903, loss = 0.03421259\n",
      "Iteration 35904, loss = 0.03421259\n",
      "Iteration 35905, loss = 0.03421634\n",
      "Iteration 35906, loss = 0.03423008\n",
      "Iteration 35907, loss = 0.03422649\n",
      "Iteration 35908, loss = 0.03420672\n",
      "Iteration 35909, loss = 0.03421678\n",
      "Iteration 35910, loss = 0.03422208\n",
      "Iteration 35911, loss = 0.03423681\n",
      "Iteration 35912, loss = 0.03423708\n",
      "Iteration 35913, loss = 0.03422112\n",
      "Iteration 35914, loss = 0.03419872\n",
      "Iteration 35915, loss = 0.03420587\n",
      "Iteration 35916, loss = 0.03420663\n",
      "Iteration 35917, loss = 0.03420486\n",
      "Iteration 35918, loss = 0.03419881\n",
      "Iteration 35919, loss = 0.03419984\n",
      "Iteration 35920, loss = 0.03419872\n",
      "Iteration 35921, loss = 0.03419984\n",
      "Iteration 35922, loss = 0.03420077\n",
      "Iteration 35923, loss = 0.03420088\n",
      "Iteration 35924, loss = 0.03419936\n",
      "Iteration 35925, loss = 0.03419578\n",
      "Iteration 35926, loss = 0.03419895\n",
      "Iteration 35927, loss = 0.03419932\n",
      "Iteration 35928, loss = 0.03419530\n",
      "Iteration 35929, loss = 0.03420156\n",
      "Iteration 35930, loss = 0.03420112\n",
      "Iteration 35931, loss = 0.03418977\n",
      "Iteration 35932, loss = 0.03419751\n",
      "Iteration 35933, loss = 0.03419867\n",
      "Iteration 35934, loss = 0.03419250\n",
      "Iteration 35935, loss = 0.03419599\n",
      "Iteration 35936, loss = 0.03420303\n",
      "Iteration 35937, loss = 0.03420300\n",
      "Iteration 35938, loss = 0.03419892\n",
      "Iteration 35939, loss = 0.03419814\n",
      "Iteration 35940, loss = 0.03418687\n",
      "Iteration 35941, loss = 0.03419982\n",
      "Iteration 35942, loss = 0.03420555\n",
      "Iteration 35943, loss = 0.03419929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 35944, loss = 0.03419164\n",
      "Iteration 35945, loss = 0.03419243\n",
      "Iteration 35946, loss = 0.03419221\n",
      "Iteration 35947, loss = 0.03419002\n",
      "Iteration 35948, loss = 0.03419252\n",
      "Iteration 35949, loss = 0.03418941\n",
      "Iteration 35950, loss = 0.03419073\n",
      "Iteration 35951, loss = 0.03419779\n",
      "Iteration 35952, loss = 0.03419624\n",
      "Iteration 35953, loss = 0.03418329\n",
      "Iteration 35954, loss = 0.03419202\n",
      "Iteration 35955, loss = 0.03419876\n",
      "Iteration 35956, loss = 0.03420507\n",
      "Iteration 35957, loss = 0.03420322\n",
      "Iteration 35958, loss = 0.03419498\n",
      "Iteration 35959, loss = 0.03418831\n",
      "Iteration 35960, loss = 0.03419184\n",
      "Iteration 35961, loss = 0.03420317\n",
      "Iteration 35962, loss = 0.03420208\n",
      "Iteration 35963, loss = 0.03419134\n",
      "Iteration 35964, loss = 0.03419123\n",
      "Iteration 35965, loss = 0.03418633\n",
      "Iteration 35966, loss = 0.03419167\n",
      "Iteration 35967, loss = 0.03419607\n",
      "Iteration 35968, loss = 0.03419340\n",
      "Iteration 35969, loss = 0.03418228\n",
      "Iteration 35970, loss = 0.03418260\n",
      "Iteration 35971, loss = 0.03418894\n",
      "Iteration 35972, loss = 0.03417933\n",
      "Iteration 35973, loss = 0.03417721\n",
      "Iteration 35974, loss = 0.03419033\n",
      "Iteration 35975, loss = 0.03419621\n",
      "Iteration 35976, loss = 0.03419333\n",
      "Iteration 35977, loss = 0.03418106\n",
      "Iteration 35978, loss = 0.03419135\n",
      "Iteration 35979, loss = 0.03418748\n",
      "Iteration 35980, loss = 0.03417516\n",
      "Iteration 35981, loss = 0.03417644\n",
      "Iteration 35982, loss = 0.03418215\n",
      "Iteration 35983, loss = 0.03418316\n",
      "Iteration 35984, loss = 0.03418204\n",
      "Iteration 35985, loss = 0.03417392\n",
      "Iteration 35986, loss = 0.03417277\n",
      "Iteration 35987, loss = 0.03418328\n",
      "Iteration 35988, loss = 0.03418451\n",
      "Iteration 35989, loss = 0.03418301\n",
      "Iteration 35990, loss = 0.03417279\n",
      "Iteration 35991, loss = 0.03417699\n",
      "Iteration 35992, loss = 0.03417946\n",
      "Iteration 35993, loss = 0.03418072\n",
      "Iteration 35994, loss = 0.03418614\n",
      "Iteration 35995, loss = 0.03418693\n",
      "Iteration 35996, loss = 0.03417345\n",
      "Iteration 35997, loss = 0.03416853\n",
      "Iteration 35998, loss = 0.03417157\n",
      "Iteration 35999, loss = 0.03417498\n",
      "Iteration 36000, loss = 0.03417583\n",
      "Iteration 36001, loss = 0.03417314\n",
      "Iteration 36002, loss = 0.03416916\n",
      "Iteration 36003, loss = 0.03417452\n",
      "Iteration 36004, loss = 0.03417259\n",
      "Iteration 36005, loss = 0.03416970\n",
      "Iteration 36006, loss = 0.03417565\n",
      "Iteration 36007, loss = 0.03417558\n",
      "Iteration 36008, loss = 0.03417612\n",
      "Iteration 36009, loss = 0.03417362\n",
      "Iteration 36010, loss = 0.03416147\n",
      "Iteration 36011, loss = 0.03416557\n",
      "Iteration 36012, loss = 0.03417049\n",
      "Iteration 36013, loss = 0.03416672\n",
      "Iteration 36014, loss = 0.03417310\n",
      "Iteration 36015, loss = 0.03417607\n",
      "Iteration 36016, loss = 0.03417797\n",
      "Iteration 36017, loss = 0.03417376\n",
      "Iteration 36018, loss = 0.03416815\n",
      "Iteration 36019, loss = 0.03416952\n",
      "Iteration 36020, loss = 0.03416683\n",
      "Iteration 36021, loss = 0.03416251\n",
      "Iteration 36022, loss = 0.03416907\n",
      "Iteration 36023, loss = 0.03417471\n",
      "Iteration 36024, loss = 0.03417672\n",
      "Iteration 36025, loss = 0.03417092\n",
      "Iteration 36026, loss = 0.03417518\n",
      "Iteration 36027, loss = 0.03417756\n",
      "Iteration 36028, loss = 0.03417198\n",
      "Iteration 36029, loss = 0.03417206\n",
      "Iteration 36030, loss = 0.03417808\n",
      "Iteration 36031, loss = 0.03418515\n",
      "Iteration 36032, loss = 0.03418021\n",
      "Iteration 36033, loss = 0.03416552\n",
      "Iteration 36034, loss = 0.03415206\n",
      "Iteration 36035, loss = 0.03416642\n",
      "Iteration 36036, loss = 0.03417574\n",
      "Iteration 36037, loss = 0.03416577\n",
      "Iteration 36038, loss = 0.03415493\n",
      "Iteration 36039, loss = 0.03415960\n",
      "Iteration 36040, loss = 0.03416922\n",
      "Iteration 36041, loss = 0.03417515\n",
      "Iteration 36042, loss = 0.03416717\n",
      "Iteration 36043, loss = 0.03415438\n",
      "Iteration 36044, loss = 0.03414971\n",
      "Iteration 36045, loss = 0.03416667\n",
      "Iteration 36046, loss = 0.03417224\n",
      "Iteration 36047, loss = 0.03417166\n",
      "Iteration 36048, loss = 0.03416636\n",
      "Iteration 36049, loss = 0.03414917\n",
      "Iteration 36050, loss = 0.03415186\n",
      "Iteration 36051, loss = 0.03416548\n",
      "Iteration 36052, loss = 0.03415466\n",
      "Iteration 36053, loss = 0.03415512\n",
      "Iteration 36054, loss = 0.03415413\n",
      "Iteration 36055, loss = 0.03414495\n",
      "Iteration 36056, loss = 0.03414733\n",
      "Iteration 36057, loss = 0.03416051\n",
      "Iteration 36058, loss = 0.03415916\n",
      "Iteration 36059, loss = 0.03415105\n",
      "Iteration 36060, loss = 0.03414513\n",
      "Iteration 36061, loss = 0.03415663\n",
      "Iteration 36062, loss = 0.03416421\n",
      "Iteration 36063, loss = 0.03416824\n",
      "Iteration 36064, loss = 0.03416208\n",
      "Iteration 36065, loss = 0.03415052\n",
      "Iteration 36066, loss = 0.03415165\n",
      "Iteration 36067, loss = 0.03414963\n",
      "Iteration 36068, loss = 0.03414673\n",
      "Iteration 36069, loss = 0.03414240\n",
      "Iteration 36070, loss = 0.03414132\n",
      "Iteration 36071, loss = 0.03414504\n",
      "Iteration 36072, loss = 0.03414637\n",
      "Iteration 36073, loss = 0.03414000\n",
      "Iteration 36074, loss = 0.03414462\n",
      "Iteration 36075, loss = 0.03414388\n",
      "Iteration 36076, loss = 0.03414224\n",
      "Iteration 36077, loss = 0.03413804\n",
      "Iteration 36078, loss = 0.03413662\n",
      "Iteration 36079, loss = 0.03413782\n",
      "Iteration 36080, loss = 0.03413341\n",
      "Iteration 36081, loss = 0.03413092\n",
      "Iteration 36082, loss = 0.03413500\n",
      "Iteration 36083, loss = 0.03413389\n",
      "Iteration 36084, loss = 0.03413187\n",
      "Iteration 36085, loss = 0.03412478\n",
      "Iteration 36086, loss = 0.03412910\n",
      "Iteration 36087, loss = 0.03412817\n",
      "Iteration 36088, loss = 0.03412875\n",
      "Iteration 36089, loss = 0.03413738\n",
      "Iteration 36090, loss = 0.03413458\n",
      "Iteration 36091, loss = 0.03412663\n",
      "Iteration 36092, loss = 0.03413034\n",
      "Iteration 36093, loss = 0.03413600\n",
      "Iteration 36094, loss = 0.03413187\n",
      "Iteration 36095, loss = 0.03412962\n",
      "Iteration 36096, loss = 0.03412875\n",
      "Iteration 36097, loss = 0.03413780\n",
      "Iteration 36098, loss = 0.03413619\n",
      "Iteration 36099, loss = 0.03412263\n",
      "Iteration 36100, loss = 0.03413154\n",
      "Iteration 36101, loss = 0.03413542\n",
      "Iteration 36102, loss = 0.03413943\n",
      "Iteration 36103, loss = 0.03413467\n",
      "Iteration 36104, loss = 0.03412857\n",
      "Iteration 36105, loss = 0.03412994\n",
      "Iteration 36106, loss = 0.03413254\n",
      "Iteration 36107, loss = 0.03412946\n",
      "Iteration 36108, loss = 0.03412817\n",
      "Iteration 36109, loss = 0.03412098\n",
      "Iteration 36110, loss = 0.03412434\n",
      "Iteration 36111, loss = 0.03413594\n",
      "Iteration 36112, loss = 0.03413861\n",
      "Iteration 36113, loss = 0.03413319\n",
      "Iteration 36114, loss = 0.03413688\n",
      "Iteration 36115, loss = 0.03413767\n",
      "Iteration 36116, loss = 0.03413080\n",
      "Iteration 36117, loss = 0.03412768\n",
      "Iteration 36118, loss = 0.03413142\n",
      "Iteration 36119, loss = 0.03413714\n",
      "Iteration 36120, loss = 0.03413286\n",
      "Iteration 36121, loss = 0.03412792\n",
      "Iteration 36122, loss = 0.03413314\n",
      "Iteration 36123, loss = 0.03413433\n",
      "Iteration 36124, loss = 0.03413082\n",
      "Iteration 36125, loss = 0.03413592\n",
      "Iteration 36126, loss = 0.03413034\n",
      "Iteration 36127, loss = 0.03411229\n",
      "Iteration 36128, loss = 0.03412967\n",
      "Iteration 36129, loss = 0.03414303\n",
      "Iteration 36130, loss = 0.03414241\n",
      "Iteration 36131, loss = 0.03413155\n",
      "Iteration 36132, loss = 0.03412138\n",
      "Iteration 36133, loss = 0.03413028\n",
      "Iteration 36134, loss = 0.03412719\n",
      "Iteration 36135, loss = 0.03411899\n",
      "Iteration 36136, loss = 0.03411597\n",
      "Iteration 36137, loss = 0.03411311\n",
      "Iteration 36138, loss = 0.03411308\n",
      "Iteration 36139, loss = 0.03411621\n",
      "Iteration 36140, loss = 0.03412000\n",
      "Iteration 36141, loss = 0.03411292\n",
      "Iteration 36142, loss = 0.03411404\n",
      "Iteration 36143, loss = 0.03412163\n",
      "Iteration 36144, loss = 0.03412021\n",
      "Iteration 36145, loss = 0.03412816\n",
      "Iteration 36146, loss = 0.03412089\n",
      "Iteration 36147, loss = 0.03411061\n",
      "Iteration 36148, loss = 0.03412533\n",
      "Iteration 36149, loss = 0.03413469\n",
      "Iteration 36150, loss = 0.03412735\n",
      "Iteration 36151, loss = 0.03412106\n",
      "Iteration 36152, loss = 0.03411770\n",
      "Iteration 36153, loss = 0.03411571\n",
      "Iteration 36154, loss = 0.03412425\n",
      "Iteration 36155, loss = 0.03411512\n",
      "Iteration 36156, loss = 0.03411614\n",
      "Iteration 36157, loss = 0.03411363\n",
      "Iteration 36158, loss = 0.03412025\n",
      "Iteration 36159, loss = 0.03412439\n",
      "Iteration 36160, loss = 0.03411366\n",
      "Iteration 36161, loss = 0.03410604\n",
      "Iteration 36162, loss = 0.03411384\n",
      "Iteration 36163, loss = 0.03411611\n",
      "Iteration 36164, loss = 0.03410782\n",
      "Iteration 36165, loss = 0.03410367\n",
      "Iteration 36166, loss = 0.03410267\n",
      "Iteration 36167, loss = 0.03410004\n",
      "Iteration 36168, loss = 0.03409901\n",
      "Iteration 36169, loss = 0.03409711\n",
      "Iteration 36170, loss = 0.03410478\n",
      "Iteration 36171, loss = 0.03410775\n",
      "Iteration 36172, loss = 0.03410114\n",
      "Iteration 36173, loss = 0.03409753\n",
      "Iteration 36174, loss = 0.03410213\n",
      "Iteration 36175, loss = 0.03410599\n",
      "Iteration 36176, loss = 0.03410998\n",
      "Iteration 36177, loss = 0.03411047\n",
      "Iteration 36178, loss = 0.03410279\n",
      "Iteration 36179, loss = 0.03410144\n",
      "Iteration 36180, loss = 0.03410462\n",
      "Iteration 36181, loss = 0.03410860\n",
      "Iteration 36182, loss = 0.03410153\n",
      "Iteration 36183, loss = 0.03409878\n",
      "Iteration 36184, loss = 0.03409569\n",
      "Iteration 36185, loss = 0.03409279\n",
      "Iteration 36186, loss = 0.03410141\n",
      "Iteration 36187, loss = 0.03411173\n",
      "Iteration 36188, loss = 0.03410740\n",
      "Iteration 36189, loss = 0.03410075\n",
      "Iteration 36190, loss = 0.03410057\n",
      "Iteration 36191, loss = 0.03409509\n",
      "Iteration 36192, loss = 0.03410840\n",
      "Iteration 36193, loss = 0.03410899\n",
      "Iteration 36194, loss = 0.03411395\n",
      "Iteration 36195, loss = 0.03410707\n",
      "Iteration 36196, loss = 0.03410396\n",
      "Iteration 36197, loss = 0.03410374\n",
      "Iteration 36198, loss = 0.03410789\n",
      "Iteration 36199, loss = 0.03410213\n",
      "Iteration 36200, loss = 0.03409660\n",
      "Iteration 36201, loss = 0.03409157\n",
      "Iteration 36202, loss = 0.03409404\n",
      "Iteration 36203, loss = 0.03409870\n",
      "Iteration 36204, loss = 0.03409917\n",
      "Iteration 36205, loss = 0.03409857\n",
      "Iteration 36206, loss = 0.03409731\n",
      "Iteration 36207, loss = 0.03409401\n",
      "Iteration 36208, loss = 0.03409217\n",
      "Iteration 36209, loss = 0.03409263\n",
      "Iteration 36210, loss = 0.03408742\n",
      "Iteration 36211, loss = 0.03409137\n",
      "Iteration 36212, loss = 0.03409471\n",
      "Iteration 36213, loss = 0.03409260\n",
      "Iteration 36214, loss = 0.03409577\n",
      "Iteration 36215, loss = 0.03409476\n",
      "Iteration 36216, loss = 0.03408103\n",
      "Iteration 36217, loss = 0.03408450\n",
      "Iteration 36218, loss = 0.03409214\n",
      "Iteration 36219, loss = 0.03409023\n",
      "Iteration 36220, loss = 0.03409094\n",
      "Iteration 36221, loss = 0.03408952\n",
      "Iteration 36222, loss = 0.03407921\n",
      "Iteration 36223, loss = 0.03408234\n",
      "Iteration 36224, loss = 0.03408932\n",
      "Iteration 36225, loss = 0.03408353\n",
      "Iteration 36226, loss = 0.03407553\n",
      "Iteration 36227, loss = 0.03407804\n",
      "Iteration 36228, loss = 0.03407530\n",
      "Iteration 36229, loss = 0.03407258\n",
      "Iteration 36230, loss = 0.03408007\n",
      "Iteration 36231, loss = 0.03407772\n",
      "Iteration 36232, loss = 0.03407552\n",
      "Iteration 36233, loss = 0.03408025\n",
      "Iteration 36234, loss = 0.03407568\n",
      "Iteration 36235, loss = 0.03407782\n",
      "Iteration 36236, loss = 0.03407896\n",
      "Iteration 36237, loss = 0.03407436\n",
      "Iteration 36238, loss = 0.03407075\n",
      "Iteration 36239, loss = 0.03407838\n",
      "Iteration 36240, loss = 0.03407973\n",
      "Iteration 36241, loss = 0.03407855\n",
      "Iteration 36242, loss = 0.03407281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 36243, loss = 0.03407252\n",
      "Iteration 36244, loss = 0.03407840\n",
      "Iteration 36245, loss = 0.03407733\n",
      "Iteration 36246, loss = 0.03407978\n",
      "Iteration 36247, loss = 0.03408446\n",
      "Iteration 36248, loss = 0.03408323\n",
      "Iteration 36249, loss = 0.03407488\n",
      "Iteration 36250, loss = 0.03407160\n",
      "Iteration 36251, loss = 0.03407265\n",
      "Iteration 36252, loss = 0.03407895\n",
      "Iteration 36253, loss = 0.03408177\n",
      "Iteration 36254, loss = 0.03407863\n",
      "Iteration 36255, loss = 0.03407635\n",
      "Iteration 36256, loss = 0.03408344\n",
      "Iteration 36257, loss = 0.03407483\n",
      "Iteration 36258, loss = 0.03407389\n",
      "Iteration 36259, loss = 0.03406898\n",
      "Iteration 36260, loss = 0.03407059\n",
      "Iteration 36261, loss = 0.03406929\n",
      "Iteration 36262, loss = 0.03407962\n",
      "Iteration 36263, loss = 0.03407616\n",
      "Iteration 36264, loss = 0.03406702\n",
      "Iteration 36265, loss = 0.03406968\n",
      "Iteration 36266, loss = 0.03407684\n",
      "Iteration 36267, loss = 0.03407281\n",
      "Iteration 36268, loss = 0.03406929\n",
      "Iteration 36269, loss = 0.03406718\n",
      "Iteration 36270, loss = 0.03405970\n",
      "Iteration 36271, loss = 0.03406566\n",
      "Iteration 36272, loss = 0.03406799\n",
      "Iteration 36273, loss = 0.03406198\n",
      "Iteration 36274, loss = 0.03406283\n",
      "Iteration 36275, loss = 0.03406851\n",
      "Iteration 36276, loss = 0.03406912\n",
      "Iteration 36277, loss = 0.03405936\n",
      "Iteration 36278, loss = 0.03405788\n",
      "Iteration 36279, loss = 0.03406213\n",
      "Iteration 36280, loss = 0.03405653\n",
      "Iteration 36281, loss = 0.03405360\n",
      "Iteration 36282, loss = 0.03406028\n",
      "Iteration 36283, loss = 0.03406773\n",
      "Iteration 36284, loss = 0.03405863\n",
      "Iteration 36285, loss = 0.03405092\n",
      "Iteration 36286, loss = 0.03406757\n",
      "Iteration 36287, loss = 0.03407696\n",
      "Iteration 36288, loss = 0.03407306\n",
      "Iteration 36289, loss = 0.03406668\n",
      "Iteration 36290, loss = 0.03406047\n",
      "Iteration 36291, loss = 0.03405545\n",
      "Iteration 36292, loss = 0.03406738\n",
      "Iteration 36293, loss = 0.03407023\n",
      "Iteration 36294, loss = 0.03407090\n",
      "Iteration 36295, loss = 0.03406440\n",
      "Iteration 36296, loss = 0.03404812\n",
      "Iteration 36297, loss = 0.03406045\n",
      "Iteration 36298, loss = 0.03407346\n",
      "Iteration 36299, loss = 0.03406965\n",
      "Iteration 36300, loss = 0.03406751\n",
      "Iteration 36301, loss = 0.03406227\n",
      "Iteration 36302, loss = 0.03406388\n",
      "Iteration 36303, loss = 0.03407373\n",
      "Iteration 36304, loss = 0.03406862\n",
      "Iteration 36305, loss = 0.03406571\n",
      "Iteration 36306, loss = 0.03406198\n",
      "Iteration 36307, loss = 0.03405178\n",
      "Iteration 36308, loss = 0.03405483\n",
      "Iteration 36309, loss = 0.03404217\n",
      "Iteration 36310, loss = 0.03404655\n",
      "Iteration 36311, loss = 0.03405675\n",
      "Iteration 36312, loss = 0.03405481\n",
      "Iteration 36313, loss = 0.03404953\n",
      "Iteration 36314, loss = 0.03405530\n",
      "Iteration 36315, loss = 0.03406118\n",
      "Iteration 36316, loss = 0.03405749\n",
      "Iteration 36317, loss = 0.03404477\n",
      "Iteration 36318, loss = 0.03405158\n",
      "Iteration 36319, loss = 0.03406169\n",
      "Iteration 36320, loss = 0.03406816\n",
      "Iteration 36321, loss = 0.03405595\n",
      "Iteration 36322, loss = 0.03405245\n",
      "Iteration 36323, loss = 0.03405685\n",
      "Iteration 36324, loss = 0.03406273\n",
      "Iteration 36325, loss = 0.03405785\n",
      "Iteration 36326, loss = 0.03405745\n",
      "Iteration 36327, loss = 0.03404872\n",
      "Iteration 36328, loss = 0.03404386\n",
      "Iteration 36329, loss = 0.03403351\n",
      "Iteration 36330, loss = 0.03404696\n",
      "Iteration 36331, loss = 0.03405390\n",
      "Iteration 36332, loss = 0.03404334\n",
      "Iteration 36333, loss = 0.03403420\n",
      "Iteration 36334, loss = 0.03403937\n",
      "Iteration 36335, loss = 0.03403715\n",
      "Iteration 36336, loss = 0.03404031\n",
      "Iteration 36337, loss = 0.03403492\n",
      "Iteration 36338, loss = 0.03403352\n",
      "Iteration 36339, loss = 0.03403846\n",
      "Iteration 36340, loss = 0.03403470\n",
      "Iteration 36341, loss = 0.03403221\n",
      "Iteration 36342, loss = 0.03403249\n",
      "Iteration 36343, loss = 0.03402894\n",
      "Iteration 36344, loss = 0.03402924\n",
      "Iteration 36345, loss = 0.03402877\n",
      "Iteration 36346, loss = 0.03402801\n",
      "Iteration 36347, loss = 0.03402748\n",
      "Iteration 36348, loss = 0.03402733\n",
      "Iteration 36349, loss = 0.03403513\n",
      "Iteration 36350, loss = 0.03403174\n",
      "Iteration 36351, loss = 0.03402839\n",
      "Iteration 36352, loss = 0.03403109\n",
      "Iteration 36353, loss = 0.03404008\n",
      "Iteration 36354, loss = 0.03403827\n",
      "Iteration 36355, loss = 0.03403468\n",
      "Iteration 36356, loss = 0.03403338\n",
      "Iteration 36357, loss = 0.03403308\n",
      "Iteration 36358, loss = 0.03402463\n",
      "Iteration 36359, loss = 0.03403912\n",
      "Iteration 36360, loss = 0.03404571\n",
      "Iteration 36361, loss = 0.03403480\n",
      "Iteration 36362, loss = 0.03401774\n",
      "Iteration 36363, loss = 0.03404226\n",
      "Iteration 36364, loss = 0.03404868\n",
      "Iteration 36365, loss = 0.03403571\n",
      "Iteration 36366, loss = 0.03402979\n",
      "Iteration 36367, loss = 0.03403337\n",
      "Iteration 36368, loss = 0.03404047\n",
      "Iteration 36369, loss = 0.03404228\n",
      "Iteration 36370, loss = 0.03403791\n",
      "Iteration 36371, loss = 0.03402852\n",
      "Iteration 36372, loss = 0.03402843\n",
      "Iteration 36373, loss = 0.03403373\n",
      "Iteration 36374, loss = 0.03403152\n",
      "Iteration 36375, loss = 0.03401757\n",
      "Iteration 36376, loss = 0.03402160\n",
      "Iteration 36377, loss = 0.03402492\n",
      "Iteration 36378, loss = 0.03402417\n",
      "Iteration 36379, loss = 0.03402057\n",
      "Iteration 36380, loss = 0.03401124\n",
      "Iteration 36381, loss = 0.03402026\n",
      "Iteration 36382, loss = 0.03402324\n",
      "Iteration 36383, loss = 0.03401643\n",
      "Iteration 36384, loss = 0.03401984\n",
      "Iteration 36385, loss = 0.03402382\n",
      "Iteration 36386, loss = 0.03401864\n",
      "Iteration 36387, loss = 0.03402024\n",
      "Iteration 36388, loss = 0.03401862\n",
      "Iteration 36389, loss = 0.03400971\n",
      "Iteration 36390, loss = 0.03401849\n",
      "Iteration 36391, loss = 0.03401782\n",
      "Iteration 36392, loss = 0.03401564\n",
      "Iteration 36393, loss = 0.03401819\n",
      "Iteration 36394, loss = 0.03401848\n",
      "Iteration 36395, loss = 0.03401338\n",
      "Iteration 36396, loss = 0.03401226\n",
      "Iteration 36397, loss = 0.03401065\n",
      "Iteration 36398, loss = 0.03401182\n",
      "Iteration 36399, loss = 0.03400862\n",
      "Iteration 36400, loss = 0.03401546\n",
      "Iteration 36401, loss = 0.03401294\n",
      "Iteration 36402, loss = 0.03400644\n",
      "Iteration 36403, loss = 0.03401939\n",
      "Iteration 36404, loss = 0.03401470\n",
      "Iteration 36405, loss = 0.03401311\n",
      "Iteration 36406, loss = 0.03401181\n",
      "Iteration 36407, loss = 0.03401268\n",
      "Iteration 36408, loss = 0.03401338\n",
      "Iteration 36409, loss = 0.03401132\n",
      "Iteration 36410, loss = 0.03400692\n",
      "Iteration 36411, loss = 0.03400456\n",
      "Iteration 36412, loss = 0.03400535\n",
      "Iteration 36413, loss = 0.03400489\n",
      "Iteration 36414, loss = 0.03400480\n",
      "Iteration 36415, loss = 0.03400352\n",
      "Iteration 36416, loss = 0.03400010\n",
      "Iteration 36417, loss = 0.03400010\n",
      "Iteration 36418, loss = 0.03400823\n",
      "Iteration 36419, loss = 0.03400698\n",
      "Iteration 36420, loss = 0.03400637\n",
      "Iteration 36421, loss = 0.03400911\n",
      "Iteration 36422, loss = 0.03400202\n",
      "Iteration 36423, loss = 0.03400170\n",
      "Iteration 36424, loss = 0.03401098\n",
      "Iteration 36425, loss = 0.03400238\n",
      "Iteration 36426, loss = 0.03399807\n",
      "Iteration 36427, loss = 0.03399810\n",
      "Iteration 36428, loss = 0.03399739\n",
      "Iteration 36429, loss = 0.03400539\n",
      "Iteration 36430, loss = 0.03400263\n",
      "Iteration 36431, loss = 0.03399499\n",
      "Iteration 36432, loss = 0.03400239\n",
      "Iteration 36433, loss = 0.03400443\n",
      "Iteration 36434, loss = 0.03399666\n",
      "Iteration 36435, loss = 0.03399180\n",
      "Iteration 36436, loss = 0.03400777\n",
      "Iteration 36437, loss = 0.03401039\n",
      "Iteration 36438, loss = 0.03400198\n",
      "Iteration 36439, loss = 0.03400353\n",
      "Iteration 36440, loss = 0.03400365\n",
      "Iteration 36441, loss = 0.03400992\n",
      "Iteration 36442, loss = 0.03400661\n",
      "Iteration 36443, loss = 0.03400110\n",
      "Iteration 36444, loss = 0.03399668\n",
      "Iteration 36445, loss = 0.03399892\n",
      "Iteration 36446, loss = 0.03399430\n",
      "Iteration 36447, loss = 0.03400447\n",
      "Iteration 36448, loss = 0.03401244\n",
      "Iteration 36449, loss = 0.03400224\n",
      "Iteration 36450, loss = 0.03398803\n",
      "Iteration 36451, loss = 0.03399702\n",
      "Iteration 36452, loss = 0.03400160\n",
      "Iteration 36453, loss = 0.03400598\n",
      "Iteration 36454, loss = 0.03400538\n",
      "Iteration 36455, loss = 0.03400612\n",
      "Iteration 36456, loss = 0.03400318\n",
      "Iteration 36457, loss = 0.03400249\n",
      "Iteration 36458, loss = 0.03399568\n",
      "Iteration 36459, loss = 0.03399044\n",
      "Iteration 36460, loss = 0.03399168\n",
      "Iteration 36461, loss = 0.03399780\n",
      "Iteration 36462, loss = 0.03399543\n",
      "Iteration 36463, loss = 0.03398306\n",
      "Iteration 36464, loss = 0.03399126\n",
      "Iteration 36465, loss = 0.03398920\n",
      "Iteration 36466, loss = 0.03398551\n",
      "Iteration 36467, loss = 0.03398081\n",
      "Iteration 36468, loss = 0.03399075\n",
      "Iteration 36469, loss = 0.03399030\n",
      "Iteration 36470, loss = 0.03398028\n",
      "Iteration 36471, loss = 0.03397770\n",
      "Iteration 36472, loss = 0.03397903\n",
      "Iteration 36473, loss = 0.03397952\n",
      "Iteration 36474, loss = 0.03398053\n",
      "Iteration 36475, loss = 0.03398300\n",
      "Iteration 36476, loss = 0.03398562\n",
      "Iteration 36477, loss = 0.03398228\n",
      "Iteration 36478, loss = 0.03398644\n",
      "Iteration 36479, loss = 0.03398622\n",
      "Iteration 36480, loss = 0.03398369\n",
      "Iteration 36481, loss = 0.03398402\n",
      "Iteration 36482, loss = 0.03398231\n",
      "Iteration 36483, loss = 0.03397998\n",
      "Iteration 36484, loss = 0.03397966\n",
      "Iteration 36485, loss = 0.03397605\n",
      "Iteration 36486, loss = 0.03397733\n",
      "Iteration 36487, loss = 0.03397990\n",
      "Iteration 36488, loss = 0.03397451\n",
      "Iteration 36489, loss = 0.03397834\n",
      "Iteration 36490, loss = 0.03397890\n",
      "Iteration 36491, loss = 0.03397972\n",
      "Iteration 36492, loss = 0.03398399\n",
      "Iteration 36493, loss = 0.03397664\n",
      "Iteration 36494, loss = 0.03397837\n",
      "Iteration 36495, loss = 0.03398156\n",
      "Iteration 36496, loss = 0.03397754\n",
      "Iteration 36497, loss = 0.03397296\n",
      "Iteration 36498, loss = 0.03396965\n",
      "Iteration 36499, loss = 0.03397418\n",
      "Iteration 36500, loss = 0.03397689\n",
      "Iteration 36501, loss = 0.03397456\n",
      "Iteration 36502, loss = 0.03397596\n",
      "Iteration 36503, loss = 0.03397720\n",
      "Iteration 36504, loss = 0.03398201\n",
      "Iteration 36505, loss = 0.03398000\n",
      "Iteration 36506, loss = 0.03396789\n",
      "Iteration 36507, loss = 0.03396965\n",
      "Iteration 36508, loss = 0.03398434\n",
      "Iteration 36509, loss = 0.03397829\n",
      "Iteration 36510, loss = 0.03397510\n",
      "Iteration 36511, loss = 0.03396467\n",
      "Iteration 36512, loss = 0.03397172\n",
      "Iteration 36513, loss = 0.03398063\n",
      "Iteration 36514, loss = 0.03397873\n",
      "Iteration 36515, loss = 0.03397238\n",
      "Iteration 36516, loss = 0.03397954\n",
      "Iteration 36517, loss = 0.03397774\n",
      "Iteration 36518, loss = 0.03396871\n",
      "Iteration 36519, loss = 0.03396831\n",
      "Iteration 36520, loss = 0.03397803\n",
      "Iteration 36521, loss = 0.03397574\n",
      "Iteration 36522, loss = 0.03396574\n",
      "Iteration 36523, loss = 0.03396968\n",
      "Iteration 36524, loss = 0.03397138\n",
      "Iteration 36525, loss = 0.03396957\n",
      "Iteration 36526, loss = 0.03397299\n",
      "Iteration 36527, loss = 0.03396492\n",
      "Iteration 36528, loss = 0.03396288\n",
      "Iteration 36529, loss = 0.03397064\n",
      "Iteration 36530, loss = 0.03396642\n",
      "Iteration 36531, loss = 0.03396633\n",
      "Iteration 36532, loss = 0.03396528\n",
      "Iteration 36533, loss = 0.03396359\n",
      "Iteration 36534, loss = 0.03396433\n",
      "Iteration 36535, loss = 0.03396568\n",
      "Iteration 36536, loss = 0.03396640\n",
      "Iteration 36537, loss = 0.03396464\n",
      "Iteration 36538, loss = 0.03395773\n",
      "Iteration 36539, loss = 0.03395423\n",
      "Iteration 36540, loss = 0.03396063\n",
      "Iteration 36541, loss = 0.03395450\n",
      "Iteration 36542, loss = 0.03395726\n",
      "Iteration 36543, loss = 0.03395867\n",
      "Iteration 36544, loss = 0.03396085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 36545, loss = 0.03395684\n",
      "Iteration 36546, loss = 0.03395262\n",
      "Iteration 36547, loss = 0.03396140\n",
      "Iteration 36548, loss = 0.03395739\n",
      "Iteration 36549, loss = 0.03395843\n",
      "Iteration 36550, loss = 0.03395623\n",
      "Iteration 36551, loss = 0.03395182\n",
      "Iteration 36552, loss = 0.03396080\n",
      "Iteration 36553, loss = 0.03395845\n",
      "Iteration 36554, loss = 0.03395371\n",
      "Iteration 36555, loss = 0.03395252\n",
      "Iteration 36556, loss = 0.03395211\n",
      "Iteration 36557, loss = 0.03394659\n",
      "Iteration 36558, loss = 0.03395215\n",
      "Iteration 36559, loss = 0.03395352\n",
      "Iteration 36560, loss = 0.03395509\n",
      "Iteration 36561, loss = 0.03395318\n",
      "Iteration 36562, loss = 0.03394432\n",
      "Iteration 36563, loss = 0.03394750\n",
      "Iteration 36564, loss = 0.03395433\n",
      "Iteration 36565, loss = 0.03395400\n",
      "Iteration 36566, loss = 0.03394571\n",
      "Iteration 36567, loss = 0.03394529\n",
      "Iteration 36568, loss = 0.03394740\n",
      "Iteration 36569, loss = 0.03394216\n",
      "Iteration 36570, loss = 0.03393875\n",
      "Iteration 36571, loss = 0.03394188\n",
      "Iteration 36572, loss = 0.03394210\n",
      "Iteration 36573, loss = 0.03394336\n",
      "Iteration 36574, loss = 0.03394686\n",
      "Iteration 36575, loss = 0.03394472\n",
      "Iteration 36576, loss = 0.03394289\n",
      "Iteration 36577, loss = 0.03394085\n",
      "Iteration 36578, loss = 0.03394149\n",
      "Iteration 36579, loss = 0.03395044\n",
      "Iteration 36580, loss = 0.03394356\n",
      "Iteration 36581, loss = 0.03395145\n",
      "Iteration 36582, loss = 0.03394604\n",
      "Iteration 36583, loss = 0.03393497\n",
      "Iteration 36584, loss = 0.03394736\n",
      "Iteration 36585, loss = 0.03394476\n",
      "Iteration 36586, loss = 0.03394780\n",
      "Iteration 36587, loss = 0.03395176\n",
      "Iteration 36588, loss = 0.03394472\n",
      "Iteration 36589, loss = 0.03393921\n",
      "Iteration 36590, loss = 0.03393087\n",
      "Iteration 36591, loss = 0.03394292\n",
      "Iteration 36592, loss = 0.03394200\n",
      "Iteration 36593, loss = 0.03393122\n",
      "Iteration 36594, loss = 0.03393918\n",
      "Iteration 36595, loss = 0.03394514\n",
      "Iteration 36596, loss = 0.03394125\n",
      "Iteration 36597, loss = 0.03394129\n",
      "Iteration 36598, loss = 0.03394294\n",
      "Iteration 36599, loss = 0.03393979\n",
      "Iteration 36600, loss = 0.03393569\n",
      "Iteration 36601, loss = 0.03392975\n",
      "Iteration 36602, loss = 0.03394039\n",
      "Iteration 36603, loss = 0.03394607\n",
      "Iteration 36604, loss = 0.03394243\n",
      "Iteration 36605, loss = 0.03392541\n",
      "Iteration 36606, loss = 0.03393842\n",
      "Iteration 36607, loss = 0.03394779\n",
      "Iteration 36608, loss = 0.03395026\n",
      "Iteration 36609, loss = 0.03394761\n",
      "Iteration 36610, loss = 0.03394324\n",
      "Iteration 36611, loss = 0.03394528\n",
      "Iteration 36612, loss = 0.03393687\n",
      "Iteration 36613, loss = 0.03393268\n",
      "Iteration 36614, loss = 0.03392489\n",
      "Iteration 36615, loss = 0.03393417\n",
      "Iteration 36616, loss = 0.03393865\n",
      "Iteration 36617, loss = 0.03393452\n",
      "Iteration 36618, loss = 0.03392756\n",
      "Iteration 36619, loss = 0.03393251\n",
      "Iteration 36620, loss = 0.03393772\n",
      "Iteration 36621, loss = 0.03394148\n",
      "Iteration 36622, loss = 0.03394340\n",
      "Iteration 36623, loss = 0.03393637\n",
      "Iteration 36624, loss = 0.03393753\n",
      "Iteration 36625, loss = 0.03393787\n",
      "Iteration 36626, loss = 0.03393537\n",
      "Iteration 36627, loss = 0.03392946\n",
      "Iteration 36628, loss = 0.03393517\n",
      "Iteration 36629, loss = 0.03393848\n",
      "Iteration 36630, loss = 0.03394864\n",
      "Iteration 36631, loss = 0.03394270\n",
      "Iteration 36632, loss = 0.03393244\n",
      "Iteration 36633, loss = 0.03392863\n",
      "Iteration 36634, loss = 0.03393337\n",
      "Iteration 36635, loss = 0.03393924\n",
      "Iteration 36636, loss = 0.03394106\n",
      "Iteration 36637, loss = 0.03393211\n",
      "Iteration 36638, loss = 0.03392754\n",
      "Iteration 36639, loss = 0.03392960\n",
      "Iteration 36640, loss = 0.03392363\n",
      "Iteration 36641, loss = 0.03391750\n",
      "Iteration 36642, loss = 0.03392117\n",
      "Iteration 36643, loss = 0.03392973\n",
      "Iteration 36644, loss = 0.03393036\n",
      "Iteration 36645, loss = 0.03392797\n",
      "Iteration 36646, loss = 0.03392025\n",
      "Iteration 36647, loss = 0.03392811\n",
      "Iteration 36648, loss = 0.03392794\n",
      "Iteration 36649, loss = 0.03392600\n",
      "Iteration 36650, loss = 0.03392490\n",
      "Iteration 36651, loss = 0.03391225\n",
      "Iteration 36652, loss = 0.03391242\n",
      "Iteration 36653, loss = 0.03391365\n",
      "Iteration 36654, loss = 0.03390935\n",
      "Iteration 36655, loss = 0.03391075\n",
      "Iteration 36656, loss = 0.03391100\n",
      "Iteration 36657, loss = 0.03390863\n",
      "Iteration 36658, loss = 0.03391287\n",
      "Iteration 36659, loss = 0.03391352\n",
      "Iteration 36660, loss = 0.03391039\n",
      "Iteration 36661, loss = 0.03391219\n",
      "Iteration 36662, loss = 0.03390998\n",
      "Iteration 36663, loss = 0.03390961\n",
      "Iteration 36664, loss = 0.03390713\n",
      "Iteration 36665, loss = 0.03391286\n",
      "Iteration 36666, loss = 0.03391197\n",
      "Iteration 36667, loss = 0.03390912\n",
      "Iteration 36668, loss = 0.03390889\n",
      "Iteration 36669, loss = 0.03390868\n",
      "Iteration 36670, loss = 0.03390571\n",
      "Iteration 36671, loss = 0.03390698\n",
      "Iteration 36672, loss = 0.03391602\n",
      "Iteration 36673, loss = 0.03391191\n",
      "Iteration 36674, loss = 0.03390746\n",
      "Iteration 36675, loss = 0.03390265\n",
      "Iteration 36676, loss = 0.03390714\n",
      "Iteration 36677, loss = 0.03390696\n",
      "Iteration 36678, loss = 0.03390769\n",
      "Iteration 36679, loss = 0.03390464\n",
      "Iteration 36680, loss = 0.03390021\n",
      "Iteration 36681, loss = 0.03390514\n",
      "Iteration 36682, loss = 0.03390927\n",
      "Iteration 36683, loss = 0.03390275\n",
      "Iteration 36684, loss = 0.03390305\n",
      "Iteration 36685, loss = 0.03390385\n",
      "Iteration 36686, loss = 0.03389847\n",
      "Iteration 36687, loss = 0.03389819\n",
      "Iteration 36688, loss = 0.03390211\n",
      "Iteration 36689, loss = 0.03390321\n",
      "Iteration 36690, loss = 0.03390475\n",
      "Iteration 36691, loss = 0.03389965\n",
      "Iteration 36692, loss = 0.03390030\n",
      "Iteration 36693, loss = 0.03389287\n",
      "Iteration 36694, loss = 0.03390164\n",
      "Iteration 36695, loss = 0.03390769\n",
      "Iteration 36696, loss = 0.03389746\n",
      "Iteration 36697, loss = 0.03389595\n",
      "Iteration 36698, loss = 0.03390293\n",
      "Iteration 36699, loss = 0.03391218\n",
      "Iteration 36700, loss = 0.03390720\n",
      "Iteration 36701, loss = 0.03390995\n",
      "Iteration 36702, loss = 0.03390739\n",
      "Iteration 36703, loss = 0.03389407\n",
      "Iteration 36704, loss = 0.03388611\n",
      "Iteration 36705, loss = 0.03390738\n",
      "Iteration 36706, loss = 0.03391591\n",
      "Iteration 36707, loss = 0.03390568\n",
      "Iteration 36708, loss = 0.03389383\n",
      "Iteration 36709, loss = 0.03390413\n",
      "Iteration 36710, loss = 0.03391119\n",
      "Iteration 36711, loss = 0.03392029\n",
      "Iteration 36712, loss = 0.03392035\n",
      "Iteration 36713, loss = 0.03390956\n",
      "Iteration 36714, loss = 0.03390083\n",
      "Iteration 36715, loss = 0.03390177\n",
      "Iteration 36716, loss = 0.03389074\n",
      "Iteration 36717, loss = 0.03388678\n",
      "Iteration 36718, loss = 0.03389740\n",
      "Iteration 36719, loss = 0.03390061\n",
      "Iteration 36720, loss = 0.03389617\n",
      "Iteration 36721, loss = 0.03388865\n",
      "Iteration 36722, loss = 0.03389107\n",
      "Iteration 36723, loss = 0.03389674\n",
      "Iteration 36724, loss = 0.03389849\n",
      "Iteration 36725, loss = 0.03389708\n",
      "Iteration 36726, loss = 0.03390126\n",
      "Iteration 36727, loss = 0.03390060\n",
      "Iteration 36728, loss = 0.03389466\n",
      "Iteration 36729, loss = 0.03388392\n",
      "Iteration 36730, loss = 0.03388198\n",
      "Iteration 36731, loss = 0.03388970\n",
      "Iteration 36732, loss = 0.03388926\n",
      "Iteration 36733, loss = 0.03388682\n",
      "Iteration 36734, loss = 0.03387904\n",
      "Iteration 36735, loss = 0.03388086\n",
      "Iteration 36736, loss = 0.03388511\n",
      "Iteration 36737, loss = 0.03388598\n",
      "Iteration 36738, loss = 0.03388611\n",
      "Iteration 36739, loss = 0.03387696\n",
      "Iteration 36740, loss = 0.03388427\n",
      "Iteration 36741, loss = 0.03388436\n",
      "Iteration 36742, loss = 0.03388466\n",
      "Iteration 36743, loss = 0.03388043\n",
      "Iteration 36744, loss = 0.03387626\n",
      "Iteration 36745, loss = 0.03388145\n",
      "Iteration 36746, loss = 0.03388808\n",
      "Iteration 36747, loss = 0.03388930\n",
      "Iteration 36748, loss = 0.03388808\n",
      "Iteration 36749, loss = 0.03388685\n",
      "Iteration 36750, loss = 0.03388660\n",
      "Iteration 36751, loss = 0.03388001\n",
      "Iteration 36752, loss = 0.03388379\n",
      "Iteration 36753, loss = 0.03388657\n",
      "Iteration 36754, loss = 0.03387865\n",
      "Iteration 36755, loss = 0.03387145\n",
      "Iteration 36756, loss = 0.03387469\n",
      "Iteration 36757, loss = 0.03387687\n",
      "Iteration 36758, loss = 0.03387164\n",
      "Iteration 36759, loss = 0.03387693\n",
      "Iteration 36760, loss = 0.03388138\n",
      "Iteration 36761, loss = 0.03388217\n",
      "Iteration 36762, loss = 0.03387431\n",
      "Iteration 36763, loss = 0.03387947\n",
      "Iteration 36764, loss = 0.03388342\n",
      "Iteration 36765, loss = 0.03387870\n",
      "Iteration 36766, loss = 0.03388534\n",
      "Iteration 36767, loss = 0.03388184\n",
      "Iteration 36768, loss = 0.03387262\n",
      "Iteration 36769, loss = 0.03387882\n",
      "Iteration 36770, loss = 0.03388725\n",
      "Iteration 36771, loss = 0.03389173\n",
      "Iteration 36772, loss = 0.03388961\n",
      "Iteration 36773, loss = 0.03387634\n",
      "Iteration 36774, loss = 0.03387927\n",
      "Iteration 36775, loss = 0.03388367\n",
      "Iteration 36776, loss = 0.03387532\n",
      "Iteration 36777, loss = 0.03387573\n",
      "Iteration 36778, loss = 0.03387111\n",
      "Iteration 36779, loss = 0.03386439\n",
      "Iteration 36780, loss = 0.03387143\n",
      "Iteration 36781, loss = 0.03387584\n",
      "Iteration 36782, loss = 0.03387697\n",
      "Iteration 36783, loss = 0.03386916\n",
      "Iteration 36784, loss = 0.03386597\n",
      "Iteration 36785, loss = 0.03386959\n",
      "Iteration 36786, loss = 0.03386654\n",
      "Iteration 36787, loss = 0.03385988\n",
      "Iteration 36788, loss = 0.03386612\n",
      "Iteration 36789, loss = 0.03386553\n",
      "Iteration 36790, loss = 0.03386053\n",
      "Iteration 36791, loss = 0.03386127\n",
      "Iteration 36792, loss = 0.03385951\n",
      "Iteration 36793, loss = 0.03386472\n",
      "Iteration 36794, loss = 0.03386051\n",
      "Iteration 36795, loss = 0.03386623\n",
      "Iteration 36796, loss = 0.03386813\n",
      "Iteration 36797, loss = 0.03386756\n",
      "Iteration 36798, loss = 0.03386345\n",
      "Iteration 36799, loss = 0.03386350\n",
      "Iteration 36800, loss = 0.03386512\n",
      "Iteration 36801, loss = 0.03386454\n",
      "Iteration 36802, loss = 0.03385354\n",
      "Iteration 36803, loss = 0.03385869\n",
      "Iteration 36804, loss = 0.03386436\n",
      "Iteration 36805, loss = 0.03385883\n",
      "Iteration 36806, loss = 0.03384917\n",
      "Iteration 36807, loss = 0.03385502\n",
      "Iteration 36808, loss = 0.03385539\n",
      "Iteration 36809, loss = 0.03386044\n",
      "Iteration 36810, loss = 0.03386684\n",
      "Iteration 36811, loss = 0.03386224\n",
      "Iteration 36812, loss = 0.03385577\n",
      "Iteration 36813, loss = 0.03385654\n",
      "Iteration 36814, loss = 0.03387147\n",
      "Iteration 36815, loss = 0.03387400\n",
      "Iteration 36816, loss = 0.03387574\n",
      "Iteration 36817, loss = 0.03386549\n",
      "Iteration 36818, loss = 0.03385129\n",
      "Iteration 36819, loss = 0.03385365\n",
      "Iteration 36820, loss = 0.03386614\n",
      "Iteration 36821, loss = 0.03386530\n",
      "Iteration 36822, loss = 0.03386050\n",
      "Iteration 36823, loss = 0.03385263\n",
      "Iteration 36824, loss = 0.03384229\n",
      "Iteration 36825, loss = 0.03385052\n",
      "Iteration 36826, loss = 0.03384840\n",
      "Iteration 36827, loss = 0.03385037\n",
      "Iteration 36828, loss = 0.03385373\n",
      "Iteration 36829, loss = 0.03385426\n",
      "Iteration 36830, loss = 0.03385221\n",
      "Iteration 36831, loss = 0.03384562\n",
      "Iteration 36832, loss = 0.03384956\n",
      "Iteration 36833, loss = 0.03384604\n",
      "Iteration 36834, loss = 0.03384201\n",
      "Iteration 36835, loss = 0.03384406\n",
      "Iteration 36836, loss = 0.03384542\n",
      "Iteration 36837, loss = 0.03384035\n",
      "Iteration 36838, loss = 0.03385100\n",
      "Iteration 36839, loss = 0.03386004\n",
      "Iteration 36840, loss = 0.03385969\n",
      "Iteration 36841, loss = 0.03385503\n",
      "Iteration 36842, loss = 0.03384764\n",
      "Iteration 36843, loss = 0.03384650\n",
      "Iteration 36844, loss = 0.03383318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 36845, loss = 0.03385968\n",
      "Iteration 36846, loss = 0.03386679\n",
      "Iteration 36847, loss = 0.03385265\n",
      "Iteration 36848, loss = 0.03384745\n",
      "Iteration 36849, loss = 0.03385232\n",
      "Iteration 36850, loss = 0.03385715\n",
      "Iteration 36851, loss = 0.03385549\n",
      "Iteration 36852, loss = 0.03384783\n",
      "Iteration 36853, loss = 0.03385224\n",
      "Iteration 36854, loss = 0.03384636\n",
      "Iteration 36855, loss = 0.03383635\n",
      "Iteration 36856, loss = 0.03384022\n",
      "Iteration 36857, loss = 0.03383602\n",
      "Iteration 36858, loss = 0.03384112\n",
      "Iteration 36859, loss = 0.03383955\n",
      "Iteration 36860, loss = 0.03383291\n",
      "Iteration 36861, loss = 0.03383660\n",
      "Iteration 36862, loss = 0.03384770\n",
      "Iteration 36863, loss = 0.03384917\n",
      "Iteration 36864, loss = 0.03384556\n",
      "Iteration 36865, loss = 0.03384213\n",
      "Iteration 36866, loss = 0.03383093\n",
      "Iteration 36867, loss = 0.03383048\n",
      "Iteration 36868, loss = 0.03384960\n",
      "Iteration 36869, loss = 0.03384617\n",
      "Iteration 36870, loss = 0.03383179\n",
      "Iteration 36871, loss = 0.03384117\n",
      "Iteration 36872, loss = 0.03385206\n",
      "Iteration 36873, loss = 0.03385192\n",
      "Iteration 36874, loss = 0.03385690\n",
      "Iteration 36875, loss = 0.03385073\n",
      "Iteration 36876, loss = 0.03383586\n",
      "Iteration 36877, loss = 0.03382234\n",
      "Iteration 36878, loss = 0.03383146\n",
      "Iteration 36879, loss = 0.03383586\n",
      "Iteration 36880, loss = 0.03383624\n",
      "Iteration 36881, loss = 0.03383656\n",
      "Iteration 36882, loss = 0.03382803\n",
      "Iteration 36883, loss = 0.03382805\n",
      "Iteration 36884, loss = 0.03382802\n",
      "Iteration 36885, loss = 0.03382392\n",
      "Iteration 36886, loss = 0.03382159\n",
      "Iteration 36887, loss = 0.03382603\n",
      "Iteration 36888, loss = 0.03382086\n",
      "Iteration 36889, loss = 0.03382340\n",
      "Iteration 36890, loss = 0.03382789\n",
      "Iteration 36891, loss = 0.03382759\n",
      "Iteration 36892, loss = 0.03382638\n",
      "Iteration 36893, loss = 0.03383873\n",
      "Iteration 36894, loss = 0.03384053\n",
      "Iteration 36895, loss = 0.03384020\n",
      "Iteration 36896, loss = 0.03383476\n",
      "Iteration 36897, loss = 0.03382948\n",
      "Iteration 36898, loss = 0.03382263\n",
      "Iteration 36899, loss = 0.03381931\n",
      "Iteration 36900, loss = 0.03381333\n",
      "Iteration 36901, loss = 0.03381626\n",
      "Iteration 36902, loss = 0.03381392\n",
      "Iteration 36903, loss = 0.03381571\n",
      "Iteration 36904, loss = 0.03381511\n",
      "Iteration 36905, loss = 0.03381658\n",
      "Iteration 36906, loss = 0.03381642\n",
      "Iteration 36907, loss = 0.03381433\n",
      "Iteration 36908, loss = 0.03382002\n",
      "Iteration 36909, loss = 0.03382569\n",
      "Iteration 36910, loss = 0.03382070\n",
      "Iteration 36911, loss = 0.03381775\n",
      "Iteration 36912, loss = 0.03381635\n",
      "Iteration 36913, loss = 0.03381214\n",
      "Iteration 36914, loss = 0.03381310\n",
      "Iteration 36915, loss = 0.03381432\n",
      "Iteration 36916, loss = 0.03381413\n",
      "Iteration 36917, loss = 0.03381163\n",
      "Iteration 36918, loss = 0.03380735\n",
      "Iteration 36919, loss = 0.03381823\n",
      "Iteration 36920, loss = 0.03381457\n",
      "Iteration 36921, loss = 0.03381152\n",
      "Iteration 36922, loss = 0.03381002\n",
      "Iteration 36923, loss = 0.03381766\n",
      "Iteration 36924, loss = 0.03382104\n",
      "Iteration 36925, loss = 0.03381720\n",
      "Iteration 36926, loss = 0.03381966\n",
      "Iteration 36927, loss = 0.03381539\n",
      "Iteration 36928, loss = 0.03381089\n",
      "Iteration 36929, loss = 0.03381661\n",
      "Iteration 36930, loss = 0.03381913\n",
      "Iteration 36931, loss = 0.03380907\n",
      "Iteration 36932, loss = 0.03380892\n",
      "Iteration 36933, loss = 0.03381452\n",
      "Iteration 36934, loss = 0.03381637\n",
      "Iteration 36935, loss = 0.03381987\n",
      "Iteration 36936, loss = 0.03381215\n",
      "Iteration 36937, loss = 0.03380938\n",
      "Iteration 36938, loss = 0.03381434\n",
      "Iteration 36939, loss = 0.03381634\n",
      "Iteration 36940, loss = 0.03380544\n",
      "Iteration 36941, loss = 0.03381443\n",
      "Iteration 36942, loss = 0.03381927\n",
      "Iteration 36943, loss = 0.03381963\n",
      "Iteration 36944, loss = 0.03382209\n",
      "Iteration 36945, loss = 0.03381093\n",
      "Iteration 36946, loss = 0.03379729\n",
      "Iteration 36947, loss = 0.03381418\n",
      "Iteration 36948, loss = 0.03381952\n",
      "Iteration 36949, loss = 0.03381702\n",
      "Iteration 36950, loss = 0.03380800\n",
      "Iteration 36951, loss = 0.03380382\n",
      "Iteration 36952, loss = 0.03380651\n",
      "Iteration 36953, loss = 0.03380917\n",
      "Iteration 36954, loss = 0.03381221\n",
      "Iteration 36955, loss = 0.03381043\n",
      "Iteration 36956, loss = 0.03380682\n",
      "Iteration 36957, loss = 0.03380876\n",
      "Iteration 36958, loss = 0.03381302\n",
      "Iteration 36959, loss = 0.03380365\n",
      "Iteration 36960, loss = 0.03381073\n",
      "Iteration 36961, loss = 0.03381117\n",
      "Iteration 36962, loss = 0.03380149\n",
      "Iteration 36963, loss = 0.03379904\n",
      "Iteration 36964, loss = 0.03380259\n",
      "Iteration 36965, loss = 0.03380315\n",
      "Iteration 36966, loss = 0.03380533\n",
      "Iteration 36967, loss = 0.03380831\n",
      "Iteration 36968, loss = 0.03379991\n",
      "Iteration 36969, loss = 0.03379407\n",
      "Iteration 36970, loss = 0.03379816\n",
      "Iteration 36971, loss = 0.03380330\n",
      "Iteration 36972, loss = 0.03379304\n",
      "Iteration 36973, loss = 0.03379507\n",
      "Iteration 36974, loss = 0.03379488\n",
      "Iteration 36975, loss = 0.03379417\n",
      "Iteration 36976, loss = 0.03379116\n",
      "Iteration 36977, loss = 0.03379076\n",
      "Iteration 36978, loss = 0.03379221\n",
      "Iteration 36979, loss = 0.03379649\n",
      "Iteration 36980, loss = 0.03380110\n",
      "Iteration 36981, loss = 0.03380077\n",
      "Iteration 36982, loss = 0.03380164\n",
      "Iteration 36983, loss = 0.03379535\n",
      "Iteration 36984, loss = 0.03379234\n",
      "Iteration 36985, loss = 0.03378516\n",
      "Iteration 36986, loss = 0.03379369\n",
      "Iteration 36987, loss = 0.03379352\n",
      "Iteration 36988, loss = 0.03378891\n",
      "Iteration 36989, loss = 0.03378518\n",
      "Iteration 36990, loss = 0.03378193\n",
      "Iteration 36991, loss = 0.03378182\n",
      "Iteration 36992, loss = 0.03378489\n",
      "Iteration 36993, loss = 0.03378086\n",
      "Iteration 36994, loss = 0.03377759\n",
      "Iteration 36995, loss = 0.03378180\n",
      "Iteration 36996, loss = 0.03378271\n",
      "Iteration 36997, loss = 0.03378175\n",
      "Iteration 36998, loss = 0.03377947\n",
      "Iteration 36999, loss = 0.03377749\n",
      "Iteration 37000, loss = 0.03377846\n",
      "Iteration 37001, loss = 0.03378434\n",
      "Iteration 37002, loss = 0.03377770\n",
      "Iteration 37003, loss = 0.03377577\n",
      "Iteration 37004, loss = 0.03378505\n",
      "Iteration 37005, loss = 0.03378539\n",
      "Iteration 37006, loss = 0.03378335\n",
      "Iteration 37007, loss = 0.03378344\n",
      "Iteration 37008, loss = 0.03377738\n",
      "Iteration 37009, loss = 0.03379116\n",
      "Iteration 37010, loss = 0.03378894\n",
      "Iteration 37011, loss = 0.03378120\n",
      "Iteration 37012, loss = 0.03377769\n",
      "Iteration 37013, loss = 0.03378906\n",
      "Iteration 37014, loss = 0.03379179\n",
      "Iteration 37015, loss = 0.03378732\n",
      "Iteration 37016, loss = 0.03378361\n",
      "Iteration 37017, loss = 0.03378427\n",
      "Iteration 37018, loss = 0.03378370\n",
      "Iteration 37019, loss = 0.03378773\n",
      "Iteration 37020, loss = 0.03379092\n",
      "Iteration 37021, loss = 0.03378247\n",
      "Iteration 37022, loss = 0.03377222\n",
      "Iteration 37023, loss = 0.03377737\n",
      "Iteration 37024, loss = 0.03378337\n",
      "Iteration 37025, loss = 0.03377423\n",
      "Iteration 37026, loss = 0.03377288\n",
      "Iteration 37027, loss = 0.03377653\n",
      "Iteration 37028, loss = 0.03378242\n",
      "Iteration 37029, loss = 0.03378692\n",
      "Iteration 37030, loss = 0.03377878\n",
      "Iteration 37031, loss = 0.03377099\n",
      "Iteration 37032, loss = 0.03377613\n",
      "Iteration 37033, loss = 0.03378112\n",
      "Iteration 37034, loss = 0.03378044\n",
      "Iteration 37035, loss = 0.03377327\n",
      "Iteration 37036, loss = 0.03377468\n",
      "Iteration 37037, loss = 0.03377521\n",
      "Iteration 37038, loss = 0.03377242\n",
      "Iteration 37039, loss = 0.03377727\n",
      "Iteration 37040, loss = 0.03377419\n",
      "Iteration 37041, loss = 0.03377308\n",
      "Iteration 37042, loss = 0.03377084\n",
      "Iteration 37043, loss = 0.03377888\n",
      "Iteration 37044, loss = 0.03377751\n",
      "Iteration 37045, loss = 0.03377824\n",
      "Iteration 37046, loss = 0.03377253\n",
      "Iteration 37047, loss = 0.03377142\n",
      "Iteration 37048, loss = 0.03377387\n",
      "Iteration 37049, loss = 0.03377883\n",
      "Iteration 37050, loss = 0.03378158\n",
      "Iteration 37051, loss = 0.03377532\n",
      "Iteration 37052, loss = 0.03376690\n",
      "Iteration 37053, loss = 0.03376941\n",
      "Iteration 37054, loss = 0.03376034\n",
      "Iteration 37055, loss = 0.03375586\n",
      "Iteration 37056, loss = 0.03376469\n",
      "Iteration 37057, loss = 0.03377172\n",
      "Iteration 37058, loss = 0.03376812\n",
      "Iteration 37059, loss = 0.03376503\n",
      "Iteration 37060, loss = 0.03375843\n",
      "Iteration 37061, loss = 0.03375987\n",
      "Iteration 37062, loss = 0.03376367\n",
      "Iteration 37063, loss = 0.03376106\n",
      "Iteration 37064, loss = 0.03376087\n",
      "Iteration 37065, loss = 0.03375224\n",
      "Iteration 37066, loss = 0.03375613\n",
      "Iteration 37067, loss = 0.03375688\n",
      "Iteration 37068, loss = 0.03375232\n",
      "Iteration 37069, loss = 0.03375600\n",
      "Iteration 37070, loss = 0.03374897\n",
      "Iteration 37071, loss = 0.03375295\n",
      "Iteration 37072, loss = 0.03376291\n",
      "Iteration 37073, loss = 0.03375836\n",
      "Iteration 37074, loss = 0.03375562\n",
      "Iteration 37075, loss = 0.03375488\n",
      "Iteration 37076, loss = 0.03374838\n",
      "Iteration 37077, loss = 0.03375063\n",
      "Iteration 37078, loss = 0.03375575\n",
      "Iteration 37079, loss = 0.03374982\n",
      "Iteration 37080, loss = 0.03374271\n",
      "Iteration 37081, loss = 0.03374741\n",
      "Iteration 37082, loss = 0.03375322\n",
      "Iteration 37083, loss = 0.03374591\n",
      "Iteration 37084, loss = 0.03374432\n",
      "Iteration 37085, loss = 0.03375130\n",
      "Iteration 37086, loss = 0.03374680\n",
      "Iteration 37087, loss = 0.03374118\n",
      "Iteration 37088, loss = 0.03374836\n",
      "Iteration 37089, loss = 0.03375207\n",
      "Iteration 37090, loss = 0.03374329\n",
      "Iteration 37091, loss = 0.03374204\n",
      "Iteration 37092, loss = 0.03374293\n",
      "Iteration 37093, loss = 0.03373812\n",
      "Iteration 37094, loss = 0.03374737\n",
      "Iteration 37095, loss = 0.03374547\n",
      "Iteration 37096, loss = 0.03373594\n",
      "Iteration 37097, loss = 0.03373935\n",
      "Iteration 37098, loss = 0.03374152\n",
      "Iteration 37099, loss = 0.03373558\n",
      "Iteration 37100, loss = 0.03373656\n",
      "Iteration 37101, loss = 0.03374260\n",
      "Iteration 37102, loss = 0.03374119\n",
      "Iteration 37103, loss = 0.03373204\n",
      "Iteration 37104, loss = 0.03374827\n",
      "Iteration 37105, loss = 0.03374903\n",
      "Iteration 37106, loss = 0.03374475\n",
      "Iteration 37107, loss = 0.03373712\n",
      "Iteration 37108, loss = 0.03374037\n",
      "Iteration 37109, loss = 0.03374312\n",
      "Iteration 37110, loss = 0.03373711\n",
      "Iteration 37111, loss = 0.03374778\n",
      "Iteration 37112, loss = 0.03374602\n",
      "Iteration 37113, loss = 0.03374014\n",
      "Iteration 37114, loss = 0.03374571\n",
      "Iteration 37115, loss = 0.03374345\n",
      "Iteration 37116, loss = 0.03374430\n",
      "Iteration 37117, loss = 0.03375317\n",
      "Iteration 37118, loss = 0.03374769\n",
      "Iteration 37119, loss = 0.03374988\n",
      "Iteration 37120, loss = 0.03374826\n",
      "Iteration 37121, loss = 0.03374065\n",
      "Iteration 37122, loss = 0.03374020\n",
      "Iteration 37123, loss = 0.03373280\n",
      "Iteration 37124, loss = 0.03373334\n",
      "Iteration 37125, loss = 0.03373114\n",
      "Iteration 37126, loss = 0.03373350\n",
      "Iteration 37127, loss = 0.03372732\n",
      "Iteration 37128, loss = 0.03372556\n",
      "Iteration 37129, loss = 0.03372416\n",
      "Iteration 37130, loss = 0.03372735\n",
      "Iteration 37131, loss = 0.03372204\n",
      "Iteration 37132, loss = 0.03372399\n",
      "Iteration 37133, loss = 0.03372324\n",
      "Iteration 37134, loss = 0.03371750\n",
      "Iteration 37135, loss = 0.03372515\n",
      "Iteration 37136, loss = 0.03373083\n",
      "Iteration 37137, loss = 0.03373276\n",
      "Iteration 37138, loss = 0.03373102\n",
      "Iteration 37139, loss = 0.03371869\n",
      "Iteration 37140, loss = 0.03371853\n",
      "Iteration 37141, loss = 0.03373140\n",
      "Iteration 37142, loss = 0.03373369\n",
      "Iteration 37143, loss = 0.03372077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 37144, loss = 0.03372524\n",
      "Iteration 37145, loss = 0.03373556\n",
      "Iteration 37146, loss = 0.03373402\n",
      "Iteration 37147, loss = 0.03373162\n",
      "Iteration 37148, loss = 0.03373815\n",
      "Iteration 37149, loss = 0.03373423\n",
      "Iteration 37150, loss = 0.03372175\n",
      "Iteration 37151, loss = 0.03372124\n",
      "Iteration 37152, loss = 0.03372907\n",
      "Iteration 37153, loss = 0.03373160\n",
      "Iteration 37154, loss = 0.03371912\n",
      "Iteration 37155, loss = 0.03371032\n",
      "Iteration 37156, loss = 0.03371671\n",
      "Iteration 37157, loss = 0.03372718\n",
      "Iteration 37158, loss = 0.03372291\n",
      "Iteration 37159, loss = 0.03371746\n",
      "Iteration 37160, loss = 0.03371685\n",
      "Iteration 37161, loss = 0.03371483\n",
      "Iteration 37162, loss = 0.03371374\n",
      "Iteration 37163, loss = 0.03371533\n",
      "Iteration 37164, loss = 0.03371816\n",
      "Iteration 37165, loss = 0.03371393\n",
      "Iteration 37166, loss = 0.03371384\n",
      "Iteration 37167, loss = 0.03370859\n",
      "Iteration 37168, loss = 0.03371125\n",
      "Iteration 37169, loss = 0.03370704\n",
      "Iteration 37170, loss = 0.03370852\n",
      "Iteration 37171, loss = 0.03371118\n",
      "Iteration 37172, loss = 0.03371026\n",
      "Iteration 37173, loss = 0.03370764\n",
      "Iteration 37174, loss = 0.03370303\n",
      "Iteration 37175, loss = 0.03370762\n",
      "Iteration 37176, loss = 0.03371113\n",
      "Iteration 37177, loss = 0.03370306\n",
      "Iteration 37178, loss = 0.03370444\n",
      "Iteration 37179, loss = 0.03371424\n",
      "Iteration 37180, loss = 0.03371737\n",
      "Iteration 37181, loss = 0.03371102\n",
      "Iteration 37182, loss = 0.03371547\n",
      "Iteration 37183, loss = 0.03371382\n",
      "Iteration 37184, loss = 0.03369871\n",
      "Iteration 37185, loss = 0.03370617\n",
      "Iteration 37186, loss = 0.03371163\n",
      "Iteration 37187, loss = 0.03370968\n",
      "Iteration 37188, loss = 0.03370769\n",
      "Iteration 37189, loss = 0.03370669\n",
      "Iteration 37190, loss = 0.03371151\n",
      "Iteration 37191, loss = 0.03370935\n",
      "Iteration 37192, loss = 0.03370309\n",
      "Iteration 37193, loss = 0.03370588\n",
      "Iteration 37194, loss = 0.03370113\n",
      "Iteration 37195, loss = 0.03370689\n",
      "Iteration 37196, loss = 0.03371297\n",
      "Iteration 37197, loss = 0.03371016\n",
      "Iteration 37198, loss = 0.03369494\n",
      "Iteration 37199, loss = 0.03369784\n",
      "Iteration 37200, loss = 0.03369886\n",
      "Iteration 37201, loss = 0.03369852\n",
      "Iteration 37202, loss = 0.03370022\n",
      "Iteration 37203, loss = 0.03369410\n",
      "Iteration 37204, loss = 0.03369093\n",
      "Iteration 37205, loss = 0.03370196\n",
      "Iteration 37206, loss = 0.03370229\n",
      "Iteration 37207, loss = 0.03370050\n",
      "Iteration 37208, loss = 0.03369120\n",
      "Iteration 37209, loss = 0.03369647\n",
      "Iteration 37210, loss = 0.03370497\n",
      "Iteration 37211, loss = 0.03369340\n",
      "Iteration 37212, loss = 0.03369197\n",
      "Iteration 37213, loss = 0.03369990\n",
      "Iteration 37214, loss = 0.03370079\n",
      "Iteration 37215, loss = 0.03369819\n",
      "Iteration 37216, loss = 0.03369031\n",
      "Iteration 37217, loss = 0.03369542\n",
      "Iteration 37218, loss = 0.03370465\n",
      "Iteration 37219, loss = 0.03370121\n",
      "Iteration 37220, loss = 0.03369916\n",
      "Iteration 37221, loss = 0.03369042\n",
      "Iteration 37222, loss = 0.03369798\n",
      "Iteration 37223, loss = 0.03369616\n",
      "Iteration 37224, loss = 0.03369047\n",
      "Iteration 37225, loss = 0.03368904\n",
      "Iteration 37226, loss = 0.03368996\n",
      "Iteration 37227, loss = 0.03368522\n",
      "Iteration 37228, loss = 0.03368335\n",
      "Iteration 37229, loss = 0.03369412\n",
      "Iteration 37230, loss = 0.03369695\n",
      "Iteration 37231, loss = 0.03368560\n",
      "Iteration 37232, loss = 0.03368835\n",
      "Iteration 37233, loss = 0.03369227\n",
      "Iteration 37234, loss = 0.03368795\n",
      "Iteration 37235, loss = 0.03368624\n",
      "Iteration 37236, loss = 0.03368973\n",
      "Iteration 37237, loss = 0.03368464\n",
      "Iteration 37238, loss = 0.03368817\n",
      "Iteration 37239, loss = 0.03369433\n",
      "Iteration 37240, loss = 0.03369247\n",
      "Iteration 37241, loss = 0.03368492\n",
      "Iteration 37242, loss = 0.03367561\n",
      "Iteration 37243, loss = 0.03368210\n",
      "Iteration 37244, loss = 0.03369651\n",
      "Iteration 37245, loss = 0.03369923\n",
      "Iteration 37246, loss = 0.03368717\n",
      "Iteration 37247, loss = 0.03368695\n",
      "Iteration 37248, loss = 0.03369311\n",
      "Iteration 37249, loss = 0.03370367\n",
      "Iteration 37250, loss = 0.03369911\n",
      "Iteration 37251, loss = 0.03369674\n",
      "Iteration 37252, loss = 0.03368668\n",
      "Iteration 37253, loss = 0.03367665\n",
      "Iteration 37254, loss = 0.03368884\n",
      "Iteration 37255, loss = 0.03368978\n",
      "Iteration 37256, loss = 0.03368765\n",
      "Iteration 37257, loss = 0.03368904\n",
      "Iteration 37258, loss = 0.03368621\n",
      "Iteration 37259, loss = 0.03367465\n",
      "Iteration 37260, loss = 0.03367133\n",
      "Iteration 37261, loss = 0.03368778\n",
      "Iteration 37262, loss = 0.03368877\n",
      "Iteration 37263, loss = 0.03367550\n",
      "Iteration 37264, loss = 0.03367112\n",
      "Iteration 37265, loss = 0.03367755\n",
      "Iteration 37266, loss = 0.03367733\n",
      "Iteration 37267, loss = 0.03366783\n",
      "Iteration 37268, loss = 0.03366731\n",
      "Iteration 37269, loss = 0.03366628\n",
      "Iteration 37270, loss = 0.03366659\n",
      "Iteration 37271, loss = 0.03367366\n",
      "Iteration 37272, loss = 0.03367547\n",
      "Iteration 37273, loss = 0.03367870\n",
      "Iteration 37274, loss = 0.03366932\n",
      "Iteration 37275, loss = 0.03367174\n",
      "Iteration 37276, loss = 0.03367426\n",
      "Iteration 37277, loss = 0.03367423\n",
      "Iteration 37278, loss = 0.03367751\n",
      "Iteration 37279, loss = 0.03367373\n",
      "Iteration 37280, loss = 0.03366619\n",
      "Iteration 37281, loss = 0.03366900\n",
      "Iteration 37282, loss = 0.03367997\n",
      "Iteration 37283, loss = 0.03367869\n",
      "Iteration 37284, loss = 0.03367004\n",
      "Iteration 37285, loss = 0.03366020\n",
      "Iteration 37286, loss = 0.03367493\n",
      "Iteration 37287, loss = 0.03367719\n",
      "Iteration 37288, loss = 0.03367322\n",
      "Iteration 37289, loss = 0.03367721\n",
      "Iteration 37290, loss = 0.03367668\n",
      "Iteration 37291, loss = 0.03366616\n",
      "Iteration 37292, loss = 0.03366356\n",
      "Iteration 37293, loss = 0.03367165\n",
      "Iteration 37294, loss = 0.03367580\n",
      "Iteration 37295, loss = 0.03366918\n",
      "Iteration 37296, loss = 0.03366334\n",
      "Iteration 37297, loss = 0.03366645\n",
      "Iteration 37298, loss = 0.03367184\n",
      "Iteration 37299, loss = 0.03366563\n",
      "Iteration 37300, loss = 0.03366315\n",
      "Iteration 37301, loss = 0.03366279\n",
      "Iteration 37302, loss = 0.03365353\n",
      "Iteration 37303, loss = 0.03365862\n",
      "Iteration 37304, loss = 0.03366599\n",
      "Iteration 37305, loss = 0.03365542\n",
      "Iteration 37306, loss = 0.03365594\n",
      "Iteration 37307, loss = 0.03365952\n",
      "Iteration 37308, loss = 0.03366389\n",
      "Iteration 37309, loss = 0.03366020\n",
      "Iteration 37310, loss = 0.03365546\n",
      "Iteration 37311, loss = 0.03365012\n",
      "Iteration 37312, loss = 0.03364955\n",
      "Iteration 37313, loss = 0.03365755\n",
      "Iteration 37314, loss = 0.03366574\n",
      "Iteration 37315, loss = 0.03366120\n",
      "Iteration 37316, loss = 0.03364973\n",
      "Iteration 37317, loss = 0.03365512\n",
      "Iteration 37318, loss = 0.03364935\n",
      "Iteration 37319, loss = 0.03365394\n",
      "Iteration 37320, loss = 0.03365617\n",
      "Iteration 37321, loss = 0.03364490\n",
      "Iteration 37322, loss = 0.03365218\n",
      "Iteration 37323, loss = 0.03366212\n",
      "Iteration 37324, loss = 0.03366021\n",
      "Iteration 37325, loss = 0.03366106\n",
      "Iteration 37326, loss = 0.03365967\n",
      "Iteration 37327, loss = 0.03365260\n",
      "Iteration 37328, loss = 0.03366060\n",
      "Iteration 37329, loss = 0.03366102\n",
      "Iteration 37330, loss = 0.03365132\n",
      "Iteration 37331, loss = 0.03364920\n",
      "Iteration 37332, loss = 0.03365736\n",
      "Iteration 37333, loss = 0.03366613\n",
      "Iteration 37334, loss = 0.03366598\n",
      "Iteration 37335, loss = 0.03366300\n",
      "Iteration 37336, loss = 0.03365876\n",
      "Iteration 37337, loss = 0.03365199\n",
      "Iteration 37338, loss = 0.03364818\n",
      "Iteration 37339, loss = 0.03365370\n",
      "Iteration 37340, loss = 0.03364881\n",
      "Iteration 37341, loss = 0.03363879\n",
      "Iteration 37342, loss = 0.03364385\n",
      "Iteration 37343, loss = 0.03364783\n",
      "Iteration 37344, loss = 0.03364923\n",
      "Iteration 37345, loss = 0.03364807\n",
      "Iteration 37346, loss = 0.03364544\n",
      "Iteration 37347, loss = 0.03364395\n",
      "Iteration 37348, loss = 0.03363904\n",
      "Iteration 37349, loss = 0.03364980\n",
      "Iteration 37350, loss = 0.03365230\n",
      "Iteration 37351, loss = 0.03365574\n",
      "Iteration 37352, loss = 0.03364490\n",
      "Iteration 37353, loss = 0.03364453\n",
      "Iteration 37354, loss = 0.03364930\n",
      "Iteration 37355, loss = 0.03364961\n",
      "Iteration 37356, loss = 0.03364999\n",
      "Iteration 37357, loss = 0.03364277\n",
      "Iteration 37358, loss = 0.03363971\n",
      "Iteration 37359, loss = 0.03363376\n",
      "Iteration 37360, loss = 0.03363685\n",
      "Iteration 37361, loss = 0.03364187\n",
      "Iteration 37362, loss = 0.03364512\n",
      "Iteration 37363, loss = 0.03364181\n",
      "Iteration 37364, loss = 0.03363473\n",
      "Iteration 37365, loss = 0.03363661\n",
      "Iteration 37366, loss = 0.03364134\n",
      "Iteration 37367, loss = 0.03363741\n",
      "Iteration 37368, loss = 0.03363252\n",
      "Iteration 37369, loss = 0.03363424\n",
      "Iteration 37370, loss = 0.03364030\n",
      "Iteration 37371, loss = 0.03363837\n",
      "Iteration 37372, loss = 0.03364044\n",
      "Iteration 37373, loss = 0.03363539\n",
      "Iteration 37374, loss = 0.03363084\n",
      "Iteration 37375, loss = 0.03364470\n",
      "Iteration 37376, loss = 0.03365287\n",
      "Iteration 37377, loss = 0.03364951\n",
      "Iteration 37378, loss = 0.03363868\n",
      "Iteration 37379, loss = 0.03363565\n",
      "Iteration 37380, loss = 0.03363575\n",
      "Iteration 37381, loss = 0.03363354\n",
      "Iteration 37382, loss = 0.03363085\n",
      "Iteration 37383, loss = 0.03363517\n",
      "Iteration 37384, loss = 0.03363577\n",
      "Iteration 37385, loss = 0.03364623\n",
      "Iteration 37386, loss = 0.03364389\n",
      "Iteration 37387, loss = 0.03363237\n",
      "Iteration 37388, loss = 0.03362435\n",
      "Iteration 37389, loss = 0.03363368\n",
      "Iteration 37390, loss = 0.03363869\n",
      "Iteration 37391, loss = 0.03364120\n",
      "Iteration 37392, loss = 0.03363114\n",
      "Iteration 37393, loss = 0.03362485\n",
      "Iteration 37394, loss = 0.03363125\n",
      "Iteration 37395, loss = 0.03363558\n",
      "Iteration 37396, loss = 0.03364232\n",
      "Iteration 37397, loss = 0.03364057\n",
      "Iteration 37398, loss = 0.03363984\n",
      "Iteration 37399, loss = 0.03363000\n",
      "Iteration 37400, loss = 0.03363183\n",
      "Iteration 37401, loss = 0.03363726\n",
      "Iteration 37402, loss = 0.03363516\n",
      "Iteration 37403, loss = 0.03362799\n",
      "Iteration 37404, loss = 0.03361903\n",
      "Iteration 37405, loss = 0.03361896\n",
      "Iteration 37406, loss = 0.03361580\n",
      "Iteration 37407, loss = 0.03362329\n",
      "Iteration 37408, loss = 0.03362211\n",
      "Iteration 37409, loss = 0.03361825\n",
      "Iteration 37410, loss = 0.03361406\n",
      "Iteration 37411, loss = 0.03361702\n",
      "Iteration 37412, loss = 0.03361765\n",
      "Iteration 37413, loss = 0.03361582\n",
      "Iteration 37414, loss = 0.03361202\n",
      "Iteration 37415, loss = 0.03361144\n",
      "Iteration 37416, loss = 0.03361829\n",
      "Iteration 37417, loss = 0.03361296\n",
      "Iteration 37418, loss = 0.03361448\n",
      "Iteration 37419, loss = 0.03361930\n",
      "Iteration 37420, loss = 0.03361861\n",
      "Iteration 37421, loss = 0.03360958\n",
      "Iteration 37422, loss = 0.03361413\n",
      "Iteration 37423, loss = 0.03362030\n",
      "Iteration 37424, loss = 0.03361360\n",
      "Iteration 37425, loss = 0.03361304\n",
      "Iteration 37426, loss = 0.03362027\n",
      "Iteration 37427, loss = 0.03362641\n",
      "Iteration 37428, loss = 0.03361905\n",
      "Iteration 37429, loss = 0.03362518\n",
      "Iteration 37430, loss = 0.03361865\n",
      "Iteration 37431, loss = 0.03361665\n",
      "Iteration 37432, loss = 0.03362078\n",
      "Iteration 37433, loss = 0.03361444\n",
      "Iteration 37434, loss = 0.03360934\n",
      "Iteration 37435, loss = 0.03360678\n",
      "Iteration 37436, loss = 0.03361135\n",
      "Iteration 37437, loss = 0.03361437\n",
      "Iteration 37438, loss = 0.03361037\n",
      "Iteration 37439, loss = 0.03360771\n",
      "Iteration 37440, loss = 0.03360778\n",
      "Iteration 37441, loss = 0.03360690\n",
      "Iteration 37442, loss = 0.03361267\n",
      "Iteration 37443, loss = 0.03360259\n",
      "Iteration 37444, loss = 0.03359897\n",
      "Iteration 37445, loss = 0.03360047\n",
      "Iteration 37446, loss = 0.03360748\n",
      "Iteration 37447, loss = 0.03360834\n",
      "Iteration 37448, loss = 0.03360062\n",
      "Iteration 37449, loss = 0.03360316\n",
      "Iteration 37450, loss = 0.03360800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 37451, loss = 0.03360841\n",
      "Iteration 37452, loss = 0.03361127\n",
      "Iteration 37453, loss = 0.03360339\n",
      "Iteration 37454, loss = 0.03359595\n",
      "Iteration 37455, loss = 0.03360159\n",
      "Iteration 37456, loss = 0.03360786\n",
      "Iteration 37457, loss = 0.03359998\n",
      "Iteration 37458, loss = 0.03359595\n",
      "Iteration 37459, loss = 0.03360403\n",
      "Iteration 37460, loss = 0.03360334\n",
      "Iteration 37461, loss = 0.03360106\n",
      "Iteration 37462, loss = 0.03359771\n",
      "Iteration 37463, loss = 0.03360060\n",
      "Iteration 37464, loss = 0.03359789\n",
      "Iteration 37465, loss = 0.03359580\n",
      "Iteration 37466, loss = 0.03359410\n",
      "Iteration 37467, loss = 0.03359507\n",
      "Iteration 37468, loss = 0.03360749\n",
      "Iteration 37469, loss = 0.03360412\n",
      "Iteration 37470, loss = 0.03359111\n",
      "Iteration 37471, loss = 0.03359977\n",
      "Iteration 37472, loss = 0.03360932\n",
      "Iteration 37473, loss = 0.03360448\n",
      "Iteration 37474, loss = 0.03360078\n",
      "Iteration 37475, loss = 0.03360298\n",
      "Iteration 37476, loss = 0.03359825\n",
      "Iteration 37477, loss = 0.03359325\n",
      "Iteration 37478, loss = 0.03359870\n",
      "Iteration 37479, loss = 0.03360664\n",
      "Iteration 37480, loss = 0.03359422\n",
      "Iteration 37481, loss = 0.03359772\n",
      "Iteration 37482, loss = 0.03360236\n",
      "Iteration 37483, loss = 0.03360666\n",
      "Iteration 37484, loss = 0.03360512\n",
      "Iteration 37485, loss = 0.03359820\n",
      "Iteration 37486, loss = 0.03358823\n",
      "Iteration 37487, loss = 0.03359152\n",
      "Iteration 37488, loss = 0.03359894\n",
      "Iteration 37489, loss = 0.03359182\n",
      "Iteration 37490, loss = 0.03358129\n",
      "Iteration 37491, loss = 0.03358777\n",
      "Iteration 37492, loss = 0.03358521\n",
      "Iteration 37493, loss = 0.03358299\n",
      "Iteration 37494, loss = 0.03358720\n",
      "Iteration 37495, loss = 0.03358596\n",
      "Iteration 37496, loss = 0.03358172\n",
      "Iteration 37497, loss = 0.03358452\n",
      "Iteration 37498, loss = 0.03357615\n",
      "Iteration 37499, loss = 0.03359329\n",
      "Iteration 37500, loss = 0.03359332\n",
      "Iteration 37501, loss = 0.03358324\n",
      "Iteration 37502, loss = 0.03358748\n",
      "Iteration 37503, loss = 0.03358063\n",
      "Iteration 37504, loss = 0.03358543\n",
      "Iteration 37505, loss = 0.03358231\n",
      "Iteration 37506, loss = 0.03358232\n",
      "Iteration 37507, loss = 0.03358399\n",
      "Iteration 37508, loss = 0.03357936\n",
      "Iteration 37509, loss = 0.03357576\n",
      "Iteration 37510, loss = 0.03357617\n",
      "Iteration 37511, loss = 0.03358091\n",
      "Iteration 37512, loss = 0.03358344\n",
      "Iteration 37513, loss = 0.03357884\n",
      "Iteration 37514, loss = 0.03357074\n",
      "Iteration 37515, loss = 0.03358405\n",
      "Iteration 37516, loss = 0.03357905\n",
      "Iteration 37517, loss = 0.03357473\n",
      "Iteration 37518, loss = 0.03357913\n",
      "Iteration 37519, loss = 0.03357671\n",
      "Iteration 37520, loss = 0.03357458\n",
      "Iteration 37521, loss = 0.03357999\n",
      "Iteration 37522, loss = 0.03358505\n",
      "Iteration 37523, loss = 0.03357535\n",
      "Iteration 37524, loss = 0.03356871\n",
      "Iteration 37525, loss = 0.03357020\n",
      "Iteration 37526, loss = 0.03357899\n",
      "Iteration 37527, loss = 0.03358072\n",
      "Iteration 37528, loss = 0.03357680\n",
      "Iteration 37529, loss = 0.03357885\n",
      "Iteration 37530, loss = 0.03357769\n",
      "Iteration 37531, loss = 0.03357689\n",
      "Iteration 37532, loss = 0.03357182\n",
      "Iteration 37533, loss = 0.03357058\n",
      "Iteration 37534, loss = 0.03357401\n",
      "Iteration 37535, loss = 0.03357650\n",
      "Iteration 37536, loss = 0.03357054\n",
      "Iteration 37537, loss = 0.03357783\n",
      "Iteration 37538, loss = 0.03358165\n",
      "Iteration 37539, loss = 0.03357731\n",
      "Iteration 37540, loss = 0.03357433\n",
      "Iteration 37541, loss = 0.03357099\n",
      "Iteration 37542, loss = 0.03356373\n",
      "Iteration 37543, loss = 0.03357290\n",
      "Iteration 37544, loss = 0.03357838\n",
      "Iteration 37545, loss = 0.03357572\n",
      "Iteration 37546, loss = 0.03356578\n",
      "Iteration 37547, loss = 0.03356402\n",
      "Iteration 37548, loss = 0.03356570\n",
      "Iteration 37549, loss = 0.03356839\n",
      "Iteration 37550, loss = 0.03355631\n",
      "Iteration 37551, loss = 0.03357037\n",
      "Iteration 37552, loss = 0.03357713\n",
      "Iteration 37553, loss = 0.03357397\n",
      "Iteration 37554, loss = 0.03356576\n",
      "Iteration 37555, loss = 0.03355705\n",
      "Iteration 37556, loss = 0.03356993\n",
      "Iteration 37557, loss = 0.03356481\n",
      "Iteration 37558, loss = 0.03355767\n",
      "Iteration 37559, loss = 0.03356246\n",
      "Iteration 37560, loss = 0.03356797\n",
      "Iteration 37561, loss = 0.03356642\n",
      "Iteration 37562, loss = 0.03356045\n",
      "Iteration 37563, loss = 0.03356198\n",
      "Iteration 37564, loss = 0.03355468\n",
      "Iteration 37565, loss = 0.03356237\n",
      "Iteration 37566, loss = 0.03357076\n",
      "Iteration 37567, loss = 0.03356513\n",
      "Iteration 37568, loss = 0.03354886\n",
      "Iteration 37569, loss = 0.03356497\n",
      "Iteration 37570, loss = 0.03357244\n",
      "Iteration 37571, loss = 0.03356620\n",
      "Iteration 37572, loss = 0.03356027\n",
      "Iteration 37573, loss = 0.03356915\n",
      "Iteration 37574, loss = 0.03356858\n",
      "Iteration 37575, loss = 0.03356070\n",
      "Iteration 37576, loss = 0.03356368\n",
      "Iteration 37577, loss = 0.03355530\n",
      "Iteration 37578, loss = 0.03355192\n",
      "Iteration 37579, loss = 0.03356053\n",
      "Iteration 37580, loss = 0.03355510\n",
      "Iteration 37581, loss = 0.03354988\n",
      "Iteration 37582, loss = 0.03355878\n",
      "Iteration 37583, loss = 0.03355919\n",
      "Iteration 37584, loss = 0.03354384\n",
      "Iteration 37585, loss = 0.03354527\n",
      "Iteration 37586, loss = 0.03354491\n",
      "Iteration 37587, loss = 0.03354981\n",
      "Iteration 37588, loss = 0.03354737\n",
      "Iteration 37589, loss = 0.03355216\n",
      "Iteration 37590, loss = 0.03355102\n",
      "Iteration 37591, loss = 0.03355284\n",
      "Iteration 37592, loss = 0.03354540\n",
      "Iteration 37593, loss = 0.03355254\n",
      "Iteration 37594, loss = 0.03355300\n",
      "Iteration 37595, loss = 0.03354689\n",
      "Iteration 37596, loss = 0.03355528\n",
      "Iteration 37597, loss = 0.03355537\n",
      "Iteration 37598, loss = 0.03356341\n",
      "Iteration 37599, loss = 0.03355523\n",
      "Iteration 37600, loss = 0.03354430\n",
      "Iteration 37601, loss = 0.03354096\n",
      "Iteration 37602, loss = 0.03355075\n",
      "Iteration 37603, loss = 0.03355081\n",
      "Iteration 37604, loss = 0.03354850\n",
      "Iteration 37605, loss = 0.03354101\n",
      "Iteration 37606, loss = 0.03354007\n",
      "Iteration 37607, loss = 0.03355712\n",
      "Iteration 37608, loss = 0.03356269\n",
      "Iteration 37609, loss = 0.03355701\n",
      "Iteration 37610, loss = 0.03356090\n",
      "Iteration 37611, loss = 0.03355440\n",
      "Iteration 37612, loss = 0.03354362\n",
      "Iteration 37613, loss = 0.03354228\n",
      "Iteration 37614, loss = 0.03354880\n",
      "Iteration 37615, loss = 0.03355038\n",
      "Iteration 37616, loss = 0.03354002\n",
      "Iteration 37617, loss = 0.03354547\n",
      "Iteration 37618, loss = 0.03355120\n",
      "Iteration 37619, loss = 0.03354389\n",
      "Iteration 37620, loss = 0.03353979\n",
      "Iteration 37621, loss = 0.03354741\n",
      "Iteration 37622, loss = 0.03354621\n",
      "Iteration 37623, loss = 0.03354457\n",
      "Iteration 37624, loss = 0.03353709\n",
      "Iteration 37625, loss = 0.03354142\n",
      "Iteration 37626, loss = 0.03354280\n",
      "Iteration 37627, loss = 0.03355140\n",
      "Iteration 37628, loss = 0.03355094\n",
      "Iteration 37629, loss = 0.03355190\n",
      "Iteration 37630, loss = 0.03354221\n",
      "Iteration 37631, loss = 0.03354966\n",
      "Iteration 37632, loss = 0.03355485\n",
      "Iteration 37633, loss = 0.03355186\n",
      "Iteration 37634, loss = 0.03354601\n",
      "Iteration 37635, loss = 0.03352937\n",
      "Iteration 37636, loss = 0.03354329\n",
      "Iteration 37637, loss = 0.03355358\n",
      "Iteration 37638, loss = 0.03354955\n",
      "Iteration 37639, loss = 0.03354725\n",
      "Iteration 37640, loss = 0.03353667\n",
      "Iteration 37641, loss = 0.03352624\n",
      "Iteration 37642, loss = 0.03353855\n",
      "Iteration 37643, loss = 0.03354739\n",
      "Iteration 37644, loss = 0.03354322\n",
      "Iteration 37645, loss = 0.03352512\n",
      "Iteration 37646, loss = 0.03353430\n",
      "Iteration 37647, loss = 0.03353990\n",
      "Iteration 37648, loss = 0.03354765\n",
      "Iteration 37649, loss = 0.03354491\n",
      "Iteration 37650, loss = 0.03354087\n",
      "Iteration 37651, loss = 0.03353975\n",
      "Iteration 37652, loss = 0.03353312\n",
      "Iteration 37653, loss = 0.03353127\n",
      "Iteration 37654, loss = 0.03353489\n",
      "Iteration 37655, loss = 0.03354981\n",
      "Iteration 37656, loss = 0.03354937\n",
      "Iteration 37657, loss = 0.03353764\n",
      "Iteration 37658, loss = 0.03353061\n",
      "Iteration 37659, loss = 0.03352444\n",
      "Iteration 37660, loss = 0.03352803\n",
      "Iteration 37661, loss = 0.03353613\n",
      "Iteration 37662, loss = 0.03353222\n",
      "Iteration 37663, loss = 0.03352863\n",
      "Iteration 37664, loss = 0.03353526\n",
      "Iteration 37665, loss = 0.03353396\n",
      "Iteration 37666, loss = 0.03352582\n",
      "Iteration 37667, loss = 0.03352350\n",
      "Iteration 37668, loss = 0.03353084\n",
      "Iteration 37669, loss = 0.03353242\n",
      "Iteration 37670, loss = 0.03353670\n",
      "Iteration 37671, loss = 0.03354138\n",
      "Iteration 37672, loss = 0.03353887\n",
      "Iteration 37673, loss = 0.03352747\n",
      "Iteration 37674, loss = 0.03353952\n",
      "Iteration 37675, loss = 0.03353785\n",
      "Iteration 37676, loss = 0.03352796\n",
      "Iteration 37677, loss = 0.03353074\n",
      "Iteration 37678, loss = 0.03351487\n",
      "Iteration 37679, loss = 0.03351522\n",
      "Iteration 37680, loss = 0.03353274\n",
      "Iteration 37681, loss = 0.03353177\n",
      "Iteration 37682, loss = 0.03352696\n",
      "Iteration 37683, loss = 0.03352356\n",
      "Iteration 37684, loss = 0.03351617\n",
      "Iteration 37685, loss = 0.03351408\n",
      "Iteration 37686, loss = 0.03352150\n",
      "Iteration 37687, loss = 0.03351983\n",
      "Iteration 37688, loss = 0.03351855\n",
      "Iteration 37689, loss = 0.03351531\n",
      "Iteration 37690, loss = 0.03352088\n",
      "Iteration 37691, loss = 0.03352109\n",
      "Iteration 37692, loss = 0.03351324\n",
      "Iteration 37693, loss = 0.03351340\n",
      "Iteration 37694, loss = 0.03351190\n",
      "Iteration 37695, loss = 0.03351137\n",
      "Iteration 37696, loss = 0.03351175\n",
      "Iteration 37697, loss = 0.03351551\n",
      "Iteration 37698, loss = 0.03351127\n",
      "Iteration 37699, loss = 0.03351005\n",
      "Iteration 37700, loss = 0.03352224\n",
      "Iteration 37701, loss = 0.03352181\n",
      "Iteration 37702, loss = 0.03351713\n",
      "Iteration 37703, loss = 0.03351500\n",
      "Iteration 37704, loss = 0.03351428\n",
      "Iteration 37705, loss = 0.03350496\n",
      "Iteration 37706, loss = 0.03350058\n",
      "Iteration 37707, loss = 0.03352019\n",
      "Iteration 37708, loss = 0.03352200\n",
      "Iteration 37709, loss = 0.03351554\n",
      "Iteration 37710, loss = 0.03350477\n",
      "Iteration 37711, loss = 0.03351756\n",
      "Iteration 37712, loss = 0.03352494\n",
      "Iteration 37713, loss = 0.03352106\n",
      "Iteration 37714, loss = 0.03351311\n",
      "Iteration 37715, loss = 0.03351291\n",
      "Iteration 37716, loss = 0.03350115\n",
      "Iteration 37717, loss = 0.03350409\n",
      "Iteration 37718, loss = 0.03352502\n",
      "Iteration 37719, loss = 0.03352333\n",
      "Iteration 37720, loss = 0.03350511\n",
      "Iteration 37721, loss = 0.03350306\n",
      "Iteration 37722, loss = 0.03351397\n",
      "Iteration 37723, loss = 0.03351786\n",
      "Iteration 37724, loss = 0.03351974\n",
      "Iteration 37725, loss = 0.03351084\n",
      "Iteration 37726, loss = 0.03350916\n",
      "Iteration 37727, loss = 0.03351550\n",
      "Iteration 37728, loss = 0.03351269\n",
      "Iteration 37729, loss = 0.03351332\n",
      "Iteration 37730, loss = 0.03349767\n",
      "Iteration 37731, loss = 0.03350110\n",
      "Iteration 37732, loss = 0.03350398\n",
      "Iteration 37733, loss = 0.03350028\n",
      "Iteration 37734, loss = 0.03350002\n",
      "Iteration 37735, loss = 0.03350217\n",
      "Iteration 37736, loss = 0.03349412\n",
      "Iteration 37737, loss = 0.03350682\n",
      "Iteration 37738, loss = 0.03350540\n",
      "Iteration 37739, loss = 0.03350234\n",
      "Iteration 37740, loss = 0.03349730\n",
      "Iteration 37741, loss = 0.03348962\n",
      "Iteration 37742, loss = 0.03349829\n",
      "Iteration 37743, loss = 0.03349749\n",
      "Iteration 37744, loss = 0.03348928\n",
      "Iteration 37745, loss = 0.03349056\n",
      "Iteration 37746, loss = 0.03350260\n",
      "Iteration 37747, loss = 0.03350539\n",
      "Iteration 37748, loss = 0.03350383\n",
      "Iteration 37749, loss = 0.03349418\n",
      "Iteration 37750, loss = 0.03349084\n",
      "Iteration 37751, loss = 0.03349778\n",
      "Iteration 37752, loss = 0.03349700\n",
      "Iteration 37753, loss = 0.03349548\n",
      "Iteration 37754, loss = 0.03349150\n",
      "Iteration 37755, loss = 0.03349222\n",
      "Iteration 37756, loss = 0.03349907\n",
      "Iteration 37757, loss = 0.03349593\n",
      "Iteration 37758, loss = 0.03348532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 37759, loss = 0.03348764\n",
      "Iteration 37760, loss = 0.03349288\n",
      "Iteration 37761, loss = 0.03349214\n",
      "Iteration 37762, loss = 0.03349264\n",
      "Iteration 37763, loss = 0.03348329\n",
      "Iteration 37764, loss = 0.03348964\n",
      "Iteration 37765, loss = 0.03349671\n",
      "Iteration 37766, loss = 0.03349166\n",
      "Iteration 37767, loss = 0.03348558\n",
      "Iteration 37768, loss = 0.03349305\n",
      "Iteration 37769, loss = 0.03350313\n",
      "Iteration 37770, loss = 0.03350198\n",
      "Iteration 37771, loss = 0.03348947\n",
      "Iteration 37772, loss = 0.03348582\n",
      "Iteration 37773, loss = 0.03348555\n",
      "Iteration 37774, loss = 0.03348428\n",
      "Iteration 37775, loss = 0.03348521\n",
      "Iteration 37776, loss = 0.03347712\n",
      "Iteration 37777, loss = 0.03348043\n",
      "Iteration 37778, loss = 0.03348230\n",
      "Iteration 37779, loss = 0.03348659\n",
      "Iteration 37780, loss = 0.03348544\n",
      "Iteration 37781, loss = 0.03348203\n",
      "Iteration 37782, loss = 0.03347477\n",
      "Iteration 37783, loss = 0.03347994\n",
      "Iteration 37784, loss = 0.03348448\n",
      "Iteration 37785, loss = 0.03348083\n",
      "Iteration 37786, loss = 0.03347812\n",
      "Iteration 37787, loss = 0.03347671\n",
      "Iteration 37788, loss = 0.03348056\n",
      "Iteration 37789, loss = 0.03347720\n",
      "Iteration 37790, loss = 0.03347061\n",
      "Iteration 37791, loss = 0.03347380\n",
      "Iteration 37792, loss = 0.03347040\n",
      "Iteration 37793, loss = 0.03346847\n",
      "Iteration 37794, loss = 0.03346843\n",
      "Iteration 37795, loss = 0.03346828\n",
      "Iteration 37796, loss = 0.03346669\n",
      "Iteration 37797, loss = 0.03346725\n",
      "Iteration 37798, loss = 0.03346448\n",
      "Iteration 37799, loss = 0.03346245\n",
      "Iteration 37800, loss = 0.03346908\n",
      "Iteration 37801, loss = 0.03347103\n",
      "Iteration 37802, loss = 0.03346811\n",
      "Iteration 37803, loss = 0.03346807\n",
      "Iteration 37804, loss = 0.03346779\n",
      "Iteration 37805, loss = 0.03346993\n",
      "Iteration 37806, loss = 0.03346982\n",
      "Iteration 37807, loss = 0.03347351\n",
      "Iteration 37808, loss = 0.03347204\n",
      "Iteration 37809, loss = 0.03345950\n",
      "Iteration 37810, loss = 0.03347086\n",
      "Iteration 37811, loss = 0.03347267\n",
      "Iteration 37812, loss = 0.03347065\n",
      "Iteration 37813, loss = 0.03346816\n",
      "Iteration 37814, loss = 0.03346684\n",
      "Iteration 37815, loss = 0.03346688\n",
      "Iteration 37816, loss = 0.03346537\n",
      "Iteration 37817, loss = 0.03346630\n",
      "Iteration 37818, loss = 0.03346530\n",
      "Iteration 37819, loss = 0.03346733\n",
      "Iteration 37820, loss = 0.03346291\n",
      "Iteration 37821, loss = 0.03346275\n",
      "Iteration 37822, loss = 0.03346328\n",
      "Iteration 37823, loss = 0.03346596\n",
      "Iteration 37824, loss = 0.03346798\n",
      "Iteration 37825, loss = 0.03346570\n",
      "Iteration 37826, loss = 0.03346867\n",
      "Iteration 37827, loss = 0.03346776\n",
      "Iteration 37828, loss = 0.03346876\n",
      "Iteration 37829, loss = 0.03346015\n",
      "Iteration 37830, loss = 0.03345649\n",
      "Iteration 37831, loss = 0.03345907\n",
      "Iteration 37832, loss = 0.03346106\n",
      "Iteration 37833, loss = 0.03346564\n",
      "Iteration 37834, loss = 0.03346124\n",
      "Iteration 37835, loss = 0.03346002\n",
      "Iteration 37836, loss = 0.03346075\n",
      "Iteration 37837, loss = 0.03345977\n",
      "Iteration 37838, loss = 0.03346376\n",
      "Iteration 37839, loss = 0.03346402\n",
      "Iteration 37840, loss = 0.03345446\n",
      "Iteration 37841, loss = 0.03344528\n",
      "Iteration 37842, loss = 0.03346135\n",
      "Iteration 37843, loss = 0.03346690\n",
      "Iteration 37844, loss = 0.03345783\n",
      "Iteration 37845, loss = 0.03345700\n",
      "Iteration 37846, loss = 0.03345776\n",
      "Iteration 37847, loss = 0.03345217\n",
      "Iteration 37848, loss = 0.03345470\n",
      "Iteration 37849, loss = 0.03345420\n",
      "Iteration 37850, loss = 0.03345173\n",
      "Iteration 37851, loss = 0.03344531\n",
      "Iteration 37852, loss = 0.03344523\n",
      "Iteration 37853, loss = 0.03345312\n",
      "Iteration 37854, loss = 0.03344778\n",
      "Iteration 37855, loss = 0.03344424\n",
      "Iteration 37856, loss = 0.03344318\n",
      "Iteration 37857, loss = 0.03345424\n",
      "Iteration 37858, loss = 0.03344914\n",
      "Iteration 37859, loss = 0.03343974\n",
      "Iteration 37860, loss = 0.03345433\n",
      "Iteration 37861, loss = 0.03345245\n",
      "Iteration 37862, loss = 0.03345512\n",
      "Iteration 37863, loss = 0.03346554\n",
      "Iteration 37864, loss = 0.03346253\n",
      "Iteration 37865, loss = 0.03345042\n",
      "Iteration 37866, loss = 0.03344052\n",
      "Iteration 37867, loss = 0.03345527\n",
      "Iteration 37868, loss = 0.03346294\n",
      "Iteration 37869, loss = 0.03345209\n",
      "Iteration 37870, loss = 0.03344169\n",
      "Iteration 37871, loss = 0.03344527\n",
      "Iteration 37872, loss = 0.03344918\n",
      "Iteration 37873, loss = 0.03345282\n",
      "Iteration 37874, loss = 0.03345388\n",
      "Iteration 37875, loss = 0.03344881\n",
      "Iteration 37876, loss = 0.03344557\n",
      "Iteration 37877, loss = 0.03344077\n",
      "Iteration 37878, loss = 0.03344348\n",
      "Iteration 37879, loss = 0.03344305\n",
      "Iteration 37880, loss = 0.03344087\n",
      "Iteration 37881, loss = 0.03344117\n",
      "Iteration 37882, loss = 0.03344799\n",
      "Iteration 37883, loss = 0.03344641\n",
      "Iteration 37884, loss = 0.03344603\n",
      "Iteration 37885, loss = 0.03344438\n",
      "Iteration 37886, loss = 0.03343990\n",
      "Iteration 37887, loss = 0.03343785\n",
      "Iteration 37888, loss = 0.03343948\n",
      "Iteration 37889, loss = 0.03344576\n",
      "Iteration 37890, loss = 0.03344090\n",
      "Iteration 37891, loss = 0.03342798\n",
      "Iteration 37892, loss = 0.03344244\n",
      "Iteration 37893, loss = 0.03344506\n",
      "Iteration 37894, loss = 0.03344392\n",
      "Iteration 37895, loss = 0.03344281\n",
      "Iteration 37896, loss = 0.03344395\n",
      "Iteration 37897, loss = 0.03343986\n",
      "Iteration 37898, loss = 0.03343387\n",
      "Iteration 37899, loss = 0.03343247\n",
      "Iteration 37900, loss = 0.03343829\n",
      "Iteration 37901, loss = 0.03344368\n",
      "Iteration 37902, loss = 0.03344007\n",
      "Iteration 37903, loss = 0.03343004\n",
      "Iteration 37904, loss = 0.03343539\n",
      "Iteration 37905, loss = 0.03344974\n",
      "Iteration 37906, loss = 0.03344385\n",
      "Iteration 37907, loss = 0.03342590\n",
      "Iteration 37908, loss = 0.03343277\n",
      "Iteration 37909, loss = 0.03343630\n",
      "Iteration 37910, loss = 0.03343089\n",
      "Iteration 37911, loss = 0.03343615\n",
      "Iteration 37912, loss = 0.03343644\n",
      "Iteration 37913, loss = 0.03342894\n",
      "Iteration 37914, loss = 0.03342324\n",
      "Iteration 37915, loss = 0.03343232\n",
      "Iteration 37916, loss = 0.03342127\n",
      "Iteration 37917, loss = 0.03343060\n",
      "Iteration 37918, loss = 0.03343281\n",
      "Iteration 37919, loss = 0.03343289\n",
      "Iteration 37920, loss = 0.03343218\n",
      "Iteration 37921, loss = 0.03342360\n",
      "Iteration 37922, loss = 0.03343174\n",
      "Iteration 37923, loss = 0.03343299\n",
      "Iteration 37924, loss = 0.03342697\n",
      "Iteration 37925, loss = 0.03343304\n",
      "Iteration 37926, loss = 0.03343308\n",
      "Iteration 37927, loss = 0.03344025\n",
      "Iteration 37928, loss = 0.03343801\n",
      "Iteration 37929, loss = 0.03343275\n",
      "Iteration 37930, loss = 0.03343106\n",
      "Iteration 37931, loss = 0.03343593\n",
      "Iteration 37932, loss = 0.03343916\n",
      "Iteration 37933, loss = 0.03343169\n",
      "Iteration 37934, loss = 0.03342277\n",
      "Iteration 37935, loss = 0.03342273\n",
      "Iteration 37936, loss = 0.03342649\n",
      "Iteration 37937, loss = 0.03343527\n",
      "Iteration 37938, loss = 0.03343362\n",
      "Iteration 37939, loss = 0.03342751\n",
      "Iteration 37940, loss = 0.03342515\n",
      "Iteration 37941, loss = 0.03342644\n",
      "Iteration 37942, loss = 0.03342313\n",
      "Iteration 37943, loss = 0.03341458\n",
      "Iteration 37944, loss = 0.03341027\n",
      "Iteration 37945, loss = 0.03341337\n",
      "Iteration 37946, loss = 0.03341129\n",
      "Iteration 37947, loss = 0.03341936\n",
      "Iteration 37948, loss = 0.03342242\n",
      "Iteration 37949, loss = 0.03341793\n",
      "Iteration 37950, loss = 0.03341388\n",
      "Iteration 37951, loss = 0.03342272\n",
      "Iteration 37952, loss = 0.03341822\n",
      "Iteration 37953, loss = 0.03341025\n",
      "Iteration 37954, loss = 0.03341652\n",
      "Iteration 37955, loss = 0.03342419\n",
      "Iteration 37956, loss = 0.03341336\n",
      "Iteration 37957, loss = 0.03341217\n",
      "Iteration 37958, loss = 0.03341731\n",
      "Iteration 37959, loss = 0.03341422\n",
      "Iteration 37960, loss = 0.03341183\n",
      "Iteration 37961, loss = 0.03341612\n",
      "Iteration 37962, loss = 0.03342133\n",
      "Iteration 37963, loss = 0.03341607\n",
      "Iteration 37964, loss = 0.03340773\n",
      "Iteration 37965, loss = 0.03341470\n",
      "Iteration 37966, loss = 0.03341169\n",
      "Iteration 37967, loss = 0.03340922\n",
      "Iteration 37968, loss = 0.03341091\n",
      "Iteration 37969, loss = 0.03341160\n",
      "Iteration 37970, loss = 0.03341310\n",
      "Iteration 37971, loss = 0.03341477\n",
      "Iteration 37972, loss = 0.03341297\n",
      "Iteration 37973, loss = 0.03340809\n",
      "Iteration 37974, loss = 0.03340436\n",
      "Iteration 37975, loss = 0.03341087\n",
      "Iteration 37976, loss = 0.03341464\n",
      "Iteration 37977, loss = 0.03340253\n",
      "Iteration 37978, loss = 0.03340849\n",
      "Iteration 37979, loss = 0.03341608\n",
      "Iteration 37980, loss = 0.03341215\n",
      "Iteration 37981, loss = 0.03341898\n",
      "Iteration 37982, loss = 0.03341706\n",
      "Iteration 37983, loss = 0.03340550\n",
      "Iteration 37984, loss = 0.03340872\n",
      "Iteration 37985, loss = 0.03341784\n",
      "Iteration 37986, loss = 0.03340679\n",
      "Iteration 37987, loss = 0.03339740\n",
      "Iteration 37988, loss = 0.03339964\n",
      "Iteration 37989, loss = 0.03340611\n",
      "Iteration 37990, loss = 0.03339234\n",
      "Iteration 37991, loss = 0.03340439\n",
      "Iteration 37992, loss = 0.03340926\n",
      "Iteration 37993, loss = 0.03340477\n",
      "Iteration 37994, loss = 0.03340093\n",
      "Iteration 37995, loss = 0.03339910\n",
      "Iteration 37996, loss = 0.03340555\n",
      "Iteration 37997, loss = 0.03341320\n",
      "Iteration 37998, loss = 0.03340648\n",
      "Iteration 37999, loss = 0.03340586\n",
      "Iteration 38000, loss = 0.03339701\n",
      "Iteration 38001, loss = 0.03339632\n",
      "Iteration 38002, loss = 0.03340661\n",
      "Iteration 38003, loss = 0.03340731\n",
      "Iteration 38004, loss = 0.03339622\n",
      "Iteration 38005, loss = 0.03339387\n",
      "Iteration 38006, loss = 0.03341054\n",
      "Iteration 38007, loss = 0.03341536\n",
      "Iteration 38008, loss = 0.03340047\n",
      "Iteration 38009, loss = 0.03340050\n",
      "Iteration 38010, loss = 0.03340410\n",
      "Iteration 38011, loss = 0.03341308\n",
      "Iteration 38012, loss = 0.03340876\n",
      "Iteration 38013, loss = 0.03340747\n",
      "Iteration 38014, loss = 0.03340158\n",
      "Iteration 38015, loss = 0.03339666\n",
      "Iteration 38016, loss = 0.03340269\n",
      "Iteration 38017, loss = 0.03340844\n",
      "Iteration 38018, loss = 0.03340795\n",
      "Iteration 38019, loss = 0.03340308\n",
      "Iteration 38020, loss = 0.03340438\n",
      "Iteration 38021, loss = 0.03339735\n",
      "Iteration 38022, loss = 0.03340374\n",
      "Iteration 38023, loss = 0.03339852\n",
      "Iteration 38024, loss = 0.03339317\n",
      "Iteration 38025, loss = 0.03338744\n",
      "Iteration 38026, loss = 0.03338970\n",
      "Iteration 38027, loss = 0.03338825\n",
      "Iteration 38028, loss = 0.03339537\n",
      "Iteration 38029, loss = 0.03339820\n",
      "Iteration 38030, loss = 0.03339256\n",
      "Iteration 38031, loss = 0.03338946\n",
      "Iteration 38032, loss = 0.03339111\n",
      "Iteration 38033, loss = 0.03339228\n",
      "Iteration 38034, loss = 0.03338283\n",
      "Iteration 38035, loss = 0.03338478\n",
      "Iteration 38036, loss = 0.03339647\n",
      "Iteration 38037, loss = 0.03339046\n",
      "Iteration 38038, loss = 0.03337596\n",
      "Iteration 38039, loss = 0.03338784\n",
      "Iteration 38040, loss = 0.03339923\n",
      "Iteration 38041, loss = 0.03339610\n",
      "Iteration 38042, loss = 0.03339087\n",
      "Iteration 38043, loss = 0.03339215\n",
      "Iteration 38044, loss = 0.03340251\n",
      "Iteration 38045, loss = 0.03339679\n",
      "Iteration 38046, loss = 0.03339334\n",
      "Iteration 38047, loss = 0.03339078\n",
      "Iteration 38048, loss = 0.03339981\n",
      "Iteration 38049, loss = 0.03339345\n",
      "Iteration 38050, loss = 0.03339133\n",
      "Iteration 38051, loss = 0.03339138\n",
      "Iteration 38052, loss = 0.03339092\n",
      "Iteration 38053, loss = 0.03338169\n",
      "Iteration 38054, loss = 0.03338301\n",
      "Iteration 38055, loss = 0.03338049\n",
      "Iteration 38056, loss = 0.03337483\n",
      "Iteration 38057, loss = 0.03337762\n",
      "Iteration 38058, loss = 0.03337605\n",
      "Iteration 38059, loss = 0.03337788\n",
      "Iteration 38060, loss = 0.03337362\n",
      "Iteration 38061, loss = 0.03336848\n",
      "Iteration 38062, loss = 0.03337377\n",
      "Iteration 38063, loss = 0.03337435\n",
      "Iteration 38064, loss = 0.03336948\n",
      "Iteration 38065, loss = 0.03337082\n",
      "Iteration 38066, loss = 0.03337581\n",
      "Iteration 38067, loss = 0.03337408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 38068, loss = 0.03337969\n",
      "Iteration 38069, loss = 0.03338337\n",
      "Iteration 38070, loss = 0.03337874\n",
      "Iteration 38071, loss = 0.03337266\n",
      "Iteration 38072, loss = 0.03337503\n",
      "Iteration 38073, loss = 0.03337789\n",
      "Iteration 38074, loss = 0.03336471\n",
      "Iteration 38075, loss = 0.03337305\n",
      "Iteration 38076, loss = 0.03337674\n",
      "Iteration 38077, loss = 0.03337597\n",
      "Iteration 38078, loss = 0.03337985\n",
      "Iteration 38079, loss = 0.03337977\n",
      "Iteration 38080, loss = 0.03337957\n",
      "Iteration 38081, loss = 0.03337289\n",
      "Iteration 38082, loss = 0.03336597\n",
      "Iteration 38083, loss = 0.03336685\n",
      "Iteration 38084, loss = 0.03335916\n",
      "Iteration 38085, loss = 0.03336301\n",
      "Iteration 38086, loss = 0.03336713\n",
      "Iteration 38087, loss = 0.03337209\n",
      "Iteration 38088, loss = 0.03336945\n",
      "Iteration 38089, loss = 0.03336461\n",
      "Iteration 38090, loss = 0.03336645\n",
      "Iteration 38091, loss = 0.03336819\n",
      "Iteration 38092, loss = 0.03336345\n",
      "Iteration 38093, loss = 0.03336412\n",
      "Iteration 38094, loss = 0.03336690\n",
      "Iteration 38095, loss = 0.03336081\n",
      "Iteration 38096, loss = 0.03336094\n",
      "Iteration 38097, loss = 0.03336309\n",
      "Iteration 38098, loss = 0.03336039\n",
      "Iteration 38099, loss = 0.03335579\n",
      "Iteration 38100, loss = 0.03336346\n",
      "Iteration 38101, loss = 0.03336505\n",
      "Iteration 38102, loss = 0.03336109\n",
      "Iteration 38103, loss = 0.03336154\n",
      "Iteration 38104, loss = 0.03336690\n",
      "Iteration 38105, loss = 0.03336401\n",
      "Iteration 38106, loss = 0.03336144\n",
      "Iteration 38107, loss = 0.03335537\n",
      "Iteration 38108, loss = 0.03335241\n",
      "Iteration 38109, loss = 0.03335026\n",
      "Iteration 38110, loss = 0.03335056\n",
      "Iteration 38111, loss = 0.03334943\n",
      "Iteration 38112, loss = 0.03335141\n",
      "Iteration 38113, loss = 0.03335710\n",
      "Iteration 38114, loss = 0.03335053\n",
      "Iteration 38115, loss = 0.03335478\n",
      "Iteration 38116, loss = 0.03335672\n",
      "Iteration 38117, loss = 0.03335269\n",
      "Iteration 38118, loss = 0.03335253\n",
      "Iteration 38119, loss = 0.03335384\n",
      "Iteration 38120, loss = 0.03335658\n",
      "Iteration 38121, loss = 0.03335567\n",
      "Iteration 38122, loss = 0.03335777\n",
      "Iteration 38123, loss = 0.03335518\n",
      "Iteration 38124, loss = 0.03334807\n",
      "Iteration 38125, loss = 0.03334477\n",
      "Iteration 38126, loss = 0.03335941\n",
      "Iteration 38127, loss = 0.03335881\n",
      "Iteration 38128, loss = 0.03335099\n",
      "Iteration 38129, loss = 0.03335798\n",
      "Iteration 38130, loss = 0.03336198\n",
      "Iteration 38131, loss = 0.03336176\n",
      "Iteration 38132, loss = 0.03335391\n",
      "Iteration 38133, loss = 0.03335660\n",
      "Iteration 38134, loss = 0.03335268\n",
      "Iteration 38135, loss = 0.03334481\n",
      "Iteration 38136, loss = 0.03334473\n",
      "Iteration 38137, loss = 0.03334478\n",
      "Iteration 38138, loss = 0.03334818\n",
      "Iteration 38139, loss = 0.03334223\n",
      "Iteration 38140, loss = 0.03334451\n",
      "Iteration 38141, loss = 0.03334754\n",
      "Iteration 38142, loss = 0.03334598\n",
      "Iteration 38143, loss = 0.03334677\n",
      "Iteration 38144, loss = 0.03334439\n",
      "Iteration 38145, loss = 0.03334546\n",
      "Iteration 38146, loss = 0.03335147\n",
      "Iteration 38147, loss = 0.03334599\n",
      "Iteration 38148, loss = 0.03334028\n",
      "Iteration 38149, loss = 0.03334157\n",
      "Iteration 38150, loss = 0.03334682\n",
      "Iteration 38151, loss = 0.03335122\n",
      "Iteration 38152, loss = 0.03334558\n",
      "Iteration 38153, loss = 0.03334348\n",
      "Iteration 38154, loss = 0.03334262\n",
      "Iteration 38155, loss = 0.03334919\n",
      "Iteration 38156, loss = 0.03335241\n",
      "Iteration 38157, loss = 0.03334218\n",
      "Iteration 38158, loss = 0.03333778\n",
      "Iteration 38159, loss = 0.03333422\n",
      "Iteration 38160, loss = 0.03333692\n",
      "Iteration 38161, loss = 0.03333573\n",
      "Iteration 38162, loss = 0.03333474\n",
      "Iteration 38163, loss = 0.03333559\n",
      "Iteration 38164, loss = 0.03332856\n",
      "Iteration 38165, loss = 0.03333349\n",
      "Iteration 38166, loss = 0.03334077\n",
      "Iteration 38167, loss = 0.03333596\n",
      "Iteration 38168, loss = 0.03333019\n",
      "Iteration 38169, loss = 0.03333399\n",
      "Iteration 38170, loss = 0.03333180\n",
      "Iteration 38171, loss = 0.03332962\n",
      "Iteration 38172, loss = 0.03333356\n",
      "Iteration 38173, loss = 0.03333183\n",
      "Iteration 38174, loss = 0.03332769\n",
      "Iteration 38175, loss = 0.03333754\n",
      "Iteration 38176, loss = 0.03334070\n",
      "Iteration 38177, loss = 0.03333507\n",
      "Iteration 38178, loss = 0.03332907\n",
      "Iteration 38179, loss = 0.03334431\n",
      "Iteration 38180, loss = 0.03335117\n",
      "Iteration 38181, loss = 0.03334183\n",
      "Iteration 38182, loss = 0.03333180\n",
      "Iteration 38183, loss = 0.03332510\n",
      "Iteration 38184, loss = 0.03332734\n",
      "Iteration 38185, loss = 0.03333614\n",
      "Iteration 38186, loss = 0.03332712\n",
      "Iteration 38187, loss = 0.03332600\n",
      "Iteration 38188, loss = 0.03333161\n",
      "Iteration 38189, loss = 0.03332871\n",
      "Iteration 38190, loss = 0.03333221\n",
      "Iteration 38191, loss = 0.03333299\n",
      "Iteration 38192, loss = 0.03332431\n",
      "Iteration 38193, loss = 0.03332925\n",
      "Iteration 38194, loss = 0.03333986\n",
      "Iteration 38195, loss = 0.03333466\n",
      "Iteration 38196, loss = 0.03332911\n",
      "Iteration 38197, loss = 0.03332918\n",
      "Iteration 38198, loss = 0.03333281\n",
      "Iteration 38199, loss = 0.03333059\n",
      "Iteration 38200, loss = 0.03332828\n",
      "Iteration 38201, loss = 0.03333026\n",
      "Iteration 38202, loss = 0.03332641\n",
      "Iteration 38203, loss = 0.03332349\n",
      "Iteration 38204, loss = 0.03333268\n",
      "Iteration 38205, loss = 0.03333100\n",
      "Iteration 38206, loss = 0.03331579\n",
      "Iteration 38207, loss = 0.03332526\n",
      "Iteration 38208, loss = 0.03333046\n",
      "Iteration 38209, loss = 0.03332298\n",
      "Iteration 38210, loss = 0.03332548\n",
      "Iteration 38211, loss = 0.03332308\n",
      "Iteration 38212, loss = 0.03331852\n",
      "Iteration 38213, loss = 0.03331759\n",
      "Iteration 38214, loss = 0.03332306\n",
      "Iteration 38215, loss = 0.03333313\n",
      "Iteration 38216, loss = 0.03333585\n",
      "Iteration 38217, loss = 0.03331862\n",
      "Iteration 38218, loss = 0.03331862\n",
      "Iteration 38219, loss = 0.03332420\n",
      "Iteration 38220, loss = 0.03333815\n",
      "Iteration 38221, loss = 0.03333465\n",
      "Iteration 38222, loss = 0.03331999\n",
      "Iteration 38223, loss = 0.03332772\n",
      "Iteration 38224, loss = 0.03332475\n",
      "Iteration 38225, loss = 0.03332748\n",
      "Iteration 38226, loss = 0.03333857\n",
      "Iteration 38227, loss = 0.03333711\n",
      "Iteration 38228, loss = 0.03332541\n",
      "Iteration 38229, loss = 0.03332315\n",
      "Iteration 38230, loss = 0.03332592\n",
      "Iteration 38231, loss = 0.03332206\n",
      "Iteration 38232, loss = 0.03332250\n",
      "Iteration 38233, loss = 0.03333041\n",
      "Iteration 38234, loss = 0.03332250\n",
      "Iteration 38235, loss = 0.03331627\n",
      "Iteration 38236, loss = 0.03332234\n",
      "Iteration 38237, loss = 0.03331938\n",
      "Iteration 38238, loss = 0.03330879\n",
      "Iteration 38239, loss = 0.03330609\n",
      "Iteration 38240, loss = 0.03331690\n",
      "Iteration 38241, loss = 0.03331209\n",
      "Iteration 38242, loss = 0.03330408\n",
      "Iteration 38243, loss = 0.03330476\n",
      "Iteration 38244, loss = 0.03331611\n",
      "Iteration 38245, loss = 0.03330840\n",
      "Iteration 38246, loss = 0.03330942\n",
      "Iteration 38247, loss = 0.03331080\n",
      "Iteration 38248, loss = 0.03332239\n",
      "Iteration 38249, loss = 0.03331579\n",
      "Iteration 38250, loss = 0.03330753\n",
      "Iteration 38251, loss = 0.03330972\n",
      "Iteration 38252, loss = 0.03332899\n",
      "Iteration 38253, loss = 0.03332291\n",
      "Iteration 38254, loss = 0.03330900\n",
      "Iteration 38255, loss = 0.03330387\n",
      "Iteration 38256, loss = 0.03331364\n",
      "Iteration 38257, loss = 0.03331260\n",
      "Iteration 38258, loss = 0.03331140\n",
      "Iteration 38259, loss = 0.03330474\n",
      "Iteration 38260, loss = 0.03329860\n",
      "Iteration 38261, loss = 0.03330383\n",
      "Iteration 38262, loss = 0.03330637\n",
      "Iteration 38263, loss = 0.03329838\n",
      "Iteration 38264, loss = 0.03330474\n",
      "Iteration 38265, loss = 0.03331108\n",
      "Iteration 38266, loss = 0.03331293\n",
      "Iteration 38267, loss = 0.03330504\n",
      "Iteration 38268, loss = 0.03331071\n",
      "Iteration 38269, loss = 0.03330839\n",
      "Iteration 38270, loss = 0.03330243\n",
      "Iteration 38271, loss = 0.03330392\n",
      "Iteration 38272, loss = 0.03330418\n",
      "Iteration 38273, loss = 0.03330124\n",
      "Iteration 38274, loss = 0.03329626\n",
      "Iteration 38275, loss = 0.03330119\n",
      "Iteration 38276, loss = 0.03330349\n",
      "Iteration 38277, loss = 0.03330325\n",
      "Iteration 38278, loss = 0.03329700\n",
      "Iteration 38279, loss = 0.03328964\n",
      "Iteration 38280, loss = 0.03330472\n",
      "Iteration 38281, loss = 0.03330380\n",
      "Iteration 38282, loss = 0.03329004\n",
      "Iteration 38283, loss = 0.03329629\n",
      "Iteration 38284, loss = 0.03330580\n",
      "Iteration 38285, loss = 0.03330058\n",
      "Iteration 38286, loss = 0.03331170\n",
      "Iteration 38287, loss = 0.03331119\n",
      "Iteration 38288, loss = 0.03330089\n",
      "Iteration 38289, loss = 0.03329106\n",
      "Iteration 38290, loss = 0.03331164\n",
      "Iteration 38291, loss = 0.03332426\n",
      "Iteration 38292, loss = 0.03331757\n",
      "Iteration 38293, loss = 0.03330170\n",
      "Iteration 38294, loss = 0.03330004\n",
      "Iteration 38295, loss = 0.03329712\n",
      "Iteration 38296, loss = 0.03330662\n",
      "Iteration 38297, loss = 0.03330640\n",
      "Iteration 38298, loss = 0.03330291\n",
      "Iteration 38299, loss = 0.03329353\n",
      "Iteration 38300, loss = 0.03328853\n",
      "Iteration 38301, loss = 0.03329403\n",
      "Iteration 38302, loss = 0.03329943\n",
      "Iteration 38303, loss = 0.03330068\n",
      "Iteration 38304, loss = 0.03329404\n",
      "Iteration 38305, loss = 0.03329095\n",
      "Iteration 38306, loss = 0.03328741\n",
      "Iteration 38307, loss = 0.03328662\n",
      "Iteration 38308, loss = 0.03328820\n",
      "Iteration 38309, loss = 0.03329258\n",
      "Iteration 38310, loss = 0.03328730\n",
      "Iteration 38311, loss = 0.03328100\n",
      "Iteration 38312, loss = 0.03328971\n",
      "Iteration 38313, loss = 0.03329770\n",
      "Iteration 38314, loss = 0.03328743\n",
      "Iteration 38315, loss = 0.03328753\n",
      "Iteration 38316, loss = 0.03329881\n",
      "Iteration 38317, loss = 0.03330201\n",
      "Iteration 38318, loss = 0.03329897\n",
      "Iteration 38319, loss = 0.03328487\n",
      "Iteration 38320, loss = 0.03327366\n",
      "Iteration 38321, loss = 0.03329513\n",
      "Iteration 38322, loss = 0.03330159\n",
      "Iteration 38323, loss = 0.03329212\n",
      "Iteration 38324, loss = 0.03327822\n",
      "Iteration 38325, loss = 0.03329141\n",
      "Iteration 38326, loss = 0.03330005\n",
      "Iteration 38327, loss = 0.03329874\n",
      "Iteration 38328, loss = 0.03328839\n",
      "Iteration 38329, loss = 0.03328638\n",
      "Iteration 38330, loss = 0.03328842\n",
      "Iteration 38331, loss = 0.03328566\n",
      "Iteration 38332, loss = 0.03328358\n",
      "Iteration 38333, loss = 0.03328618\n",
      "Iteration 38334, loss = 0.03328403\n",
      "Iteration 38335, loss = 0.03328019\n",
      "Iteration 38336, loss = 0.03327535\n",
      "Iteration 38337, loss = 0.03327154\n",
      "Iteration 38338, loss = 0.03328574\n",
      "Iteration 38339, loss = 0.03329141\n",
      "Iteration 38340, loss = 0.03329220\n",
      "Iteration 38341, loss = 0.03328232\n",
      "Iteration 38342, loss = 0.03327432\n",
      "Iteration 38343, loss = 0.03326989\n",
      "Iteration 38344, loss = 0.03326681\n",
      "Iteration 38345, loss = 0.03327323\n",
      "Iteration 38346, loss = 0.03327704\n",
      "Iteration 38347, loss = 0.03327657\n",
      "Iteration 38348, loss = 0.03326903\n",
      "Iteration 38349, loss = 0.03326381\n",
      "Iteration 38350, loss = 0.03327525\n",
      "Iteration 38351, loss = 0.03327633\n",
      "Iteration 38352, loss = 0.03327347\n",
      "Iteration 38353, loss = 0.03326383\n",
      "Iteration 38354, loss = 0.03326466\n",
      "Iteration 38355, loss = 0.03327416\n",
      "Iteration 38356, loss = 0.03327731\n",
      "Iteration 38357, loss = 0.03326673\n",
      "Iteration 38358, loss = 0.03327341\n",
      "Iteration 38359, loss = 0.03326966\n",
      "Iteration 38360, loss = 0.03326155\n",
      "Iteration 38361, loss = 0.03326176\n",
      "Iteration 38362, loss = 0.03326919\n",
      "Iteration 38363, loss = 0.03327002\n",
      "Iteration 38364, loss = 0.03325412\n",
      "Iteration 38365, loss = 0.03326231\n",
      "Iteration 38366, loss = 0.03326778\n",
      "Iteration 38367, loss = 0.03326890\n",
      "Iteration 38368, loss = 0.03326768\n",
      "Iteration 38369, loss = 0.03326934\n",
      "Iteration 38370, loss = 0.03326808\n",
      "Iteration 38371, loss = 0.03325928\n",
      "Iteration 38372, loss = 0.03325881\n",
      "Iteration 38373, loss = 0.03326984\n",
      "Iteration 38374, loss = 0.03326239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 38375, loss = 0.03325672\n",
      "Iteration 38376, loss = 0.03326941\n",
      "Iteration 38377, loss = 0.03327294\n",
      "Iteration 38378, loss = 0.03326945\n",
      "Iteration 38379, loss = 0.03326155\n",
      "Iteration 38380, loss = 0.03326191\n",
      "Iteration 38381, loss = 0.03325423\n",
      "Iteration 38382, loss = 0.03325402\n",
      "Iteration 38383, loss = 0.03326379\n",
      "Iteration 38384, loss = 0.03325967\n",
      "Iteration 38385, loss = 0.03324848\n",
      "Iteration 38386, loss = 0.03326538\n",
      "Iteration 38387, loss = 0.03326441\n",
      "Iteration 38388, loss = 0.03324817\n",
      "Iteration 38389, loss = 0.03325683\n",
      "Iteration 38390, loss = 0.03325859\n",
      "Iteration 38391, loss = 0.03324790\n",
      "Iteration 38392, loss = 0.03325222\n",
      "Iteration 38393, loss = 0.03326104\n",
      "Iteration 38394, loss = 0.03326033\n",
      "Iteration 38395, loss = 0.03325967\n",
      "Iteration 38396, loss = 0.03325450\n",
      "Iteration 38397, loss = 0.03326327\n",
      "Iteration 38398, loss = 0.03326300\n",
      "Iteration 38399, loss = 0.03326665\n",
      "Iteration 38400, loss = 0.03326273\n",
      "Iteration 38401, loss = 0.03325167\n",
      "Iteration 38402, loss = 0.03324840\n",
      "Iteration 38403, loss = 0.03326701\n",
      "Iteration 38404, loss = 0.03327710\n",
      "Iteration 38405, loss = 0.03327881\n",
      "Iteration 38406, loss = 0.03326798\n",
      "Iteration 38407, loss = 0.03325131\n",
      "Iteration 38408, loss = 0.03325511\n",
      "Iteration 38409, loss = 0.03326626\n",
      "Iteration 38410, loss = 0.03326849\n",
      "Iteration 38411, loss = 0.03326178\n",
      "Iteration 38412, loss = 0.03326054\n",
      "Iteration 38413, loss = 0.03326065\n",
      "Iteration 38414, loss = 0.03325898\n",
      "Iteration 38415, loss = 0.03325614\n",
      "Iteration 38416, loss = 0.03325148\n",
      "Iteration 38417, loss = 0.03324158\n",
      "Iteration 38418, loss = 0.03324313\n",
      "Iteration 38419, loss = 0.03324810\n",
      "Iteration 38420, loss = 0.03324609\n",
      "Iteration 38421, loss = 0.03324839\n",
      "Iteration 38422, loss = 0.03324938\n",
      "Iteration 38423, loss = 0.03325762\n",
      "Iteration 38424, loss = 0.03326571\n",
      "Iteration 38425, loss = 0.03326135\n",
      "Iteration 38426, loss = 0.03324405\n",
      "Iteration 38427, loss = 0.03324206\n",
      "Iteration 38428, loss = 0.03325859\n",
      "Iteration 38429, loss = 0.03326552\n",
      "Iteration 38430, loss = 0.03325451\n",
      "Iteration 38431, loss = 0.03323741\n",
      "Iteration 38432, loss = 0.03324348\n",
      "Iteration 38433, loss = 0.03325817\n",
      "Iteration 38434, loss = 0.03325833\n",
      "Iteration 38435, loss = 0.03324170\n",
      "Iteration 38436, loss = 0.03323500\n",
      "Iteration 38437, loss = 0.03324978\n",
      "Iteration 38438, loss = 0.03325529\n",
      "Iteration 38439, loss = 0.03324501\n",
      "Iteration 38440, loss = 0.03323983\n",
      "Iteration 38441, loss = 0.03324839\n",
      "Iteration 38442, loss = 0.03325332\n",
      "Iteration 38443, loss = 0.03325654\n",
      "Iteration 38444, loss = 0.03324497\n",
      "Iteration 38445, loss = 0.03324406\n",
      "Iteration 38446, loss = 0.03325257\n",
      "Iteration 38447, loss = 0.03326122\n",
      "Iteration 38448, loss = 0.03325341\n",
      "Iteration 38449, loss = 0.03324873\n",
      "Iteration 38450, loss = 0.03324553\n",
      "Iteration 38451, loss = 0.03324236\n",
      "Iteration 38452, loss = 0.03322773\n",
      "Iteration 38453, loss = 0.03323736\n",
      "Iteration 38454, loss = 0.03324314\n",
      "Iteration 38455, loss = 0.03323201\n",
      "Iteration 38456, loss = 0.03322838\n",
      "Iteration 38457, loss = 0.03323058\n",
      "Iteration 38458, loss = 0.03322884\n",
      "Iteration 38459, loss = 0.03322859\n",
      "Iteration 38460, loss = 0.03322152\n",
      "Iteration 38461, loss = 0.03321853\n",
      "Iteration 38462, loss = 0.03323158\n",
      "Iteration 38463, loss = 0.03323405\n",
      "Iteration 38464, loss = 0.03322280\n",
      "Iteration 38465, loss = 0.03322413\n",
      "Iteration 38466, loss = 0.03323159\n",
      "Iteration 38467, loss = 0.03323031\n",
      "Iteration 38468, loss = 0.03322764\n",
      "Iteration 38469, loss = 0.03322894\n",
      "Iteration 38470, loss = 0.03322124\n",
      "Iteration 38471, loss = 0.03321692\n",
      "Iteration 38472, loss = 0.03321687\n",
      "Iteration 38473, loss = 0.03321986\n",
      "Iteration 38474, loss = 0.03322771\n",
      "Iteration 38475, loss = 0.03322578\n",
      "Iteration 38476, loss = 0.03322147\n",
      "Iteration 38477, loss = 0.03322673\n",
      "Iteration 38478, loss = 0.03322297\n",
      "Iteration 38479, loss = 0.03322212\n",
      "Iteration 38480, loss = 0.03321999\n",
      "Iteration 38481, loss = 0.03322060\n",
      "Iteration 38482, loss = 0.03322463\n",
      "Iteration 38483, loss = 0.03323277\n",
      "Iteration 38484, loss = 0.03323294\n",
      "Iteration 38485, loss = 0.03322184\n",
      "Iteration 38486, loss = 0.03321676\n",
      "Iteration 38487, loss = 0.03322778\n",
      "Iteration 38488, loss = 0.03322694\n",
      "Iteration 38489, loss = 0.03321788\n",
      "Iteration 38490, loss = 0.03322170\n",
      "Iteration 38491, loss = 0.03323292\n",
      "Iteration 38492, loss = 0.03322781\n",
      "Iteration 38493, loss = 0.03322488\n",
      "Iteration 38494, loss = 0.03323072\n",
      "Iteration 38495, loss = 0.03323342\n",
      "Iteration 38496, loss = 0.03323157\n",
      "Iteration 38497, loss = 0.03322430\n",
      "Iteration 38498, loss = 0.03322413\n",
      "Iteration 38499, loss = 0.03322223\n",
      "Iteration 38500, loss = 0.03323121\n",
      "Iteration 38501, loss = 0.03323095\n",
      "Iteration 38502, loss = 0.03322324\n",
      "Iteration 38503, loss = 0.03321311\n",
      "Iteration 38504, loss = 0.03322614\n",
      "Iteration 38505, loss = 0.03323153\n",
      "Iteration 38506, loss = 0.03322460\n",
      "Iteration 38507, loss = 0.03322497\n",
      "Iteration 38508, loss = 0.03322487\n",
      "Iteration 38509, loss = 0.03323112\n",
      "Iteration 38510, loss = 0.03322169\n",
      "Iteration 38511, loss = 0.03320645\n",
      "Iteration 38512, loss = 0.03320977\n",
      "Iteration 38513, loss = 0.03321333\n",
      "Iteration 38514, loss = 0.03321394\n",
      "Iteration 38515, loss = 0.03321685\n",
      "Iteration 38516, loss = 0.03321015\n",
      "Iteration 38517, loss = 0.03320619\n",
      "Iteration 38518, loss = 0.03320239\n",
      "Iteration 38519, loss = 0.03321025\n",
      "Iteration 38520, loss = 0.03322402\n",
      "Iteration 38521, loss = 0.03321838\n",
      "Iteration 38522, loss = 0.03320119\n",
      "Iteration 38523, loss = 0.03320852\n",
      "Iteration 38524, loss = 0.03321054\n",
      "Iteration 38525, loss = 0.03321290\n",
      "Iteration 38526, loss = 0.03320441\n",
      "Iteration 38527, loss = 0.03319976\n",
      "Iteration 38528, loss = 0.03320452\n",
      "Iteration 38529, loss = 0.03319990\n",
      "Iteration 38530, loss = 0.03320293\n",
      "Iteration 38531, loss = 0.03321276\n",
      "Iteration 38532, loss = 0.03320794\n",
      "Iteration 38533, loss = 0.03319633\n",
      "Iteration 38534, loss = 0.03319957\n",
      "Iteration 38535, loss = 0.03321011\n",
      "Iteration 38536, loss = 0.03320615\n",
      "Iteration 38537, loss = 0.03320916\n",
      "Iteration 38538, loss = 0.03320772\n",
      "Iteration 38539, loss = 0.03320571\n",
      "Iteration 38540, loss = 0.03319526\n",
      "Iteration 38541, loss = 0.03321184\n",
      "Iteration 38542, loss = 0.03321368\n",
      "Iteration 38543, loss = 0.03320755\n",
      "Iteration 38544, loss = 0.03320134\n",
      "Iteration 38545, loss = 0.03320693\n",
      "Iteration 38546, loss = 0.03320552\n",
      "Iteration 38547, loss = 0.03321115\n",
      "Iteration 38548, loss = 0.03320848\n",
      "Iteration 38549, loss = 0.03319967\n",
      "Iteration 38550, loss = 0.03319899\n",
      "Iteration 38551, loss = 0.03320926\n",
      "Iteration 38552, loss = 0.03321996\n",
      "Iteration 38553, loss = 0.03321915\n",
      "Iteration 38554, loss = 0.03320987\n",
      "Iteration 38555, loss = 0.03319866\n",
      "Iteration 38556, loss = 0.03320291\n",
      "Iteration 38557, loss = 0.03320972\n",
      "Iteration 38558, loss = 0.03320571\n",
      "Iteration 38559, loss = 0.03320198\n",
      "Iteration 38560, loss = 0.03320372\n",
      "Iteration 38561, loss = 0.03319800\n",
      "Iteration 38562, loss = 0.03319141\n",
      "Iteration 38563, loss = 0.03320412\n",
      "Iteration 38564, loss = 0.03320352\n",
      "Iteration 38565, loss = 0.03318457\n",
      "Iteration 38566, loss = 0.03319642\n",
      "Iteration 38567, loss = 0.03320955\n",
      "Iteration 38568, loss = 0.03321288\n",
      "Iteration 38569, loss = 0.03320694\n",
      "Iteration 38570, loss = 0.03319295\n",
      "Iteration 38571, loss = 0.03319929\n",
      "Iteration 38572, loss = 0.03319887\n",
      "Iteration 38573, loss = 0.03319883\n",
      "Iteration 38574, loss = 0.03320180\n",
      "Iteration 38575, loss = 0.03319997\n",
      "Iteration 38576, loss = 0.03319565\n",
      "Iteration 38577, loss = 0.03319903\n",
      "Iteration 38578, loss = 0.03319581\n",
      "Iteration 38579, loss = 0.03319408\n",
      "Iteration 38580, loss = 0.03320331\n",
      "Iteration 38581, loss = 0.03320013\n",
      "Iteration 38582, loss = 0.03319508\n",
      "Iteration 38583, loss = 0.03318310\n",
      "Iteration 38584, loss = 0.03319972\n",
      "Iteration 38585, loss = 0.03320114\n",
      "Iteration 38586, loss = 0.03319403\n",
      "Iteration 38587, loss = 0.03319151\n",
      "Iteration 38588, loss = 0.03320152\n",
      "Iteration 38589, loss = 0.03320578\n",
      "Iteration 38590, loss = 0.03320190\n",
      "Iteration 38591, loss = 0.03318750\n",
      "Iteration 38592, loss = 0.03318079\n",
      "Iteration 38593, loss = 0.03317854\n",
      "Iteration 38594, loss = 0.03318163\n",
      "Iteration 38595, loss = 0.03318971\n",
      "Iteration 38596, loss = 0.03318222\n",
      "Iteration 38597, loss = 0.03317464\n",
      "Iteration 38598, loss = 0.03318468\n",
      "Iteration 38599, loss = 0.03318730\n",
      "Iteration 38600, loss = 0.03318979\n",
      "Iteration 38601, loss = 0.03318127\n",
      "Iteration 38602, loss = 0.03317384\n",
      "Iteration 38603, loss = 0.03319158\n",
      "Iteration 38604, loss = 0.03319615\n",
      "Iteration 38605, loss = 0.03318297\n",
      "Iteration 38606, loss = 0.03318501\n",
      "Iteration 38607, loss = 0.03318796\n",
      "Iteration 38608, loss = 0.03318439\n",
      "Iteration 38609, loss = 0.03319338\n",
      "Iteration 38610, loss = 0.03319516\n",
      "Iteration 38611, loss = 0.03318671\n",
      "Iteration 38612, loss = 0.03318758\n",
      "Iteration 38613, loss = 0.03319834\n",
      "Iteration 38614, loss = 0.03319467\n",
      "Iteration 38615, loss = 0.03318469\n",
      "Iteration 38616, loss = 0.03317212\n",
      "Iteration 38617, loss = 0.03318195\n",
      "Iteration 38618, loss = 0.03318295\n",
      "Iteration 38619, loss = 0.03318166\n",
      "Iteration 38620, loss = 0.03317125\n",
      "Iteration 38621, loss = 0.03317030\n",
      "Iteration 38622, loss = 0.03317689\n",
      "Iteration 38623, loss = 0.03317956\n",
      "Iteration 38624, loss = 0.03316993\n",
      "Iteration 38625, loss = 0.03317483\n",
      "Iteration 38626, loss = 0.03317824\n",
      "Iteration 38627, loss = 0.03317895\n",
      "Iteration 38628, loss = 0.03317493\n",
      "Iteration 38629, loss = 0.03316641\n",
      "Iteration 38630, loss = 0.03316649\n",
      "Iteration 38631, loss = 0.03317134\n",
      "Iteration 38632, loss = 0.03316267\n",
      "Iteration 38633, loss = 0.03316551\n",
      "Iteration 38634, loss = 0.03316801\n",
      "Iteration 38635, loss = 0.03317170\n",
      "Iteration 38636, loss = 0.03317285\n",
      "Iteration 38637, loss = 0.03316810\n",
      "Iteration 38638, loss = 0.03316116\n",
      "Iteration 38639, loss = 0.03316743\n",
      "Iteration 38640, loss = 0.03316819\n",
      "Iteration 38641, loss = 0.03316404\n",
      "Iteration 38642, loss = 0.03316053\n",
      "Iteration 38643, loss = 0.03317083\n",
      "Iteration 38644, loss = 0.03317162\n",
      "Iteration 38645, loss = 0.03316773\n",
      "Iteration 38646, loss = 0.03318260\n",
      "Iteration 38647, loss = 0.03318205\n",
      "Iteration 38648, loss = 0.03317141\n",
      "Iteration 38649, loss = 0.03315606\n",
      "Iteration 38650, loss = 0.03317481\n",
      "Iteration 38651, loss = 0.03318448\n",
      "Iteration 38652, loss = 0.03317896\n",
      "Iteration 38653, loss = 0.03316425\n",
      "Iteration 38654, loss = 0.03316250\n",
      "Iteration 38655, loss = 0.03316003\n",
      "Iteration 38656, loss = 0.03316292\n",
      "Iteration 38657, loss = 0.03316190\n",
      "Iteration 38658, loss = 0.03316317\n",
      "Iteration 38659, loss = 0.03315428\n",
      "Iteration 38660, loss = 0.03316310\n",
      "Iteration 38661, loss = 0.03316834\n",
      "Iteration 38662, loss = 0.03316760\n",
      "Iteration 38663, loss = 0.03316081\n",
      "Iteration 38664, loss = 0.03315617\n",
      "Iteration 38665, loss = 0.03314610\n",
      "Iteration 38666, loss = 0.03315880\n",
      "Iteration 38667, loss = 0.03316453\n",
      "Iteration 38668, loss = 0.03315578\n",
      "Iteration 38669, loss = 0.03315066\n",
      "Iteration 38670, loss = 0.03315317\n",
      "Iteration 38671, loss = 0.03315504\n",
      "Iteration 38672, loss = 0.03314959\n",
      "Iteration 38673, loss = 0.03314922\n",
      "Iteration 38674, loss = 0.03315320\n",
      "Iteration 38675, loss = 0.03314867\n",
      "Iteration 38676, loss = 0.03314675\n",
      "Iteration 38677, loss = 0.03314948\n",
      "Iteration 38678, loss = 0.03314685\n",
      "Iteration 38679, loss = 0.03314503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 38680, loss = 0.03315589\n",
      "Iteration 38681, loss = 0.03315834\n",
      "Iteration 38682, loss = 0.03315163\n",
      "Iteration 38683, loss = 0.03314336\n",
      "Iteration 38684, loss = 0.03314655\n",
      "Iteration 38685, loss = 0.03314686\n",
      "Iteration 38686, loss = 0.03314013\n",
      "Iteration 38687, loss = 0.03314097\n",
      "Iteration 38688, loss = 0.03313984\n",
      "Iteration 38689, loss = 0.03314056\n",
      "Iteration 38690, loss = 0.03313810\n",
      "Iteration 38691, loss = 0.03314144\n",
      "Iteration 38692, loss = 0.03314539\n",
      "Iteration 38693, loss = 0.03313903\n",
      "Iteration 38694, loss = 0.03314250\n",
      "Iteration 38695, loss = 0.03314980\n",
      "Iteration 38696, loss = 0.03314563\n",
      "Iteration 38697, loss = 0.03314263\n",
      "Iteration 38698, loss = 0.03313659\n",
      "Iteration 38699, loss = 0.03313839\n",
      "Iteration 38700, loss = 0.03314148\n",
      "Iteration 38701, loss = 0.03314357\n",
      "Iteration 38702, loss = 0.03314510\n",
      "Iteration 38703, loss = 0.03314737\n",
      "Iteration 38704, loss = 0.03314528\n",
      "Iteration 38705, loss = 0.03315005\n",
      "Iteration 38706, loss = 0.03315156\n",
      "Iteration 38707, loss = 0.03314257\n",
      "Iteration 38708, loss = 0.03313854\n",
      "Iteration 38709, loss = 0.03314383\n",
      "Iteration 38710, loss = 0.03314597\n",
      "Iteration 38711, loss = 0.03313446\n",
      "Iteration 38712, loss = 0.03314094\n",
      "Iteration 38713, loss = 0.03314264\n",
      "Iteration 38714, loss = 0.03313692\n",
      "Iteration 38715, loss = 0.03313328\n",
      "Iteration 38716, loss = 0.03313848\n",
      "Iteration 38717, loss = 0.03313464\n",
      "Iteration 38718, loss = 0.03313387\n",
      "Iteration 38719, loss = 0.03313661\n",
      "Iteration 38720, loss = 0.03313344\n",
      "Iteration 38721, loss = 0.03313931\n",
      "Iteration 38722, loss = 0.03313655\n",
      "Iteration 38723, loss = 0.03312783\n",
      "Iteration 38724, loss = 0.03313302\n",
      "Iteration 38725, loss = 0.03313763\n",
      "Iteration 38726, loss = 0.03313335\n",
      "Iteration 38727, loss = 0.03313448\n",
      "Iteration 38728, loss = 0.03313549\n",
      "Iteration 38729, loss = 0.03313239\n",
      "Iteration 38730, loss = 0.03313106\n",
      "Iteration 38731, loss = 0.03313010\n",
      "Iteration 38732, loss = 0.03313126\n",
      "Iteration 38733, loss = 0.03312777\n",
      "Iteration 38734, loss = 0.03312046\n",
      "Iteration 38735, loss = 0.03312848\n",
      "Iteration 38736, loss = 0.03313363\n",
      "Iteration 38737, loss = 0.03313493\n",
      "Iteration 38738, loss = 0.03312618\n",
      "Iteration 38739, loss = 0.03312208\n",
      "Iteration 38740, loss = 0.03312057\n",
      "Iteration 38741, loss = 0.03312007\n",
      "Iteration 38742, loss = 0.03312166\n",
      "Iteration 38743, loss = 0.03312611\n",
      "Iteration 38744, loss = 0.03312506\n",
      "Iteration 38745, loss = 0.03311907\n",
      "Iteration 38746, loss = 0.03312280\n",
      "Iteration 38747, loss = 0.03312124\n",
      "Iteration 38748, loss = 0.03312191\n",
      "Iteration 38749, loss = 0.03312439\n",
      "Iteration 38750, loss = 0.03312207\n",
      "Iteration 38751, loss = 0.03311734\n",
      "Iteration 38752, loss = 0.03312818\n",
      "Iteration 38753, loss = 0.03313040\n",
      "Iteration 38754, loss = 0.03312769\n",
      "Iteration 38755, loss = 0.03312346\n",
      "Iteration 38756, loss = 0.03312502\n",
      "Iteration 38757, loss = 0.03312948\n",
      "Iteration 38758, loss = 0.03312731\n",
      "Iteration 38759, loss = 0.03312362\n",
      "Iteration 38760, loss = 0.03311867\n",
      "Iteration 38761, loss = 0.03311815\n",
      "Iteration 38762, loss = 0.03311157\n",
      "Iteration 38763, loss = 0.03312120\n",
      "Iteration 38764, loss = 0.03312918\n",
      "Iteration 38765, loss = 0.03312256\n",
      "Iteration 38766, loss = 0.03311373\n",
      "Iteration 38767, loss = 0.03311757\n",
      "Iteration 38768, loss = 0.03312029\n",
      "Iteration 38769, loss = 0.03312373\n",
      "Iteration 38770, loss = 0.03311801\n",
      "Iteration 38771, loss = 0.03311544\n",
      "Iteration 38772, loss = 0.03311551\n",
      "Iteration 38773, loss = 0.03311037\n",
      "Iteration 38774, loss = 0.03311682\n",
      "Iteration 38775, loss = 0.03311347\n",
      "Iteration 38776, loss = 0.03311397\n",
      "Iteration 38777, loss = 0.03311936\n",
      "Iteration 38778, loss = 0.03311830\n",
      "Iteration 38779, loss = 0.03311300\n",
      "Iteration 38780, loss = 0.03310578\n",
      "Iteration 38781, loss = 0.03311987\n",
      "Iteration 38782, loss = 0.03312352\n",
      "Iteration 38783, loss = 0.03311383\n",
      "Iteration 38784, loss = 0.03310745\n",
      "Iteration 38785, loss = 0.03311865\n",
      "Iteration 38786, loss = 0.03312360\n",
      "Iteration 38787, loss = 0.03311960\n",
      "Iteration 38788, loss = 0.03311531\n",
      "Iteration 38789, loss = 0.03311692\n",
      "Iteration 38790, loss = 0.03310773\n",
      "Iteration 38791, loss = 0.03310820\n",
      "Iteration 38792, loss = 0.03311667\n",
      "Iteration 38793, loss = 0.03311860\n",
      "Iteration 38794, loss = 0.03311378\n",
      "Iteration 38795, loss = 0.03309981\n",
      "Iteration 38796, loss = 0.03310997\n",
      "Iteration 38797, loss = 0.03311348\n",
      "Iteration 38798, loss = 0.03311369\n",
      "Iteration 38799, loss = 0.03311713\n",
      "Iteration 38800, loss = 0.03312085\n",
      "Iteration 38801, loss = 0.03311463\n",
      "Iteration 38802, loss = 0.03310334\n",
      "Iteration 38803, loss = 0.03310537\n",
      "Iteration 38804, loss = 0.03311458\n",
      "Iteration 38805, loss = 0.03311293\n",
      "Iteration 38806, loss = 0.03310917\n",
      "Iteration 38807, loss = 0.03311343\n",
      "Iteration 38808, loss = 0.03312612\n",
      "Iteration 38809, loss = 0.03313258\n",
      "Iteration 38810, loss = 0.03312885\n",
      "Iteration 38811, loss = 0.03311448\n",
      "Iteration 38812, loss = 0.03309919\n",
      "Iteration 38813, loss = 0.03310816\n",
      "Iteration 38814, loss = 0.03311433\n",
      "Iteration 38815, loss = 0.03311049\n",
      "Iteration 38816, loss = 0.03310419\n",
      "Iteration 38817, loss = 0.03311211\n",
      "Iteration 38818, loss = 0.03311266\n",
      "Iteration 38819, loss = 0.03311123\n",
      "Iteration 38820, loss = 0.03311718\n",
      "Iteration 38821, loss = 0.03310836\n",
      "Iteration 38822, loss = 0.03310309\n",
      "Iteration 38823, loss = 0.03310771\n",
      "Iteration 38824, loss = 0.03310447\n",
      "Iteration 38825, loss = 0.03310260\n",
      "Iteration 38826, loss = 0.03311396\n",
      "Iteration 38827, loss = 0.03311868\n",
      "Iteration 38828, loss = 0.03311050\n",
      "Iteration 38829, loss = 0.03310253\n",
      "Iteration 38830, loss = 0.03310040\n",
      "Iteration 38831, loss = 0.03309077\n",
      "Iteration 38832, loss = 0.03310358\n",
      "Iteration 38833, loss = 0.03310995\n",
      "Iteration 38834, loss = 0.03309952\n",
      "Iteration 38835, loss = 0.03309305\n",
      "Iteration 38836, loss = 0.03308997\n",
      "Iteration 38837, loss = 0.03309067\n",
      "Iteration 38838, loss = 0.03309272\n",
      "Iteration 38839, loss = 0.03309238\n",
      "Iteration 38840, loss = 0.03309431\n",
      "Iteration 38841, loss = 0.03309273\n",
      "Iteration 38842, loss = 0.03309878\n",
      "Iteration 38843, loss = 0.03310230\n",
      "Iteration 38844, loss = 0.03310086\n",
      "Iteration 38845, loss = 0.03309514\n",
      "Iteration 38846, loss = 0.03308800\n",
      "Iteration 38847, loss = 0.03309004\n",
      "Iteration 38848, loss = 0.03310102\n",
      "Iteration 38849, loss = 0.03310556\n",
      "Iteration 38850, loss = 0.03309613\n",
      "Iteration 38851, loss = 0.03308879\n",
      "Iteration 38852, loss = 0.03309174\n",
      "Iteration 38853, loss = 0.03309986\n",
      "Iteration 38854, loss = 0.03309443\n",
      "Iteration 38855, loss = 0.03308382\n",
      "Iteration 38856, loss = 0.03309667\n",
      "Iteration 38857, loss = 0.03310746\n",
      "Iteration 38858, loss = 0.03310861\n",
      "Iteration 38859, loss = 0.03309874\n",
      "Iteration 38860, loss = 0.03309368\n",
      "Iteration 38861, loss = 0.03310423\n",
      "Iteration 38862, loss = 0.03310238\n",
      "Iteration 38863, loss = 0.03309757\n",
      "Iteration 38864, loss = 0.03308956\n",
      "Iteration 38865, loss = 0.03308571\n",
      "Iteration 38866, loss = 0.03308837\n",
      "Iteration 38867, loss = 0.03308811\n",
      "Iteration 38868, loss = 0.03307931\n",
      "Iteration 38869, loss = 0.03308580\n",
      "Iteration 38870, loss = 0.03308304\n",
      "Iteration 38871, loss = 0.03308108\n",
      "Iteration 38872, loss = 0.03308196\n",
      "Iteration 38873, loss = 0.03308399\n",
      "Iteration 38874, loss = 0.03308307\n",
      "Iteration 38875, loss = 0.03309059\n",
      "Iteration 38876, loss = 0.03309338\n",
      "Iteration 38877, loss = 0.03309126\n",
      "Iteration 38878, loss = 0.03308295\n",
      "Iteration 38879, loss = 0.03308762\n",
      "Iteration 38880, loss = 0.03308767\n",
      "Iteration 38881, loss = 0.03307998\n",
      "Iteration 38882, loss = 0.03307470\n",
      "Iteration 38883, loss = 0.03307244\n",
      "Iteration 38884, loss = 0.03308270\n",
      "Iteration 38885, loss = 0.03308115\n",
      "Iteration 38886, loss = 0.03307295\n",
      "Iteration 38887, loss = 0.03308074\n",
      "Iteration 38888, loss = 0.03308639\n",
      "Iteration 38889, loss = 0.03307819\n",
      "Iteration 38890, loss = 0.03307927\n",
      "Iteration 38891, loss = 0.03307344\n",
      "Iteration 38892, loss = 0.03307417\n",
      "Iteration 38893, loss = 0.03308319\n",
      "Iteration 38894, loss = 0.03307804\n",
      "Iteration 38895, loss = 0.03307761\n",
      "Iteration 38896, loss = 0.03307868\n",
      "Iteration 38897, loss = 0.03307471\n",
      "Iteration 38898, loss = 0.03307225\n",
      "Iteration 38899, loss = 0.03307019\n",
      "Iteration 38900, loss = 0.03307002\n",
      "Iteration 38901, loss = 0.03307470\n",
      "Iteration 38902, loss = 0.03306889\n",
      "Iteration 38903, loss = 0.03307707\n",
      "Iteration 38904, loss = 0.03307457\n",
      "Iteration 38905, loss = 0.03305909\n",
      "Iteration 38906, loss = 0.03307276\n",
      "Iteration 38907, loss = 0.03307862\n",
      "Iteration 38908, loss = 0.03306419\n",
      "Iteration 38909, loss = 0.03306938\n",
      "Iteration 38910, loss = 0.03306927\n",
      "Iteration 38911, loss = 0.03306823\n",
      "Iteration 38912, loss = 0.03306335\n",
      "Iteration 38913, loss = 0.03307492\n",
      "Iteration 38914, loss = 0.03308147\n",
      "Iteration 38915, loss = 0.03306801\n",
      "Iteration 38916, loss = 0.03306420\n",
      "Iteration 38917, loss = 0.03307726\n",
      "Iteration 38918, loss = 0.03308644\n",
      "Iteration 38919, loss = 0.03308563\n",
      "Iteration 38920, loss = 0.03307827\n",
      "Iteration 38921, loss = 0.03307186\n",
      "Iteration 38922, loss = 0.03305685\n",
      "Iteration 38923, loss = 0.03306709\n",
      "Iteration 38924, loss = 0.03307257\n",
      "Iteration 38925, loss = 0.03307223\n",
      "Iteration 38926, loss = 0.03306247\n",
      "Iteration 38927, loss = 0.03305597\n",
      "Iteration 38928, loss = 0.03306894\n",
      "Iteration 38929, loss = 0.03307210\n",
      "Iteration 38930, loss = 0.03306781\n",
      "Iteration 38931, loss = 0.03306501\n",
      "Iteration 38932, loss = 0.03305852\n",
      "Iteration 38933, loss = 0.03307013\n",
      "Iteration 38934, loss = 0.03307465\n",
      "Iteration 38935, loss = 0.03306323\n",
      "Iteration 38936, loss = 0.03306132\n",
      "Iteration 38937, loss = 0.03306205\n",
      "Iteration 38938, loss = 0.03307427\n",
      "Iteration 38939, loss = 0.03307140\n",
      "Iteration 38940, loss = 0.03306640\n",
      "Iteration 38941, loss = 0.03305487\n",
      "Iteration 38942, loss = 0.03305710\n",
      "Iteration 38943, loss = 0.03306997\n",
      "Iteration 38944, loss = 0.03307050\n",
      "Iteration 38945, loss = 0.03306131\n",
      "Iteration 38946, loss = 0.03305942\n",
      "Iteration 38947, loss = 0.03306319\n",
      "Iteration 38948, loss = 0.03306482\n",
      "Iteration 38949, loss = 0.03307183\n",
      "Iteration 38950, loss = 0.03307138\n",
      "Iteration 38951, loss = 0.03306500\n",
      "Iteration 38952, loss = 0.03304352\n",
      "Iteration 38953, loss = 0.03306164\n",
      "Iteration 38954, loss = 0.03306848\n",
      "Iteration 38955, loss = 0.03306060\n",
      "Iteration 38956, loss = 0.03305042\n",
      "Iteration 38957, loss = 0.03305981\n",
      "Iteration 38958, loss = 0.03306866\n",
      "Iteration 38959, loss = 0.03306746\n",
      "Iteration 38960, loss = 0.03305914\n",
      "Iteration 38961, loss = 0.03306243\n",
      "Iteration 38962, loss = 0.03305636\n",
      "Iteration 38963, loss = 0.03306332\n",
      "Iteration 38964, loss = 0.03306753\n",
      "Iteration 38965, loss = 0.03306326\n",
      "Iteration 38966, loss = 0.03305534\n",
      "Iteration 38967, loss = 0.03304784\n",
      "Iteration 38968, loss = 0.03304209\n",
      "Iteration 38969, loss = 0.03305217\n",
      "Iteration 38970, loss = 0.03305039\n",
      "Iteration 38971, loss = 0.03304434\n",
      "Iteration 38972, loss = 0.03303925\n",
      "Iteration 38973, loss = 0.03304323\n",
      "Iteration 38974, loss = 0.03305407\n",
      "Iteration 38975, loss = 0.03304835\n",
      "Iteration 38976, loss = 0.03303481\n",
      "Iteration 38977, loss = 0.03304152\n",
      "Iteration 38978, loss = 0.03304192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 38979, loss = 0.03304370\n",
      "Iteration 38980, loss = 0.03304549\n",
      "Iteration 38981, loss = 0.03304153\n",
      "Iteration 38982, loss = 0.03303576\n",
      "Iteration 38983, loss = 0.03303187\n",
      "Iteration 38984, loss = 0.03303645\n",
      "Iteration 38985, loss = 0.03303990\n",
      "Iteration 38986, loss = 0.03303184\n",
      "Iteration 38987, loss = 0.03303445\n",
      "Iteration 38988, loss = 0.03304420\n",
      "Iteration 38989, loss = 0.03304752\n",
      "Iteration 38990, loss = 0.03304707\n",
      "Iteration 38991, loss = 0.03303823\n",
      "Iteration 38992, loss = 0.03303773\n",
      "Iteration 38993, loss = 0.03303333\n",
      "Iteration 38994, loss = 0.03302982\n",
      "Iteration 38995, loss = 0.03304237\n",
      "Iteration 38996, loss = 0.03304315\n",
      "Iteration 38997, loss = 0.03303386\n",
      "Iteration 38998, loss = 0.03302950\n",
      "Iteration 38999, loss = 0.03303957\n",
      "Iteration 39000, loss = 0.03303692\n",
      "Iteration 39001, loss = 0.03302918\n",
      "Iteration 39002, loss = 0.03302765\n",
      "Iteration 39003, loss = 0.03303490\n",
      "Iteration 39004, loss = 0.03303930\n",
      "Iteration 39005, loss = 0.03303152\n",
      "Iteration 39006, loss = 0.03303229\n",
      "Iteration 39007, loss = 0.03304012\n",
      "Iteration 39008, loss = 0.03303363\n",
      "Iteration 39009, loss = 0.03304029\n",
      "Iteration 39010, loss = 0.03304092\n",
      "Iteration 39011, loss = 0.03303641\n",
      "Iteration 39012, loss = 0.03302737\n",
      "Iteration 39013, loss = 0.03302497\n",
      "Iteration 39014, loss = 0.03302427\n",
      "Iteration 39015, loss = 0.03302770\n",
      "Iteration 39016, loss = 0.03302458\n",
      "Iteration 39017, loss = 0.03301961\n",
      "Iteration 39018, loss = 0.03303341\n",
      "Iteration 39019, loss = 0.03303335\n",
      "Iteration 39020, loss = 0.03303390\n",
      "Iteration 39021, loss = 0.03302836\n",
      "Iteration 39022, loss = 0.03303256\n",
      "Iteration 39023, loss = 0.03303100\n",
      "Iteration 39024, loss = 0.03302065\n",
      "Iteration 39025, loss = 0.03303130\n",
      "Iteration 39026, loss = 0.03303613\n",
      "Iteration 39027, loss = 0.03303598\n",
      "Iteration 39028, loss = 0.03302779\n",
      "Iteration 39029, loss = 0.03302538\n",
      "Iteration 39030, loss = 0.03303764\n",
      "Iteration 39031, loss = 0.03303139\n",
      "Iteration 39032, loss = 0.03302508\n",
      "Iteration 39033, loss = 0.03302942\n",
      "Iteration 39034, loss = 0.03302262\n",
      "Iteration 39035, loss = 0.03301171\n",
      "Iteration 39036, loss = 0.03302130\n",
      "Iteration 39037, loss = 0.03303378\n",
      "Iteration 39038, loss = 0.03303571\n",
      "Iteration 39039, loss = 0.03302248\n",
      "Iteration 39040, loss = 0.03302193\n",
      "Iteration 39041, loss = 0.03302714\n",
      "Iteration 39042, loss = 0.03302617\n",
      "Iteration 39043, loss = 0.03302485\n",
      "Iteration 39044, loss = 0.03302790\n",
      "Iteration 39045, loss = 0.03302757\n",
      "Iteration 39046, loss = 0.03302011\n",
      "Iteration 39047, loss = 0.03301505\n",
      "Iteration 39048, loss = 0.03302315\n",
      "Iteration 39049, loss = 0.03301969\n",
      "Iteration 39050, loss = 0.03301576\n",
      "Iteration 39051, loss = 0.03301948\n",
      "Iteration 39052, loss = 0.03301505\n",
      "Iteration 39053, loss = 0.03302461\n",
      "Iteration 39054, loss = 0.03302508\n",
      "Iteration 39055, loss = 0.03301182\n",
      "Iteration 39056, loss = 0.03301170\n",
      "Iteration 39057, loss = 0.03302718\n",
      "Iteration 39058, loss = 0.03302514\n",
      "Iteration 39059, loss = 0.03302188\n",
      "Iteration 39060, loss = 0.03301433\n",
      "Iteration 39061, loss = 0.03303095\n",
      "Iteration 39062, loss = 0.03303896\n",
      "Iteration 39063, loss = 0.03302986\n",
      "Iteration 39064, loss = 0.03302566\n",
      "Iteration 39065, loss = 0.03301731\n",
      "Iteration 39066, loss = 0.03302154\n",
      "Iteration 39067, loss = 0.03303036\n",
      "Iteration 39068, loss = 0.03301992\n",
      "Iteration 39069, loss = 0.03300315\n",
      "Iteration 39070, loss = 0.03301228\n",
      "Iteration 39071, loss = 0.03301549\n",
      "Iteration 39072, loss = 0.03301113\n",
      "Iteration 39073, loss = 0.03299923\n",
      "Iteration 39074, loss = 0.03300266\n",
      "Iteration 39075, loss = 0.03300781\n",
      "Iteration 39076, loss = 0.03300232\n",
      "Iteration 39077, loss = 0.03301223\n",
      "Iteration 39078, loss = 0.03301238\n",
      "Iteration 39079, loss = 0.03300310\n",
      "Iteration 39080, loss = 0.03301007\n",
      "Iteration 39081, loss = 0.03301513\n",
      "Iteration 39082, loss = 0.03301350\n",
      "Iteration 39083, loss = 0.03301301\n",
      "Iteration 39084, loss = 0.03301322\n",
      "Iteration 39085, loss = 0.03300372\n",
      "Iteration 39086, loss = 0.03300189\n",
      "Iteration 39087, loss = 0.03301925\n",
      "Iteration 39088, loss = 0.03302166\n",
      "Iteration 39089, loss = 0.03300451\n",
      "Iteration 39090, loss = 0.03300369\n",
      "Iteration 39091, loss = 0.03301789\n",
      "Iteration 39092, loss = 0.03302783\n",
      "Iteration 39093, loss = 0.03302027\n",
      "Iteration 39094, loss = 0.03301391\n",
      "Iteration 39095, loss = 0.03301118\n",
      "Iteration 39096, loss = 0.03299991\n",
      "Iteration 39097, loss = 0.03301176\n",
      "Iteration 39098, loss = 0.03301734\n",
      "Iteration 39099, loss = 0.03300888\n",
      "Iteration 39100, loss = 0.03299810\n",
      "Iteration 39101, loss = 0.03299866\n",
      "Iteration 39102, loss = 0.03301039\n",
      "Iteration 39103, loss = 0.03301242\n",
      "Iteration 39104, loss = 0.03300520\n",
      "Iteration 39105, loss = 0.03300342\n",
      "Iteration 39106, loss = 0.03301181\n",
      "Iteration 39107, loss = 0.03301226\n",
      "Iteration 39108, loss = 0.03301151\n",
      "Iteration 39109, loss = 0.03299877\n",
      "Iteration 39110, loss = 0.03300850\n",
      "Iteration 39111, loss = 0.03300596\n",
      "Iteration 39112, loss = 0.03300211\n",
      "Iteration 39113, loss = 0.03300704\n",
      "Iteration 39114, loss = 0.03300060\n",
      "Iteration 39115, loss = 0.03299371\n",
      "Iteration 39116, loss = 0.03300191\n",
      "Iteration 39117, loss = 0.03299265\n",
      "Iteration 39118, loss = 0.03299578\n",
      "Iteration 39119, loss = 0.03299118\n",
      "Iteration 39120, loss = 0.03300174\n",
      "Iteration 39121, loss = 0.03298982\n",
      "Iteration 39122, loss = 0.03298807\n",
      "Iteration 39123, loss = 0.03298810\n",
      "Iteration 39124, loss = 0.03299301\n",
      "Iteration 39125, loss = 0.03300110\n",
      "Iteration 39126, loss = 0.03299596\n",
      "Iteration 39127, loss = 0.03298182\n",
      "Iteration 39128, loss = 0.03298394\n",
      "Iteration 39129, loss = 0.03299133\n",
      "Iteration 39130, loss = 0.03299255\n",
      "Iteration 39131, loss = 0.03298987\n",
      "Iteration 39132, loss = 0.03298422\n",
      "Iteration 39133, loss = 0.03297942\n",
      "Iteration 39134, loss = 0.03298468\n",
      "Iteration 39135, loss = 0.03298286\n",
      "Iteration 39136, loss = 0.03297898\n",
      "Iteration 39137, loss = 0.03298647\n",
      "Iteration 39138, loss = 0.03298633\n",
      "Iteration 39139, loss = 0.03298802\n",
      "Iteration 39140, loss = 0.03298615\n",
      "Iteration 39141, loss = 0.03297955\n",
      "Iteration 39142, loss = 0.03298430\n",
      "Iteration 39143, loss = 0.03297993\n",
      "Iteration 39144, loss = 0.03298003\n",
      "Iteration 39145, loss = 0.03298411\n",
      "Iteration 39146, loss = 0.03297887\n",
      "Iteration 39147, loss = 0.03298299\n",
      "Iteration 39148, loss = 0.03299645\n",
      "Iteration 39149, loss = 0.03299689\n",
      "Iteration 39150, loss = 0.03299485\n",
      "Iteration 39151, loss = 0.03298726\n",
      "Iteration 39152, loss = 0.03297917\n",
      "Iteration 39153, loss = 0.03297776\n",
      "Iteration 39154, loss = 0.03298122\n",
      "Iteration 39155, loss = 0.03298881\n",
      "Iteration 39156, loss = 0.03298241\n",
      "Iteration 39157, loss = 0.03297369\n",
      "Iteration 39158, loss = 0.03297612\n",
      "Iteration 39159, loss = 0.03297981\n",
      "Iteration 39160, loss = 0.03297050\n",
      "Iteration 39161, loss = 0.03297391\n",
      "Iteration 39162, loss = 0.03297516\n",
      "Iteration 39163, loss = 0.03297717\n",
      "Iteration 39164, loss = 0.03297772\n",
      "Iteration 39165, loss = 0.03297124\n",
      "Iteration 39166, loss = 0.03296870\n",
      "Iteration 39167, loss = 0.03298060\n",
      "Iteration 39168, loss = 0.03297850\n",
      "Iteration 39169, loss = 0.03298014\n",
      "Iteration 39170, loss = 0.03297368\n",
      "Iteration 39171, loss = 0.03298801\n",
      "Iteration 39172, loss = 0.03298675\n",
      "Iteration 39173, loss = 0.03297906\n",
      "Iteration 39174, loss = 0.03299335\n",
      "Iteration 39175, loss = 0.03299646\n",
      "Iteration 39176, loss = 0.03300023\n",
      "Iteration 39177, loss = 0.03299107\n",
      "Iteration 39178, loss = 0.03298691\n",
      "Iteration 39179, loss = 0.03299310\n",
      "Iteration 39180, loss = 0.03298552\n",
      "Iteration 39181, loss = 0.03297805\n",
      "Iteration 39182, loss = 0.03297513\n",
      "Iteration 39183, loss = 0.03297767\n",
      "Iteration 39184, loss = 0.03296944\n",
      "Iteration 39185, loss = 0.03297126\n",
      "Iteration 39186, loss = 0.03297267\n",
      "Iteration 39187, loss = 0.03296657\n",
      "Iteration 39188, loss = 0.03297470\n",
      "Iteration 39189, loss = 0.03296986\n",
      "Iteration 39190, loss = 0.03297472\n",
      "Iteration 39191, loss = 0.03297197\n",
      "Iteration 39192, loss = 0.03296862\n",
      "Iteration 39193, loss = 0.03296267\n",
      "Iteration 39194, loss = 0.03296677\n",
      "Iteration 39195, loss = 0.03296225\n",
      "Iteration 39196, loss = 0.03296633\n",
      "Iteration 39197, loss = 0.03296694\n",
      "Iteration 39198, loss = 0.03295783\n",
      "Iteration 39199, loss = 0.03295504\n",
      "Iteration 39200, loss = 0.03297190\n",
      "Iteration 39201, loss = 0.03296590\n",
      "Iteration 39202, loss = 0.03295655\n",
      "Iteration 39203, loss = 0.03295591\n",
      "Iteration 39204, loss = 0.03296545\n",
      "Iteration 39205, loss = 0.03296695\n",
      "Iteration 39206, loss = 0.03296573\n",
      "Iteration 39207, loss = 0.03296732\n",
      "Iteration 39208, loss = 0.03296592\n",
      "Iteration 39209, loss = 0.03296080\n",
      "Iteration 39210, loss = 0.03296120\n",
      "Iteration 39211, loss = 0.03295708\n",
      "Iteration 39212, loss = 0.03294915\n",
      "Iteration 39213, loss = 0.03295635\n",
      "Iteration 39214, loss = 0.03296004\n",
      "Iteration 39215, loss = 0.03295462\n",
      "Iteration 39216, loss = 0.03295284\n",
      "Iteration 39217, loss = 0.03296193\n",
      "Iteration 39218, loss = 0.03295779\n",
      "Iteration 39219, loss = 0.03295692\n",
      "Iteration 39220, loss = 0.03294839\n",
      "Iteration 39221, loss = 0.03296204\n",
      "Iteration 39222, loss = 0.03296324\n",
      "Iteration 39223, loss = 0.03296004\n",
      "Iteration 39224, loss = 0.03296127\n",
      "Iteration 39225, loss = 0.03295537\n",
      "Iteration 39226, loss = 0.03294623\n",
      "Iteration 39227, loss = 0.03295661\n",
      "Iteration 39228, loss = 0.03296164\n",
      "Iteration 39229, loss = 0.03296469\n",
      "Iteration 39230, loss = 0.03295576\n",
      "Iteration 39231, loss = 0.03294613\n",
      "Iteration 39232, loss = 0.03295503\n",
      "Iteration 39233, loss = 0.03296388\n",
      "Iteration 39234, loss = 0.03296765\n",
      "Iteration 39235, loss = 0.03296319\n",
      "Iteration 39236, loss = 0.03295532\n",
      "Iteration 39237, loss = 0.03295136\n",
      "Iteration 39238, loss = 0.03294951\n",
      "Iteration 39239, loss = 0.03295764\n",
      "Iteration 39240, loss = 0.03295027\n",
      "Iteration 39241, loss = 0.03294656\n",
      "Iteration 39242, loss = 0.03296074\n",
      "Iteration 39243, loss = 0.03296309\n",
      "Iteration 39244, loss = 0.03296525\n",
      "Iteration 39245, loss = 0.03296384\n",
      "Iteration 39246, loss = 0.03294810\n",
      "Iteration 39247, loss = 0.03294233\n",
      "Iteration 39248, loss = 0.03295671\n",
      "Iteration 39249, loss = 0.03295037\n",
      "Iteration 39250, loss = 0.03293831\n",
      "Iteration 39251, loss = 0.03295184\n",
      "Iteration 39252, loss = 0.03295673\n",
      "Iteration 39253, loss = 0.03294938\n",
      "Iteration 39254, loss = 0.03294560\n",
      "Iteration 39255, loss = 0.03295234\n",
      "Iteration 39256, loss = 0.03295542\n",
      "Iteration 39257, loss = 0.03295118\n",
      "Iteration 39258, loss = 0.03294559\n",
      "Iteration 39259, loss = 0.03294167\n",
      "Iteration 39260, loss = 0.03295260\n",
      "Iteration 39261, loss = 0.03295269\n",
      "Iteration 39262, loss = 0.03295401\n",
      "Iteration 39263, loss = 0.03293976\n",
      "Iteration 39264, loss = 0.03293201\n",
      "Iteration 39265, loss = 0.03294778\n",
      "Iteration 39266, loss = 0.03295590\n",
      "Iteration 39267, loss = 0.03295235\n",
      "Iteration 39268, loss = 0.03293904\n",
      "Iteration 39269, loss = 0.03293389\n",
      "Iteration 39270, loss = 0.03294922\n",
      "Iteration 39271, loss = 0.03295127\n",
      "Iteration 39272, loss = 0.03294073\n",
      "Iteration 39273, loss = 0.03293713\n",
      "Iteration 39274, loss = 0.03293979\n",
      "Iteration 39275, loss = 0.03295115\n",
      "Iteration 39276, loss = 0.03295264\n",
      "Iteration 39277, loss = 0.03294455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 39278, loss = 0.03292927\n",
      "Iteration 39279, loss = 0.03293462\n",
      "Iteration 39280, loss = 0.03294463\n",
      "Iteration 39281, loss = 0.03294106\n",
      "Iteration 39282, loss = 0.03293215\n",
      "Iteration 39283, loss = 0.03293855\n",
      "Iteration 39284, loss = 0.03294305\n",
      "Iteration 39285, loss = 0.03294130\n",
      "Iteration 39286, loss = 0.03293655\n",
      "Iteration 39287, loss = 0.03293683\n",
      "Iteration 39288, loss = 0.03294227\n",
      "Iteration 39289, loss = 0.03294362\n",
      "Iteration 39290, loss = 0.03293738\n",
      "Iteration 39291, loss = 0.03292222\n",
      "Iteration 39292, loss = 0.03293704\n",
      "Iteration 39293, loss = 0.03293756\n",
      "Iteration 39294, loss = 0.03292990\n",
      "Iteration 39295, loss = 0.03293057\n",
      "Iteration 39296, loss = 0.03292871\n",
      "Iteration 39297, loss = 0.03292396\n",
      "Iteration 39298, loss = 0.03292205\n",
      "Iteration 39299, loss = 0.03291856\n",
      "Iteration 39300, loss = 0.03292900\n",
      "Iteration 39301, loss = 0.03293074\n",
      "Iteration 39302, loss = 0.03292131\n",
      "Iteration 39303, loss = 0.03292204\n",
      "Iteration 39304, loss = 0.03293685\n",
      "Iteration 39305, loss = 0.03293563\n",
      "Iteration 39306, loss = 0.03292485\n",
      "Iteration 39307, loss = 0.03292480\n",
      "Iteration 39308, loss = 0.03293578\n",
      "Iteration 39309, loss = 0.03293954\n",
      "Iteration 39310, loss = 0.03293666\n",
      "Iteration 39311, loss = 0.03293146\n",
      "Iteration 39312, loss = 0.03292832\n",
      "Iteration 39313, loss = 0.03291381\n",
      "Iteration 39314, loss = 0.03291439\n",
      "Iteration 39315, loss = 0.03291555\n",
      "Iteration 39316, loss = 0.03292208\n",
      "Iteration 39317, loss = 0.03292011\n",
      "Iteration 39318, loss = 0.03291539\n",
      "Iteration 39319, loss = 0.03290689\n",
      "Iteration 39320, loss = 0.03292731\n",
      "Iteration 39321, loss = 0.03293024\n",
      "Iteration 39322, loss = 0.03291578\n",
      "Iteration 39323, loss = 0.03291396\n",
      "Iteration 39324, loss = 0.03292963\n",
      "Iteration 39325, loss = 0.03294369\n",
      "Iteration 39326, loss = 0.03294659\n",
      "Iteration 39327, loss = 0.03293571\n",
      "Iteration 39328, loss = 0.03292809\n",
      "Iteration 39329, loss = 0.03292077\n",
      "Iteration 39330, loss = 0.03292703\n",
      "Iteration 39331, loss = 0.03293401\n",
      "Iteration 39332, loss = 0.03292804\n",
      "Iteration 39333, loss = 0.03292235\n",
      "Iteration 39334, loss = 0.03292077\n",
      "Iteration 39335, loss = 0.03293036\n",
      "Iteration 39336, loss = 0.03293310\n",
      "Iteration 39337, loss = 0.03293411\n",
      "Iteration 39338, loss = 0.03292527\n",
      "Iteration 39339, loss = 0.03291668\n",
      "Iteration 39340, loss = 0.03291467\n",
      "Iteration 39341, loss = 0.03290847\n",
      "Iteration 39342, loss = 0.03291515\n",
      "Iteration 39343, loss = 0.03290792\n",
      "Iteration 39344, loss = 0.03290398\n",
      "Iteration 39345, loss = 0.03290974\n",
      "Iteration 39346, loss = 0.03292013\n",
      "Iteration 39347, loss = 0.03292063\n",
      "Iteration 39348, loss = 0.03292274\n",
      "Iteration 39349, loss = 0.03291373\n",
      "Iteration 39350, loss = 0.03291009\n",
      "Iteration 39351, loss = 0.03291625\n",
      "Iteration 39352, loss = 0.03291296\n",
      "Iteration 39353, loss = 0.03291415\n",
      "Iteration 39354, loss = 0.03292759\n",
      "Iteration 39355, loss = 0.03293084\n",
      "Iteration 39356, loss = 0.03292274\n",
      "Iteration 39357, loss = 0.03291703\n",
      "Iteration 39358, loss = 0.03290868\n",
      "Iteration 39359, loss = 0.03291054\n",
      "Iteration 39360, loss = 0.03291582\n",
      "Iteration 39361, loss = 0.03290977\n",
      "Iteration 39362, loss = 0.03289750\n",
      "Iteration 39363, loss = 0.03289974\n",
      "Iteration 39364, loss = 0.03290153\n",
      "Iteration 39365, loss = 0.03290749\n",
      "Iteration 39366, loss = 0.03290282\n",
      "Iteration 39367, loss = 0.03290734\n",
      "Iteration 39368, loss = 0.03290958\n",
      "Iteration 39369, loss = 0.03290177\n",
      "Iteration 39370, loss = 0.03290240\n",
      "Iteration 39371, loss = 0.03290372\n",
      "Iteration 39372, loss = 0.03289429\n",
      "Iteration 39373, loss = 0.03290064\n",
      "Iteration 39374, loss = 0.03290289\n",
      "Iteration 39375, loss = 0.03291152\n",
      "Iteration 39376, loss = 0.03291006\n",
      "Iteration 39377, loss = 0.03290646\n",
      "Iteration 39378, loss = 0.03289308\n",
      "Iteration 39379, loss = 0.03289296\n",
      "Iteration 39380, loss = 0.03289441\n",
      "Iteration 39381, loss = 0.03290291\n",
      "Iteration 39382, loss = 0.03290169\n",
      "Iteration 39383, loss = 0.03289196\n",
      "Iteration 39384, loss = 0.03289128\n",
      "Iteration 39385, loss = 0.03289484\n",
      "Iteration 39386, loss = 0.03289184\n",
      "Iteration 39387, loss = 0.03288985\n",
      "Iteration 39388, loss = 0.03288750\n",
      "Iteration 39389, loss = 0.03289776\n",
      "Iteration 39390, loss = 0.03289867\n",
      "Iteration 39391, loss = 0.03289498\n",
      "Iteration 39392, loss = 0.03289511\n",
      "Iteration 39393, loss = 0.03289729\n",
      "Iteration 39394, loss = 0.03289783\n",
      "Iteration 39395, loss = 0.03290741\n",
      "Iteration 39396, loss = 0.03290617\n",
      "Iteration 39397, loss = 0.03289302\n",
      "Iteration 39398, loss = 0.03289913\n",
      "Iteration 39399, loss = 0.03290200\n",
      "Iteration 39400, loss = 0.03289761\n",
      "Iteration 39401, loss = 0.03290227\n",
      "Iteration 39402, loss = 0.03289040\n",
      "Iteration 39403, loss = 0.03289441\n",
      "Iteration 39404, loss = 0.03290515\n",
      "Iteration 39405, loss = 0.03291040\n",
      "Iteration 39406, loss = 0.03290650\n",
      "Iteration 39407, loss = 0.03290138\n",
      "Iteration 39408, loss = 0.03290603\n",
      "Iteration 39409, loss = 0.03289909\n",
      "Iteration 39410, loss = 0.03288580\n",
      "Iteration 39411, loss = 0.03287578\n",
      "Iteration 39412, loss = 0.03288379\n",
      "Iteration 39413, loss = 0.03288627\n",
      "Iteration 39414, loss = 0.03288362\n",
      "Iteration 39415, loss = 0.03287996\n",
      "Iteration 39416, loss = 0.03287822\n",
      "Iteration 39417, loss = 0.03288789\n",
      "Iteration 39418, loss = 0.03288786\n",
      "Iteration 39419, loss = 0.03288533\n",
      "Iteration 39420, loss = 0.03288608\n",
      "Iteration 39421, loss = 0.03289622\n",
      "Iteration 39422, loss = 0.03289733\n",
      "Iteration 39423, loss = 0.03289408\n",
      "Iteration 39424, loss = 0.03288843\n",
      "Iteration 39425, loss = 0.03288471\n",
      "Iteration 39426, loss = 0.03290022\n",
      "Iteration 39427, loss = 0.03290359\n",
      "Iteration 39428, loss = 0.03289417\n",
      "Iteration 39429, loss = 0.03288813\n",
      "Iteration 39430, loss = 0.03288677\n",
      "Iteration 39431, loss = 0.03288030\n",
      "Iteration 39432, loss = 0.03288314\n",
      "Iteration 39433, loss = 0.03288169\n",
      "Iteration 39434, loss = 0.03289094\n",
      "Iteration 39435, loss = 0.03288980\n",
      "Iteration 39436, loss = 0.03288611\n",
      "Iteration 39437, loss = 0.03288014\n",
      "Iteration 39438, loss = 0.03287644\n",
      "Iteration 39439, loss = 0.03288244\n",
      "Iteration 39440, loss = 0.03288098\n",
      "Iteration 39441, loss = 0.03287866\n",
      "Iteration 39442, loss = 0.03288237\n",
      "Iteration 39443, loss = 0.03288489\n",
      "Iteration 39444, loss = 0.03288124\n",
      "Iteration 39445, loss = 0.03287636\n",
      "Iteration 39446, loss = 0.03287720\n",
      "Iteration 39447, loss = 0.03286721\n",
      "Iteration 39448, loss = 0.03287806\n",
      "Iteration 39449, loss = 0.03288471\n",
      "Iteration 39450, loss = 0.03287468\n",
      "Iteration 39451, loss = 0.03286876\n",
      "Iteration 39452, loss = 0.03287753\n",
      "Iteration 39453, loss = 0.03287618\n",
      "Iteration 39454, loss = 0.03287428\n",
      "Iteration 39455, loss = 0.03287555\n",
      "Iteration 39456, loss = 0.03287226\n",
      "Iteration 39457, loss = 0.03286602\n",
      "Iteration 39458, loss = 0.03286816\n",
      "Iteration 39459, loss = 0.03286859\n",
      "Iteration 39460, loss = 0.03286375\n",
      "Iteration 39461, loss = 0.03285630\n",
      "Iteration 39462, loss = 0.03287143\n",
      "Iteration 39463, loss = 0.03287071\n",
      "Iteration 39464, loss = 0.03287177\n",
      "Iteration 39465, loss = 0.03286920\n",
      "Iteration 39466, loss = 0.03287138\n",
      "Iteration 39467, loss = 0.03286665\n",
      "Iteration 39468, loss = 0.03286135\n",
      "Iteration 39469, loss = 0.03286426\n",
      "Iteration 39470, loss = 0.03286447\n",
      "Iteration 39471, loss = 0.03286516\n",
      "Iteration 39472, loss = 0.03286259\n",
      "Iteration 39473, loss = 0.03286680\n",
      "Iteration 39474, loss = 0.03286713\n",
      "Iteration 39475, loss = 0.03286305\n",
      "Iteration 39476, loss = 0.03286279\n",
      "Iteration 39477, loss = 0.03286309\n",
      "Iteration 39478, loss = 0.03286885\n",
      "Iteration 39479, loss = 0.03286346\n",
      "Iteration 39480, loss = 0.03285851\n",
      "Iteration 39481, loss = 0.03286139\n",
      "Iteration 39482, loss = 0.03285966\n",
      "Iteration 39483, loss = 0.03286043\n",
      "Iteration 39484, loss = 0.03285867\n",
      "Iteration 39485, loss = 0.03286338\n",
      "Iteration 39486, loss = 0.03285881\n",
      "Iteration 39487, loss = 0.03285968\n",
      "Iteration 39488, loss = 0.03286541\n",
      "Iteration 39489, loss = 0.03286758\n",
      "Iteration 39490, loss = 0.03285795\n",
      "Iteration 39491, loss = 0.03286255\n",
      "Iteration 39492, loss = 0.03286232\n",
      "Iteration 39493, loss = 0.03286393\n",
      "Iteration 39494, loss = 0.03287167\n",
      "Iteration 39495, loss = 0.03286835\n",
      "Iteration 39496, loss = 0.03286146\n",
      "Iteration 39497, loss = 0.03285555\n",
      "Iteration 39498, loss = 0.03286644\n",
      "Iteration 39499, loss = 0.03286666\n",
      "Iteration 39500, loss = 0.03285537\n",
      "Iteration 39501, loss = 0.03284717\n",
      "Iteration 39502, loss = 0.03285710\n",
      "Iteration 39503, loss = 0.03285524\n",
      "Iteration 39504, loss = 0.03285641\n",
      "Iteration 39505, loss = 0.03285301\n",
      "Iteration 39506, loss = 0.03284970\n",
      "Iteration 39507, loss = 0.03284830\n",
      "Iteration 39508, loss = 0.03284742\n",
      "Iteration 39509, loss = 0.03285316\n",
      "Iteration 39510, loss = 0.03285308\n",
      "Iteration 39511, loss = 0.03285096\n",
      "Iteration 39512, loss = 0.03285259\n",
      "Iteration 39513, loss = 0.03284736\n",
      "Iteration 39514, loss = 0.03285423\n",
      "Iteration 39515, loss = 0.03285444\n",
      "Iteration 39516, loss = 0.03284620\n",
      "Iteration 39517, loss = 0.03284682\n",
      "Iteration 39518, loss = 0.03285295\n",
      "Iteration 39519, loss = 0.03284741\n",
      "Iteration 39520, loss = 0.03285323\n",
      "Iteration 39521, loss = 0.03285869\n",
      "Iteration 39522, loss = 0.03285407\n",
      "Iteration 39523, loss = 0.03285960\n",
      "Iteration 39524, loss = 0.03285537\n",
      "Iteration 39525, loss = 0.03285113\n",
      "Iteration 39526, loss = 0.03284488\n",
      "Iteration 39527, loss = 0.03284365\n",
      "Iteration 39528, loss = 0.03285378\n",
      "Iteration 39529, loss = 0.03285295\n",
      "Iteration 39530, loss = 0.03284916\n",
      "Iteration 39531, loss = 0.03284833\n",
      "Iteration 39532, loss = 0.03284591\n",
      "Iteration 39533, loss = 0.03284512\n",
      "Iteration 39534, loss = 0.03284949\n",
      "Iteration 39535, loss = 0.03284462\n",
      "Iteration 39536, loss = 0.03285112\n",
      "Iteration 39537, loss = 0.03283834\n",
      "Iteration 39538, loss = 0.03284059\n",
      "Iteration 39539, loss = 0.03284936\n",
      "Iteration 39540, loss = 0.03284673\n",
      "Iteration 39541, loss = 0.03283866\n",
      "Iteration 39542, loss = 0.03283926\n",
      "Iteration 39543, loss = 0.03283582\n",
      "Iteration 39544, loss = 0.03283037\n",
      "Iteration 39545, loss = 0.03283214\n",
      "Iteration 39546, loss = 0.03284440\n",
      "Iteration 39547, loss = 0.03284006\n",
      "Iteration 39548, loss = 0.03283514\n",
      "Iteration 39549, loss = 0.03283346\n",
      "Iteration 39550, loss = 0.03284634\n",
      "Iteration 39551, loss = 0.03284886\n",
      "Iteration 39552, loss = 0.03284352\n",
      "Iteration 39553, loss = 0.03284395\n",
      "Iteration 39554, loss = 0.03283638\n",
      "Iteration 39555, loss = 0.03283594\n",
      "Iteration 39556, loss = 0.03284822\n",
      "Iteration 39557, loss = 0.03284383\n",
      "Iteration 39558, loss = 0.03283923\n",
      "Iteration 39559, loss = 0.03284740\n",
      "Iteration 39560, loss = 0.03285118\n",
      "Iteration 39561, loss = 0.03284676\n",
      "Iteration 39562, loss = 0.03284305\n",
      "Iteration 39563, loss = 0.03284079\n",
      "Iteration 39564, loss = 0.03283832\n",
      "Iteration 39565, loss = 0.03282514\n",
      "Iteration 39566, loss = 0.03283608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 39567, loss = 0.03284862\n",
      "Iteration 39568, loss = 0.03284486\n",
      "Iteration 39569, loss = 0.03283564\n",
      "Iteration 39570, loss = 0.03282949\n",
      "Iteration 39571, loss = 0.03283092\n",
      "Iteration 39572, loss = 0.03284693\n",
      "Iteration 39573, loss = 0.03284936\n",
      "Iteration 39574, loss = 0.03283997\n",
      "Iteration 39575, loss = 0.03283212\n",
      "Iteration 39576, loss = 0.03283283\n",
      "Iteration 39577, loss = 0.03283539\n",
      "Iteration 39578, loss = 0.03283216\n",
      "Iteration 39579, loss = 0.03282947\n",
      "Iteration 39580, loss = 0.03282353\n",
      "Iteration 39581, loss = 0.03282441\n",
      "Iteration 39582, loss = 0.03283293\n",
      "Iteration 39583, loss = 0.03282816\n",
      "Iteration 39584, loss = 0.03282452\n",
      "Iteration 39585, loss = 0.03282106\n",
      "Iteration 39586, loss = 0.03282340\n",
      "Iteration 39587, loss = 0.03282498\n",
      "Iteration 39588, loss = 0.03282863\n",
      "Iteration 39589, loss = 0.03282677\n",
      "Iteration 39590, loss = 0.03282716\n",
      "Iteration 39591, loss = 0.03283274\n",
      "Iteration 39592, loss = 0.03282880\n",
      "Iteration 39593, loss = 0.03283020\n",
      "Iteration 39594, loss = 0.03283108\n",
      "Iteration 39595, loss = 0.03283241\n",
      "Iteration 39596, loss = 0.03282565\n",
      "Iteration 39597, loss = 0.03281528\n",
      "Iteration 39598, loss = 0.03282829\n",
      "Iteration 39599, loss = 0.03283745\n",
      "Iteration 39600, loss = 0.03283801\n",
      "Iteration 39601, loss = 0.03282683\n",
      "Iteration 39602, loss = 0.03282688\n",
      "Iteration 39603, loss = 0.03282316\n",
      "Iteration 39604, loss = 0.03282751\n",
      "Iteration 39605, loss = 0.03283670\n",
      "Iteration 39606, loss = 0.03282751\n",
      "Iteration 39607, loss = 0.03281741\n",
      "Iteration 39608, loss = 0.03281883\n",
      "Iteration 39609, loss = 0.03281878\n",
      "Iteration 39610, loss = 0.03281447\n",
      "Iteration 39611, loss = 0.03281418\n",
      "Iteration 39612, loss = 0.03282334\n",
      "Iteration 39613, loss = 0.03282193\n",
      "Iteration 39614, loss = 0.03282251\n",
      "Iteration 39615, loss = 0.03282828\n",
      "Iteration 39616, loss = 0.03283108\n",
      "Iteration 39617, loss = 0.03282446\n",
      "Iteration 39618, loss = 0.03282678\n",
      "Iteration 39619, loss = 0.03282804\n",
      "Iteration 39620, loss = 0.03282537\n",
      "Iteration 39621, loss = 0.03280706\n",
      "Iteration 39622, loss = 0.03282378\n",
      "Iteration 39623, loss = 0.03283657\n",
      "Iteration 39624, loss = 0.03283176\n",
      "Iteration 39625, loss = 0.03281623\n",
      "Iteration 39626, loss = 0.03281755\n",
      "Iteration 39627, loss = 0.03281154\n",
      "Iteration 39628, loss = 0.03281972\n",
      "Iteration 39629, loss = 0.03282027\n",
      "Iteration 39630, loss = 0.03281974\n",
      "Iteration 39631, loss = 0.03281689\n",
      "Iteration 39632, loss = 0.03280519\n",
      "Iteration 39633, loss = 0.03281779\n",
      "Iteration 39634, loss = 0.03282229\n",
      "Iteration 39635, loss = 0.03282294\n",
      "Iteration 39636, loss = 0.03282031\n",
      "Iteration 39637, loss = 0.03281087\n",
      "Iteration 39638, loss = 0.03280642\n",
      "Iteration 39639, loss = 0.03279966\n",
      "Iteration 39640, loss = 0.03281216\n",
      "Iteration 39641, loss = 0.03281095\n",
      "Iteration 39642, loss = 0.03280056\n",
      "Iteration 39643, loss = 0.03281742\n",
      "Iteration 39644, loss = 0.03281864\n",
      "Iteration 39645, loss = 0.03281198\n",
      "Iteration 39646, loss = 0.03280999\n",
      "Iteration 39647, loss = 0.03280481\n",
      "Iteration 39648, loss = 0.03280859\n",
      "Iteration 39649, loss = 0.03281179\n",
      "Iteration 39650, loss = 0.03280670\n",
      "Iteration 39651, loss = 0.03279405\n",
      "Iteration 39652, loss = 0.03279950\n",
      "Iteration 39653, loss = 0.03281711\n",
      "Iteration 39654, loss = 0.03282172\n",
      "Iteration 39655, loss = 0.03281586\n",
      "Iteration 39656, loss = 0.03279772\n",
      "Iteration 39657, loss = 0.03279174\n",
      "Iteration 39658, loss = 0.03280036\n",
      "Iteration 39659, loss = 0.03279886\n",
      "Iteration 39660, loss = 0.03279114\n",
      "Iteration 39661, loss = 0.03279968\n",
      "Iteration 39662, loss = 0.03279479\n",
      "Iteration 39663, loss = 0.03279390\n",
      "Iteration 39664, loss = 0.03279602\n",
      "Iteration 39665, loss = 0.03279616\n",
      "Iteration 39666, loss = 0.03279459\n",
      "Iteration 39667, loss = 0.03279212\n",
      "Iteration 39668, loss = 0.03279512\n",
      "Iteration 39669, loss = 0.03279802\n",
      "Iteration 39670, loss = 0.03279713\n",
      "Iteration 39671, loss = 0.03279244\n",
      "Iteration 39672, loss = 0.03279466\n",
      "Iteration 39673, loss = 0.03278796\n",
      "Iteration 39674, loss = 0.03278675\n",
      "Iteration 39675, loss = 0.03278768\n",
      "Iteration 39676, loss = 0.03278823\n",
      "Iteration 39677, loss = 0.03278530\n",
      "Iteration 39678, loss = 0.03278449\n",
      "Iteration 39679, loss = 0.03279030\n",
      "Iteration 39680, loss = 0.03278631\n",
      "Iteration 39681, loss = 0.03278485\n",
      "Iteration 39682, loss = 0.03278468\n",
      "Iteration 39683, loss = 0.03279266\n",
      "Iteration 39684, loss = 0.03278967\n",
      "Iteration 39685, loss = 0.03277775\n",
      "Iteration 39686, loss = 0.03278152\n",
      "Iteration 39687, loss = 0.03278109\n",
      "Iteration 39688, loss = 0.03278414\n",
      "Iteration 39689, loss = 0.03278584\n",
      "Iteration 39690, loss = 0.03279205\n",
      "Iteration 39691, loss = 0.03279116\n",
      "Iteration 39692, loss = 0.03278442\n",
      "Iteration 39693, loss = 0.03278431\n",
      "Iteration 39694, loss = 0.03278325\n",
      "Iteration 39695, loss = 0.03278866\n",
      "Iteration 39696, loss = 0.03278638\n",
      "Iteration 39697, loss = 0.03278246\n",
      "Iteration 39698, loss = 0.03278358\n",
      "Iteration 39699, loss = 0.03278199\n",
      "Iteration 39700, loss = 0.03278097\n",
      "Iteration 39701, loss = 0.03278370\n",
      "Iteration 39702, loss = 0.03277889\n",
      "Iteration 39703, loss = 0.03278420\n",
      "Iteration 39704, loss = 0.03278479\n",
      "Iteration 39705, loss = 0.03278718\n",
      "Iteration 39706, loss = 0.03278712\n",
      "Iteration 39707, loss = 0.03277700\n",
      "Iteration 39708, loss = 0.03277742\n",
      "Iteration 39709, loss = 0.03277468\n",
      "Iteration 39710, loss = 0.03277360\n",
      "Iteration 39711, loss = 0.03277913\n",
      "Iteration 39712, loss = 0.03277235\n",
      "Iteration 39713, loss = 0.03277213\n",
      "Iteration 39714, loss = 0.03277027\n",
      "Iteration 39715, loss = 0.03277066\n",
      "Iteration 39716, loss = 0.03277945\n",
      "Iteration 39717, loss = 0.03278495\n",
      "Iteration 39718, loss = 0.03277567\n",
      "Iteration 39719, loss = 0.03277964\n",
      "Iteration 39720, loss = 0.03278157\n",
      "Iteration 39721, loss = 0.03278188\n",
      "Iteration 39722, loss = 0.03278001\n",
      "Iteration 39723, loss = 0.03277318\n",
      "Iteration 39724, loss = 0.03278281\n",
      "Iteration 39725, loss = 0.03278400\n",
      "Iteration 39726, loss = 0.03278550\n",
      "Iteration 39727, loss = 0.03277670\n",
      "Iteration 39728, loss = 0.03276729\n",
      "Iteration 39729, loss = 0.03278373\n",
      "Iteration 39730, loss = 0.03278714\n",
      "Iteration 39731, loss = 0.03277452\n",
      "Iteration 39732, loss = 0.03276868\n",
      "Iteration 39733, loss = 0.03278571\n",
      "Iteration 39734, loss = 0.03279208\n",
      "Iteration 39735, loss = 0.03278500\n",
      "Iteration 39736, loss = 0.03277094\n",
      "Iteration 39737, loss = 0.03276779\n",
      "Iteration 39738, loss = 0.03278873\n",
      "Iteration 39739, loss = 0.03278804\n",
      "Iteration 39740, loss = 0.03278301\n",
      "Iteration 39741, loss = 0.03277326\n",
      "Iteration 39742, loss = 0.03277528\n",
      "Iteration 39743, loss = 0.03278229\n",
      "Iteration 39744, loss = 0.03277800\n",
      "Iteration 39745, loss = 0.03278791\n",
      "Iteration 39746, loss = 0.03278901\n",
      "Iteration 39747, loss = 0.03276792\n",
      "Iteration 39748, loss = 0.03276515\n",
      "Iteration 39749, loss = 0.03277989\n",
      "Iteration 39750, loss = 0.03278383\n",
      "Iteration 39751, loss = 0.03278009\n",
      "Iteration 39752, loss = 0.03277007\n",
      "Iteration 39753, loss = 0.03276408\n",
      "Iteration 39754, loss = 0.03276248\n",
      "Iteration 39755, loss = 0.03277473\n",
      "Iteration 39756, loss = 0.03277786\n",
      "Iteration 39757, loss = 0.03277660\n",
      "Iteration 39758, loss = 0.03276280\n",
      "Iteration 39759, loss = 0.03275776\n",
      "Iteration 39760, loss = 0.03276914\n",
      "Iteration 39761, loss = 0.03277067\n",
      "Iteration 39762, loss = 0.03277298\n",
      "Iteration 39763, loss = 0.03276588\n",
      "Iteration 39764, loss = 0.03275622\n",
      "Iteration 39765, loss = 0.03276514\n",
      "Iteration 39766, loss = 0.03277024\n",
      "Iteration 39767, loss = 0.03276940\n",
      "Iteration 39768, loss = 0.03276561\n",
      "Iteration 39769, loss = 0.03275607\n",
      "Iteration 39770, loss = 0.03275047\n",
      "Iteration 39771, loss = 0.03275080\n",
      "Iteration 39772, loss = 0.03275388\n",
      "Iteration 39773, loss = 0.03275551\n",
      "Iteration 39774, loss = 0.03275171\n",
      "Iteration 39775, loss = 0.03274593\n",
      "Iteration 39776, loss = 0.03276336\n",
      "Iteration 39777, loss = 0.03276153\n",
      "Iteration 39778, loss = 0.03275688\n",
      "Iteration 39779, loss = 0.03276151\n",
      "Iteration 39780, loss = 0.03275629\n",
      "Iteration 39781, loss = 0.03276266\n",
      "Iteration 39782, loss = 0.03277166\n",
      "Iteration 39783, loss = 0.03276960\n",
      "Iteration 39784, loss = 0.03275637\n",
      "Iteration 39785, loss = 0.03274463\n",
      "Iteration 39786, loss = 0.03276654\n",
      "Iteration 39787, loss = 0.03277406\n",
      "Iteration 39788, loss = 0.03276661\n",
      "Iteration 39789, loss = 0.03274926\n",
      "Iteration 39790, loss = 0.03274766\n",
      "Iteration 39791, loss = 0.03275253\n",
      "Iteration 39792, loss = 0.03275530\n",
      "Iteration 39793, loss = 0.03274580\n",
      "Iteration 39794, loss = 0.03274697\n",
      "Iteration 39795, loss = 0.03275374\n",
      "Iteration 39796, loss = 0.03276073\n",
      "Iteration 39797, loss = 0.03276068\n",
      "Iteration 39798, loss = 0.03275161\n",
      "Iteration 39799, loss = 0.03275147\n",
      "Iteration 39800, loss = 0.03275202\n",
      "Iteration 39801, loss = 0.03275316\n",
      "Iteration 39802, loss = 0.03275386\n",
      "Iteration 39803, loss = 0.03275027\n",
      "Iteration 39804, loss = 0.03275052\n",
      "Iteration 39805, loss = 0.03275430\n",
      "Iteration 39806, loss = 0.03275129\n",
      "Iteration 39807, loss = 0.03275768\n",
      "Iteration 39808, loss = 0.03275449\n",
      "Iteration 39809, loss = 0.03273903\n",
      "Iteration 39810, loss = 0.03274931\n",
      "Iteration 39811, loss = 0.03275809\n",
      "Iteration 39812, loss = 0.03275781\n",
      "Iteration 39813, loss = 0.03274877\n",
      "Iteration 39814, loss = 0.03274475\n",
      "Iteration 39815, loss = 0.03275196\n",
      "Iteration 39816, loss = 0.03275139\n",
      "Iteration 39817, loss = 0.03275985\n",
      "Iteration 39818, loss = 0.03276045\n",
      "Iteration 39819, loss = 0.03275164\n",
      "Iteration 39820, loss = 0.03274412\n",
      "Iteration 39821, loss = 0.03274864\n",
      "Iteration 39822, loss = 0.03274228\n",
      "Iteration 39823, loss = 0.03274208\n",
      "Iteration 39824, loss = 0.03273933\n",
      "Iteration 39825, loss = 0.03274798\n",
      "Iteration 39826, loss = 0.03274596\n",
      "Iteration 39827, loss = 0.03274113\n",
      "Iteration 39828, loss = 0.03273600\n",
      "Iteration 39829, loss = 0.03274222\n",
      "Iteration 39830, loss = 0.03274482\n",
      "Iteration 39831, loss = 0.03274039\n",
      "Iteration 39832, loss = 0.03273446\n",
      "Iteration 39833, loss = 0.03273716\n",
      "Iteration 39834, loss = 0.03273830\n",
      "Iteration 39835, loss = 0.03273368\n",
      "Iteration 39836, loss = 0.03273576\n",
      "Iteration 39837, loss = 0.03273648\n",
      "Iteration 39838, loss = 0.03273108\n",
      "Iteration 39839, loss = 0.03273240\n",
      "Iteration 39840, loss = 0.03273929\n",
      "Iteration 39841, loss = 0.03273832\n",
      "Iteration 39842, loss = 0.03272973\n",
      "Iteration 39843, loss = 0.03272821\n",
      "Iteration 39844, loss = 0.03273801\n",
      "Iteration 39845, loss = 0.03273988\n",
      "Iteration 39846, loss = 0.03272948\n",
      "Iteration 39847, loss = 0.03274195\n",
      "Iteration 39848, loss = 0.03274449\n",
      "Iteration 39849, loss = 0.03273724\n",
      "Iteration 39850, loss = 0.03274313\n",
      "Iteration 39851, loss = 0.03274469\n",
      "Iteration 39852, loss = 0.03274105\n",
      "Iteration 39853, loss = 0.03273008\n",
      "Iteration 39854, loss = 0.03272985\n",
      "Iteration 39855, loss = 0.03274777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 39856, loss = 0.03274772\n",
      "Iteration 39857, loss = 0.03273307\n",
      "Iteration 39858, loss = 0.03273506\n",
      "Iteration 39859, loss = 0.03273463\n",
      "Iteration 39860, loss = 0.03273851\n",
      "Iteration 39861, loss = 0.03273417\n",
      "Iteration 39862, loss = 0.03272549\n",
      "Iteration 39863, loss = 0.03271404\n",
      "Iteration 39864, loss = 0.03273659\n",
      "Iteration 39865, loss = 0.03274568\n",
      "Iteration 39866, loss = 0.03273935\n",
      "Iteration 39867, loss = 0.03272191\n",
      "Iteration 39868, loss = 0.03272594\n",
      "Iteration 39869, loss = 0.03273275\n",
      "Iteration 39870, loss = 0.03274486\n",
      "Iteration 39871, loss = 0.03274425\n",
      "Iteration 39872, loss = 0.03272914\n",
      "Iteration 39873, loss = 0.03272535\n",
      "Iteration 39874, loss = 0.03271930\n",
      "Iteration 39875, loss = 0.03273329\n",
      "Iteration 39876, loss = 0.03273138\n",
      "Iteration 39877, loss = 0.03272415\n",
      "Iteration 39878, loss = 0.03271945\n",
      "Iteration 39879, loss = 0.03272916\n",
      "Iteration 39880, loss = 0.03273251\n",
      "Iteration 39881, loss = 0.03273357\n",
      "Iteration 39882, loss = 0.03273101\n",
      "Iteration 39883, loss = 0.03271963\n",
      "Iteration 39884, loss = 0.03271002\n",
      "Iteration 39885, loss = 0.03272117\n",
      "Iteration 39886, loss = 0.03272763\n",
      "Iteration 39887, loss = 0.03271687\n",
      "Iteration 39888, loss = 0.03271244\n",
      "Iteration 39889, loss = 0.03271976\n",
      "Iteration 39890, loss = 0.03271831\n",
      "Iteration 39891, loss = 0.03271282\n",
      "Iteration 39892, loss = 0.03270757\n",
      "Iteration 39893, loss = 0.03271857\n",
      "Iteration 39894, loss = 0.03271291\n",
      "Iteration 39895, loss = 0.03270792\n",
      "Iteration 39896, loss = 0.03271784\n",
      "Iteration 39897, loss = 0.03272677\n",
      "Iteration 39898, loss = 0.03271932\n",
      "Iteration 39899, loss = 0.03271584\n",
      "Iteration 39900, loss = 0.03270874\n",
      "Iteration 39901, loss = 0.03272097\n",
      "Iteration 39902, loss = 0.03272230\n",
      "Iteration 39903, loss = 0.03272390\n",
      "Iteration 39904, loss = 0.03271768\n",
      "Iteration 39905, loss = 0.03270787\n",
      "Iteration 39906, loss = 0.03270935\n",
      "Iteration 39907, loss = 0.03271199\n",
      "Iteration 39908, loss = 0.03272007\n",
      "Iteration 39909, loss = 0.03271411\n",
      "Iteration 39910, loss = 0.03271510\n",
      "Iteration 39911, loss = 0.03271838\n",
      "Iteration 39912, loss = 0.03272644\n",
      "Iteration 39913, loss = 0.03271745\n",
      "Iteration 39914, loss = 0.03271420\n",
      "Iteration 39915, loss = 0.03272291\n",
      "Iteration 39916, loss = 0.03272057\n",
      "Iteration 39917, loss = 0.03272614\n",
      "Iteration 39918, loss = 0.03272491\n",
      "Iteration 39919, loss = 0.03272188\n",
      "Iteration 39920, loss = 0.03271553\n",
      "Iteration 39921, loss = 0.03270650\n",
      "Iteration 39922, loss = 0.03270374\n",
      "Iteration 39923, loss = 0.03271341\n",
      "Iteration 39924, loss = 0.03271650\n",
      "Iteration 39925, loss = 0.03270956\n",
      "Iteration 39926, loss = 0.03270123\n",
      "Iteration 39927, loss = 0.03270699\n",
      "Iteration 39928, loss = 0.03271461\n",
      "Iteration 39929, loss = 0.03271340\n",
      "Iteration 39930, loss = 0.03271057\n",
      "Iteration 39931, loss = 0.03270599\n",
      "Iteration 39932, loss = 0.03270263\n",
      "Iteration 39933, loss = 0.03270069\n",
      "Iteration 39934, loss = 0.03269627\n",
      "Iteration 39935, loss = 0.03269680\n",
      "Iteration 39936, loss = 0.03269491\n",
      "Iteration 39937, loss = 0.03269193\n",
      "Iteration 39938, loss = 0.03269238\n",
      "Iteration 39939, loss = 0.03270092\n",
      "Iteration 39940, loss = 0.03269866\n",
      "Iteration 39941, loss = 0.03269331\n",
      "Iteration 39942, loss = 0.03269958\n",
      "Iteration 39943, loss = 0.03271588\n",
      "Iteration 39944, loss = 0.03271343\n",
      "Iteration 39945, loss = 0.03269623\n",
      "Iteration 39946, loss = 0.03269909\n",
      "Iteration 39947, loss = 0.03271845\n",
      "Iteration 39948, loss = 0.03271405\n",
      "Iteration 39949, loss = 0.03271563\n",
      "Iteration 39950, loss = 0.03271399\n",
      "Iteration 39951, loss = 0.03270575\n",
      "Iteration 39952, loss = 0.03270153\n",
      "Iteration 39953, loss = 0.03270912\n",
      "Iteration 39954, loss = 0.03270843\n",
      "Iteration 39955, loss = 0.03270034\n",
      "Iteration 39956, loss = 0.03268024\n",
      "Iteration 39957, loss = 0.03268988\n",
      "Iteration 39958, loss = 0.03269402\n",
      "Iteration 39959, loss = 0.03268888\n",
      "Iteration 39960, loss = 0.03268363\n",
      "Iteration 39961, loss = 0.03268653\n",
      "Iteration 39962, loss = 0.03268906\n",
      "Iteration 39963, loss = 0.03268528\n",
      "Iteration 39964, loss = 0.03268860\n",
      "Iteration 39965, loss = 0.03268315\n",
      "Iteration 39966, loss = 0.03268487\n",
      "Iteration 39967, loss = 0.03268928\n",
      "Iteration 39968, loss = 0.03269213\n",
      "Iteration 39969, loss = 0.03268902\n",
      "Iteration 39970, loss = 0.03268601\n",
      "Iteration 39971, loss = 0.03268188\n",
      "Iteration 39972, loss = 0.03267956\n",
      "Iteration 39973, loss = 0.03268102\n",
      "Iteration 39974, loss = 0.03267968\n",
      "Iteration 39975, loss = 0.03267936\n",
      "Iteration 39976, loss = 0.03268350\n",
      "Iteration 39977, loss = 0.03268391\n",
      "Iteration 39978, loss = 0.03268283\n",
      "Iteration 39979, loss = 0.03267331\n",
      "Iteration 39980, loss = 0.03268852\n",
      "Iteration 39981, loss = 0.03268788\n",
      "Iteration 39982, loss = 0.03267548\n",
      "Iteration 39983, loss = 0.03268484\n",
      "Iteration 39984, loss = 0.03268604\n",
      "Iteration 39985, loss = 0.03268900\n",
      "Iteration 39986, loss = 0.03269612\n",
      "Iteration 39987, loss = 0.03268811\n",
      "Iteration 39988, loss = 0.03268159\n",
      "Iteration 39989, loss = 0.03268533\n",
      "Iteration 39990, loss = 0.03269672\n",
      "Iteration 39991, loss = 0.03270135\n",
      "Iteration 39992, loss = 0.03269047\n",
      "Iteration 39993, loss = 0.03268308\n",
      "Iteration 39994, loss = 0.03268225\n",
      "Iteration 39995, loss = 0.03267360\n",
      "Iteration 39996, loss = 0.03268975\n",
      "Iteration 39997, loss = 0.03268789\n",
      "Iteration 39998, loss = 0.03268054\n",
      "Iteration 39999, loss = 0.03267354\n",
      "Iteration 40000, loss = 0.03267585\n",
      "Iteration 40001, loss = 0.03268756\n",
      "Iteration 40002, loss = 0.03268941\n",
      "Iteration 40003, loss = 0.03268345\n",
      "Iteration 40004, loss = 0.03268049\n",
      "Iteration 40005, loss = 0.03267695\n",
      "Iteration 40006, loss = 0.03267090\n",
      "Iteration 40007, loss = 0.03267543\n",
      "Iteration 40008, loss = 0.03268542\n",
      "Iteration 40009, loss = 0.03268232\n",
      "Iteration 40010, loss = 0.03266758\n",
      "Iteration 40011, loss = 0.03266686\n",
      "Iteration 40012, loss = 0.03267320\n",
      "Iteration 40013, loss = 0.03267533\n",
      "Iteration 40014, loss = 0.03267486\n",
      "Iteration 40015, loss = 0.03267727\n",
      "Iteration 40016, loss = 0.03267865\n",
      "Iteration 40017, loss = 0.03266793\n",
      "Iteration 40018, loss = 0.03267596\n",
      "Iteration 40019, loss = 0.03267974\n",
      "Iteration 40020, loss = 0.03267376\n",
      "Iteration 40021, loss = 0.03267135\n",
      "Iteration 40022, loss = 0.03266418\n",
      "Iteration 40023, loss = 0.03266967\n",
      "Iteration 40024, loss = 0.03267123\n",
      "Iteration 40025, loss = 0.03266962\n",
      "Iteration 40026, loss = 0.03266930\n",
      "Iteration 40027, loss = 0.03266816\n",
      "Iteration 40028, loss = 0.03266928\n",
      "Iteration 40029, loss = 0.03266812\n",
      "Iteration 40030, loss = 0.03267184\n",
      "Iteration 40031, loss = 0.03266963\n",
      "Iteration 40032, loss = 0.03266332\n",
      "Iteration 40033, loss = 0.03266156\n",
      "Iteration 40034, loss = 0.03266604\n",
      "Iteration 40035, loss = 0.03266113\n",
      "Iteration 40036, loss = 0.03266231\n",
      "Iteration 40037, loss = 0.03267128\n",
      "Iteration 40038, loss = 0.03266731\n",
      "Iteration 40039, loss = 0.03266367\n",
      "Iteration 40040, loss = 0.03266729\n",
      "Iteration 40041, loss = 0.03266382\n",
      "Iteration 40042, loss = 0.03266077\n",
      "Iteration 40043, loss = 0.03265429\n",
      "Iteration 40044, loss = 0.03266530\n",
      "Iteration 40045, loss = 0.03266376\n",
      "Iteration 40046, loss = 0.03265357\n",
      "Iteration 40047, loss = 0.03265814\n",
      "Iteration 40048, loss = 0.03266344\n",
      "Iteration 40049, loss = 0.03265403\n",
      "Iteration 40050, loss = 0.03265064\n",
      "Iteration 40051, loss = 0.03266614\n",
      "Iteration 40052, loss = 0.03266007\n",
      "Iteration 40053, loss = 0.03265988\n",
      "Iteration 40054, loss = 0.03266435\n",
      "Iteration 40055, loss = 0.03266711\n",
      "Iteration 40056, loss = 0.03266522\n",
      "Iteration 40057, loss = 0.03266139\n",
      "Iteration 40058, loss = 0.03265705\n",
      "Iteration 40059, loss = 0.03265471\n",
      "Iteration 40060, loss = 0.03264407\n",
      "Iteration 40061, loss = 0.03266115\n",
      "Iteration 40062, loss = 0.03266552\n",
      "Iteration 40063, loss = 0.03265785\n",
      "Iteration 40064, loss = 0.03264749\n",
      "Iteration 40065, loss = 0.03265836\n",
      "Iteration 40066, loss = 0.03265873\n",
      "Iteration 40067, loss = 0.03265372\n",
      "Iteration 40068, loss = 0.03265105\n",
      "Iteration 40069, loss = 0.03264966\n",
      "Iteration 40070, loss = 0.03264814\n",
      "Iteration 40071, loss = 0.03265469\n",
      "Iteration 40072, loss = 0.03266118\n",
      "Iteration 40073, loss = 0.03265716\n",
      "Iteration 40074, loss = 0.03264401\n",
      "Iteration 40075, loss = 0.03265609\n",
      "Iteration 40076, loss = 0.03265951\n",
      "Iteration 40077, loss = 0.03264596\n",
      "Iteration 40078, loss = 0.03265248\n",
      "Iteration 40079, loss = 0.03266136\n",
      "Iteration 40080, loss = 0.03266071\n",
      "Iteration 40081, loss = 0.03264976\n",
      "Iteration 40082, loss = 0.03265076\n",
      "Iteration 40083, loss = 0.03265102\n",
      "Iteration 40084, loss = 0.03265436\n",
      "Iteration 40085, loss = 0.03264732\n",
      "Iteration 40086, loss = 0.03264738\n",
      "Iteration 40087, loss = 0.03264435\n",
      "Iteration 40088, loss = 0.03264366\n",
      "Iteration 40089, loss = 0.03264284\n",
      "Iteration 40090, loss = 0.03264359\n",
      "Iteration 40091, loss = 0.03265251\n",
      "Iteration 40092, loss = 0.03265023\n",
      "Iteration 40093, loss = 0.03264084\n",
      "Iteration 40094, loss = 0.03264703\n",
      "Iteration 40095, loss = 0.03264723\n",
      "Iteration 40096, loss = 0.03264916\n",
      "Iteration 40097, loss = 0.03265104\n",
      "Iteration 40098, loss = 0.03264497\n",
      "Iteration 40099, loss = 0.03263787\n",
      "Iteration 40100, loss = 0.03263787\n",
      "Iteration 40101, loss = 0.03265203\n",
      "Iteration 40102, loss = 0.03265502\n",
      "Iteration 40103, loss = 0.03264245\n",
      "Iteration 40104, loss = 0.03264319\n",
      "Iteration 40105, loss = 0.03264364\n",
      "Iteration 40106, loss = 0.03264860\n",
      "Iteration 40107, loss = 0.03264855\n",
      "Iteration 40108, loss = 0.03263852\n",
      "Iteration 40109, loss = 0.03263505\n",
      "Iteration 40110, loss = 0.03265391\n",
      "Iteration 40111, loss = 0.03265214\n",
      "Iteration 40112, loss = 0.03264727\n",
      "Iteration 40113, loss = 0.03264073\n",
      "Iteration 40114, loss = 0.03265061\n",
      "Iteration 40115, loss = 0.03265852\n",
      "Iteration 40116, loss = 0.03265543\n",
      "Iteration 40117, loss = 0.03264447\n",
      "Iteration 40118, loss = 0.03263435\n",
      "Iteration 40119, loss = 0.03264124\n",
      "Iteration 40120, loss = 0.03263993\n",
      "Iteration 40121, loss = 0.03263258\n",
      "Iteration 40122, loss = 0.03263817\n",
      "Iteration 40123, loss = 0.03264539\n",
      "Iteration 40124, loss = 0.03264285\n",
      "Iteration 40125, loss = 0.03263657\n",
      "Iteration 40126, loss = 0.03263272\n",
      "Iteration 40127, loss = 0.03263036\n",
      "Iteration 40128, loss = 0.03262049\n",
      "Iteration 40129, loss = 0.03263085\n",
      "Iteration 40130, loss = 0.03263996\n",
      "Iteration 40131, loss = 0.03263107\n",
      "Iteration 40132, loss = 0.03262273\n",
      "Iteration 40133, loss = 0.03262934\n",
      "Iteration 40134, loss = 0.03263043\n",
      "Iteration 40135, loss = 0.03262455\n",
      "Iteration 40136, loss = 0.03261870\n",
      "Iteration 40137, loss = 0.03262865\n",
      "Iteration 40138, loss = 0.03262946\n",
      "Iteration 40139, loss = 0.03262594\n",
      "Iteration 40140, loss = 0.03262312\n",
      "Iteration 40141, loss = 0.03261875\n",
      "Iteration 40142, loss = 0.03262378\n",
      "Iteration 40143, loss = 0.03263190\n",
      "Iteration 40144, loss = 0.03262332\n",
      "Iteration 40145, loss = 0.03262445\n",
      "Iteration 40146, loss = 0.03262910\n",
      "Iteration 40147, loss = 0.03263272\n",
      "Iteration 40148, loss = 0.03262528\n",
      "Iteration 40149, loss = 0.03262033\n",
      "Iteration 40150, loss = 0.03262180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 40151, loss = 0.03262917\n",
      "Iteration 40152, loss = 0.03263272\n",
      "Iteration 40153, loss = 0.03263519\n",
      "Iteration 40154, loss = 0.03262673\n",
      "Iteration 40155, loss = 0.03261429\n",
      "Iteration 40156, loss = 0.03263224\n",
      "Iteration 40157, loss = 0.03263913\n",
      "Iteration 40158, loss = 0.03263074\n",
      "Iteration 40159, loss = 0.03262120\n",
      "Iteration 40160, loss = 0.03262649\n",
      "Iteration 40161, loss = 0.03263355\n",
      "Iteration 40162, loss = 0.03263123\n",
      "Iteration 40163, loss = 0.03263842\n",
      "Iteration 40164, loss = 0.03263441\n",
      "Iteration 40165, loss = 0.03261907\n",
      "Iteration 40166, loss = 0.03262496\n",
      "Iteration 40167, loss = 0.03264080\n",
      "Iteration 40168, loss = 0.03264295\n",
      "Iteration 40169, loss = 0.03261977\n",
      "Iteration 40170, loss = 0.03261793\n",
      "Iteration 40171, loss = 0.03262346\n",
      "Iteration 40172, loss = 0.03262890\n",
      "Iteration 40173, loss = 0.03262207\n",
      "Iteration 40174, loss = 0.03263458\n",
      "Iteration 40175, loss = 0.03263296\n",
      "Iteration 40176, loss = 0.03261306\n",
      "Iteration 40177, loss = 0.03261472\n",
      "Iteration 40178, loss = 0.03262749\n",
      "Iteration 40179, loss = 0.03262963\n",
      "Iteration 40180, loss = 0.03262659\n",
      "Iteration 40181, loss = 0.03262340\n",
      "Iteration 40182, loss = 0.03261759\n",
      "Iteration 40183, loss = 0.03262113\n",
      "Iteration 40184, loss = 0.03261845\n",
      "Iteration 40185, loss = 0.03261545\n",
      "Iteration 40186, loss = 0.03261008\n",
      "Iteration 40187, loss = 0.03260506\n",
      "Iteration 40188, loss = 0.03261016\n",
      "Iteration 40189, loss = 0.03261727\n",
      "Iteration 40190, loss = 0.03261441\n",
      "Iteration 40191, loss = 0.03260162\n",
      "Iteration 40192, loss = 0.03260926\n",
      "Iteration 40193, loss = 0.03262588\n",
      "Iteration 40194, loss = 0.03262331\n",
      "Iteration 40195, loss = 0.03262198\n",
      "Iteration 40196, loss = 0.03262306\n",
      "Iteration 40197, loss = 0.03262274\n",
      "Iteration 40198, loss = 0.03261640\n",
      "Iteration 40199, loss = 0.03261588\n",
      "Iteration 40200, loss = 0.03262706\n",
      "Iteration 40201, loss = 0.03262632\n",
      "Iteration 40202, loss = 0.03261076\n",
      "Iteration 40203, loss = 0.03261901\n",
      "Iteration 40204, loss = 0.03262274\n",
      "Iteration 40205, loss = 0.03262663\n",
      "Iteration 40206, loss = 0.03261302\n",
      "Iteration 40207, loss = 0.03261279\n",
      "Iteration 40208, loss = 0.03260734\n",
      "Iteration 40209, loss = 0.03261201\n",
      "Iteration 40210, loss = 0.03261135\n",
      "Iteration 40211, loss = 0.03261700\n",
      "Iteration 40212, loss = 0.03260484\n",
      "Iteration 40213, loss = 0.03259695\n",
      "Iteration 40214, loss = 0.03261318\n",
      "Iteration 40215, loss = 0.03261923\n",
      "Iteration 40216, loss = 0.03261247\n",
      "Iteration 40217, loss = 0.03260872\n",
      "Iteration 40218, loss = 0.03261145\n",
      "Iteration 40219, loss = 0.03260820\n",
      "Iteration 40220, loss = 0.03260139\n",
      "Iteration 40221, loss = 0.03259665\n",
      "Iteration 40222, loss = 0.03259718\n",
      "Iteration 40223, loss = 0.03259588\n",
      "Iteration 40224, loss = 0.03259634\n",
      "Iteration 40225, loss = 0.03259282\n",
      "Iteration 40226, loss = 0.03259409\n",
      "Iteration 40227, loss = 0.03259943\n",
      "Iteration 40228, loss = 0.03259230\n",
      "Iteration 40229, loss = 0.03258275\n",
      "Iteration 40230, loss = 0.03259657\n",
      "Iteration 40231, loss = 0.03260164\n",
      "Iteration 40232, loss = 0.03259070\n",
      "Iteration 40233, loss = 0.03258817\n",
      "Iteration 40234, loss = 0.03259927\n",
      "Iteration 40235, loss = 0.03260229\n",
      "Iteration 40236, loss = 0.03260002\n",
      "Iteration 40237, loss = 0.03259540\n",
      "Iteration 40238, loss = 0.03259635\n",
      "Iteration 40239, loss = 0.03258924\n",
      "Iteration 40240, loss = 0.03259302\n",
      "Iteration 40241, loss = 0.03260309\n",
      "Iteration 40242, loss = 0.03260079\n",
      "Iteration 40243, loss = 0.03257911\n",
      "Iteration 40244, loss = 0.03259161\n",
      "Iteration 40245, loss = 0.03259768\n",
      "Iteration 40246, loss = 0.03259568\n",
      "Iteration 40247, loss = 0.03258473\n",
      "Iteration 40248, loss = 0.03258792\n",
      "Iteration 40249, loss = 0.03258892\n",
      "Iteration 40250, loss = 0.03258654\n",
      "Iteration 40251, loss = 0.03258601\n",
      "Iteration 40252, loss = 0.03258671\n",
      "Iteration 40253, loss = 0.03258248\n",
      "Iteration 40254, loss = 0.03259185\n",
      "Iteration 40255, loss = 0.03259750\n",
      "Iteration 40256, loss = 0.03259708\n",
      "Iteration 40257, loss = 0.03260497\n",
      "Iteration 40258, loss = 0.03259632\n",
      "Iteration 40259, loss = 0.03258015\n",
      "Iteration 40260, loss = 0.03258015\n",
      "Iteration 40261, loss = 0.03258357\n",
      "Iteration 40262, loss = 0.03259534\n",
      "Iteration 40263, loss = 0.03260026\n",
      "Iteration 40264, loss = 0.03259063\n",
      "Iteration 40265, loss = 0.03258454\n",
      "Iteration 40266, loss = 0.03258611\n",
      "Iteration 40267, loss = 0.03260521\n",
      "Iteration 40268, loss = 0.03260355\n",
      "Iteration 40269, loss = 0.03259617\n",
      "Iteration 40270, loss = 0.03259280\n",
      "Iteration 40271, loss = 0.03258712\n",
      "Iteration 40272, loss = 0.03258826\n",
      "Iteration 40273, loss = 0.03259312\n",
      "Iteration 40274, loss = 0.03259265\n",
      "Iteration 40275, loss = 0.03258011\n",
      "Iteration 40276, loss = 0.03257148\n",
      "Iteration 40277, loss = 0.03259582\n",
      "Iteration 40278, loss = 0.03260307\n",
      "Iteration 40279, loss = 0.03259242\n",
      "Iteration 40280, loss = 0.03258109\n",
      "Iteration 40281, loss = 0.03258828\n",
      "Iteration 40282, loss = 0.03259382\n",
      "Iteration 40283, loss = 0.03259491\n",
      "Iteration 40284, loss = 0.03259789\n",
      "Iteration 40285, loss = 0.03259078\n",
      "Iteration 40286, loss = 0.03258890\n",
      "Iteration 40287, loss = 0.03258262\n",
      "Iteration 40288, loss = 0.03257425\n",
      "Iteration 40289, loss = 0.03256946\n",
      "Iteration 40290, loss = 0.03258194\n",
      "Iteration 40291, loss = 0.03258906\n",
      "Iteration 40292, loss = 0.03258847\n",
      "Iteration 40293, loss = 0.03257457\n",
      "Iteration 40294, loss = 0.03258173\n",
      "Iteration 40295, loss = 0.03258775\n",
      "Iteration 40296, loss = 0.03259022\n",
      "Iteration 40297, loss = 0.03258560\n",
      "Iteration 40298, loss = 0.03257952\n",
      "Iteration 40299, loss = 0.03258234\n",
      "Iteration 40300, loss = 0.03257713\n",
      "Iteration 40301, loss = 0.03258379\n",
      "Iteration 40302, loss = 0.03257731\n",
      "Iteration 40303, loss = 0.03256060\n",
      "Iteration 40304, loss = 0.03256866\n",
      "Iteration 40305, loss = 0.03256911\n",
      "Iteration 40306, loss = 0.03257418\n",
      "Iteration 40307, loss = 0.03256893\n",
      "Iteration 40308, loss = 0.03257144\n",
      "Iteration 40309, loss = 0.03257627\n",
      "Iteration 40310, loss = 0.03256921\n",
      "Iteration 40311, loss = 0.03256535\n",
      "Iteration 40312, loss = 0.03257665\n",
      "Iteration 40313, loss = 0.03257925\n",
      "Iteration 40314, loss = 0.03258267\n",
      "Iteration 40315, loss = 0.03257841\n",
      "Iteration 40316, loss = 0.03257263\n",
      "Iteration 40317, loss = 0.03256958\n",
      "Iteration 40318, loss = 0.03256557\n",
      "Iteration 40319, loss = 0.03255718\n",
      "Iteration 40320, loss = 0.03255555\n",
      "Iteration 40321, loss = 0.03256001\n",
      "Iteration 40322, loss = 0.03255621\n",
      "Iteration 40323, loss = 0.03255646\n",
      "Iteration 40324, loss = 0.03255411\n",
      "Iteration 40325, loss = 0.03256254\n",
      "Iteration 40326, loss = 0.03256559\n",
      "Iteration 40327, loss = 0.03255389\n",
      "Iteration 40328, loss = 0.03256126\n",
      "Iteration 40329, loss = 0.03256963\n",
      "Iteration 40330, loss = 0.03256076\n",
      "Iteration 40331, loss = 0.03255810\n",
      "Iteration 40332, loss = 0.03255972\n",
      "Iteration 40333, loss = 0.03255535\n",
      "Iteration 40334, loss = 0.03256350\n",
      "Iteration 40335, loss = 0.03255398\n",
      "Iteration 40336, loss = 0.03255077\n",
      "Iteration 40337, loss = 0.03255260\n",
      "Iteration 40338, loss = 0.03255065\n",
      "Iteration 40339, loss = 0.03255149\n",
      "Iteration 40340, loss = 0.03255795\n",
      "Iteration 40341, loss = 0.03255322\n",
      "Iteration 40342, loss = 0.03255810\n",
      "Iteration 40343, loss = 0.03255883\n",
      "Iteration 40344, loss = 0.03255292\n",
      "Iteration 40345, loss = 0.03255006\n",
      "Iteration 40346, loss = 0.03254760\n",
      "Iteration 40347, loss = 0.03255632\n",
      "Iteration 40348, loss = 0.03255397\n",
      "Iteration 40349, loss = 0.03254967\n",
      "Iteration 40350, loss = 0.03255791\n",
      "Iteration 40351, loss = 0.03255678\n",
      "Iteration 40352, loss = 0.03255993\n",
      "Iteration 40353, loss = 0.03256208\n",
      "Iteration 40354, loss = 0.03255468\n",
      "Iteration 40355, loss = 0.03254385\n",
      "Iteration 40356, loss = 0.03255203\n",
      "Iteration 40357, loss = 0.03256790\n",
      "Iteration 40358, loss = 0.03256347\n",
      "Iteration 40359, loss = 0.03255689\n",
      "Iteration 40360, loss = 0.03254979\n",
      "Iteration 40361, loss = 0.03255370\n",
      "Iteration 40362, loss = 0.03255152\n",
      "Iteration 40363, loss = 0.03254684\n",
      "Iteration 40364, loss = 0.03254058\n",
      "Iteration 40365, loss = 0.03254732\n",
      "Iteration 40366, loss = 0.03255807\n",
      "Iteration 40367, loss = 0.03255196\n",
      "Iteration 40368, loss = 0.03254537\n",
      "Iteration 40369, loss = 0.03254162\n",
      "Iteration 40370, loss = 0.03255337\n",
      "Iteration 40371, loss = 0.03256050\n",
      "Iteration 40372, loss = 0.03254762\n",
      "Iteration 40373, loss = 0.03253955\n",
      "Iteration 40374, loss = 0.03255315\n",
      "Iteration 40375, loss = 0.03255367\n",
      "Iteration 40376, loss = 0.03255994\n",
      "Iteration 40377, loss = 0.03255916\n",
      "Iteration 40378, loss = 0.03254618\n",
      "Iteration 40379, loss = 0.03254359\n",
      "Iteration 40380, loss = 0.03255592\n",
      "Iteration 40381, loss = 0.03256625\n",
      "Iteration 40382, loss = 0.03256536\n",
      "Iteration 40383, loss = 0.03254877\n",
      "Iteration 40384, loss = 0.03254059\n",
      "Iteration 40385, loss = 0.03256012\n",
      "Iteration 40386, loss = 0.03256481\n",
      "Iteration 40387, loss = 0.03256574\n",
      "Iteration 40388, loss = 0.03256222\n",
      "Iteration 40389, loss = 0.03256394\n",
      "Iteration 40390, loss = 0.03256013\n",
      "Iteration 40391, loss = 0.03254925\n",
      "Iteration 40392, loss = 0.03255741\n",
      "Iteration 40393, loss = 0.03255825\n",
      "Iteration 40394, loss = 0.03257020\n",
      "Iteration 40395, loss = 0.03256300\n",
      "Iteration 40396, loss = 0.03255894\n",
      "Iteration 40397, loss = 0.03254456\n",
      "Iteration 40398, loss = 0.03255027\n",
      "Iteration 40399, loss = 0.03254615\n",
      "Iteration 40400, loss = 0.03254191\n",
      "Iteration 40401, loss = 0.03254174\n",
      "Iteration 40402, loss = 0.03253863\n",
      "Iteration 40403, loss = 0.03254926\n",
      "Iteration 40404, loss = 0.03254966\n",
      "Iteration 40405, loss = 0.03254867\n",
      "Iteration 40406, loss = 0.03253512\n",
      "Iteration 40407, loss = 0.03252993\n",
      "Iteration 40408, loss = 0.03254461\n",
      "Iteration 40409, loss = 0.03254122\n",
      "Iteration 40410, loss = 0.03253080\n",
      "Iteration 40411, loss = 0.03252838\n",
      "Iteration 40412, loss = 0.03253194\n",
      "Iteration 40413, loss = 0.03252787\n",
      "Iteration 40414, loss = 0.03253306\n",
      "Iteration 40415, loss = 0.03253452\n",
      "Iteration 40416, loss = 0.03252516\n",
      "Iteration 40417, loss = 0.03252852\n",
      "Iteration 40418, loss = 0.03253757\n",
      "Iteration 40419, loss = 0.03253317\n",
      "Iteration 40420, loss = 0.03251832\n",
      "Iteration 40421, loss = 0.03252379\n",
      "Iteration 40422, loss = 0.03252573\n",
      "Iteration 40423, loss = 0.03253221\n",
      "Iteration 40424, loss = 0.03252999\n",
      "Iteration 40425, loss = 0.03253658\n",
      "Iteration 40426, loss = 0.03253933\n",
      "Iteration 40427, loss = 0.03252374\n",
      "Iteration 40428, loss = 0.03252931\n",
      "Iteration 40429, loss = 0.03254372\n",
      "Iteration 40430, loss = 0.03254124\n",
      "Iteration 40431, loss = 0.03252344\n",
      "Iteration 40432, loss = 0.03252996\n",
      "Iteration 40433, loss = 0.03254212\n",
      "Iteration 40434, loss = 0.03254132\n",
      "Iteration 40435, loss = 0.03253311\n",
      "Iteration 40436, loss = 0.03253209\n",
      "Iteration 40437, loss = 0.03252689\n",
      "Iteration 40438, loss = 0.03251876\n",
      "Iteration 40439, loss = 0.03251814\n",
      "Iteration 40440, loss = 0.03252936\n",
      "Iteration 40441, loss = 0.03253131\n",
      "Iteration 40442, loss = 0.03251619\n",
      "Iteration 40443, loss = 0.03252895\n",
      "Iteration 40444, loss = 0.03253066\n",
      "Iteration 40445, loss = 0.03252490\n",
      "Iteration 40446, loss = 0.03252535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 40447, loss = 0.03252102\n",
      "Iteration 40448, loss = 0.03250943\n",
      "Iteration 40449, loss = 0.03252456\n",
      "Iteration 40450, loss = 0.03253047\n",
      "Iteration 40451, loss = 0.03251969\n",
      "Iteration 40452, loss = 0.03251148\n",
      "Iteration 40453, loss = 0.03252323\n",
      "Iteration 40454, loss = 0.03253179\n",
      "Iteration 40455, loss = 0.03252694\n",
      "Iteration 40456, loss = 0.03251633\n",
      "Iteration 40457, loss = 0.03252086\n",
      "Iteration 40458, loss = 0.03252800\n",
      "Iteration 40459, loss = 0.03254207\n",
      "Iteration 40460, loss = 0.03253805\n",
      "Iteration 40461, loss = 0.03252734\n",
      "Iteration 40462, loss = 0.03251907\n",
      "Iteration 40463, loss = 0.03252809\n",
      "Iteration 40464, loss = 0.03253110\n",
      "Iteration 40465, loss = 0.03253795\n",
      "Iteration 40466, loss = 0.03252734\n",
      "Iteration 40467, loss = 0.03251568\n",
      "Iteration 40468, loss = 0.03251225\n",
      "Iteration 40469, loss = 0.03250753\n",
      "Iteration 40470, loss = 0.03250445\n",
      "Iteration 40471, loss = 0.03250957\n",
      "Iteration 40472, loss = 0.03251362\n",
      "Iteration 40473, loss = 0.03250328\n",
      "Iteration 40474, loss = 0.03250808\n",
      "Iteration 40475, loss = 0.03251554\n",
      "Iteration 40476, loss = 0.03251694\n",
      "Iteration 40477, loss = 0.03251031\n",
      "Iteration 40478, loss = 0.03250267\n",
      "Iteration 40479, loss = 0.03250884\n",
      "Iteration 40480, loss = 0.03250773\n",
      "Iteration 40481, loss = 0.03250928\n",
      "Iteration 40482, loss = 0.03250703\n",
      "Iteration 40483, loss = 0.03251248\n",
      "Iteration 40484, loss = 0.03250807\n",
      "Iteration 40485, loss = 0.03250764\n",
      "Iteration 40486, loss = 0.03251133\n",
      "Iteration 40487, loss = 0.03250475\n",
      "Iteration 40488, loss = 0.03251047\n",
      "Iteration 40489, loss = 0.03251042\n",
      "Iteration 40490, loss = 0.03249936\n",
      "Iteration 40491, loss = 0.03249756\n",
      "Iteration 40492, loss = 0.03251343\n",
      "Iteration 40493, loss = 0.03251162\n",
      "Iteration 40494, loss = 0.03250840\n",
      "Iteration 40495, loss = 0.03249846\n",
      "Iteration 40496, loss = 0.03250753\n",
      "Iteration 40497, loss = 0.03251909\n",
      "Iteration 40498, loss = 0.03251231\n",
      "Iteration 40499, loss = 0.03250461\n",
      "Iteration 40500, loss = 0.03251456\n",
      "Iteration 40501, loss = 0.03251145\n",
      "Iteration 40502, loss = 0.03250870\n",
      "Iteration 40503, loss = 0.03249571\n",
      "Iteration 40504, loss = 0.03250405\n",
      "Iteration 40505, loss = 0.03250887\n",
      "Iteration 40506, loss = 0.03251789\n",
      "Iteration 40507, loss = 0.03251675\n",
      "Iteration 40508, loss = 0.03250699\n",
      "Iteration 40509, loss = 0.03250007\n",
      "Iteration 40510, loss = 0.03250376\n",
      "Iteration 40511, loss = 0.03251558\n",
      "Iteration 40512, loss = 0.03251697\n",
      "Iteration 40513, loss = 0.03251337\n",
      "Iteration 40514, loss = 0.03250874\n",
      "Iteration 40515, loss = 0.03249688\n",
      "Iteration 40516, loss = 0.03249994\n",
      "Iteration 40517, loss = 0.03250800\n",
      "Iteration 40518, loss = 0.03250638\n",
      "Iteration 40519, loss = 0.03250155\n",
      "Iteration 40520, loss = 0.03248840\n",
      "Iteration 40521, loss = 0.03248612\n",
      "Iteration 40522, loss = 0.03248372\n",
      "Iteration 40523, loss = 0.03248123\n",
      "Iteration 40524, loss = 0.03249243\n",
      "Iteration 40525, loss = 0.03248829\n",
      "Iteration 40526, loss = 0.03248410\n",
      "Iteration 40527, loss = 0.03248626\n",
      "Iteration 40528, loss = 0.03249228\n",
      "Iteration 40529, loss = 0.03249224\n",
      "Iteration 40530, loss = 0.03248774\n",
      "Iteration 40531, loss = 0.03248467\n",
      "Iteration 40532, loss = 0.03248609\n",
      "Iteration 40533, loss = 0.03248013\n",
      "Iteration 40534, loss = 0.03248172\n",
      "Iteration 40535, loss = 0.03248185\n",
      "Iteration 40536, loss = 0.03248220\n",
      "Iteration 40537, loss = 0.03248092\n",
      "Iteration 40538, loss = 0.03248899\n",
      "Iteration 40539, loss = 0.03249140\n",
      "Iteration 40540, loss = 0.03248739\n",
      "Iteration 40541, loss = 0.03248021\n",
      "Iteration 40542, loss = 0.03248650\n",
      "Iteration 40543, loss = 0.03248792\n",
      "Iteration 40544, loss = 0.03247522\n",
      "Iteration 40545, loss = 0.03248513\n",
      "Iteration 40546, loss = 0.03249339\n",
      "Iteration 40547, loss = 0.03249534\n",
      "Iteration 40548, loss = 0.03248782\n",
      "Iteration 40549, loss = 0.03247770\n",
      "Iteration 40550, loss = 0.03248476\n",
      "Iteration 40551, loss = 0.03248906\n",
      "Iteration 40552, loss = 0.03249122\n",
      "Iteration 40553, loss = 0.03249137\n",
      "Iteration 40554, loss = 0.03248595\n",
      "Iteration 40555, loss = 0.03248623\n",
      "Iteration 40556, loss = 0.03249391\n",
      "Iteration 40557, loss = 0.03249316\n",
      "Iteration 40558, loss = 0.03248413\n",
      "Iteration 40559, loss = 0.03247684\n",
      "Iteration 40560, loss = 0.03249519\n",
      "Iteration 40561, loss = 0.03249157\n",
      "Iteration 40562, loss = 0.03249161\n",
      "Iteration 40563, loss = 0.03248220\n",
      "Iteration 40564, loss = 0.03248414\n",
      "Iteration 40565, loss = 0.03247497\n",
      "Iteration 40566, loss = 0.03247942\n",
      "Iteration 40567, loss = 0.03248303\n",
      "Iteration 40568, loss = 0.03248900\n",
      "Iteration 40569, loss = 0.03247924\n",
      "Iteration 40570, loss = 0.03247074\n",
      "Iteration 40571, loss = 0.03246868\n",
      "Iteration 40572, loss = 0.03246936\n",
      "Iteration 40573, loss = 0.03246905\n",
      "Iteration 40574, loss = 0.03247762\n",
      "Iteration 40575, loss = 0.03247446\n",
      "Iteration 40576, loss = 0.03246544\n",
      "Iteration 40577, loss = 0.03247614\n",
      "Iteration 40578, loss = 0.03247500\n",
      "Iteration 40579, loss = 0.03246445\n",
      "Iteration 40580, loss = 0.03247447\n",
      "Iteration 40581, loss = 0.03247463\n",
      "Iteration 40582, loss = 0.03247165\n",
      "Iteration 40583, loss = 0.03247522\n",
      "Iteration 40584, loss = 0.03247532\n",
      "Iteration 40585, loss = 0.03247117\n",
      "Iteration 40586, loss = 0.03246531\n",
      "Iteration 40587, loss = 0.03246262\n",
      "Iteration 40588, loss = 0.03246418\n",
      "Iteration 40589, loss = 0.03246791\n",
      "Iteration 40590, loss = 0.03247736\n",
      "Iteration 40591, loss = 0.03247190\n",
      "Iteration 40592, loss = 0.03245878\n",
      "Iteration 40593, loss = 0.03246371\n",
      "Iteration 40594, loss = 0.03246537\n",
      "Iteration 40595, loss = 0.03246357\n",
      "Iteration 40596, loss = 0.03246192\n",
      "Iteration 40597, loss = 0.03247157\n",
      "Iteration 40598, loss = 0.03246852\n",
      "Iteration 40599, loss = 0.03247230\n",
      "Iteration 40600, loss = 0.03246880\n",
      "Iteration 40601, loss = 0.03246363\n",
      "Iteration 40602, loss = 0.03246032\n",
      "Iteration 40603, loss = 0.03246344\n",
      "Iteration 40604, loss = 0.03246937\n",
      "Iteration 40605, loss = 0.03246551\n",
      "Iteration 40606, loss = 0.03246037\n",
      "Iteration 40607, loss = 0.03246846\n",
      "Iteration 40608, loss = 0.03247575\n",
      "Iteration 40609, loss = 0.03247625\n",
      "Iteration 40610, loss = 0.03247345\n",
      "Iteration 40611, loss = 0.03246863\n",
      "Iteration 40612, loss = 0.03245994\n",
      "Iteration 40613, loss = 0.03247469\n",
      "Iteration 40614, loss = 0.03248612\n",
      "Iteration 40615, loss = 0.03248168\n",
      "Iteration 40616, loss = 0.03246074\n",
      "Iteration 40617, loss = 0.03246328\n",
      "Iteration 40618, loss = 0.03247964\n",
      "Iteration 40619, loss = 0.03247847\n",
      "Iteration 40620, loss = 0.03248955\n",
      "Iteration 40621, loss = 0.03249027\n",
      "Iteration 40622, loss = 0.03248010\n",
      "Iteration 40623, loss = 0.03246982\n",
      "Iteration 40624, loss = 0.03246107\n",
      "Iteration 40625, loss = 0.03247531\n",
      "Iteration 40626, loss = 0.03248433\n",
      "Iteration 40627, loss = 0.03248023\n",
      "Iteration 40628, loss = 0.03247611\n",
      "Iteration 40629, loss = 0.03246174\n",
      "Iteration 40630, loss = 0.03246323\n",
      "Iteration 40631, loss = 0.03247060\n",
      "Iteration 40632, loss = 0.03247075\n",
      "Iteration 40633, loss = 0.03247015\n",
      "Iteration 40634, loss = 0.03245906\n",
      "Iteration 40635, loss = 0.03244441\n",
      "Iteration 40636, loss = 0.03245079\n",
      "Iteration 40637, loss = 0.03245231\n",
      "Iteration 40638, loss = 0.03245296\n",
      "Iteration 40639, loss = 0.03245645\n",
      "Iteration 40640, loss = 0.03245091\n",
      "Iteration 40641, loss = 0.03244271\n",
      "Iteration 40642, loss = 0.03244437\n",
      "Iteration 40643, loss = 0.03244495\n",
      "Iteration 40644, loss = 0.03243719\n",
      "Iteration 40645, loss = 0.03243886\n",
      "Iteration 40646, loss = 0.03244092\n",
      "Iteration 40647, loss = 0.03243848\n",
      "Iteration 40648, loss = 0.03244016\n",
      "Iteration 40649, loss = 0.03244121\n",
      "Iteration 40650, loss = 0.03245528\n",
      "Iteration 40651, loss = 0.03245497\n",
      "Iteration 40652, loss = 0.03244410\n",
      "Iteration 40653, loss = 0.03243814\n",
      "Iteration 40654, loss = 0.03244840\n",
      "Iteration 40655, loss = 0.03244825\n",
      "Iteration 40656, loss = 0.03243865\n",
      "Iteration 40657, loss = 0.03243915\n",
      "Iteration 40658, loss = 0.03244132\n",
      "Iteration 40659, loss = 0.03243725\n",
      "Iteration 40660, loss = 0.03244980\n",
      "Iteration 40661, loss = 0.03244636\n",
      "Iteration 40662, loss = 0.03243535\n",
      "Iteration 40663, loss = 0.03243506\n",
      "Iteration 40664, loss = 0.03243455\n",
      "Iteration 40665, loss = 0.03243470\n",
      "Iteration 40666, loss = 0.03243639\n",
      "Iteration 40667, loss = 0.03243606\n",
      "Iteration 40668, loss = 0.03243859\n",
      "Iteration 40669, loss = 0.03243145\n",
      "Iteration 40670, loss = 0.03243547\n",
      "Iteration 40671, loss = 0.03243724\n",
      "Iteration 40672, loss = 0.03243297\n",
      "Iteration 40673, loss = 0.03243287\n",
      "Iteration 40674, loss = 0.03243987\n",
      "Iteration 40675, loss = 0.03243366\n",
      "Iteration 40676, loss = 0.03243202\n",
      "Iteration 40677, loss = 0.03243669\n",
      "Iteration 40678, loss = 0.03244115\n",
      "Iteration 40679, loss = 0.03244466\n",
      "Iteration 40680, loss = 0.03244538\n",
      "Iteration 40681, loss = 0.03245086\n",
      "Iteration 40682, loss = 0.03244767\n",
      "Iteration 40683, loss = 0.03244177\n",
      "Iteration 40684, loss = 0.03244152\n",
      "Iteration 40685, loss = 0.03244557\n",
      "Iteration 40686, loss = 0.03244356\n",
      "Iteration 40687, loss = 0.03243188\n",
      "Iteration 40688, loss = 0.03243362\n",
      "Iteration 40689, loss = 0.03244121\n",
      "Iteration 40690, loss = 0.03243693\n",
      "Iteration 40691, loss = 0.03243490\n",
      "Iteration 40692, loss = 0.03244151\n",
      "Iteration 40693, loss = 0.03243855\n",
      "Iteration 40694, loss = 0.03242275\n",
      "Iteration 40695, loss = 0.03243236\n",
      "Iteration 40696, loss = 0.03244650\n",
      "Iteration 40697, loss = 0.03244560\n",
      "Iteration 40698, loss = 0.03243315\n",
      "Iteration 40699, loss = 0.03243330\n",
      "Iteration 40700, loss = 0.03243013\n",
      "Iteration 40701, loss = 0.03243659\n",
      "Iteration 40702, loss = 0.03243651\n",
      "Iteration 40703, loss = 0.03242575\n",
      "Iteration 40704, loss = 0.03242991\n",
      "Iteration 40705, loss = 0.03243587\n",
      "Iteration 40706, loss = 0.03242925\n",
      "Iteration 40707, loss = 0.03242902\n",
      "Iteration 40708, loss = 0.03242928\n",
      "Iteration 40709, loss = 0.03243817\n",
      "Iteration 40710, loss = 0.03243277\n",
      "Iteration 40711, loss = 0.03241872\n",
      "Iteration 40712, loss = 0.03243624\n",
      "Iteration 40713, loss = 0.03244492\n",
      "Iteration 40714, loss = 0.03243643\n",
      "Iteration 40715, loss = 0.03242675\n",
      "Iteration 40716, loss = 0.03241923\n",
      "Iteration 40717, loss = 0.03243421\n",
      "Iteration 40718, loss = 0.03243896\n",
      "Iteration 40719, loss = 0.03242987\n",
      "Iteration 40720, loss = 0.03241514\n",
      "Iteration 40721, loss = 0.03242410\n",
      "Iteration 40722, loss = 0.03243689\n",
      "Iteration 40723, loss = 0.03243584\n",
      "Iteration 40724, loss = 0.03242197\n",
      "Iteration 40725, loss = 0.03242775\n",
      "Iteration 40726, loss = 0.03243549\n",
      "Iteration 40727, loss = 0.03243561\n",
      "Iteration 40728, loss = 0.03243449\n",
      "Iteration 40729, loss = 0.03243001\n",
      "Iteration 40730, loss = 0.03241257\n",
      "Iteration 40731, loss = 0.03241679\n",
      "Iteration 40732, loss = 0.03242046\n",
      "Iteration 40733, loss = 0.03242114\n",
      "Iteration 40734, loss = 0.03242033\n",
      "Iteration 40735, loss = 0.03241615\n",
      "Iteration 40736, loss = 0.03240695\n",
      "Iteration 40737, loss = 0.03241025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 40738, loss = 0.03241992\n",
      "Iteration 40739, loss = 0.03242294\n",
      "Iteration 40740, loss = 0.03240442\n",
      "Iteration 40741, loss = 0.03241665\n",
      "Iteration 40742, loss = 0.03242710\n",
      "Iteration 40743, loss = 0.03242606\n",
      "Iteration 40744, loss = 0.03243174\n",
      "Iteration 40745, loss = 0.03242851\n",
      "Iteration 40746, loss = 0.03241954\n",
      "Iteration 40747, loss = 0.03241004\n",
      "Iteration 40748, loss = 0.03240465\n",
      "Iteration 40749, loss = 0.03242304\n",
      "Iteration 40750, loss = 0.03242398\n",
      "Iteration 40751, loss = 0.03241928\n",
      "Iteration 40752, loss = 0.03241623\n",
      "Iteration 40753, loss = 0.03240392\n",
      "Iteration 40754, loss = 0.03240763\n",
      "Iteration 40755, loss = 0.03240895\n",
      "Iteration 40756, loss = 0.03241112\n",
      "Iteration 40757, loss = 0.03240366\n",
      "Iteration 40758, loss = 0.03241424\n",
      "Iteration 40759, loss = 0.03241577\n",
      "Iteration 40760, loss = 0.03241317\n",
      "Iteration 40761, loss = 0.03241141\n",
      "Iteration 40762, loss = 0.03240625\n",
      "Iteration 40763, loss = 0.03241201\n",
      "Iteration 40764, loss = 0.03241457\n",
      "Iteration 40765, loss = 0.03240905\n",
      "Iteration 40766, loss = 0.03240447\n",
      "Iteration 40767, loss = 0.03240469\n",
      "Iteration 40768, loss = 0.03241175\n",
      "Iteration 40769, loss = 0.03240922\n",
      "Iteration 40770, loss = 0.03239735\n",
      "Iteration 40771, loss = 0.03241167\n",
      "Iteration 40772, loss = 0.03241250\n",
      "Iteration 40773, loss = 0.03240637\n",
      "Iteration 40774, loss = 0.03240674\n",
      "Iteration 40775, loss = 0.03240720\n",
      "Iteration 40776, loss = 0.03240766\n",
      "Iteration 40777, loss = 0.03241277\n",
      "Iteration 40778, loss = 0.03241589\n",
      "Iteration 40779, loss = 0.03240986\n",
      "Iteration 40780, loss = 0.03239019\n",
      "Iteration 40781, loss = 0.03241330\n",
      "Iteration 40782, loss = 0.03242626\n",
      "Iteration 40783, loss = 0.03242191\n",
      "Iteration 40784, loss = 0.03240434\n",
      "Iteration 40785, loss = 0.03240413\n",
      "Iteration 40786, loss = 0.03241435\n",
      "Iteration 40787, loss = 0.03240944\n",
      "Iteration 40788, loss = 0.03241955\n",
      "Iteration 40789, loss = 0.03242944\n",
      "Iteration 40790, loss = 0.03242340\n",
      "Iteration 40791, loss = 0.03241006\n",
      "Iteration 40792, loss = 0.03239693\n",
      "Iteration 40793, loss = 0.03239573\n",
      "Iteration 40794, loss = 0.03242197\n",
      "Iteration 40795, loss = 0.03242464\n",
      "Iteration 40796, loss = 0.03241123\n",
      "Iteration 40797, loss = 0.03239958\n",
      "Iteration 40798, loss = 0.03240108\n",
      "Iteration 40799, loss = 0.03240619\n",
      "Iteration 40800, loss = 0.03240898\n",
      "Iteration 40801, loss = 0.03240829\n",
      "Iteration 40802, loss = 0.03241175\n",
      "Iteration 40803, loss = 0.03240441\n",
      "Iteration 40804, loss = 0.03240758\n",
      "Iteration 40805, loss = 0.03241491\n",
      "Iteration 40806, loss = 0.03241427\n",
      "Iteration 40807, loss = 0.03239289\n",
      "Iteration 40808, loss = 0.03239498\n",
      "Iteration 40809, loss = 0.03241493\n",
      "Iteration 40810, loss = 0.03242577\n",
      "Iteration 40811, loss = 0.03241537\n",
      "Iteration 40812, loss = 0.03239426\n",
      "Iteration 40813, loss = 0.03238636\n",
      "Iteration 40814, loss = 0.03240014\n",
      "Iteration 40815, loss = 0.03240523\n",
      "Iteration 40816, loss = 0.03239650\n",
      "Iteration 40817, loss = 0.03240399\n",
      "Iteration 40818, loss = 0.03239502\n",
      "Iteration 40819, loss = 0.03239766\n",
      "Iteration 40820, loss = 0.03239239\n",
      "Iteration 40821, loss = 0.03239069\n",
      "Iteration 40822, loss = 0.03239083\n",
      "Iteration 40823, loss = 0.03238331\n",
      "Iteration 40824, loss = 0.03239116\n",
      "Iteration 40825, loss = 0.03240123\n",
      "Iteration 40826, loss = 0.03240633\n",
      "Iteration 40827, loss = 0.03238775\n",
      "Iteration 40828, loss = 0.03239048\n",
      "Iteration 40829, loss = 0.03239449\n",
      "Iteration 40830, loss = 0.03239971\n",
      "Iteration 40831, loss = 0.03239587\n",
      "Iteration 40832, loss = 0.03239645\n",
      "Iteration 40833, loss = 0.03240406\n",
      "Iteration 40834, loss = 0.03239252\n",
      "Iteration 40835, loss = 0.03238404\n",
      "Iteration 40836, loss = 0.03238398\n",
      "Iteration 40837, loss = 0.03239278\n",
      "Iteration 40838, loss = 0.03239174\n",
      "Iteration 40839, loss = 0.03238969\n",
      "Iteration 40840, loss = 0.03238107\n",
      "Iteration 40841, loss = 0.03237621\n",
      "Iteration 40842, loss = 0.03239017\n",
      "Iteration 40843, loss = 0.03238519\n",
      "Iteration 40844, loss = 0.03237810\n",
      "Iteration 40845, loss = 0.03238809\n",
      "Iteration 40846, loss = 0.03239658\n",
      "Iteration 40847, loss = 0.03238698\n",
      "Iteration 40848, loss = 0.03239040\n",
      "Iteration 40849, loss = 0.03239946\n",
      "Iteration 40850, loss = 0.03239096\n",
      "Iteration 40851, loss = 0.03238340\n",
      "Iteration 40852, loss = 0.03238997\n",
      "Iteration 40853, loss = 0.03239005\n",
      "Iteration 40854, loss = 0.03239186\n",
      "Iteration 40855, loss = 0.03238981\n",
      "Iteration 40856, loss = 0.03237699\n",
      "Iteration 40857, loss = 0.03237815\n",
      "Iteration 40858, loss = 0.03238936\n",
      "Iteration 40859, loss = 0.03239108\n",
      "Iteration 40860, loss = 0.03237942\n",
      "Iteration 40861, loss = 0.03238267\n",
      "Iteration 40862, loss = 0.03237398\n",
      "Iteration 40863, loss = 0.03238414\n",
      "Iteration 40864, loss = 0.03238472\n",
      "Iteration 40865, loss = 0.03239066\n",
      "Iteration 40866, loss = 0.03240097\n",
      "Iteration 40867, loss = 0.03238837\n",
      "Iteration 40868, loss = 0.03237847\n",
      "Iteration 40869, loss = 0.03237542\n",
      "Iteration 40870, loss = 0.03239430\n",
      "Iteration 40871, loss = 0.03240250\n",
      "Iteration 40872, loss = 0.03239776\n",
      "Iteration 40873, loss = 0.03238637\n",
      "Iteration 40874, loss = 0.03237363\n",
      "Iteration 40875, loss = 0.03235761\n",
      "Iteration 40876, loss = 0.03238449\n",
      "Iteration 40877, loss = 0.03239686\n",
      "Iteration 40878, loss = 0.03238897\n",
      "Iteration 40879, loss = 0.03236738\n",
      "Iteration 40880, loss = 0.03236722\n",
      "Iteration 40881, loss = 0.03238191\n",
      "Iteration 40882, loss = 0.03239788\n",
      "Iteration 40883, loss = 0.03239806\n",
      "Iteration 40884, loss = 0.03239112\n",
      "Iteration 40885, loss = 0.03237882\n",
      "Iteration 40886, loss = 0.03237517\n",
      "Iteration 40887, loss = 0.03237192\n",
      "Iteration 40888, loss = 0.03235943\n",
      "Iteration 40889, loss = 0.03236499\n",
      "Iteration 40890, loss = 0.03236491\n",
      "Iteration 40891, loss = 0.03236211\n",
      "Iteration 40892, loss = 0.03236758\n",
      "Iteration 40893, loss = 0.03237310\n",
      "Iteration 40894, loss = 0.03236721\n",
      "Iteration 40895, loss = 0.03236176\n",
      "Iteration 40896, loss = 0.03235664\n",
      "Iteration 40897, loss = 0.03236490\n",
      "Iteration 40898, loss = 0.03236749\n",
      "Iteration 40899, loss = 0.03236031\n",
      "Iteration 40900, loss = 0.03235224\n",
      "Iteration 40901, loss = 0.03235605\n",
      "Iteration 40902, loss = 0.03235464\n",
      "Iteration 40903, loss = 0.03235801\n",
      "Iteration 40904, loss = 0.03235739\n",
      "Iteration 40905, loss = 0.03235028\n",
      "Iteration 40906, loss = 0.03235176\n",
      "Iteration 40907, loss = 0.03235720\n",
      "Iteration 40908, loss = 0.03235513\n",
      "Iteration 40909, loss = 0.03234831\n",
      "Iteration 40910, loss = 0.03234733\n",
      "Iteration 40911, loss = 0.03234812\n",
      "Iteration 40912, loss = 0.03234506\n",
      "Iteration 40913, loss = 0.03234392\n",
      "Iteration 40914, loss = 0.03235334\n",
      "Iteration 40915, loss = 0.03235052\n",
      "Iteration 40916, loss = 0.03234478\n",
      "Iteration 40917, loss = 0.03235450\n",
      "Iteration 40918, loss = 0.03235690\n",
      "Iteration 40919, loss = 0.03234987\n",
      "Iteration 40920, loss = 0.03234131\n",
      "Iteration 40921, loss = 0.03236315\n",
      "Iteration 40922, loss = 0.03236160\n",
      "Iteration 40923, loss = 0.03234239\n",
      "Iteration 40924, loss = 0.03235447\n",
      "Iteration 40925, loss = 0.03236237\n",
      "Iteration 40926, loss = 0.03235356\n",
      "Iteration 40927, loss = 0.03235844\n",
      "Iteration 40928, loss = 0.03236305\n",
      "Iteration 40929, loss = 0.03235362\n",
      "Iteration 40930, loss = 0.03234639\n",
      "Iteration 40931, loss = 0.03236482\n",
      "Iteration 40932, loss = 0.03237256\n",
      "Iteration 40933, loss = 0.03236227\n",
      "Iteration 40934, loss = 0.03234114\n",
      "Iteration 40935, loss = 0.03234594\n",
      "Iteration 40936, loss = 0.03235795\n",
      "Iteration 40937, loss = 0.03235539\n",
      "Iteration 40938, loss = 0.03235139\n",
      "Iteration 40939, loss = 0.03234972\n",
      "Iteration 40940, loss = 0.03235146\n",
      "Iteration 40941, loss = 0.03234365\n",
      "Iteration 40942, loss = 0.03234147\n",
      "Iteration 40943, loss = 0.03234004\n",
      "Iteration 40944, loss = 0.03234201\n",
      "Iteration 40945, loss = 0.03236307\n",
      "Iteration 40946, loss = 0.03235980\n",
      "Iteration 40947, loss = 0.03234395\n",
      "Iteration 40948, loss = 0.03233693\n",
      "Iteration 40949, loss = 0.03234756\n",
      "Iteration 40950, loss = 0.03234482\n",
      "Iteration 40951, loss = 0.03234065\n",
      "Iteration 40952, loss = 0.03233767\n",
      "Iteration 40953, loss = 0.03234367\n",
      "Iteration 40954, loss = 0.03234523\n",
      "Iteration 40955, loss = 0.03233528\n",
      "Iteration 40956, loss = 0.03233320\n",
      "Iteration 40957, loss = 0.03233453\n",
      "Iteration 40958, loss = 0.03232924\n",
      "Iteration 40959, loss = 0.03233303\n",
      "Iteration 40960, loss = 0.03233594\n",
      "Iteration 40961, loss = 0.03233014\n",
      "Iteration 40962, loss = 0.03233396\n",
      "Iteration 40963, loss = 0.03233460\n",
      "Iteration 40964, loss = 0.03234527\n",
      "Iteration 40965, loss = 0.03233986\n",
      "Iteration 40966, loss = 0.03233922\n",
      "Iteration 40967, loss = 0.03234710\n",
      "Iteration 40968, loss = 0.03234797\n",
      "Iteration 40969, loss = 0.03233387\n",
      "Iteration 40970, loss = 0.03234628\n",
      "Iteration 40971, loss = 0.03235484\n",
      "Iteration 40972, loss = 0.03235316\n",
      "Iteration 40973, loss = 0.03234239\n",
      "Iteration 40974, loss = 0.03233629\n",
      "Iteration 40975, loss = 0.03232493\n",
      "Iteration 40976, loss = 0.03233605\n",
      "Iteration 40977, loss = 0.03234231\n",
      "Iteration 40978, loss = 0.03234181\n",
      "Iteration 40979, loss = 0.03233935\n",
      "Iteration 40980, loss = 0.03233077\n",
      "Iteration 40981, loss = 0.03233519\n",
      "Iteration 40982, loss = 0.03233146\n",
      "Iteration 40983, loss = 0.03232795\n",
      "Iteration 40984, loss = 0.03233492\n",
      "Iteration 40985, loss = 0.03234246\n",
      "Iteration 40986, loss = 0.03233626\n",
      "Iteration 40987, loss = 0.03233499\n",
      "Iteration 40988, loss = 0.03234874\n",
      "Iteration 40989, loss = 0.03235168\n",
      "Iteration 40990, loss = 0.03234610\n",
      "Iteration 40991, loss = 0.03233657\n",
      "Iteration 40992, loss = 0.03233719\n",
      "Iteration 40993, loss = 0.03233565\n",
      "Iteration 40994, loss = 0.03233173\n",
      "Iteration 40995, loss = 0.03233542\n",
      "Iteration 40996, loss = 0.03234077\n",
      "Iteration 40997, loss = 0.03232891\n",
      "Iteration 40998, loss = 0.03232170\n",
      "Iteration 40999, loss = 0.03233377\n",
      "Iteration 41000, loss = 0.03233176\n",
      "Iteration 41001, loss = 0.03233100\n",
      "Iteration 41002, loss = 0.03233006\n",
      "Iteration 41003, loss = 0.03232770\n",
      "Iteration 41004, loss = 0.03233781\n",
      "Iteration 41005, loss = 0.03234333\n",
      "Iteration 41006, loss = 0.03233604\n",
      "Iteration 41007, loss = 0.03231664\n",
      "Iteration 41008, loss = 0.03232750\n",
      "Iteration 41009, loss = 0.03233184\n",
      "Iteration 41010, loss = 0.03232397\n",
      "Iteration 41011, loss = 0.03231957\n",
      "Iteration 41012, loss = 0.03232328\n",
      "Iteration 41013, loss = 0.03233490\n",
      "Iteration 41014, loss = 0.03233077\n",
      "Iteration 41015, loss = 0.03231369\n",
      "Iteration 41016, loss = 0.03231379\n",
      "Iteration 41017, loss = 0.03232277\n",
      "Iteration 41018, loss = 0.03231592\n",
      "Iteration 41019, loss = 0.03232854\n",
      "Iteration 41020, loss = 0.03232963\n",
      "Iteration 41021, loss = 0.03232156\n",
      "Iteration 41022, loss = 0.03231065\n",
      "Iteration 41023, loss = 0.03231485\n",
      "Iteration 41024, loss = 0.03231159\n",
      "Iteration 41025, loss = 0.03231367\n",
      "Iteration 41026, loss = 0.03231078\n",
      "Iteration 41027, loss = 0.03231570\n",
      "Iteration 41028, loss = 0.03231293\n",
      "Iteration 41029, loss = 0.03231359\n",
      "Iteration 41030, loss = 0.03231437\n",
      "Iteration 41031, loss = 0.03231491\n",
      "Iteration 41032, loss = 0.03231338\n",
      "Iteration 41033, loss = 0.03231237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 41034, loss = 0.03231349\n",
      "Iteration 41035, loss = 0.03230880\n",
      "Iteration 41036, loss = 0.03230926\n",
      "Iteration 41037, loss = 0.03231227\n",
      "Iteration 41038, loss = 0.03230580\n",
      "Iteration 41039, loss = 0.03229973\n",
      "Iteration 41040, loss = 0.03231186\n",
      "Iteration 41041, loss = 0.03231625\n",
      "Iteration 41042, loss = 0.03231401\n",
      "Iteration 41043, loss = 0.03230301\n",
      "Iteration 41044, loss = 0.03231737\n",
      "Iteration 41045, loss = 0.03232284\n",
      "Iteration 41046, loss = 0.03232978\n",
      "Iteration 41047, loss = 0.03232997\n",
      "Iteration 41048, loss = 0.03231903\n",
      "Iteration 41049, loss = 0.03229994\n",
      "Iteration 41050, loss = 0.03232504\n",
      "Iteration 41051, loss = 0.03233696\n",
      "Iteration 41052, loss = 0.03232374\n",
      "Iteration 41053, loss = 0.03231206\n",
      "Iteration 41054, loss = 0.03231092\n",
      "Iteration 41055, loss = 0.03232278\n",
      "Iteration 41056, loss = 0.03232230\n",
      "Iteration 41057, loss = 0.03231167\n",
      "Iteration 41058, loss = 0.03230276\n",
      "Iteration 41059, loss = 0.03230879\n",
      "Iteration 41060, loss = 0.03231616\n",
      "Iteration 41061, loss = 0.03231283\n",
      "Iteration 41062, loss = 0.03232037\n",
      "Iteration 41063, loss = 0.03232064\n",
      "Iteration 41064, loss = 0.03231854\n",
      "Iteration 41065, loss = 0.03231453\n",
      "Iteration 41066, loss = 0.03231394\n",
      "Iteration 41067, loss = 0.03230499\n",
      "Iteration 41068, loss = 0.03230597\n",
      "Iteration 41069, loss = 0.03231021\n",
      "Iteration 41070, loss = 0.03229474\n",
      "Iteration 41071, loss = 0.03229292\n",
      "Iteration 41072, loss = 0.03229139\n",
      "Iteration 41073, loss = 0.03230016\n",
      "Iteration 41074, loss = 0.03229876\n",
      "Iteration 41075, loss = 0.03229859\n",
      "Iteration 41076, loss = 0.03229656\n",
      "Iteration 41077, loss = 0.03229504\n",
      "Iteration 41078, loss = 0.03230061\n",
      "Iteration 41079, loss = 0.03229623\n",
      "Iteration 41080, loss = 0.03230117\n",
      "Iteration 41081, loss = 0.03229803\n",
      "Iteration 41082, loss = 0.03229239\n",
      "Iteration 41083, loss = 0.03228565\n",
      "Iteration 41084, loss = 0.03228979\n",
      "Iteration 41085, loss = 0.03230112\n",
      "Iteration 41086, loss = 0.03229438\n",
      "Iteration 41087, loss = 0.03228538\n",
      "Iteration 41088, loss = 0.03229232\n",
      "Iteration 41089, loss = 0.03228735\n",
      "Iteration 41090, loss = 0.03229077\n",
      "Iteration 41091, loss = 0.03228691\n",
      "Iteration 41092, loss = 0.03228874\n",
      "Iteration 41093, loss = 0.03228959\n",
      "Iteration 41094, loss = 0.03227826\n",
      "Iteration 41095, loss = 0.03228551\n",
      "Iteration 41096, loss = 0.03228864\n",
      "Iteration 41097, loss = 0.03228411\n",
      "Iteration 41098, loss = 0.03228984\n",
      "Iteration 41099, loss = 0.03228957\n",
      "Iteration 41100, loss = 0.03228701\n",
      "Iteration 41101, loss = 0.03229046\n",
      "Iteration 41102, loss = 0.03228681\n",
      "Iteration 41103, loss = 0.03229177\n",
      "Iteration 41104, loss = 0.03229599\n",
      "Iteration 41105, loss = 0.03229356\n",
      "Iteration 41106, loss = 0.03228450\n",
      "Iteration 41107, loss = 0.03227943\n",
      "Iteration 41108, loss = 0.03228226\n",
      "Iteration 41109, loss = 0.03228534\n",
      "Iteration 41110, loss = 0.03228814\n",
      "Iteration 41111, loss = 0.03227705\n",
      "Iteration 41112, loss = 0.03228867\n",
      "Iteration 41113, loss = 0.03229360\n",
      "Iteration 41114, loss = 0.03229024\n",
      "Iteration 41115, loss = 0.03228735\n",
      "Iteration 41116, loss = 0.03228801\n",
      "Iteration 41117, loss = 0.03228460\n",
      "Iteration 41118, loss = 0.03229321\n",
      "Iteration 41119, loss = 0.03229662\n",
      "Iteration 41120, loss = 0.03228990\n",
      "Iteration 41121, loss = 0.03228152\n",
      "Iteration 41122, loss = 0.03228547\n",
      "Iteration 41123, loss = 0.03227566\n",
      "Iteration 41124, loss = 0.03227729\n",
      "Iteration 41125, loss = 0.03228630\n",
      "Iteration 41126, loss = 0.03228120\n",
      "Iteration 41127, loss = 0.03227103\n",
      "Iteration 41128, loss = 0.03227060\n",
      "Iteration 41129, loss = 0.03227901\n",
      "Iteration 41130, loss = 0.03227445\n",
      "Iteration 41131, loss = 0.03227323\n",
      "Iteration 41132, loss = 0.03227476\n",
      "Iteration 41133, loss = 0.03226913\n",
      "Iteration 41134, loss = 0.03227562\n",
      "Iteration 41135, loss = 0.03227388\n",
      "Iteration 41136, loss = 0.03228049\n",
      "Iteration 41137, loss = 0.03228664\n",
      "Iteration 41138, loss = 0.03228142\n",
      "Iteration 41139, loss = 0.03227369\n",
      "Iteration 41140, loss = 0.03228019\n",
      "Iteration 41141, loss = 0.03227153\n",
      "Iteration 41142, loss = 0.03227722\n",
      "Iteration 41143, loss = 0.03227586\n",
      "Iteration 41144, loss = 0.03227772\n",
      "Iteration 41145, loss = 0.03227422\n",
      "Iteration 41146, loss = 0.03226234\n",
      "Iteration 41147, loss = 0.03227857\n",
      "Iteration 41148, loss = 0.03228774\n",
      "Iteration 41149, loss = 0.03228300\n",
      "Iteration 41150, loss = 0.03227507\n",
      "Iteration 41151, loss = 0.03226692\n",
      "Iteration 41152, loss = 0.03227026\n",
      "Iteration 41153, loss = 0.03228062\n",
      "Iteration 41154, loss = 0.03227599\n",
      "Iteration 41155, loss = 0.03225786\n",
      "Iteration 41156, loss = 0.03227548\n",
      "Iteration 41157, loss = 0.03228126\n",
      "Iteration 41158, loss = 0.03227543\n",
      "Iteration 41159, loss = 0.03226373\n",
      "Iteration 41160, loss = 0.03227006\n",
      "Iteration 41161, loss = 0.03227805\n",
      "Iteration 41162, loss = 0.03227880\n",
      "Iteration 41163, loss = 0.03227277\n",
      "Iteration 41164, loss = 0.03226615\n",
      "Iteration 41165, loss = 0.03225845\n",
      "Iteration 41166, loss = 0.03225848\n",
      "Iteration 41167, loss = 0.03226009\n",
      "Iteration 41168, loss = 0.03226502\n",
      "Iteration 41169, loss = 0.03226664\n",
      "Iteration 41170, loss = 0.03226582\n",
      "Iteration 41171, loss = 0.03226363\n",
      "Iteration 41172, loss = 0.03226530\n",
      "Iteration 41173, loss = 0.03226700\n",
      "Iteration 41174, loss = 0.03226974\n",
      "Iteration 41175, loss = 0.03226295\n",
      "Iteration 41176, loss = 0.03226036\n",
      "Iteration 41177, loss = 0.03226067\n",
      "Iteration 41178, loss = 0.03226961\n",
      "Iteration 41179, loss = 0.03226934\n",
      "Iteration 41180, loss = 0.03225712\n",
      "Iteration 41181, loss = 0.03226489\n",
      "Iteration 41182, loss = 0.03226537\n",
      "Iteration 41183, loss = 0.03227403\n",
      "Iteration 41184, loss = 0.03227541\n",
      "Iteration 41185, loss = 0.03226343\n",
      "Iteration 41186, loss = 0.03226217\n",
      "Iteration 41187, loss = 0.03226448\n",
      "Iteration 41188, loss = 0.03227547\n",
      "Iteration 41189, loss = 0.03227488\n",
      "Iteration 41190, loss = 0.03225605\n",
      "Iteration 41191, loss = 0.03226150\n",
      "Iteration 41192, loss = 0.03227268\n",
      "Iteration 41193, loss = 0.03226688\n",
      "Iteration 41194, loss = 0.03226108\n",
      "Iteration 41195, loss = 0.03225672\n",
      "Iteration 41196, loss = 0.03226428\n",
      "Iteration 41197, loss = 0.03226504\n",
      "Iteration 41198, loss = 0.03225308\n",
      "Iteration 41199, loss = 0.03224751\n",
      "Iteration 41200, loss = 0.03225817\n",
      "Iteration 41201, loss = 0.03225704\n",
      "Iteration 41202, loss = 0.03226441\n",
      "Iteration 41203, loss = 0.03225792\n",
      "Iteration 41204, loss = 0.03225711\n",
      "Iteration 41205, loss = 0.03226497\n",
      "Iteration 41206, loss = 0.03226498\n",
      "Iteration 41207, loss = 0.03226117\n",
      "Iteration 41208, loss = 0.03225910\n",
      "Iteration 41209, loss = 0.03223893\n",
      "Iteration 41210, loss = 0.03225324\n",
      "Iteration 41211, loss = 0.03226418\n",
      "Iteration 41212, loss = 0.03226559\n",
      "Iteration 41213, loss = 0.03225478\n",
      "Iteration 41214, loss = 0.03224072\n",
      "Iteration 41215, loss = 0.03224599\n",
      "Iteration 41216, loss = 0.03225906\n",
      "Iteration 41217, loss = 0.03227096\n",
      "Iteration 41218, loss = 0.03226575\n",
      "Iteration 41219, loss = 0.03225692\n",
      "Iteration 41220, loss = 0.03224639\n",
      "Iteration 41221, loss = 0.03225102\n",
      "Iteration 41222, loss = 0.03225601\n",
      "Iteration 41223, loss = 0.03224647\n",
      "Iteration 41224, loss = 0.03224275\n",
      "Iteration 41225, loss = 0.03224463\n",
      "Iteration 41226, loss = 0.03224522\n",
      "Iteration 41227, loss = 0.03225330\n",
      "Iteration 41228, loss = 0.03225338\n",
      "Iteration 41229, loss = 0.03224457\n",
      "Iteration 41230, loss = 0.03224412\n",
      "Iteration 41231, loss = 0.03225009\n",
      "Iteration 41232, loss = 0.03224611\n",
      "Iteration 41233, loss = 0.03224165\n",
      "Iteration 41234, loss = 0.03224057\n",
      "Iteration 41235, loss = 0.03225566\n",
      "Iteration 41236, loss = 0.03225164\n",
      "Iteration 41237, loss = 0.03224644\n",
      "Iteration 41238, loss = 0.03224199\n",
      "Iteration 41239, loss = 0.03224458\n",
      "Iteration 41240, loss = 0.03224543\n",
      "Iteration 41241, loss = 0.03224069\n",
      "Iteration 41242, loss = 0.03225102\n",
      "Iteration 41243, loss = 0.03224622\n",
      "Iteration 41244, loss = 0.03223300\n",
      "Iteration 41245, loss = 0.03224394\n",
      "Iteration 41246, loss = 0.03225438\n",
      "Iteration 41247, loss = 0.03225110\n",
      "Iteration 41248, loss = 0.03224881\n",
      "Iteration 41249, loss = 0.03223784\n",
      "Iteration 41250, loss = 0.03224696\n",
      "Iteration 41251, loss = 0.03224193\n",
      "Iteration 41252, loss = 0.03222989\n",
      "Iteration 41253, loss = 0.03223502\n",
      "Iteration 41254, loss = 0.03222983\n",
      "Iteration 41255, loss = 0.03223174\n",
      "Iteration 41256, loss = 0.03223102\n",
      "Iteration 41257, loss = 0.03223587\n",
      "Iteration 41258, loss = 0.03223548\n",
      "Iteration 41259, loss = 0.03222675\n",
      "Iteration 41260, loss = 0.03222101\n",
      "Iteration 41261, loss = 0.03224055\n",
      "Iteration 41262, loss = 0.03223923\n",
      "Iteration 41263, loss = 0.03222777\n",
      "Iteration 41264, loss = 0.03222558\n",
      "Iteration 41265, loss = 0.03223482\n",
      "Iteration 41266, loss = 0.03223812\n",
      "Iteration 41267, loss = 0.03223143\n",
      "Iteration 41268, loss = 0.03223223\n",
      "Iteration 41269, loss = 0.03222551\n",
      "Iteration 41270, loss = 0.03223050\n",
      "Iteration 41271, loss = 0.03223150\n",
      "Iteration 41272, loss = 0.03222875\n",
      "Iteration 41273, loss = 0.03222436\n",
      "Iteration 41274, loss = 0.03223646\n",
      "Iteration 41275, loss = 0.03223679\n",
      "Iteration 41276, loss = 0.03222586\n",
      "Iteration 41277, loss = 0.03222629\n",
      "Iteration 41278, loss = 0.03222264\n",
      "Iteration 41279, loss = 0.03221678\n",
      "Iteration 41280, loss = 0.03222274\n",
      "Iteration 41281, loss = 0.03221787\n",
      "Iteration 41282, loss = 0.03222739\n",
      "Iteration 41283, loss = 0.03223253\n",
      "Iteration 41284, loss = 0.03222668\n",
      "Iteration 41285, loss = 0.03223280\n",
      "Iteration 41286, loss = 0.03223057\n",
      "Iteration 41287, loss = 0.03221949\n",
      "Iteration 41288, loss = 0.03221803\n",
      "Iteration 41289, loss = 0.03224124\n",
      "Iteration 41290, loss = 0.03224567\n",
      "Iteration 41291, loss = 0.03223391\n",
      "Iteration 41292, loss = 0.03221416\n",
      "Iteration 41293, loss = 0.03222884\n",
      "Iteration 41294, loss = 0.03224304\n",
      "Iteration 41295, loss = 0.03224585\n",
      "Iteration 41296, loss = 0.03223894\n",
      "Iteration 41297, loss = 0.03223507\n",
      "Iteration 41298, loss = 0.03224057\n",
      "Iteration 41299, loss = 0.03223567\n",
      "Iteration 41300, loss = 0.03222248\n",
      "Iteration 41301, loss = 0.03221018\n",
      "Iteration 41302, loss = 0.03222072\n",
      "Iteration 41303, loss = 0.03221719\n",
      "Iteration 41304, loss = 0.03221939\n",
      "Iteration 41305, loss = 0.03221628\n",
      "Iteration 41306, loss = 0.03221351\n",
      "Iteration 41307, loss = 0.03221359\n",
      "Iteration 41308, loss = 0.03221602\n",
      "Iteration 41309, loss = 0.03221023\n",
      "Iteration 41310, loss = 0.03220290\n",
      "Iteration 41311, loss = 0.03220911\n",
      "Iteration 41312, loss = 0.03221109\n",
      "Iteration 41313, loss = 0.03220937\n",
      "Iteration 41314, loss = 0.03220493\n",
      "Iteration 41315, loss = 0.03221095\n",
      "Iteration 41316, loss = 0.03220918\n",
      "Iteration 41317, loss = 0.03220678\n",
      "Iteration 41318, loss = 0.03220648\n",
      "Iteration 41319, loss = 0.03220523\n",
      "Iteration 41320, loss = 0.03220451\n",
      "Iteration 41321, loss = 0.03220419\n",
      "Iteration 41322, loss = 0.03220860\n",
      "Iteration 41323, loss = 0.03220477\n",
      "Iteration 41324, loss = 0.03220504\n",
      "Iteration 41325, loss = 0.03220840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 41326, loss = 0.03220146\n",
      "Iteration 41327, loss = 0.03220056\n",
      "Iteration 41328, loss = 0.03219906\n",
      "Iteration 41329, loss = 0.03220837\n",
      "Iteration 41330, loss = 0.03220640\n",
      "Iteration 41331, loss = 0.03220563\n",
      "Iteration 41332, loss = 0.03220056\n",
      "Iteration 41333, loss = 0.03220067\n",
      "Iteration 41334, loss = 0.03221392\n",
      "Iteration 41335, loss = 0.03220980\n",
      "Iteration 41336, loss = 0.03220587\n",
      "Iteration 41337, loss = 0.03221388\n",
      "Iteration 41338, loss = 0.03221717\n",
      "Iteration 41339, loss = 0.03221961\n",
      "Iteration 41340, loss = 0.03221744\n",
      "Iteration 41341, loss = 0.03220349\n",
      "Iteration 41342, loss = 0.03220770\n",
      "Iteration 41343, loss = 0.03221028\n",
      "Iteration 41344, loss = 0.03220210\n",
      "Iteration 41345, loss = 0.03219158\n",
      "Iteration 41346, loss = 0.03220499\n",
      "Iteration 41347, loss = 0.03220645\n",
      "Iteration 41348, loss = 0.03220247\n",
      "Iteration 41349, loss = 0.03218768\n",
      "Iteration 41350, loss = 0.03219746\n",
      "Iteration 41351, loss = 0.03221037\n",
      "Iteration 41352, loss = 0.03221240\n",
      "Iteration 41353, loss = 0.03221204\n",
      "Iteration 41354, loss = 0.03220272\n",
      "Iteration 41355, loss = 0.03219507\n",
      "Iteration 41356, loss = 0.03220162\n",
      "Iteration 41357, loss = 0.03220645\n",
      "Iteration 41358, loss = 0.03221020\n",
      "Iteration 41359, loss = 0.03221128\n",
      "Iteration 41360, loss = 0.03220117\n",
      "Iteration 41361, loss = 0.03219898\n",
      "Iteration 41362, loss = 0.03220164\n",
      "Iteration 41363, loss = 0.03219677\n",
      "Iteration 41364, loss = 0.03219153\n",
      "Iteration 41365, loss = 0.03220104\n",
      "Iteration 41366, loss = 0.03220393\n",
      "Iteration 41367, loss = 0.03220718\n",
      "Iteration 41368, loss = 0.03219758\n",
      "Iteration 41369, loss = 0.03220625\n",
      "Iteration 41370, loss = 0.03220320\n",
      "Iteration 41371, loss = 0.03219600\n",
      "Iteration 41372, loss = 0.03220704\n",
      "Iteration 41373, loss = 0.03220461\n",
      "Iteration 41374, loss = 0.03219734\n",
      "Iteration 41375, loss = 0.03219714\n",
      "Iteration 41376, loss = 0.03219429\n",
      "Iteration 41377, loss = 0.03219864\n",
      "Iteration 41378, loss = 0.03219569\n",
      "Iteration 41379, loss = 0.03219160\n",
      "Iteration 41380, loss = 0.03218232\n",
      "Iteration 41381, loss = 0.03219563\n",
      "Iteration 41382, loss = 0.03219916\n",
      "Iteration 41383, loss = 0.03219557\n",
      "Iteration 41384, loss = 0.03218199\n",
      "Iteration 41385, loss = 0.03219302\n",
      "Iteration 41386, loss = 0.03220216\n",
      "Iteration 41387, loss = 0.03220757\n",
      "Iteration 41388, loss = 0.03220674\n",
      "Iteration 41389, loss = 0.03218959\n",
      "Iteration 41390, loss = 0.03217722\n",
      "Iteration 41391, loss = 0.03219472\n",
      "Iteration 41392, loss = 0.03219855\n",
      "Iteration 41393, loss = 0.03219082\n",
      "Iteration 41394, loss = 0.03218097\n",
      "Iteration 41395, loss = 0.03219614\n",
      "Iteration 41396, loss = 0.03219280\n",
      "Iteration 41397, loss = 0.03218908\n",
      "Iteration 41398, loss = 0.03219170\n",
      "Iteration 41399, loss = 0.03219863\n",
      "Iteration 41400, loss = 0.03218844\n",
      "Iteration 41401, loss = 0.03219117\n",
      "Iteration 41402, loss = 0.03218889\n",
      "Iteration 41403, loss = 0.03219216\n",
      "Iteration 41404, loss = 0.03218513\n",
      "Iteration 41405, loss = 0.03217976\n",
      "Iteration 41406, loss = 0.03217077\n",
      "Iteration 41407, loss = 0.03218443\n",
      "Iteration 41408, loss = 0.03219419\n",
      "Iteration 41409, loss = 0.03218596\n",
      "Iteration 41410, loss = 0.03216999\n",
      "Iteration 41411, loss = 0.03218346\n",
      "Iteration 41412, loss = 0.03219730\n",
      "Iteration 41413, loss = 0.03219452\n",
      "Iteration 41414, loss = 0.03219269\n",
      "Iteration 41415, loss = 0.03218742\n",
      "Iteration 41416, loss = 0.03219493\n",
      "Iteration 41417, loss = 0.03219290\n",
      "Iteration 41418, loss = 0.03218602\n",
      "Iteration 41419, loss = 0.03217856\n",
      "Iteration 41420, loss = 0.03217536\n",
      "Iteration 41421, loss = 0.03217881\n",
      "Iteration 41422, loss = 0.03218787\n",
      "Iteration 41423, loss = 0.03218666\n",
      "Iteration 41424, loss = 0.03217598\n",
      "Iteration 41425, loss = 0.03219074\n",
      "Iteration 41426, loss = 0.03219042\n",
      "Iteration 41427, loss = 0.03217212\n",
      "Iteration 41428, loss = 0.03216560\n",
      "Iteration 41429, loss = 0.03217861\n",
      "Iteration 41430, loss = 0.03217741\n",
      "Iteration 41431, loss = 0.03218889\n",
      "Iteration 41432, loss = 0.03218739\n",
      "Iteration 41433, loss = 0.03217197\n",
      "Iteration 41434, loss = 0.03216803\n",
      "Iteration 41435, loss = 0.03218362\n",
      "Iteration 41436, loss = 0.03219015\n",
      "Iteration 41437, loss = 0.03218894\n",
      "Iteration 41438, loss = 0.03218045\n",
      "Iteration 41439, loss = 0.03216164\n",
      "Iteration 41440, loss = 0.03218007\n",
      "Iteration 41441, loss = 0.03219204\n",
      "Iteration 41442, loss = 0.03219315\n",
      "Iteration 41443, loss = 0.03218236\n",
      "Iteration 41444, loss = 0.03218057\n",
      "Iteration 41445, loss = 0.03217829\n",
      "Iteration 41446, loss = 0.03217621\n",
      "Iteration 41447, loss = 0.03218975\n",
      "Iteration 41448, loss = 0.03218588\n",
      "Iteration 41449, loss = 0.03218808\n",
      "Iteration 41450, loss = 0.03217430\n",
      "Iteration 41451, loss = 0.03216956\n",
      "Iteration 41452, loss = 0.03216813\n",
      "Iteration 41453, loss = 0.03218036\n",
      "Iteration 41454, loss = 0.03218331\n",
      "Iteration 41455, loss = 0.03217431\n",
      "Iteration 41456, loss = 0.03217171\n",
      "Iteration 41457, loss = 0.03216778\n",
      "Iteration 41458, loss = 0.03217847\n",
      "Iteration 41459, loss = 0.03218595\n",
      "Iteration 41460, loss = 0.03218827\n",
      "Iteration 41461, loss = 0.03218166\n",
      "Iteration 41462, loss = 0.03218224\n",
      "Iteration 41463, loss = 0.03218750\n",
      "Iteration 41464, loss = 0.03218042\n",
      "Iteration 41465, loss = 0.03216344\n",
      "Iteration 41466, loss = 0.03216531\n",
      "Iteration 41467, loss = 0.03219264\n",
      "Iteration 41468, loss = 0.03219035\n",
      "Iteration 41469, loss = 0.03216668\n",
      "Iteration 41470, loss = 0.03215901\n",
      "Iteration 41471, loss = 0.03217073\n",
      "Iteration 41472, loss = 0.03217935\n",
      "Iteration 41473, loss = 0.03217649\n",
      "Iteration 41474, loss = 0.03217437\n",
      "Iteration 41475, loss = 0.03216946\n",
      "Iteration 41476, loss = 0.03215855\n",
      "Iteration 41477, loss = 0.03215909\n",
      "Iteration 41478, loss = 0.03215387\n",
      "Iteration 41479, loss = 0.03216219\n",
      "Iteration 41480, loss = 0.03217182\n",
      "Iteration 41481, loss = 0.03215807\n",
      "Iteration 41482, loss = 0.03214787\n",
      "Iteration 41483, loss = 0.03216755\n",
      "Iteration 41484, loss = 0.03217239\n",
      "Iteration 41485, loss = 0.03216275\n",
      "Iteration 41486, loss = 0.03215282\n",
      "Iteration 41487, loss = 0.03216360\n",
      "Iteration 41488, loss = 0.03216825\n",
      "Iteration 41489, loss = 0.03217064\n",
      "Iteration 41490, loss = 0.03216408\n",
      "Iteration 41491, loss = 0.03215543\n",
      "Iteration 41492, loss = 0.03215409\n",
      "Iteration 41493, loss = 0.03215704\n",
      "Iteration 41494, loss = 0.03215649\n",
      "Iteration 41495, loss = 0.03216504\n",
      "Iteration 41496, loss = 0.03216076\n",
      "Iteration 41497, loss = 0.03214069\n",
      "Iteration 41498, loss = 0.03214465\n",
      "Iteration 41499, loss = 0.03214517\n",
      "Iteration 41500, loss = 0.03214980\n",
      "Iteration 41501, loss = 0.03214876\n",
      "Iteration 41502, loss = 0.03214558\n",
      "Iteration 41503, loss = 0.03214517\n",
      "Iteration 41504, loss = 0.03214129\n",
      "Iteration 41505, loss = 0.03214728\n",
      "Iteration 41506, loss = 0.03215717\n",
      "Iteration 41507, loss = 0.03215248\n",
      "Iteration 41508, loss = 0.03214956\n",
      "Iteration 41509, loss = 0.03215820\n",
      "Iteration 41510, loss = 0.03215761\n",
      "Iteration 41511, loss = 0.03215362\n",
      "Iteration 41512, loss = 0.03215965\n",
      "Iteration 41513, loss = 0.03215660\n",
      "Iteration 41514, loss = 0.03213844\n",
      "Iteration 41515, loss = 0.03214027\n",
      "Iteration 41516, loss = 0.03214749\n",
      "Iteration 41517, loss = 0.03214032\n",
      "Iteration 41518, loss = 0.03213810\n",
      "Iteration 41519, loss = 0.03214386\n",
      "Iteration 41520, loss = 0.03214221\n",
      "Iteration 41521, loss = 0.03213909\n",
      "Iteration 41522, loss = 0.03213641\n",
      "Iteration 41523, loss = 0.03214574\n",
      "Iteration 41524, loss = 0.03213827\n",
      "Iteration 41525, loss = 0.03214418\n",
      "Iteration 41526, loss = 0.03214600\n",
      "Iteration 41527, loss = 0.03214464\n",
      "Iteration 41528, loss = 0.03214249\n",
      "Iteration 41529, loss = 0.03213431\n",
      "Iteration 41530, loss = 0.03213737\n",
      "Iteration 41531, loss = 0.03214249\n",
      "Iteration 41532, loss = 0.03214260\n",
      "Iteration 41533, loss = 0.03214664\n",
      "Iteration 41534, loss = 0.03214133\n",
      "Iteration 41535, loss = 0.03213168\n",
      "Iteration 41536, loss = 0.03213769\n",
      "Iteration 41537, loss = 0.03214135\n",
      "Iteration 41538, loss = 0.03213731\n",
      "Iteration 41539, loss = 0.03213196\n",
      "Iteration 41540, loss = 0.03213257\n",
      "Iteration 41541, loss = 0.03213707\n",
      "Iteration 41542, loss = 0.03213125\n",
      "Iteration 41543, loss = 0.03213096\n",
      "Iteration 41544, loss = 0.03213303\n",
      "Iteration 41545, loss = 0.03213047\n",
      "Iteration 41546, loss = 0.03213000\n",
      "Iteration 41547, loss = 0.03214118\n",
      "Iteration 41548, loss = 0.03213443\n",
      "Iteration 41549, loss = 0.03212931\n",
      "Iteration 41550, loss = 0.03213147\n",
      "Iteration 41551, loss = 0.03212965\n",
      "Iteration 41552, loss = 0.03212584\n",
      "Iteration 41553, loss = 0.03212675\n",
      "Iteration 41554, loss = 0.03212935\n",
      "Iteration 41555, loss = 0.03212371\n",
      "Iteration 41556, loss = 0.03212939\n",
      "Iteration 41557, loss = 0.03213341\n",
      "Iteration 41558, loss = 0.03213485\n",
      "Iteration 41559, loss = 0.03212851\n",
      "Iteration 41560, loss = 0.03212264\n",
      "Iteration 41561, loss = 0.03212673\n",
      "Iteration 41562, loss = 0.03212117\n",
      "Iteration 41563, loss = 0.03211845\n",
      "Iteration 41564, loss = 0.03212272\n",
      "Iteration 41565, loss = 0.03213357\n",
      "Iteration 41566, loss = 0.03212369\n",
      "Iteration 41567, loss = 0.03212157\n",
      "Iteration 41568, loss = 0.03212737\n",
      "Iteration 41569, loss = 0.03212541\n",
      "Iteration 41570, loss = 0.03211916\n",
      "Iteration 41571, loss = 0.03212268\n",
      "Iteration 41572, loss = 0.03213478\n",
      "Iteration 41573, loss = 0.03213657\n",
      "Iteration 41574, loss = 0.03212092\n",
      "Iteration 41575, loss = 0.03211993\n",
      "Iteration 41576, loss = 0.03213260\n",
      "Iteration 41577, loss = 0.03213416\n",
      "Iteration 41578, loss = 0.03213093\n",
      "Iteration 41579, loss = 0.03213161\n",
      "Iteration 41580, loss = 0.03211995\n",
      "Iteration 41581, loss = 0.03211535\n",
      "Iteration 41582, loss = 0.03213134\n",
      "Iteration 41583, loss = 0.03214184\n",
      "Iteration 41584, loss = 0.03213015\n",
      "Iteration 41585, loss = 0.03211673\n",
      "Iteration 41586, loss = 0.03212173\n",
      "Iteration 41587, loss = 0.03212146\n",
      "Iteration 41588, loss = 0.03211477\n",
      "Iteration 41589, loss = 0.03211134\n",
      "Iteration 41590, loss = 0.03212255\n",
      "Iteration 41591, loss = 0.03212071\n",
      "Iteration 41592, loss = 0.03211336\n",
      "Iteration 41593, loss = 0.03211684\n",
      "Iteration 41594, loss = 0.03211776\n",
      "Iteration 41595, loss = 0.03210842\n",
      "Iteration 41596, loss = 0.03210970\n",
      "Iteration 41597, loss = 0.03211606\n",
      "Iteration 41598, loss = 0.03210908\n",
      "Iteration 41599, loss = 0.03209993\n",
      "Iteration 41600, loss = 0.03210260\n",
      "Iteration 41601, loss = 0.03210591\n",
      "Iteration 41602, loss = 0.03210994\n",
      "Iteration 41603, loss = 0.03211241\n",
      "Iteration 41604, loss = 0.03210787\n",
      "Iteration 41605, loss = 0.03210188\n",
      "Iteration 41606, loss = 0.03211126\n",
      "Iteration 41607, loss = 0.03211098\n",
      "Iteration 41608, loss = 0.03211307\n",
      "Iteration 41609, loss = 0.03210101\n",
      "Iteration 41610, loss = 0.03210558\n",
      "Iteration 41611, loss = 0.03212007\n",
      "Iteration 41612, loss = 0.03212176\n",
      "Iteration 41613, loss = 0.03211306\n",
      "Iteration 41614, loss = 0.03210984\n",
      "Iteration 41615, loss = 0.03210580\n",
      "Iteration 41616, loss = 0.03210854\n",
      "Iteration 41617, loss = 0.03211897\n",
      "Iteration 41618, loss = 0.03211548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 41619, loss = 0.03211530\n",
      "Iteration 41620, loss = 0.03210791\n",
      "Iteration 41621, loss = 0.03210446\n",
      "Iteration 41622, loss = 0.03210366\n",
      "Iteration 41623, loss = 0.03209877\n",
      "Iteration 41624, loss = 0.03210735\n",
      "Iteration 41625, loss = 0.03210669\n",
      "Iteration 41626, loss = 0.03209468\n",
      "Iteration 41627, loss = 0.03210651\n",
      "Iteration 41628, loss = 0.03211450\n",
      "Iteration 41629, loss = 0.03210481\n",
      "Iteration 41630, loss = 0.03211238\n",
      "Iteration 41631, loss = 0.03211471\n",
      "Iteration 41632, loss = 0.03210726\n",
      "Iteration 41633, loss = 0.03209916\n",
      "Iteration 41634, loss = 0.03209713\n",
      "Iteration 41635, loss = 0.03209228\n",
      "Iteration 41636, loss = 0.03209670\n",
      "Iteration 41637, loss = 0.03209664\n",
      "Iteration 41638, loss = 0.03209831\n",
      "Iteration 41639, loss = 0.03209473\n",
      "Iteration 41640, loss = 0.03209157\n",
      "Iteration 41641, loss = 0.03209812\n",
      "Iteration 41642, loss = 0.03210194\n",
      "Iteration 41643, loss = 0.03210305\n",
      "Iteration 41644, loss = 0.03209526\n",
      "Iteration 41645, loss = 0.03209658\n",
      "Iteration 41646, loss = 0.03210147\n",
      "Iteration 41647, loss = 0.03208736\n",
      "Iteration 41648, loss = 0.03209186\n",
      "Iteration 41649, loss = 0.03210143\n",
      "Iteration 41650, loss = 0.03210376\n",
      "Iteration 41651, loss = 0.03210983\n",
      "Iteration 41652, loss = 0.03210481\n",
      "Iteration 41653, loss = 0.03209474\n",
      "Iteration 41654, loss = 0.03209606\n",
      "Iteration 41655, loss = 0.03209867\n",
      "Iteration 41656, loss = 0.03209245\n",
      "Iteration 41657, loss = 0.03209016\n",
      "Iteration 41658, loss = 0.03209349\n",
      "Iteration 41659, loss = 0.03209591\n",
      "Iteration 41660, loss = 0.03209822\n",
      "Iteration 41661, loss = 0.03209434\n",
      "Iteration 41662, loss = 0.03208661\n",
      "Iteration 41663, loss = 0.03209198\n",
      "Iteration 41664, loss = 0.03210650\n",
      "Iteration 41665, loss = 0.03210573\n",
      "Iteration 41666, loss = 0.03209176\n",
      "Iteration 41667, loss = 0.03208379\n",
      "Iteration 41668, loss = 0.03209074\n",
      "Iteration 41669, loss = 0.03209268\n",
      "Iteration 41670, loss = 0.03208582\n",
      "Iteration 41671, loss = 0.03208913\n",
      "Iteration 41672, loss = 0.03208660\n",
      "Iteration 41673, loss = 0.03208570\n",
      "Iteration 41674, loss = 0.03209218\n",
      "Iteration 41675, loss = 0.03209173\n",
      "Iteration 41676, loss = 0.03209084\n",
      "Iteration 41677, loss = 0.03208755\n",
      "Iteration 41678, loss = 0.03209281\n",
      "Iteration 41679, loss = 0.03209839\n",
      "Iteration 41680, loss = 0.03209675\n",
      "Iteration 41681, loss = 0.03208303\n",
      "Iteration 41682, loss = 0.03208513\n",
      "Iteration 41683, loss = 0.03208587\n",
      "Iteration 41684, loss = 0.03207854\n",
      "Iteration 41685, loss = 0.03208266\n",
      "Iteration 41686, loss = 0.03208273\n",
      "Iteration 41687, loss = 0.03208756\n",
      "Iteration 41688, loss = 0.03208814\n",
      "Iteration 41689, loss = 0.03208692\n",
      "Iteration 41690, loss = 0.03208467\n",
      "Iteration 41691, loss = 0.03207275\n",
      "Iteration 41692, loss = 0.03209560\n",
      "Iteration 41693, loss = 0.03210504\n",
      "Iteration 41694, loss = 0.03209974\n",
      "Iteration 41695, loss = 0.03208593\n",
      "Iteration 41696, loss = 0.03209084\n",
      "Iteration 41697, loss = 0.03210293\n",
      "Iteration 41698, loss = 0.03210779\n",
      "Iteration 41699, loss = 0.03210810\n",
      "Iteration 41700, loss = 0.03209589\n",
      "Iteration 41701, loss = 0.03209978\n",
      "Iteration 41702, loss = 0.03209831\n",
      "Iteration 41703, loss = 0.03208897\n",
      "Iteration 41704, loss = 0.03208465\n",
      "Iteration 41705, loss = 0.03208463\n",
      "Iteration 41706, loss = 0.03208291\n",
      "Iteration 41707, loss = 0.03208393\n",
      "Iteration 41708, loss = 0.03208393\n",
      "Iteration 41709, loss = 0.03207744\n",
      "Iteration 41710, loss = 0.03207321\n",
      "Iteration 41711, loss = 0.03207074\n",
      "Iteration 41712, loss = 0.03208105\n",
      "Iteration 41713, loss = 0.03207871\n",
      "Iteration 41714, loss = 0.03207214\n",
      "Iteration 41715, loss = 0.03207595\n",
      "Iteration 41716, loss = 0.03207043\n",
      "Iteration 41717, loss = 0.03207328\n",
      "Iteration 41718, loss = 0.03208104\n",
      "Iteration 41719, loss = 0.03207415\n",
      "Iteration 41720, loss = 0.03207284\n",
      "Iteration 41721, loss = 0.03207726\n",
      "Iteration 41722, loss = 0.03207662\n",
      "Iteration 41723, loss = 0.03207089\n",
      "Iteration 41724, loss = 0.03206221\n",
      "Iteration 41725, loss = 0.03206646\n",
      "Iteration 41726, loss = 0.03206369\n",
      "Iteration 41727, loss = 0.03206745\n",
      "Iteration 41728, loss = 0.03206871\n",
      "Iteration 41729, loss = 0.03206091\n",
      "Iteration 41730, loss = 0.03207279\n",
      "Iteration 41731, loss = 0.03207435\n",
      "Iteration 41732, loss = 0.03206477\n",
      "Iteration 41733, loss = 0.03206907\n",
      "Iteration 41734, loss = 0.03206762\n",
      "Iteration 41735, loss = 0.03205625\n",
      "Iteration 41736, loss = 0.03206006\n",
      "Iteration 41737, loss = 0.03205618\n",
      "Iteration 41738, loss = 0.03206989\n",
      "Iteration 41739, loss = 0.03207248\n",
      "Iteration 41740, loss = 0.03206373\n",
      "Iteration 41741, loss = 0.03205989\n",
      "Iteration 41742, loss = 0.03206431\n",
      "Iteration 41743, loss = 0.03206929\n",
      "Iteration 41744, loss = 0.03207109\n",
      "Iteration 41745, loss = 0.03206418\n",
      "Iteration 41746, loss = 0.03206794\n",
      "Iteration 41747, loss = 0.03206417\n",
      "Iteration 41748, loss = 0.03206587\n",
      "Iteration 41749, loss = 0.03206889\n",
      "Iteration 41750, loss = 0.03206594\n",
      "Iteration 41751, loss = 0.03205505\n",
      "Iteration 41752, loss = 0.03205963\n",
      "Iteration 41753, loss = 0.03206356\n",
      "Iteration 41754, loss = 0.03205795\n",
      "Iteration 41755, loss = 0.03205939\n",
      "Iteration 41756, loss = 0.03205804\n",
      "Iteration 41757, loss = 0.03205543\n",
      "Iteration 41758, loss = 0.03205000\n",
      "Iteration 41759, loss = 0.03204643\n",
      "Iteration 41760, loss = 0.03204952\n",
      "Iteration 41761, loss = 0.03204939\n",
      "Iteration 41762, loss = 0.03205400\n",
      "Iteration 41763, loss = 0.03205181\n",
      "Iteration 41764, loss = 0.03205541\n",
      "Iteration 41765, loss = 0.03204871\n",
      "Iteration 41766, loss = 0.03205186\n",
      "Iteration 41767, loss = 0.03205165\n",
      "Iteration 41768, loss = 0.03205001\n",
      "Iteration 41769, loss = 0.03206388\n",
      "Iteration 41770, loss = 0.03206488\n",
      "Iteration 41771, loss = 0.03206054\n",
      "Iteration 41772, loss = 0.03205389\n",
      "Iteration 41773, loss = 0.03204909\n",
      "Iteration 41774, loss = 0.03204948\n",
      "Iteration 41775, loss = 0.03204077\n",
      "Iteration 41776, loss = 0.03205679\n",
      "Iteration 41777, loss = 0.03206292\n",
      "Iteration 41778, loss = 0.03205062\n",
      "Iteration 41779, loss = 0.03205040\n",
      "Iteration 41780, loss = 0.03205282\n",
      "Iteration 41781, loss = 0.03204967\n",
      "Iteration 41782, loss = 0.03204453\n",
      "Iteration 41783, loss = 0.03204808\n",
      "Iteration 41784, loss = 0.03204610\n",
      "Iteration 41785, loss = 0.03205664\n",
      "Iteration 41786, loss = 0.03204861\n",
      "Iteration 41787, loss = 0.03205002\n",
      "Iteration 41788, loss = 0.03205623\n",
      "Iteration 41789, loss = 0.03204790\n",
      "Iteration 41790, loss = 0.03205288\n",
      "Iteration 41791, loss = 0.03205966\n",
      "Iteration 41792, loss = 0.03204954\n",
      "Iteration 41793, loss = 0.03204131\n",
      "Iteration 41794, loss = 0.03205006\n",
      "Iteration 41795, loss = 0.03205376\n",
      "Iteration 41796, loss = 0.03204716\n",
      "Iteration 41797, loss = 0.03204277\n",
      "Iteration 41798, loss = 0.03204352\n",
      "Iteration 41799, loss = 0.03204256\n",
      "Iteration 41800, loss = 0.03204085\n",
      "Iteration 41801, loss = 0.03205641\n",
      "Iteration 41802, loss = 0.03205680\n",
      "Iteration 41803, loss = 0.03204530\n",
      "Iteration 41804, loss = 0.03205239\n",
      "Iteration 41805, loss = 0.03205468\n",
      "Iteration 41806, loss = 0.03204163\n",
      "Iteration 41807, loss = 0.03203473\n",
      "Iteration 41808, loss = 0.03204146\n",
      "Iteration 41809, loss = 0.03205072\n",
      "Iteration 41810, loss = 0.03204479\n",
      "Iteration 41811, loss = 0.03204779\n",
      "Iteration 41812, loss = 0.03204703\n",
      "Iteration 41813, loss = 0.03204049\n",
      "Iteration 41814, loss = 0.03203966\n",
      "Iteration 41815, loss = 0.03203934\n",
      "Iteration 41816, loss = 0.03204033\n",
      "Iteration 41817, loss = 0.03203738\n",
      "Iteration 41818, loss = 0.03203641\n",
      "Iteration 41819, loss = 0.03204034\n",
      "Iteration 41820, loss = 0.03204299\n",
      "Iteration 41821, loss = 0.03203871\n",
      "Iteration 41822, loss = 0.03203672\n",
      "Iteration 41823, loss = 0.03203787\n",
      "Iteration 41824, loss = 0.03203369\n",
      "Iteration 41825, loss = 0.03204078\n",
      "Iteration 41826, loss = 0.03203987\n",
      "Iteration 41827, loss = 0.03203402\n",
      "Iteration 41828, loss = 0.03203282\n",
      "Iteration 41829, loss = 0.03203024\n",
      "Iteration 41830, loss = 0.03203052\n",
      "Iteration 41831, loss = 0.03202666\n",
      "Iteration 41832, loss = 0.03202690\n",
      "Iteration 41833, loss = 0.03202738\n",
      "Iteration 41834, loss = 0.03202342\n",
      "Iteration 41835, loss = 0.03202809\n",
      "Iteration 41836, loss = 0.03202994\n",
      "Iteration 41837, loss = 0.03204321\n",
      "Iteration 41838, loss = 0.03204397\n",
      "Iteration 41839, loss = 0.03202458\n",
      "Iteration 41840, loss = 0.03202734\n",
      "Iteration 41841, loss = 0.03204449\n",
      "Iteration 41842, loss = 0.03204604\n",
      "Iteration 41843, loss = 0.03205172\n",
      "Iteration 41844, loss = 0.03204184\n",
      "Iteration 41845, loss = 0.03204334\n",
      "Iteration 41846, loss = 0.03204599\n",
      "Iteration 41847, loss = 0.03204441\n",
      "Iteration 41848, loss = 0.03204908\n",
      "Iteration 41849, loss = 0.03205132\n",
      "Iteration 41850, loss = 0.03204792\n",
      "Iteration 41851, loss = 0.03203947\n",
      "Iteration 41852, loss = 0.03203681\n",
      "Iteration 41853, loss = 0.03204733\n",
      "Iteration 41854, loss = 0.03204704\n",
      "Iteration 41855, loss = 0.03203945\n",
      "Iteration 41856, loss = 0.03204601\n",
      "Iteration 41857, loss = 0.03203504\n",
      "Iteration 41858, loss = 0.03203454\n",
      "Iteration 41859, loss = 0.03203117\n",
      "Iteration 41860, loss = 0.03203529\n",
      "Iteration 41861, loss = 0.03203855\n",
      "Iteration 41862, loss = 0.03203377\n",
      "Iteration 41863, loss = 0.03202662\n",
      "Iteration 41864, loss = 0.03201637\n",
      "Iteration 41865, loss = 0.03202268\n",
      "Iteration 41866, loss = 0.03203386\n",
      "Iteration 41867, loss = 0.03203044\n",
      "Iteration 41868, loss = 0.03203120\n",
      "Iteration 41869, loss = 0.03202987\n",
      "Iteration 41870, loss = 0.03201979\n",
      "Iteration 41871, loss = 0.03202877\n",
      "Iteration 41872, loss = 0.03202993\n",
      "Iteration 41873, loss = 0.03203099\n",
      "Iteration 41874, loss = 0.03202847\n",
      "Iteration 41875, loss = 0.03201391\n",
      "Iteration 41876, loss = 0.03202659\n",
      "Iteration 41877, loss = 0.03204273\n",
      "Iteration 41878, loss = 0.03204324\n",
      "Iteration 41879, loss = 0.03202868\n",
      "Iteration 41880, loss = 0.03202799\n",
      "Iteration 41881, loss = 0.03202794\n",
      "Iteration 41882, loss = 0.03202122\n",
      "Iteration 41883, loss = 0.03201887\n",
      "Iteration 41884, loss = 0.03201512\n",
      "Iteration 41885, loss = 0.03200868\n",
      "Iteration 41886, loss = 0.03202137\n",
      "Iteration 41887, loss = 0.03201972\n",
      "Iteration 41888, loss = 0.03201232\n",
      "Iteration 41889, loss = 0.03202273\n",
      "Iteration 41890, loss = 0.03202847\n",
      "Iteration 41891, loss = 0.03202655\n",
      "Iteration 41892, loss = 0.03203571\n",
      "Iteration 41893, loss = 0.03203428\n",
      "Iteration 41894, loss = 0.03201644\n",
      "Iteration 41895, loss = 0.03201964\n",
      "Iteration 41896, loss = 0.03202884\n",
      "Iteration 41897, loss = 0.03201848\n",
      "Iteration 41898, loss = 0.03202259\n",
      "Iteration 41899, loss = 0.03201222\n",
      "Iteration 41900, loss = 0.03201328\n",
      "Iteration 41901, loss = 0.03200125\n",
      "Iteration 41902, loss = 0.03200133\n",
      "Iteration 41903, loss = 0.03200610\n",
      "Iteration 41904, loss = 0.03200888\n",
      "Iteration 41905, loss = 0.03200026\n",
      "Iteration 41906, loss = 0.03201365\n",
      "Iteration 41907, loss = 0.03202627\n",
      "Iteration 41908, loss = 0.03201762\n",
      "Iteration 41909, loss = 0.03200017\n",
      "Iteration 41910, loss = 0.03200264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 41911, loss = 0.03200567\n",
      "Iteration 41912, loss = 0.03199866\n",
      "Iteration 41913, loss = 0.03199637\n",
      "Iteration 41914, loss = 0.03200726\n",
      "Iteration 41915, loss = 0.03200838\n",
      "Iteration 41916, loss = 0.03200478\n",
      "Iteration 41917, loss = 0.03199880\n",
      "Iteration 41918, loss = 0.03200029\n",
      "Iteration 41919, loss = 0.03199837\n",
      "Iteration 41920, loss = 0.03199112\n",
      "Iteration 41921, loss = 0.03199135\n",
      "Iteration 41922, loss = 0.03199074\n",
      "Iteration 41923, loss = 0.03199410\n",
      "Iteration 41924, loss = 0.03200190\n",
      "Iteration 41925, loss = 0.03199922\n",
      "Iteration 41926, loss = 0.03199272\n",
      "Iteration 41927, loss = 0.03198802\n",
      "Iteration 41928, loss = 0.03200342\n",
      "Iteration 41929, loss = 0.03200402\n",
      "Iteration 41930, loss = 0.03199556\n",
      "Iteration 41931, loss = 0.03199225\n",
      "Iteration 41932, loss = 0.03199820\n",
      "Iteration 41933, loss = 0.03199152\n",
      "Iteration 41934, loss = 0.03198919\n",
      "Iteration 41935, loss = 0.03199364\n",
      "Iteration 41936, loss = 0.03199981\n",
      "Iteration 41937, loss = 0.03200007\n",
      "Iteration 41938, loss = 0.03198894\n",
      "Iteration 41939, loss = 0.03199675\n",
      "Iteration 41940, loss = 0.03198819\n",
      "Iteration 41941, loss = 0.03199465\n",
      "Iteration 41942, loss = 0.03200058\n",
      "Iteration 41943, loss = 0.03198882\n",
      "Iteration 41944, loss = 0.03198431\n",
      "Iteration 41945, loss = 0.03200253\n",
      "Iteration 41946, loss = 0.03200688\n",
      "Iteration 41947, loss = 0.03199614\n",
      "Iteration 41948, loss = 0.03199581\n",
      "Iteration 41949, loss = 0.03199328\n",
      "Iteration 41950, loss = 0.03198744\n",
      "Iteration 41951, loss = 0.03198626\n",
      "Iteration 41952, loss = 0.03198536\n",
      "Iteration 41953, loss = 0.03198759\n",
      "Iteration 41954, loss = 0.03198024\n",
      "Iteration 41955, loss = 0.03198715\n",
      "Iteration 41956, loss = 0.03199547\n",
      "Iteration 41957, loss = 0.03199603\n",
      "Iteration 41958, loss = 0.03198435\n",
      "Iteration 41959, loss = 0.03198724\n",
      "Iteration 41960, loss = 0.03199079\n",
      "Iteration 41961, loss = 0.03198247\n",
      "Iteration 41962, loss = 0.03198739\n",
      "Iteration 41963, loss = 0.03198471\n",
      "Iteration 41964, loss = 0.03198958\n",
      "Iteration 41965, loss = 0.03198186\n",
      "Iteration 41966, loss = 0.03198141\n",
      "Iteration 41967, loss = 0.03199084\n",
      "Iteration 41968, loss = 0.03198534\n",
      "Iteration 41969, loss = 0.03199170\n",
      "Iteration 41970, loss = 0.03198981\n",
      "Iteration 41971, loss = 0.03198332\n",
      "Iteration 41972, loss = 0.03197674\n",
      "Iteration 41973, loss = 0.03197652\n",
      "Iteration 41974, loss = 0.03197952\n",
      "Iteration 41975, loss = 0.03197541\n",
      "Iteration 41976, loss = 0.03197789\n",
      "Iteration 41977, loss = 0.03198548\n",
      "Iteration 41978, loss = 0.03198899\n",
      "Iteration 41979, loss = 0.03197556\n",
      "Iteration 41980, loss = 0.03197226\n",
      "Iteration 41981, loss = 0.03197908\n",
      "Iteration 41982, loss = 0.03197409\n",
      "Iteration 41983, loss = 0.03197761\n",
      "Iteration 41984, loss = 0.03197584\n",
      "Iteration 41985, loss = 0.03197787\n",
      "Iteration 41986, loss = 0.03197501\n",
      "Iteration 41987, loss = 0.03197715\n",
      "Iteration 41988, loss = 0.03197821\n",
      "Iteration 41989, loss = 0.03197763\n",
      "Iteration 41990, loss = 0.03197459\n",
      "Iteration 41991, loss = 0.03197347\n",
      "Iteration 41992, loss = 0.03198287\n",
      "Iteration 41993, loss = 0.03197528\n",
      "Iteration 41994, loss = 0.03197816\n",
      "Iteration 41995, loss = 0.03198225\n",
      "Iteration 41996, loss = 0.03197535\n",
      "Iteration 41997, loss = 0.03196650\n",
      "Iteration 41998, loss = 0.03197506\n",
      "Iteration 41999, loss = 0.03196602\n",
      "Iteration 42000, loss = 0.03196322\n",
      "Iteration 42001, loss = 0.03197668\n",
      "Iteration 42002, loss = 0.03197877\n",
      "Iteration 42003, loss = 0.03196816\n",
      "Iteration 42004, loss = 0.03196490\n",
      "Iteration 42005, loss = 0.03196478\n",
      "Iteration 42006, loss = 0.03196245\n",
      "Iteration 42007, loss = 0.03196215\n",
      "Iteration 42008, loss = 0.03196338\n",
      "Iteration 42009, loss = 0.03196143\n",
      "Iteration 42010, loss = 0.03196750\n",
      "Iteration 42011, loss = 0.03197020\n",
      "Iteration 42012, loss = 0.03196816\n",
      "Iteration 42013, loss = 0.03197656\n",
      "Iteration 42014, loss = 0.03197373\n",
      "Iteration 42015, loss = 0.03197560\n",
      "Iteration 42016, loss = 0.03197076\n",
      "Iteration 42017, loss = 0.03197408\n",
      "Iteration 42018, loss = 0.03196941\n",
      "Iteration 42019, loss = 0.03197226\n",
      "Iteration 42020, loss = 0.03197552\n",
      "Iteration 42021, loss = 0.03197686\n",
      "Iteration 42022, loss = 0.03198199\n",
      "Iteration 42023, loss = 0.03198258\n",
      "Iteration 42024, loss = 0.03197711\n",
      "Iteration 42025, loss = 0.03196107\n",
      "Iteration 42026, loss = 0.03195437\n",
      "Iteration 42027, loss = 0.03197130\n",
      "Iteration 42028, loss = 0.03197981\n",
      "Iteration 42029, loss = 0.03197009\n",
      "Iteration 42030, loss = 0.03195589\n",
      "Iteration 42031, loss = 0.03196198\n",
      "Iteration 42032, loss = 0.03196552\n",
      "Iteration 42033, loss = 0.03196081\n",
      "Iteration 42034, loss = 0.03194897\n",
      "Iteration 42035, loss = 0.03196357\n",
      "Iteration 42036, loss = 0.03196567\n",
      "Iteration 42037, loss = 0.03195436\n",
      "Iteration 42038, loss = 0.03195179\n",
      "Iteration 42039, loss = 0.03195016\n",
      "Iteration 42040, loss = 0.03195570\n",
      "Iteration 42041, loss = 0.03195127\n",
      "Iteration 42042, loss = 0.03194979\n",
      "Iteration 42043, loss = 0.03195079\n",
      "Iteration 42044, loss = 0.03194987\n",
      "Iteration 42045, loss = 0.03195168\n",
      "Iteration 42046, loss = 0.03195185\n",
      "Iteration 42047, loss = 0.03195449\n",
      "Iteration 42048, loss = 0.03195093\n",
      "Iteration 42049, loss = 0.03194565\n",
      "Iteration 42050, loss = 0.03195332\n",
      "Iteration 42051, loss = 0.03194977\n",
      "Iteration 42052, loss = 0.03194289\n",
      "Iteration 42053, loss = 0.03195355\n",
      "Iteration 42054, loss = 0.03195439\n",
      "Iteration 42055, loss = 0.03194149\n",
      "Iteration 42056, loss = 0.03195631\n",
      "Iteration 42057, loss = 0.03195831\n",
      "Iteration 42058, loss = 0.03195819\n",
      "Iteration 42059, loss = 0.03196226\n",
      "Iteration 42060, loss = 0.03195412\n",
      "Iteration 42061, loss = 0.03194581\n",
      "Iteration 42062, loss = 0.03194814\n",
      "Iteration 42063, loss = 0.03194728\n",
      "Iteration 42064, loss = 0.03194313\n",
      "Iteration 42065, loss = 0.03194408\n",
      "Iteration 42066, loss = 0.03194605\n",
      "Iteration 42067, loss = 0.03194192\n",
      "Iteration 42068, loss = 0.03194226\n",
      "Iteration 42069, loss = 0.03194458\n",
      "Iteration 42070, loss = 0.03195303\n",
      "Iteration 42071, loss = 0.03194814\n",
      "Iteration 42072, loss = 0.03194405\n",
      "Iteration 42073, loss = 0.03194869\n",
      "Iteration 42074, loss = 0.03194324\n",
      "Iteration 42075, loss = 0.03195125\n",
      "Iteration 42076, loss = 0.03195636\n",
      "Iteration 42077, loss = 0.03194532\n",
      "Iteration 42078, loss = 0.03194500\n",
      "Iteration 42079, loss = 0.03195407\n",
      "Iteration 42080, loss = 0.03194638\n",
      "Iteration 42081, loss = 0.03195169\n",
      "Iteration 42082, loss = 0.03196327\n",
      "Iteration 42083, loss = 0.03195805\n",
      "Iteration 42084, loss = 0.03194578\n",
      "Iteration 42085, loss = 0.03194494\n",
      "Iteration 42086, loss = 0.03195670\n",
      "Iteration 42087, loss = 0.03196037\n",
      "Iteration 42088, loss = 0.03194549\n",
      "Iteration 42089, loss = 0.03195350\n",
      "Iteration 42090, loss = 0.03195018\n",
      "Iteration 42091, loss = 0.03195123\n",
      "Iteration 42092, loss = 0.03194672\n",
      "Iteration 42093, loss = 0.03194342\n",
      "Iteration 42094, loss = 0.03194394\n",
      "Iteration 42095, loss = 0.03193605\n",
      "Iteration 42096, loss = 0.03193679\n",
      "Iteration 42097, loss = 0.03194070\n",
      "Iteration 42098, loss = 0.03192854\n",
      "Iteration 42099, loss = 0.03193546\n",
      "Iteration 42100, loss = 0.03194108\n",
      "Iteration 42101, loss = 0.03194917\n",
      "Iteration 42102, loss = 0.03194410\n",
      "Iteration 42103, loss = 0.03193862\n",
      "Iteration 42104, loss = 0.03194379\n",
      "Iteration 42105, loss = 0.03195758\n",
      "Iteration 42106, loss = 0.03195765\n",
      "Iteration 42107, loss = 0.03193798\n",
      "Iteration 42108, loss = 0.03193057\n",
      "Iteration 42109, loss = 0.03193849\n",
      "Iteration 42110, loss = 0.03194093\n",
      "Iteration 42111, loss = 0.03193899\n",
      "Iteration 42112, loss = 0.03193038\n",
      "Iteration 42113, loss = 0.03192397\n",
      "Iteration 42114, loss = 0.03194367\n",
      "Iteration 42115, loss = 0.03194235\n",
      "Iteration 42116, loss = 0.03192771\n",
      "Iteration 42117, loss = 0.03192751\n",
      "Iteration 42118, loss = 0.03193296\n",
      "Iteration 42119, loss = 0.03193143\n",
      "Iteration 42120, loss = 0.03192303\n",
      "Iteration 42121, loss = 0.03192598\n",
      "Iteration 42122, loss = 0.03193109\n",
      "Iteration 42123, loss = 0.03192823\n",
      "Iteration 42124, loss = 0.03192556\n",
      "Iteration 42125, loss = 0.03192781\n",
      "Iteration 42126, loss = 0.03192177\n",
      "Iteration 42127, loss = 0.03193130\n",
      "Iteration 42128, loss = 0.03192979\n",
      "Iteration 42129, loss = 0.03193346\n",
      "Iteration 42130, loss = 0.03193252\n",
      "Iteration 42131, loss = 0.03192439\n",
      "Iteration 42132, loss = 0.03193345\n",
      "Iteration 42133, loss = 0.03193354\n",
      "Iteration 42134, loss = 0.03193284\n",
      "Iteration 42135, loss = 0.03193228\n",
      "Iteration 42136, loss = 0.03192458\n",
      "Iteration 42137, loss = 0.03193456\n",
      "Iteration 42138, loss = 0.03194210\n",
      "Iteration 42139, loss = 0.03193767\n",
      "Iteration 42140, loss = 0.03192684\n",
      "Iteration 42141, loss = 0.03192331\n",
      "Iteration 42142, loss = 0.03192025\n",
      "Iteration 42143, loss = 0.03192378\n",
      "Iteration 42144, loss = 0.03191649\n",
      "Iteration 42145, loss = 0.03192228\n",
      "Iteration 42146, loss = 0.03191679\n",
      "Iteration 42147, loss = 0.03191701\n",
      "Iteration 42148, loss = 0.03192551\n",
      "Iteration 42149, loss = 0.03192371\n",
      "Iteration 42150, loss = 0.03191886\n",
      "Iteration 42151, loss = 0.03191665\n",
      "Iteration 42152, loss = 0.03192785\n",
      "Iteration 42153, loss = 0.03193171\n",
      "Iteration 42154, loss = 0.03192233\n",
      "Iteration 42155, loss = 0.03192284\n",
      "Iteration 42156, loss = 0.03192581\n",
      "Iteration 42157, loss = 0.03192085\n",
      "Iteration 42158, loss = 0.03192180\n",
      "Iteration 42159, loss = 0.03191147\n",
      "Iteration 42160, loss = 0.03191776\n",
      "Iteration 42161, loss = 0.03192357\n",
      "Iteration 42162, loss = 0.03192153\n",
      "Iteration 42163, loss = 0.03191376\n",
      "Iteration 42164, loss = 0.03191778\n",
      "Iteration 42165, loss = 0.03192674\n",
      "Iteration 42166, loss = 0.03192717\n",
      "Iteration 42167, loss = 0.03192328\n",
      "Iteration 42168, loss = 0.03191118\n",
      "Iteration 42169, loss = 0.03191525\n",
      "Iteration 42170, loss = 0.03192789\n",
      "Iteration 42171, loss = 0.03191740\n",
      "Iteration 42172, loss = 0.03191406\n",
      "Iteration 42173, loss = 0.03191740\n",
      "Iteration 42174, loss = 0.03191323\n",
      "Iteration 42175, loss = 0.03191102\n",
      "Iteration 42176, loss = 0.03190849\n",
      "Iteration 42177, loss = 0.03190434\n",
      "Iteration 42178, loss = 0.03190594\n",
      "Iteration 42179, loss = 0.03191391\n",
      "Iteration 42180, loss = 0.03191816\n",
      "Iteration 42181, loss = 0.03191472\n",
      "Iteration 42182, loss = 0.03191226\n",
      "Iteration 42183, loss = 0.03190912\n",
      "Iteration 42184, loss = 0.03191152\n",
      "Iteration 42185, loss = 0.03191125\n",
      "Iteration 42186, loss = 0.03191902\n",
      "Iteration 42187, loss = 0.03191327\n",
      "Iteration 42188, loss = 0.03191798\n",
      "Iteration 42189, loss = 0.03191847\n",
      "Iteration 42190, loss = 0.03192400\n",
      "Iteration 42191, loss = 0.03191016\n",
      "Iteration 42192, loss = 0.03190465\n",
      "Iteration 42193, loss = 0.03191640\n",
      "Iteration 42194, loss = 0.03191689\n",
      "Iteration 42195, loss = 0.03190864\n",
      "Iteration 42196, loss = 0.03190636\n",
      "Iteration 42197, loss = 0.03190598\n",
      "Iteration 42198, loss = 0.03190808\n",
      "Iteration 42199, loss = 0.03190430\n",
      "Iteration 42200, loss = 0.03189538\n",
      "Iteration 42201, loss = 0.03190360\n",
      "Iteration 42202, loss = 0.03191299\n",
      "Iteration 42203, loss = 0.03190961\n",
      "Iteration 42204, loss = 0.03191521\n",
      "Iteration 42205, loss = 0.03191101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 42206, loss = 0.03189529\n",
      "Iteration 42207, loss = 0.03190901\n",
      "Iteration 42208, loss = 0.03192096\n",
      "Iteration 42209, loss = 0.03191842\n",
      "Iteration 42210, loss = 0.03191591\n",
      "Iteration 42211, loss = 0.03190377\n",
      "Iteration 42212, loss = 0.03189969\n",
      "Iteration 42213, loss = 0.03190159\n",
      "Iteration 42214, loss = 0.03189988\n",
      "Iteration 42215, loss = 0.03191301\n",
      "Iteration 42216, loss = 0.03190971\n",
      "Iteration 42217, loss = 0.03189513\n",
      "Iteration 42218, loss = 0.03189749\n",
      "Iteration 42219, loss = 0.03190617\n",
      "Iteration 42220, loss = 0.03191107\n",
      "Iteration 42221, loss = 0.03190782\n",
      "Iteration 42222, loss = 0.03189151\n",
      "Iteration 42223, loss = 0.03190429\n",
      "Iteration 42224, loss = 0.03191440\n",
      "Iteration 42225, loss = 0.03191578\n",
      "Iteration 42226, loss = 0.03189637\n",
      "Iteration 42227, loss = 0.03189098\n",
      "Iteration 42228, loss = 0.03189577\n",
      "Iteration 42229, loss = 0.03190449\n",
      "Iteration 42230, loss = 0.03190643\n",
      "Iteration 42231, loss = 0.03189749\n",
      "Iteration 42232, loss = 0.03189903\n",
      "Iteration 42233, loss = 0.03190395\n",
      "Iteration 42234, loss = 0.03189587\n",
      "Iteration 42235, loss = 0.03188700\n",
      "Iteration 42236, loss = 0.03190591\n",
      "Iteration 42237, loss = 0.03190947\n",
      "Iteration 42238, loss = 0.03190085\n",
      "Iteration 42239, loss = 0.03190978\n",
      "Iteration 42240, loss = 0.03190747\n",
      "Iteration 42241, loss = 0.03190180\n",
      "Iteration 42242, loss = 0.03190679\n",
      "Iteration 42243, loss = 0.03191096\n",
      "Iteration 42244, loss = 0.03190815\n",
      "Iteration 42245, loss = 0.03188902\n",
      "Iteration 42246, loss = 0.03189118\n",
      "Iteration 42247, loss = 0.03190399\n",
      "Iteration 42248, loss = 0.03190263\n",
      "Iteration 42249, loss = 0.03188605\n",
      "Iteration 42250, loss = 0.03188610\n",
      "Iteration 42251, loss = 0.03188453\n",
      "Iteration 42252, loss = 0.03188537\n",
      "Iteration 42253, loss = 0.03187623\n",
      "Iteration 42254, loss = 0.03188905\n",
      "Iteration 42255, loss = 0.03189325\n",
      "Iteration 42256, loss = 0.03188516\n",
      "Iteration 42257, loss = 0.03188317\n",
      "Iteration 42258, loss = 0.03187795\n",
      "Iteration 42259, loss = 0.03187996\n",
      "Iteration 42260, loss = 0.03187519\n",
      "Iteration 42261, loss = 0.03187501\n",
      "Iteration 42262, loss = 0.03187870\n",
      "Iteration 42263, loss = 0.03187317\n",
      "Iteration 42264, loss = 0.03188246\n",
      "Iteration 42265, loss = 0.03188666\n",
      "Iteration 42266, loss = 0.03187864\n",
      "Iteration 42267, loss = 0.03188467\n",
      "Iteration 42268, loss = 0.03188750\n",
      "Iteration 42269, loss = 0.03188160\n",
      "Iteration 42270, loss = 0.03188607\n",
      "Iteration 42271, loss = 0.03188577\n",
      "Iteration 42272, loss = 0.03188975\n",
      "Iteration 42273, loss = 0.03188280\n",
      "Iteration 42274, loss = 0.03188787\n",
      "Iteration 42275, loss = 0.03188126\n",
      "Iteration 42276, loss = 0.03187859\n",
      "Iteration 42277, loss = 0.03187956\n",
      "Iteration 42278, loss = 0.03187671\n",
      "Iteration 42279, loss = 0.03189007\n",
      "Iteration 42280, loss = 0.03188828\n",
      "Iteration 42281, loss = 0.03188102\n",
      "Iteration 42282, loss = 0.03187740\n",
      "Iteration 42283, loss = 0.03188287\n",
      "Iteration 42284, loss = 0.03187385\n",
      "Iteration 42285, loss = 0.03187361\n",
      "Iteration 42286, loss = 0.03187200\n",
      "Iteration 42287, loss = 0.03187631\n",
      "Iteration 42288, loss = 0.03188255\n",
      "Iteration 42289, loss = 0.03187863\n",
      "Iteration 42290, loss = 0.03187173\n",
      "Iteration 42291, loss = 0.03186559\n",
      "Iteration 42292, loss = 0.03187691\n",
      "Iteration 42293, loss = 0.03187677\n",
      "Iteration 42294, loss = 0.03187145\n",
      "Iteration 42295, loss = 0.03187326\n",
      "Iteration 42296, loss = 0.03187016\n",
      "Iteration 42297, loss = 0.03187187\n",
      "Iteration 42298, loss = 0.03187275\n",
      "Iteration 42299, loss = 0.03186064\n",
      "Iteration 42300, loss = 0.03188353\n",
      "Iteration 42301, loss = 0.03189289\n",
      "Iteration 42302, loss = 0.03189251\n",
      "Iteration 42303, loss = 0.03188258\n",
      "Iteration 42304, loss = 0.03188022\n",
      "Iteration 42305, loss = 0.03188106\n",
      "Iteration 42306, loss = 0.03187129\n",
      "Iteration 42307, loss = 0.03187307\n",
      "Iteration 42308, loss = 0.03186557\n",
      "Iteration 42309, loss = 0.03186399\n",
      "Iteration 42310, loss = 0.03186969\n",
      "Iteration 42311, loss = 0.03187500\n",
      "Iteration 42312, loss = 0.03186444\n",
      "Iteration 42313, loss = 0.03186425\n",
      "Iteration 42314, loss = 0.03186451\n",
      "Iteration 42315, loss = 0.03186670\n",
      "Iteration 42316, loss = 0.03186745\n",
      "Iteration 42317, loss = 0.03185554\n",
      "Iteration 42318, loss = 0.03186405\n",
      "Iteration 42319, loss = 0.03186206\n",
      "Iteration 42320, loss = 0.03185723\n",
      "Iteration 42321, loss = 0.03187016\n",
      "Iteration 42322, loss = 0.03187472\n",
      "Iteration 42323, loss = 0.03188222\n",
      "Iteration 42324, loss = 0.03188101\n",
      "Iteration 42325, loss = 0.03187520\n",
      "Iteration 42326, loss = 0.03186377\n",
      "Iteration 42327, loss = 0.03185168\n",
      "Iteration 42328, loss = 0.03186364\n",
      "Iteration 42329, loss = 0.03186611\n",
      "Iteration 42330, loss = 0.03186144\n",
      "Iteration 42331, loss = 0.03184782\n",
      "Iteration 42332, loss = 0.03184793\n",
      "Iteration 42333, loss = 0.03185848\n",
      "Iteration 42334, loss = 0.03185587\n",
      "Iteration 42335, loss = 0.03184794\n",
      "Iteration 42336, loss = 0.03185160\n",
      "Iteration 42337, loss = 0.03186397\n",
      "Iteration 42338, loss = 0.03185869\n",
      "Iteration 42339, loss = 0.03185784\n",
      "Iteration 42340, loss = 0.03184612\n",
      "Iteration 42341, loss = 0.03185177\n",
      "Iteration 42342, loss = 0.03185964\n",
      "Iteration 42343, loss = 0.03186413\n",
      "Iteration 42344, loss = 0.03186415\n",
      "Iteration 42345, loss = 0.03185295\n",
      "Iteration 42346, loss = 0.03184514\n",
      "Iteration 42347, loss = 0.03185827\n",
      "Iteration 42348, loss = 0.03186099\n",
      "Iteration 42349, loss = 0.03185828\n",
      "Iteration 42350, loss = 0.03185805\n",
      "Iteration 42351, loss = 0.03184580\n",
      "Iteration 42352, loss = 0.03184829\n",
      "Iteration 42353, loss = 0.03184975\n",
      "Iteration 42354, loss = 0.03184196\n",
      "Iteration 42355, loss = 0.03184296\n",
      "Iteration 42356, loss = 0.03184114\n",
      "Iteration 42357, loss = 0.03184153\n",
      "Iteration 42358, loss = 0.03184120\n",
      "Iteration 42359, loss = 0.03184181\n",
      "Iteration 42360, loss = 0.03184128\n",
      "Iteration 42361, loss = 0.03184842\n",
      "Iteration 42362, loss = 0.03184378\n",
      "Iteration 42363, loss = 0.03184885\n",
      "Iteration 42364, loss = 0.03184410\n",
      "Iteration 42365, loss = 0.03183420\n",
      "Iteration 42366, loss = 0.03184402\n",
      "Iteration 42367, loss = 0.03184408\n",
      "Iteration 42368, loss = 0.03183910\n",
      "Iteration 42369, loss = 0.03183981\n",
      "Iteration 42370, loss = 0.03183850\n",
      "Iteration 42371, loss = 0.03184138\n",
      "Iteration 42372, loss = 0.03184972\n",
      "Iteration 42373, loss = 0.03185162\n",
      "Iteration 42374, loss = 0.03185355\n",
      "Iteration 42375, loss = 0.03184590\n",
      "Iteration 42376, loss = 0.03184157\n",
      "Iteration 42377, loss = 0.03185994\n",
      "Iteration 42378, loss = 0.03186019\n",
      "Iteration 42379, loss = 0.03185566\n",
      "Iteration 42380, loss = 0.03185308\n",
      "Iteration 42381, loss = 0.03185094\n",
      "Iteration 42382, loss = 0.03185576\n",
      "Iteration 42383, loss = 0.03185317\n",
      "Iteration 42384, loss = 0.03184395\n",
      "Iteration 42385, loss = 0.03183791\n",
      "Iteration 42386, loss = 0.03184611\n",
      "Iteration 42387, loss = 0.03184667\n",
      "Iteration 42388, loss = 0.03184503\n",
      "Iteration 42389, loss = 0.03183251\n",
      "Iteration 42390, loss = 0.03183430\n",
      "Iteration 42391, loss = 0.03183583\n",
      "Iteration 42392, loss = 0.03183849\n",
      "Iteration 42393, loss = 0.03184487\n",
      "Iteration 42394, loss = 0.03184407\n",
      "Iteration 42395, loss = 0.03184176\n",
      "Iteration 42396, loss = 0.03183229\n",
      "Iteration 42397, loss = 0.03183774\n",
      "Iteration 42398, loss = 0.03183150\n",
      "Iteration 42399, loss = 0.03183210\n",
      "Iteration 42400, loss = 0.03184274\n",
      "Iteration 42401, loss = 0.03184087\n",
      "Iteration 42402, loss = 0.03183188\n",
      "Iteration 42403, loss = 0.03184283\n",
      "Iteration 42404, loss = 0.03183526\n",
      "Iteration 42405, loss = 0.03183652\n",
      "Iteration 42406, loss = 0.03184743\n",
      "Iteration 42407, loss = 0.03184805\n",
      "Iteration 42408, loss = 0.03184795\n",
      "Iteration 42409, loss = 0.03184769\n",
      "Iteration 42410, loss = 0.03184829\n",
      "Iteration 42411, loss = 0.03183870\n",
      "Iteration 42412, loss = 0.03183565\n",
      "Iteration 42413, loss = 0.03183675\n",
      "Iteration 42414, loss = 0.03184143\n",
      "Iteration 42415, loss = 0.03183834\n",
      "Iteration 42416, loss = 0.03183045\n",
      "Iteration 42417, loss = 0.03183048\n",
      "Iteration 42418, loss = 0.03183937\n",
      "Iteration 42419, loss = 0.03184038\n",
      "Iteration 42420, loss = 0.03184663\n",
      "Iteration 42421, loss = 0.03184390\n",
      "Iteration 42422, loss = 0.03183153\n",
      "Iteration 42423, loss = 0.03182064\n",
      "Iteration 42424, loss = 0.03183508\n",
      "Iteration 42425, loss = 0.03184063\n",
      "Iteration 42426, loss = 0.03183657\n",
      "Iteration 42427, loss = 0.03182953\n",
      "Iteration 42428, loss = 0.03182325\n",
      "Iteration 42429, loss = 0.03182577\n",
      "Iteration 42430, loss = 0.03182425\n",
      "Iteration 42431, loss = 0.03182239\n",
      "Iteration 42432, loss = 0.03181980\n",
      "Iteration 42433, loss = 0.03182724\n",
      "Iteration 42434, loss = 0.03181885\n",
      "Iteration 42435, loss = 0.03181395\n",
      "Iteration 42436, loss = 0.03182522\n",
      "Iteration 42437, loss = 0.03182269\n",
      "Iteration 42438, loss = 0.03181506\n",
      "Iteration 42439, loss = 0.03182772\n",
      "Iteration 42440, loss = 0.03182699\n",
      "Iteration 42441, loss = 0.03181868\n",
      "Iteration 42442, loss = 0.03182246\n",
      "Iteration 42443, loss = 0.03182151\n",
      "Iteration 42444, loss = 0.03181544\n",
      "Iteration 42445, loss = 0.03181506\n",
      "Iteration 42446, loss = 0.03182380\n",
      "Iteration 42447, loss = 0.03182938\n",
      "Iteration 42448, loss = 0.03181794\n",
      "Iteration 42449, loss = 0.03182592\n",
      "Iteration 42450, loss = 0.03183382\n",
      "Iteration 42451, loss = 0.03183304\n",
      "Iteration 42452, loss = 0.03183593\n",
      "Iteration 42453, loss = 0.03183332\n",
      "Iteration 42454, loss = 0.03183456\n",
      "Iteration 42455, loss = 0.03183416\n",
      "Iteration 42456, loss = 0.03184058\n",
      "Iteration 42457, loss = 0.03183465\n",
      "Iteration 42458, loss = 0.03182743\n",
      "Iteration 42459, loss = 0.03182595\n",
      "Iteration 42460, loss = 0.03182170\n",
      "Iteration 42461, loss = 0.03182008\n",
      "Iteration 42462, loss = 0.03181584\n",
      "Iteration 42463, loss = 0.03182001\n",
      "Iteration 42464, loss = 0.03182387\n",
      "Iteration 42465, loss = 0.03181401\n",
      "Iteration 42466, loss = 0.03180191\n",
      "Iteration 42467, loss = 0.03181302\n",
      "Iteration 42468, loss = 0.03181949\n",
      "Iteration 42469, loss = 0.03182723\n",
      "Iteration 42470, loss = 0.03182632\n",
      "Iteration 42471, loss = 0.03181800\n",
      "Iteration 42472, loss = 0.03180189\n",
      "Iteration 42473, loss = 0.03181821\n",
      "Iteration 42474, loss = 0.03183258\n",
      "Iteration 42475, loss = 0.03182821\n",
      "Iteration 42476, loss = 0.03182040\n",
      "Iteration 42477, loss = 0.03180271\n",
      "Iteration 42478, loss = 0.03181253\n",
      "Iteration 42479, loss = 0.03180563\n",
      "Iteration 42480, loss = 0.03180963\n",
      "Iteration 42481, loss = 0.03181364\n",
      "Iteration 42482, loss = 0.03181136\n",
      "Iteration 42483, loss = 0.03181022\n",
      "Iteration 42484, loss = 0.03181332\n",
      "Iteration 42485, loss = 0.03181878\n",
      "Iteration 42486, loss = 0.03182129\n",
      "Iteration 42487, loss = 0.03181271\n",
      "Iteration 42488, loss = 0.03180155\n",
      "Iteration 42489, loss = 0.03181481\n",
      "Iteration 42490, loss = 0.03182171\n",
      "Iteration 42491, loss = 0.03182015\n",
      "Iteration 42492, loss = 0.03180317\n",
      "Iteration 42493, loss = 0.03180425\n",
      "Iteration 42494, loss = 0.03181458\n",
      "Iteration 42495, loss = 0.03181769\n",
      "Iteration 42496, loss = 0.03181699\n",
      "Iteration 42497, loss = 0.03179858\n",
      "Iteration 42498, loss = 0.03181143\n",
      "Iteration 42499, loss = 0.03182018\n",
      "Iteration 42500, loss = 0.03181564\n",
      "Iteration 42501, loss = 0.03180227\n",
      "Iteration 42502, loss = 0.03181386\n",
      "Iteration 42503, loss = 0.03181574\n",
      "Iteration 42504, loss = 0.03182869\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 42505, loss = 0.03182555\n",
      "Iteration 42506, loss = 0.03180670\n",
      "Iteration 42507, loss = 0.03179301\n",
      "Iteration 42508, loss = 0.03180500\n",
      "Iteration 42509, loss = 0.03181281\n",
      "Iteration 42510, loss = 0.03181013\n",
      "Iteration 42511, loss = 0.03179885\n",
      "Iteration 42512, loss = 0.03178298\n",
      "Iteration 42513, loss = 0.03179127\n",
      "Iteration 42514, loss = 0.03179777\n",
      "Iteration 42515, loss = 0.03178842\n",
      "Iteration 42516, loss = 0.03178952\n",
      "Iteration 42517, loss = 0.03179898\n",
      "Iteration 42518, loss = 0.03178707\n",
      "Iteration 42519, loss = 0.03178178\n",
      "Iteration 42520, loss = 0.03178661\n",
      "Iteration 42521, loss = 0.03178737\n",
      "Iteration 42522, loss = 0.03178520\n",
      "Iteration 42523, loss = 0.03178660\n",
      "Iteration 42524, loss = 0.03179490\n",
      "Iteration 42525, loss = 0.03178881\n",
      "Iteration 42526, loss = 0.03178356\n",
      "Iteration 42527, loss = 0.03178969\n",
      "Iteration 42528, loss = 0.03178525\n",
      "Iteration 42529, loss = 0.03178437\n",
      "Iteration 42530, loss = 0.03178982\n",
      "Iteration 42531, loss = 0.03178498\n",
      "Iteration 42532, loss = 0.03177952\n",
      "Iteration 42533, loss = 0.03178275\n",
      "Iteration 42534, loss = 0.03178963\n",
      "Iteration 42535, loss = 0.03178252\n",
      "Iteration 42536, loss = 0.03178960\n",
      "Iteration 42537, loss = 0.03179291\n",
      "Iteration 42538, loss = 0.03179139\n",
      "Iteration 42539, loss = 0.03179024\n",
      "Iteration 42540, loss = 0.03179747\n",
      "Iteration 42541, loss = 0.03179844\n",
      "Iteration 42542, loss = 0.03179472\n",
      "Iteration 42543, loss = 0.03179059\n",
      "Iteration 42544, loss = 0.03178069\n",
      "Iteration 42545, loss = 0.03178676\n",
      "Iteration 42546, loss = 0.03179598\n",
      "Iteration 42547, loss = 0.03180294\n",
      "Iteration 42548, loss = 0.03179750\n",
      "Iteration 42549, loss = 0.03179451\n",
      "Iteration 42550, loss = 0.03179537\n",
      "Iteration 42551, loss = 0.03179130\n",
      "Iteration 42552, loss = 0.03178246\n",
      "Iteration 42553, loss = 0.03178814\n",
      "Iteration 42554, loss = 0.03179422\n",
      "Iteration 42555, loss = 0.03179667\n",
      "Iteration 42556, loss = 0.03178126\n",
      "Iteration 42557, loss = 0.03176985\n",
      "Iteration 42558, loss = 0.03177693\n",
      "Iteration 42559, loss = 0.03177631\n",
      "Iteration 42560, loss = 0.03177778\n",
      "Iteration 42561, loss = 0.03178070\n",
      "Iteration 42562, loss = 0.03177544\n",
      "Iteration 42563, loss = 0.03177923\n",
      "Iteration 42564, loss = 0.03177952\n",
      "Iteration 42565, loss = 0.03177704\n",
      "Iteration 42566, loss = 0.03178001\n",
      "Iteration 42567, loss = 0.03177609\n",
      "Iteration 42568, loss = 0.03177701\n",
      "Iteration 42569, loss = 0.03178771\n",
      "Iteration 42570, loss = 0.03178343\n",
      "Iteration 42571, loss = 0.03178167\n",
      "Iteration 42572, loss = 0.03178633\n",
      "Iteration 42573, loss = 0.03180010\n",
      "Iteration 42574, loss = 0.03180118\n",
      "Iteration 42575, loss = 0.03178953\n",
      "Iteration 42576, loss = 0.03177187\n",
      "Iteration 42577, loss = 0.03178973\n",
      "Iteration 42578, loss = 0.03180425\n",
      "Iteration 42579, loss = 0.03180023\n",
      "Iteration 42580, loss = 0.03178651\n",
      "Iteration 42581, loss = 0.03177691\n",
      "Iteration 42582, loss = 0.03177548\n",
      "Iteration 42583, loss = 0.03178359\n",
      "Iteration 42584, loss = 0.03177154\n",
      "Iteration 42585, loss = 0.03176636\n",
      "Iteration 42586, loss = 0.03177209\n",
      "Iteration 42587, loss = 0.03177426\n",
      "Iteration 42588, loss = 0.03177521\n",
      "Iteration 42589, loss = 0.03176697\n",
      "Iteration 42590, loss = 0.03176795\n",
      "Iteration 42591, loss = 0.03177267\n",
      "Iteration 42592, loss = 0.03176195\n",
      "Iteration 42593, loss = 0.03176297\n",
      "Iteration 42594, loss = 0.03176949\n",
      "Iteration 42595, loss = 0.03176464\n",
      "Iteration 42596, loss = 0.03176395\n",
      "Iteration 42597, loss = 0.03177140\n",
      "Iteration 42598, loss = 0.03176750\n",
      "Iteration 42599, loss = 0.03176593\n",
      "Iteration 42600, loss = 0.03176867\n",
      "Iteration 42601, loss = 0.03178285\n",
      "Iteration 42602, loss = 0.03178313\n",
      "Iteration 42603, loss = 0.03177479\n",
      "Iteration 42604, loss = 0.03176375\n",
      "Iteration 42605, loss = 0.03176086\n",
      "Iteration 42606, loss = 0.03177514\n",
      "Iteration 42607, loss = 0.03177833\n",
      "Iteration 42608, loss = 0.03176611\n",
      "Iteration 42609, loss = 0.03176359\n",
      "Iteration 42610, loss = 0.03176760\n",
      "Iteration 42611, loss = 0.03176798\n",
      "Iteration 42612, loss = 0.03176781\n",
      "Iteration 42613, loss = 0.03176183\n",
      "Iteration 42614, loss = 0.03176658\n",
      "Iteration 42615, loss = 0.03176081\n",
      "Iteration 42616, loss = 0.03176735\n",
      "Iteration 42617, loss = 0.03176710\n",
      "Iteration 42618, loss = 0.03176245\n",
      "Iteration 42619, loss = 0.03176045\n",
      "Iteration 42620, loss = 0.03176553\n",
      "Iteration 42621, loss = 0.03177142\n",
      "Iteration 42622, loss = 0.03177578\n",
      "Iteration 42623, loss = 0.03176662\n",
      "Iteration 42624, loss = 0.03176131\n",
      "Iteration 42625, loss = 0.03176214\n",
      "Iteration 42626, loss = 0.03176679\n",
      "Iteration 42627, loss = 0.03175683\n",
      "Iteration 42628, loss = 0.03174998\n",
      "Iteration 42629, loss = 0.03175525\n",
      "Iteration 42630, loss = 0.03176426\n",
      "Iteration 42631, loss = 0.03176166\n",
      "Iteration 42632, loss = 0.03175331\n",
      "Iteration 42633, loss = 0.03175857\n",
      "Iteration 42634, loss = 0.03176744\n",
      "Iteration 42635, loss = 0.03176918\n",
      "Iteration 42636, loss = 0.03176022\n",
      "Iteration 42637, loss = 0.03174750\n",
      "Iteration 42638, loss = 0.03175586\n",
      "Iteration 42639, loss = 0.03175970\n",
      "Iteration 42640, loss = 0.03174954\n",
      "Iteration 42641, loss = 0.03174492\n",
      "Iteration 42642, loss = 0.03174935\n",
      "Iteration 42643, loss = 0.03174666\n",
      "Iteration 42644, loss = 0.03174427\n",
      "Iteration 42645, loss = 0.03174615\n",
      "Iteration 42646, loss = 0.03174097\n",
      "Iteration 42647, loss = 0.03174973\n",
      "Iteration 42648, loss = 0.03174699\n",
      "Iteration 42649, loss = 0.03175539\n",
      "Iteration 42650, loss = 0.03175894\n",
      "Iteration 42651, loss = 0.03175165\n",
      "Iteration 42652, loss = 0.03175273\n",
      "Iteration 42653, loss = 0.03175073\n",
      "Iteration 42654, loss = 0.03175285\n",
      "Iteration 42655, loss = 0.03175896\n",
      "Iteration 42656, loss = 0.03175436\n",
      "Iteration 42657, loss = 0.03175911\n",
      "Iteration 42658, loss = 0.03175872\n",
      "Iteration 42659, loss = 0.03175482\n",
      "Iteration 42660, loss = 0.03174327\n",
      "Iteration 42661, loss = 0.03174753\n",
      "Iteration 42662, loss = 0.03174589\n",
      "Iteration 42663, loss = 0.03175031\n",
      "Iteration 42664, loss = 0.03176022\n",
      "Iteration 42665, loss = 0.03175240\n",
      "Iteration 42666, loss = 0.03174424\n",
      "Iteration 42667, loss = 0.03174892\n",
      "Iteration 42668, loss = 0.03174010\n",
      "Iteration 42669, loss = 0.03174157\n",
      "Iteration 42670, loss = 0.03175389\n",
      "Iteration 42671, loss = 0.03174585\n",
      "Iteration 42672, loss = 0.03173538\n",
      "Iteration 42673, loss = 0.03174035\n",
      "Iteration 42674, loss = 0.03174098\n",
      "Iteration 42675, loss = 0.03173711\n",
      "Iteration 42676, loss = 0.03173317\n",
      "Iteration 42677, loss = 0.03173997\n",
      "Iteration 42678, loss = 0.03174166\n",
      "Iteration 42679, loss = 0.03173354\n",
      "Iteration 42680, loss = 0.03173438\n",
      "Iteration 42681, loss = 0.03173548\n",
      "Iteration 42682, loss = 0.03173277\n",
      "Iteration 42683, loss = 0.03173652\n",
      "Iteration 42684, loss = 0.03173881\n",
      "Iteration 42685, loss = 0.03173910\n",
      "Iteration 42686, loss = 0.03173426\n",
      "Iteration 42687, loss = 0.03173925\n",
      "Iteration 42688, loss = 0.03174416\n",
      "Iteration 42689, loss = 0.03174029\n",
      "Iteration 42690, loss = 0.03172680\n",
      "Iteration 42691, loss = 0.03173476\n",
      "Iteration 42692, loss = 0.03173353\n",
      "Iteration 42693, loss = 0.03172980\n",
      "Iteration 42694, loss = 0.03172437\n",
      "Iteration 42695, loss = 0.03173589\n",
      "Iteration 42696, loss = 0.03174084\n",
      "Iteration 42697, loss = 0.03172797\n",
      "Iteration 42698, loss = 0.03173172\n",
      "Iteration 42699, loss = 0.03174031\n",
      "Iteration 42700, loss = 0.03173909\n",
      "Iteration 42701, loss = 0.03173644\n",
      "Iteration 42702, loss = 0.03173138\n",
      "Iteration 42703, loss = 0.03172825\n",
      "Iteration 42704, loss = 0.03172928\n",
      "Iteration 42705, loss = 0.03172981\n",
      "Iteration 42706, loss = 0.03173117\n",
      "Iteration 42707, loss = 0.03172776\n",
      "Iteration 42708, loss = 0.03173956\n",
      "Iteration 42709, loss = 0.03174268\n",
      "Iteration 42710, loss = 0.03172998\n",
      "Iteration 42711, loss = 0.03172336\n",
      "Iteration 42712, loss = 0.03173326\n",
      "Iteration 42713, loss = 0.03173571\n",
      "Iteration 42714, loss = 0.03173024\n",
      "Iteration 42715, loss = 0.03171869\n",
      "Iteration 42716, loss = 0.03171961\n",
      "Iteration 42717, loss = 0.03172116\n",
      "Iteration 42718, loss = 0.03171820\n",
      "Iteration 42719, loss = 0.03172141\n",
      "Iteration 42720, loss = 0.03172801\n",
      "Iteration 42721, loss = 0.03171916\n",
      "Iteration 42722, loss = 0.03172747\n",
      "Iteration 42723, loss = 0.03173304\n",
      "Iteration 42724, loss = 0.03172626\n",
      "Iteration 42725, loss = 0.03172267\n",
      "Iteration 42726, loss = 0.03171504\n",
      "Iteration 42727, loss = 0.03170924\n",
      "Iteration 42728, loss = 0.03172885\n",
      "Iteration 42729, loss = 0.03173006\n",
      "Iteration 42730, loss = 0.03172137\n",
      "Iteration 42731, loss = 0.03172001\n",
      "Iteration 42732, loss = 0.03172740\n",
      "Iteration 42733, loss = 0.03172015\n",
      "Iteration 42734, loss = 0.03172134\n",
      "Iteration 42735, loss = 0.03172522\n",
      "Iteration 42736, loss = 0.03173470\n",
      "Iteration 42737, loss = 0.03173959\n",
      "Iteration 42738, loss = 0.03173148\n",
      "Iteration 42739, loss = 0.03172835\n",
      "Iteration 42740, loss = 0.03172194\n",
      "Iteration 42741, loss = 0.03171971\n",
      "Iteration 42742, loss = 0.03172474\n",
      "Iteration 42743, loss = 0.03172304\n",
      "Iteration 42744, loss = 0.03172336\n",
      "Iteration 42745, loss = 0.03172281\n",
      "Iteration 42746, loss = 0.03171855\n",
      "Iteration 42747, loss = 0.03171888\n",
      "Iteration 42748, loss = 0.03171075\n",
      "Iteration 42749, loss = 0.03170209\n",
      "Iteration 42750, loss = 0.03171337\n",
      "Iteration 42751, loss = 0.03172645\n",
      "Iteration 42752, loss = 0.03171499\n",
      "Iteration 42753, loss = 0.03171010\n",
      "Iteration 42754, loss = 0.03171626\n",
      "Iteration 42755, loss = 0.03171651\n",
      "Iteration 42756, loss = 0.03171559\n",
      "Iteration 42757, loss = 0.03171456\n",
      "Iteration 42758, loss = 0.03170843\n",
      "Iteration 42759, loss = 0.03172234\n",
      "Iteration 42760, loss = 0.03172172\n",
      "Iteration 42761, loss = 0.03170887\n",
      "Iteration 42762, loss = 0.03170871\n",
      "Iteration 42763, loss = 0.03171353\n",
      "Iteration 42764, loss = 0.03172718\n",
      "Iteration 42765, loss = 0.03172960\n",
      "Iteration 42766, loss = 0.03171664\n",
      "Iteration 42767, loss = 0.03170457\n",
      "Iteration 42768, loss = 0.03171788\n",
      "Iteration 42769, loss = 0.03172132\n",
      "Iteration 42770, loss = 0.03170989\n",
      "Iteration 42771, loss = 0.03170574\n",
      "Iteration 42772, loss = 0.03171165\n",
      "Iteration 42773, loss = 0.03170964\n",
      "Iteration 42774, loss = 0.03170980\n",
      "Iteration 42775, loss = 0.03170793\n",
      "Iteration 42776, loss = 0.03170164\n",
      "Iteration 42777, loss = 0.03169852\n",
      "Iteration 42778, loss = 0.03170506\n",
      "Iteration 42779, loss = 0.03170028\n",
      "Iteration 42780, loss = 0.03169950\n",
      "Iteration 42781, loss = 0.03169658\n",
      "Iteration 42782, loss = 0.03170215\n",
      "Iteration 42783, loss = 0.03170073\n",
      "Iteration 42784, loss = 0.03169976\n",
      "Iteration 42785, loss = 0.03170121\n",
      "Iteration 42786, loss = 0.03170459\n",
      "Iteration 42787, loss = 0.03170766\n",
      "Iteration 42788, loss = 0.03169680\n",
      "Iteration 42789, loss = 0.03170719\n",
      "Iteration 42790, loss = 0.03171757\n",
      "Iteration 42791, loss = 0.03170760\n",
      "Iteration 42792, loss = 0.03171025\n",
      "Iteration 42793, loss = 0.03170637\n",
      "Iteration 42794, loss = 0.03170438\n",
      "Iteration 42795, loss = 0.03171852\n",
      "Iteration 42796, loss = 0.03171730\n",
      "Iteration 42797, loss = 0.03171660\n",
      "Iteration 42798, loss = 0.03170348\n",
      "Iteration 42799, loss = 0.03171109\n",
      "Iteration 42800, loss = 0.03172416\n",
      "Iteration 42801, loss = 0.03171674\n",
      "Iteration 42802, loss = 0.03169034\n",
      "Iteration 42803, loss = 0.03170217\n",
      "Iteration 42804, loss = 0.03171568\n",
      "Iteration 42805, loss = 0.03172500\n",
      "Iteration 42806, loss = 0.03172460\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 42807, loss = 0.03170902\n",
      "Iteration 42808, loss = 0.03171167\n",
      "Iteration 42809, loss = 0.03171956\n",
      "Iteration 42810, loss = 0.03171386\n",
      "Iteration 42811, loss = 0.03170438\n",
      "Iteration 42812, loss = 0.03170137\n",
      "Iteration 42813, loss = 0.03169091\n",
      "Iteration 42814, loss = 0.03170095\n",
      "Iteration 42815, loss = 0.03171111\n",
      "Iteration 42816, loss = 0.03170914\n",
      "Iteration 42817, loss = 0.03169807\n",
      "Iteration 42818, loss = 0.03168482\n",
      "Iteration 42819, loss = 0.03169174\n",
      "Iteration 42820, loss = 0.03169158\n",
      "Iteration 42821, loss = 0.03168287\n",
      "Iteration 42822, loss = 0.03169237\n",
      "Iteration 42823, loss = 0.03169235\n",
      "Iteration 42824, loss = 0.03168011\n",
      "Iteration 42825, loss = 0.03167873\n",
      "Iteration 42826, loss = 0.03168263\n",
      "Iteration 42827, loss = 0.03168795\n",
      "Iteration 42828, loss = 0.03168774\n",
      "Iteration 42829, loss = 0.03168590\n",
      "Iteration 42830, loss = 0.03169094\n",
      "Iteration 42831, loss = 0.03168637\n",
      "Iteration 42832, loss = 0.03169174\n",
      "Iteration 42833, loss = 0.03169063\n",
      "Iteration 42834, loss = 0.03169574\n",
      "Iteration 42835, loss = 0.03168660\n",
      "Iteration 42836, loss = 0.03168739\n",
      "Iteration 42837, loss = 0.03169351\n",
      "Iteration 42838, loss = 0.03169532\n",
      "Iteration 42839, loss = 0.03169017\n",
      "Iteration 42840, loss = 0.03169478\n",
      "Iteration 42841, loss = 0.03168254\n",
      "Iteration 42842, loss = 0.03168309\n",
      "Iteration 42843, loss = 0.03169133\n",
      "Iteration 42844, loss = 0.03169076\n",
      "Iteration 42845, loss = 0.03167937\n",
      "Iteration 42846, loss = 0.03167840\n",
      "Iteration 42847, loss = 0.03168230\n",
      "Iteration 42848, loss = 0.03167574\n",
      "Iteration 42849, loss = 0.03167831\n",
      "Iteration 42850, loss = 0.03167637\n",
      "Iteration 42851, loss = 0.03167371\n",
      "Iteration 42852, loss = 0.03167160\n",
      "Iteration 42853, loss = 0.03167753\n",
      "Iteration 42854, loss = 0.03167579\n",
      "Iteration 42855, loss = 0.03167230\n",
      "Iteration 42856, loss = 0.03167691\n",
      "Iteration 42857, loss = 0.03167043\n",
      "Iteration 42858, loss = 0.03167002\n",
      "Iteration 42859, loss = 0.03167329\n",
      "Iteration 42860, loss = 0.03167361\n",
      "Iteration 42861, loss = 0.03167700\n",
      "Iteration 42862, loss = 0.03167942\n",
      "Iteration 42863, loss = 0.03166768\n",
      "Iteration 42864, loss = 0.03167184\n",
      "Iteration 42865, loss = 0.03168353\n",
      "Iteration 42866, loss = 0.03167980\n",
      "Iteration 42867, loss = 0.03167829\n",
      "Iteration 42868, loss = 0.03168008\n",
      "Iteration 42869, loss = 0.03168840\n",
      "Iteration 42870, loss = 0.03168571\n",
      "Iteration 42871, loss = 0.03167667\n",
      "Iteration 42872, loss = 0.03168469\n",
      "Iteration 42873, loss = 0.03168353\n",
      "Iteration 42874, loss = 0.03167382\n",
      "Iteration 42875, loss = 0.03167676\n",
      "Iteration 42876, loss = 0.03168003\n",
      "Iteration 42877, loss = 0.03168280\n",
      "Iteration 42878, loss = 0.03168264\n",
      "Iteration 42879, loss = 0.03168085\n",
      "Iteration 42880, loss = 0.03166922\n",
      "Iteration 42881, loss = 0.03166795\n",
      "Iteration 42882, loss = 0.03168102\n",
      "Iteration 42883, loss = 0.03167440\n",
      "Iteration 42884, loss = 0.03167002\n",
      "Iteration 42885, loss = 0.03166879\n",
      "Iteration 42886, loss = 0.03166942\n",
      "Iteration 42887, loss = 0.03166485\n",
      "Iteration 42888, loss = 0.03166329\n",
      "Iteration 42889, loss = 0.03166728\n",
      "Iteration 42890, loss = 0.03166463\n",
      "Iteration 42891, loss = 0.03166410\n",
      "Iteration 42892, loss = 0.03166258\n",
      "Iteration 42893, loss = 0.03166235\n",
      "Iteration 42894, loss = 0.03166984\n",
      "Iteration 42895, loss = 0.03166990\n",
      "Iteration 42896, loss = 0.03166794\n",
      "Iteration 42897, loss = 0.03166199\n",
      "Iteration 42898, loss = 0.03166398\n",
      "Iteration 42899, loss = 0.03166715\n",
      "Iteration 42900, loss = 0.03166165\n",
      "Iteration 42901, loss = 0.03165331\n",
      "Iteration 42902, loss = 0.03166334\n",
      "Iteration 42903, loss = 0.03166131\n",
      "Iteration 42904, loss = 0.03166138\n",
      "Iteration 42905, loss = 0.03165987\n",
      "Iteration 42906, loss = 0.03165578\n",
      "Iteration 42907, loss = 0.03165809\n",
      "Iteration 42908, loss = 0.03165471\n",
      "Iteration 42909, loss = 0.03165200\n",
      "Iteration 42910, loss = 0.03165937\n",
      "Iteration 42911, loss = 0.03166172\n",
      "Iteration 42912, loss = 0.03165421\n",
      "Iteration 42913, loss = 0.03165253\n",
      "Iteration 42914, loss = 0.03165755\n",
      "Iteration 42915, loss = 0.03165571\n",
      "Iteration 42916, loss = 0.03165697\n",
      "Iteration 42917, loss = 0.03165317\n",
      "Iteration 42918, loss = 0.03166919\n",
      "Iteration 42919, loss = 0.03166704\n",
      "Iteration 42920, loss = 0.03165069\n",
      "Iteration 42921, loss = 0.03165110\n",
      "Iteration 42922, loss = 0.03166105\n",
      "Iteration 42923, loss = 0.03166059\n",
      "Iteration 42924, loss = 0.03165387\n",
      "Iteration 42925, loss = 0.03164807\n",
      "Iteration 42926, loss = 0.03165152\n",
      "Iteration 42927, loss = 0.03165161\n",
      "Iteration 42928, loss = 0.03165840\n",
      "Iteration 42929, loss = 0.03165848\n",
      "Iteration 42930, loss = 0.03164552\n",
      "Iteration 42931, loss = 0.03165029\n",
      "Iteration 42932, loss = 0.03165436\n",
      "Iteration 42933, loss = 0.03164872\n",
      "Iteration 42934, loss = 0.03164379\n",
      "Iteration 42935, loss = 0.03164177\n",
      "Iteration 42936, loss = 0.03164410\n",
      "Iteration 42937, loss = 0.03164897\n",
      "Iteration 42938, loss = 0.03164509\n",
      "Iteration 42939, loss = 0.03165035\n",
      "Iteration 42940, loss = 0.03164462\n",
      "Iteration 42941, loss = 0.03165298\n",
      "Iteration 42942, loss = 0.03165876\n",
      "Iteration 42943, loss = 0.03165375\n",
      "Iteration 42944, loss = 0.03165002\n",
      "Iteration 42945, loss = 0.03166033\n",
      "Iteration 42946, loss = 0.03165818\n",
      "Iteration 42947, loss = 0.03165029\n",
      "Iteration 42948, loss = 0.03165233\n",
      "Iteration 42949, loss = 0.03165279\n",
      "Iteration 42950, loss = 0.03165425\n",
      "Iteration 42951, loss = 0.03165266\n",
      "Iteration 42952, loss = 0.03164633\n",
      "Iteration 42953, loss = 0.03164370\n",
      "Iteration 42954, loss = 0.03164774\n",
      "Iteration 42955, loss = 0.03164666\n",
      "Iteration 42956, loss = 0.03163995\n",
      "Iteration 42957, loss = 0.03164593\n",
      "Iteration 42958, loss = 0.03165302\n",
      "Iteration 42959, loss = 0.03164891\n",
      "Iteration 42960, loss = 0.03164669\n",
      "Iteration 42961, loss = 0.03164343\n",
      "Iteration 42962, loss = 0.03164635\n",
      "Iteration 42963, loss = 0.03164743\n",
      "Iteration 42964, loss = 0.03164286\n",
      "Iteration 42965, loss = 0.03164885\n",
      "Iteration 42966, loss = 0.03164834\n",
      "Iteration 42967, loss = 0.03164394\n",
      "Iteration 42968, loss = 0.03163617\n",
      "Iteration 42969, loss = 0.03163446\n",
      "Iteration 42970, loss = 0.03164863\n",
      "Iteration 42971, loss = 0.03165471\n",
      "Iteration 42972, loss = 0.03164437\n",
      "Iteration 42973, loss = 0.03163743\n",
      "Iteration 42974, loss = 0.03164131\n",
      "Iteration 42975, loss = 0.03163955\n",
      "Iteration 42976, loss = 0.03162837\n",
      "Iteration 42977, loss = 0.03164044\n",
      "Iteration 42978, loss = 0.03165163\n",
      "Iteration 42979, loss = 0.03164529\n",
      "Iteration 42980, loss = 0.03163829\n",
      "Iteration 42981, loss = 0.03164414\n",
      "Iteration 42982, loss = 0.03164439\n",
      "Iteration 42983, loss = 0.03164804\n",
      "Iteration 42984, loss = 0.03164627\n",
      "Iteration 42985, loss = 0.03163913\n",
      "Iteration 42986, loss = 0.03162595\n",
      "Iteration 42987, loss = 0.03163512\n",
      "Iteration 42988, loss = 0.03164610\n",
      "Iteration 42989, loss = 0.03164462\n",
      "Iteration 42990, loss = 0.03163393\n",
      "Iteration 42991, loss = 0.03162387\n",
      "Iteration 42992, loss = 0.03162985\n",
      "Iteration 42993, loss = 0.03163379\n",
      "Iteration 42994, loss = 0.03163068\n",
      "Iteration 42995, loss = 0.03162320\n",
      "Iteration 42996, loss = 0.03162717\n",
      "Iteration 42997, loss = 0.03162607\n",
      "Iteration 42998, loss = 0.03161755\n",
      "Iteration 42999, loss = 0.03162392\n",
      "Iteration 43000, loss = 0.03162210\n",
      "Iteration 43001, loss = 0.03163104\n",
      "Iteration 43002, loss = 0.03162979\n",
      "Iteration 43003, loss = 0.03162260\n",
      "Iteration 43004, loss = 0.03162699\n",
      "Iteration 43005, loss = 0.03163534\n",
      "Iteration 43006, loss = 0.03162869\n",
      "Iteration 43007, loss = 0.03163867\n",
      "Iteration 43008, loss = 0.03163115\n",
      "Iteration 43009, loss = 0.03162394\n",
      "Iteration 43010, loss = 0.03162361\n",
      "Iteration 43011, loss = 0.03162578\n",
      "Iteration 43012, loss = 0.03162333\n",
      "Iteration 43013, loss = 0.03162537\n",
      "Iteration 43014, loss = 0.03162649\n",
      "Iteration 43015, loss = 0.03161293\n",
      "Iteration 43016, loss = 0.03161896\n",
      "Iteration 43017, loss = 0.03162167\n",
      "Iteration 43018, loss = 0.03161081\n",
      "Iteration 43019, loss = 0.03161845\n",
      "Iteration 43020, loss = 0.03162390\n",
      "Iteration 43021, loss = 0.03162118\n",
      "Iteration 43022, loss = 0.03161199\n",
      "Iteration 43023, loss = 0.03161129\n",
      "Iteration 43024, loss = 0.03161855\n",
      "Iteration 43025, loss = 0.03161326\n",
      "Iteration 43026, loss = 0.03161615\n",
      "Iteration 43027, loss = 0.03161159\n",
      "Iteration 43028, loss = 0.03161322\n",
      "Iteration 43029, loss = 0.03161686\n",
      "Iteration 43030, loss = 0.03162088\n",
      "Iteration 43031, loss = 0.03161225\n",
      "Iteration 43032, loss = 0.03160120\n",
      "Iteration 43033, loss = 0.03161052\n",
      "Iteration 43034, loss = 0.03162422\n",
      "Iteration 43035, loss = 0.03161411\n",
      "Iteration 43036, loss = 0.03159048\n",
      "Iteration 43037, loss = 0.03160381\n",
      "Iteration 43038, loss = 0.03160534\n",
      "Iteration 43039, loss = 0.03160089\n",
      "Iteration 43040, loss = 0.03159476\n",
      "Iteration 43041, loss = 0.03161025\n",
      "Iteration 43042, loss = 0.03160561\n",
      "Iteration 43043, loss = 0.03158762\n",
      "Iteration 43044, loss = 0.03159450\n",
      "Iteration 43045, loss = 0.03159121\n",
      "Iteration 43046, loss = 0.03158975\n",
      "Iteration 43047, loss = 0.03158737\n",
      "Iteration 43048, loss = 0.03158943\n",
      "Iteration 43049, loss = 0.03158981\n",
      "Iteration 43050, loss = 0.03159343\n",
      "Iteration 43051, loss = 0.03159553\n",
      "Iteration 43052, loss = 0.03159498\n",
      "Iteration 43053, loss = 0.03158863\n",
      "Iteration 43054, loss = 0.03159877\n",
      "Iteration 43055, loss = 0.03160856\n",
      "Iteration 43056, loss = 0.03160428\n",
      "Iteration 43057, loss = 0.03158929\n",
      "Iteration 43058, loss = 0.03158766\n",
      "Iteration 43059, loss = 0.03159879\n",
      "Iteration 43060, loss = 0.03159863\n",
      "Iteration 43061, loss = 0.03159463\n",
      "Iteration 43062, loss = 0.03159269\n",
      "Iteration 43063, loss = 0.03159719\n",
      "Iteration 43064, loss = 0.03159081\n",
      "Iteration 43065, loss = 0.03158781\n",
      "Iteration 43066, loss = 0.03158412\n",
      "Iteration 43067, loss = 0.03158978\n",
      "Iteration 43068, loss = 0.03159031\n",
      "Iteration 43069, loss = 0.03158905\n",
      "Iteration 43070, loss = 0.03158338\n",
      "Iteration 43071, loss = 0.03158916\n",
      "Iteration 43072, loss = 0.03158734\n",
      "Iteration 43073, loss = 0.03158241\n",
      "Iteration 43074, loss = 0.03158448\n",
      "Iteration 43075, loss = 0.03159236\n",
      "Iteration 43076, loss = 0.03158778\n",
      "Iteration 43077, loss = 0.03158786\n",
      "Iteration 43078, loss = 0.03158205\n",
      "Iteration 43079, loss = 0.03158899\n",
      "Iteration 43080, loss = 0.03158604\n",
      "Iteration 43081, loss = 0.03158137\n",
      "Iteration 43082, loss = 0.03157724\n",
      "Iteration 43083, loss = 0.03158975\n",
      "Iteration 43084, loss = 0.03158661\n",
      "Iteration 43085, loss = 0.03158194\n",
      "Iteration 43086, loss = 0.03157683\n",
      "Iteration 43087, loss = 0.03158646\n",
      "Iteration 43088, loss = 0.03159099\n",
      "Iteration 43089, loss = 0.03158704\n",
      "Iteration 43090, loss = 0.03158694\n",
      "Iteration 43091, loss = 0.03158556\n",
      "Iteration 43092, loss = 0.03157864\n",
      "Iteration 43093, loss = 0.03158534\n",
      "Iteration 43094, loss = 0.03158622\n",
      "Iteration 43095, loss = 0.03158184\n",
      "Iteration 43096, loss = 0.03158082\n",
      "Iteration 43097, loss = 0.03157969\n",
      "Iteration 43098, loss = 0.03159015\n",
      "Iteration 43099, loss = 0.03159507\n",
      "Iteration 43100, loss = 0.03159842\n",
      "Iteration 43101, loss = 0.03158839\n",
      "Iteration 43102, loss = 0.03158326\n",
      "Iteration 43103, loss = 0.03158646\n",
      "Iteration 43104, loss = 0.03157924\n",
      "Iteration 43105, loss = 0.03159877\n",
      "Iteration 43106, loss = 0.03159731\n",
      "Iteration 43107, loss = 0.03159696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 43108, loss = 0.03159257\n",
      "Iteration 43109, loss = 0.03159058\n",
      "Iteration 43110, loss = 0.03159195\n",
      "Iteration 43111, loss = 0.03158434\n",
      "Iteration 43112, loss = 0.03159441\n",
      "Iteration 43113, loss = 0.03159251\n",
      "Iteration 43114, loss = 0.03158684\n",
      "Iteration 43115, loss = 0.03157047\n",
      "Iteration 43116, loss = 0.03157863\n",
      "Iteration 43117, loss = 0.03157532\n",
      "Iteration 43118, loss = 0.03157736\n",
      "Iteration 43119, loss = 0.03157329\n",
      "Iteration 43120, loss = 0.03158322\n",
      "Iteration 43121, loss = 0.03158501\n",
      "Iteration 43122, loss = 0.03157588\n",
      "Iteration 43123, loss = 0.03157873\n",
      "Iteration 43124, loss = 0.03158236\n",
      "Iteration 43125, loss = 0.03158160\n",
      "Iteration 43126, loss = 0.03158139\n",
      "Iteration 43127, loss = 0.03157747\n",
      "Iteration 43128, loss = 0.03157336\n",
      "Iteration 43129, loss = 0.03156186\n",
      "Iteration 43130, loss = 0.03156712\n",
      "Iteration 43131, loss = 0.03157159\n",
      "Iteration 43132, loss = 0.03156824\n",
      "Iteration 43133, loss = 0.03156496\n",
      "Iteration 43134, loss = 0.03156707\n",
      "Iteration 43135, loss = 0.03156049\n",
      "Iteration 43136, loss = 0.03156062\n",
      "Iteration 43137, loss = 0.03155700\n",
      "Iteration 43138, loss = 0.03155594\n",
      "Iteration 43139, loss = 0.03155157\n",
      "Iteration 43140, loss = 0.03155950\n",
      "Iteration 43141, loss = 0.03156314\n",
      "Iteration 43142, loss = 0.03155594\n",
      "Iteration 43143, loss = 0.03155455\n",
      "Iteration 43144, loss = 0.03156764\n",
      "Iteration 43145, loss = 0.03156529\n",
      "Iteration 43146, loss = 0.03155712\n",
      "Iteration 43147, loss = 0.03155596\n",
      "Iteration 43148, loss = 0.03155094\n",
      "Iteration 43149, loss = 0.03154785\n",
      "Iteration 43150, loss = 0.03155331\n",
      "Iteration 43151, loss = 0.03155506\n",
      "Iteration 43152, loss = 0.03155378\n",
      "Iteration 43153, loss = 0.03155977\n",
      "Iteration 43154, loss = 0.03155824\n",
      "Iteration 43155, loss = 0.03155404\n",
      "Iteration 43156, loss = 0.03155178\n",
      "Iteration 43157, loss = 0.03155047\n",
      "Iteration 43158, loss = 0.03155058\n",
      "Iteration 43159, loss = 0.03155112\n",
      "Iteration 43160, loss = 0.03155088\n",
      "Iteration 43161, loss = 0.03155390\n",
      "Iteration 43162, loss = 0.03155085\n",
      "Iteration 43163, loss = 0.03154787\n",
      "Iteration 43164, loss = 0.03155736\n",
      "Iteration 43165, loss = 0.03155891\n",
      "Iteration 43166, loss = 0.03155877\n",
      "Iteration 43167, loss = 0.03154951\n",
      "Iteration 43168, loss = 0.03154034\n",
      "Iteration 43169, loss = 0.03154160\n",
      "Iteration 43170, loss = 0.03154590\n",
      "Iteration 43171, loss = 0.03154690\n",
      "Iteration 43172, loss = 0.03154480\n",
      "Iteration 43173, loss = 0.03154064\n",
      "Iteration 43174, loss = 0.03155480\n",
      "Iteration 43175, loss = 0.03155331\n",
      "Iteration 43176, loss = 0.03154738\n",
      "Iteration 43177, loss = 0.03154873\n",
      "Iteration 43178, loss = 0.03155650\n",
      "Iteration 43179, loss = 0.03155583\n",
      "Iteration 43180, loss = 0.03155036\n",
      "Iteration 43181, loss = 0.03155549\n",
      "Iteration 43182, loss = 0.03155810\n",
      "Iteration 43183, loss = 0.03155299\n",
      "Iteration 43184, loss = 0.03155736\n",
      "Iteration 43185, loss = 0.03156314\n",
      "Iteration 43186, loss = 0.03155876\n",
      "Iteration 43187, loss = 0.03156242\n",
      "Iteration 43188, loss = 0.03155501\n",
      "Iteration 43189, loss = 0.03153918\n",
      "Iteration 43190, loss = 0.03154498\n",
      "Iteration 43191, loss = 0.03156595\n",
      "Iteration 43192, loss = 0.03157196\n",
      "Iteration 43193, loss = 0.03156181\n",
      "Iteration 43194, loss = 0.03154060\n",
      "Iteration 43195, loss = 0.03155032\n",
      "Iteration 43196, loss = 0.03156357\n",
      "Iteration 43197, loss = 0.03157179\n",
      "Iteration 43198, loss = 0.03156487\n",
      "Iteration 43199, loss = 0.03156423\n",
      "Iteration 43200, loss = 0.03156930\n",
      "Iteration 43201, loss = 0.03156011\n",
      "Iteration 43202, loss = 0.03153761\n",
      "Iteration 43203, loss = 0.03154183\n",
      "Iteration 43204, loss = 0.03156048\n",
      "Iteration 43205, loss = 0.03156380\n",
      "Iteration 43206, loss = 0.03155513\n",
      "Iteration 43207, loss = 0.03154483\n",
      "Iteration 43208, loss = 0.03154198\n",
      "Iteration 43209, loss = 0.03155038\n",
      "Iteration 43210, loss = 0.03155235\n",
      "Iteration 43211, loss = 0.03154878\n",
      "Iteration 43212, loss = 0.03153914\n",
      "Iteration 43213, loss = 0.03152842\n",
      "Iteration 43214, loss = 0.03153868\n",
      "Iteration 43215, loss = 0.03153500\n",
      "Iteration 43216, loss = 0.03153027\n",
      "Iteration 43217, loss = 0.03153623\n",
      "Iteration 43218, loss = 0.03153853\n",
      "Iteration 43219, loss = 0.03153074\n",
      "Iteration 43220, loss = 0.03153731\n",
      "Iteration 43221, loss = 0.03153827\n",
      "Iteration 43222, loss = 0.03154500\n",
      "Iteration 43223, loss = 0.03155112\n",
      "Iteration 43224, loss = 0.03154340\n",
      "Iteration 43225, loss = 0.03153799\n",
      "Iteration 43226, loss = 0.03152456\n",
      "Iteration 43227, loss = 0.03153312\n",
      "Iteration 43228, loss = 0.03154856\n",
      "Iteration 43229, loss = 0.03153942\n",
      "Iteration 43230, loss = 0.03151973\n",
      "Iteration 43231, loss = 0.03153718\n",
      "Iteration 43232, loss = 0.03154185\n",
      "Iteration 43233, loss = 0.03154583\n",
      "Iteration 43234, loss = 0.03153857\n",
      "Iteration 43235, loss = 0.03152681\n",
      "Iteration 43236, loss = 0.03153687\n",
      "Iteration 43237, loss = 0.03153377\n",
      "Iteration 43238, loss = 0.03153417\n",
      "Iteration 43239, loss = 0.03152333\n",
      "Iteration 43240, loss = 0.03152589\n",
      "Iteration 43241, loss = 0.03152889\n",
      "Iteration 43242, loss = 0.03152916\n",
      "Iteration 43243, loss = 0.03152345\n",
      "Iteration 43244, loss = 0.03152096\n",
      "Iteration 43245, loss = 0.03153210\n",
      "Iteration 43246, loss = 0.03153859\n",
      "Iteration 43247, loss = 0.03152548\n",
      "Iteration 43248, loss = 0.03152298\n",
      "Iteration 43249, loss = 0.03153600\n",
      "Iteration 43250, loss = 0.03153317\n",
      "Iteration 43251, loss = 0.03152149\n",
      "Iteration 43252, loss = 0.03152134\n",
      "Iteration 43253, loss = 0.03151705\n",
      "Iteration 43254, loss = 0.03152555\n",
      "Iteration 43255, loss = 0.03152545\n",
      "Iteration 43256, loss = 0.03151914\n",
      "Iteration 43257, loss = 0.03152249\n",
      "Iteration 43258, loss = 0.03152577\n",
      "Iteration 43259, loss = 0.03152539\n",
      "Iteration 43260, loss = 0.03152727\n",
      "Iteration 43261, loss = 0.03152714\n",
      "Iteration 43262, loss = 0.03152524\n",
      "Iteration 43263, loss = 0.03152638\n",
      "Iteration 43264, loss = 0.03152132\n",
      "Iteration 43265, loss = 0.03150695\n",
      "Iteration 43266, loss = 0.03152343\n",
      "Iteration 43267, loss = 0.03153208\n",
      "Iteration 43268, loss = 0.03153530\n",
      "Iteration 43269, loss = 0.03151985\n",
      "Iteration 43270, loss = 0.03152691\n",
      "Iteration 43271, loss = 0.03152769\n",
      "Iteration 43272, loss = 0.03152432\n",
      "Iteration 43273, loss = 0.03153171\n",
      "Iteration 43274, loss = 0.03152564\n",
      "Iteration 43275, loss = 0.03151612\n",
      "Iteration 43276, loss = 0.03150759\n",
      "Iteration 43277, loss = 0.03150980\n",
      "Iteration 43278, loss = 0.03151462\n",
      "Iteration 43279, loss = 0.03152479\n",
      "Iteration 43280, loss = 0.03152537\n",
      "Iteration 43281, loss = 0.03152134\n",
      "Iteration 43282, loss = 0.03151044\n",
      "Iteration 43283, loss = 0.03151135\n",
      "Iteration 43284, loss = 0.03152578\n",
      "Iteration 43285, loss = 0.03153096\n",
      "Iteration 43286, loss = 0.03151908\n",
      "Iteration 43287, loss = 0.03153086\n",
      "Iteration 43288, loss = 0.03153287\n",
      "Iteration 43289, loss = 0.03152228\n",
      "Iteration 43290, loss = 0.03151769\n",
      "Iteration 43291, loss = 0.03151371\n",
      "Iteration 43292, loss = 0.03151332\n",
      "Iteration 43293, loss = 0.03150639\n",
      "Iteration 43294, loss = 0.03151481\n",
      "Iteration 43295, loss = 0.03151826\n",
      "Iteration 43296, loss = 0.03151910\n",
      "Iteration 43297, loss = 0.03150336\n",
      "Iteration 43298, loss = 0.03151112\n",
      "Iteration 43299, loss = 0.03151248\n",
      "Iteration 43300, loss = 0.03149912\n",
      "Iteration 43301, loss = 0.03150089\n",
      "Iteration 43302, loss = 0.03151333\n",
      "Iteration 43303, loss = 0.03151155\n",
      "Iteration 43304, loss = 0.03149753\n",
      "Iteration 43305, loss = 0.03150474\n",
      "Iteration 43306, loss = 0.03150607\n",
      "Iteration 43307, loss = 0.03151158\n",
      "Iteration 43308, loss = 0.03151205\n",
      "Iteration 43309, loss = 0.03151021\n",
      "Iteration 43310, loss = 0.03150640\n",
      "Iteration 43311, loss = 0.03149626\n",
      "Iteration 43312, loss = 0.03149971\n",
      "Iteration 43313, loss = 0.03150815\n",
      "Iteration 43314, loss = 0.03149992\n",
      "Iteration 43315, loss = 0.03149169\n",
      "Iteration 43316, loss = 0.03150065\n",
      "Iteration 43317, loss = 0.03150159\n",
      "Iteration 43318, loss = 0.03149857\n",
      "Iteration 43319, loss = 0.03149149\n",
      "Iteration 43320, loss = 0.03149633\n",
      "Iteration 43321, loss = 0.03149356\n",
      "Iteration 43322, loss = 0.03149206\n",
      "Iteration 43323, loss = 0.03149831\n",
      "Iteration 43324, loss = 0.03150020\n",
      "Iteration 43325, loss = 0.03149560\n",
      "Iteration 43326, loss = 0.03149393\n",
      "Iteration 43327, loss = 0.03148767\n",
      "Iteration 43328, loss = 0.03149231\n",
      "Iteration 43329, loss = 0.03149658\n",
      "Iteration 43330, loss = 0.03149194\n",
      "Iteration 43331, loss = 0.03149268\n",
      "Iteration 43332, loss = 0.03149269\n",
      "Iteration 43333, loss = 0.03149286\n",
      "Iteration 43334, loss = 0.03149365\n",
      "Iteration 43335, loss = 0.03149149\n",
      "Iteration 43336, loss = 0.03149139\n",
      "Iteration 43337, loss = 0.03149195\n",
      "Iteration 43338, loss = 0.03148637\n",
      "Iteration 43339, loss = 0.03148743\n",
      "Iteration 43340, loss = 0.03149143\n",
      "Iteration 43341, loss = 0.03150209\n",
      "Iteration 43342, loss = 0.03150020\n",
      "Iteration 43343, loss = 0.03148673\n",
      "Iteration 43344, loss = 0.03148281\n",
      "Iteration 43345, loss = 0.03149349\n",
      "Iteration 43346, loss = 0.03149958\n",
      "Iteration 43347, loss = 0.03149426\n",
      "Iteration 43348, loss = 0.03147889\n",
      "Iteration 43349, loss = 0.03147833\n",
      "Iteration 43350, loss = 0.03148830\n",
      "Iteration 43351, loss = 0.03148980\n",
      "Iteration 43352, loss = 0.03148857\n",
      "Iteration 43353, loss = 0.03148900\n",
      "Iteration 43354, loss = 0.03148314\n",
      "Iteration 43355, loss = 0.03148765\n",
      "Iteration 43356, loss = 0.03149207\n",
      "Iteration 43357, loss = 0.03148672\n",
      "Iteration 43358, loss = 0.03148007\n",
      "Iteration 43359, loss = 0.03147920\n",
      "Iteration 43360, loss = 0.03149115\n",
      "Iteration 43361, loss = 0.03148602\n",
      "Iteration 43362, loss = 0.03148482\n",
      "Iteration 43363, loss = 0.03148332\n",
      "Iteration 43364, loss = 0.03148572\n",
      "Iteration 43365, loss = 0.03149300\n",
      "Iteration 43366, loss = 0.03149320\n",
      "Iteration 43367, loss = 0.03149438\n",
      "Iteration 43368, loss = 0.03148389\n",
      "Iteration 43369, loss = 0.03146833\n",
      "Iteration 43370, loss = 0.03148520\n",
      "Iteration 43371, loss = 0.03149037\n",
      "Iteration 43372, loss = 0.03148836\n",
      "Iteration 43373, loss = 0.03147278\n",
      "Iteration 43374, loss = 0.03147613\n",
      "Iteration 43375, loss = 0.03148065\n",
      "Iteration 43376, loss = 0.03148734\n",
      "Iteration 43377, loss = 0.03148654\n",
      "Iteration 43378, loss = 0.03147768\n",
      "Iteration 43379, loss = 0.03147328\n",
      "Iteration 43380, loss = 0.03148233\n",
      "Iteration 43381, loss = 0.03147786\n",
      "Iteration 43382, loss = 0.03146684\n",
      "Iteration 43383, loss = 0.03146999\n",
      "Iteration 43384, loss = 0.03146971\n",
      "Iteration 43385, loss = 0.03146886\n",
      "Iteration 43386, loss = 0.03146788\n",
      "Iteration 43387, loss = 0.03147708\n",
      "Iteration 43388, loss = 0.03147524\n",
      "Iteration 43389, loss = 0.03146145\n",
      "Iteration 43390, loss = 0.03147180\n",
      "Iteration 43391, loss = 0.03148530\n",
      "Iteration 43392, loss = 0.03147795\n",
      "Iteration 43393, loss = 0.03148814\n",
      "Iteration 43394, loss = 0.03149414\n",
      "Iteration 43395, loss = 0.03148789\n",
      "Iteration 43396, loss = 0.03147211\n",
      "Iteration 43397, loss = 0.03147317\n",
      "Iteration 43398, loss = 0.03149616\n",
      "Iteration 43399, loss = 0.03149164\n",
      "Iteration 43400, loss = 0.03147940\n",
      "Iteration 43401, loss = 0.03147882\n",
      "Iteration 43402, loss = 0.03148676\n",
      "Iteration 43403, loss = 0.03149019\n",
      "Iteration 43404, loss = 0.03148285\n",
      "Iteration 43405, loss = 0.03146972\n",
      "Iteration 43406, loss = 0.03147202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 43407, loss = 0.03147702\n",
      "Iteration 43408, loss = 0.03147513\n",
      "Iteration 43409, loss = 0.03146806\n",
      "Iteration 43410, loss = 0.03147160\n",
      "Iteration 43411, loss = 0.03148367\n",
      "Iteration 43412, loss = 0.03148101\n",
      "Iteration 43413, loss = 0.03147384\n",
      "Iteration 43414, loss = 0.03147332\n",
      "Iteration 43415, loss = 0.03147432\n",
      "Iteration 43416, loss = 0.03146205\n",
      "Iteration 43417, loss = 0.03146321\n",
      "Iteration 43418, loss = 0.03146989\n",
      "Iteration 43419, loss = 0.03146617\n",
      "Iteration 43420, loss = 0.03145658\n",
      "Iteration 43421, loss = 0.03145454\n",
      "Iteration 43422, loss = 0.03145909\n",
      "Iteration 43423, loss = 0.03146188\n",
      "Iteration 43424, loss = 0.03146148\n",
      "Iteration 43425, loss = 0.03146027\n",
      "Iteration 43426, loss = 0.03146064\n",
      "Iteration 43427, loss = 0.03145465\n",
      "Iteration 43428, loss = 0.03146075\n",
      "Iteration 43429, loss = 0.03146353\n",
      "Iteration 43430, loss = 0.03145693\n",
      "Iteration 43431, loss = 0.03145104\n",
      "Iteration 43432, loss = 0.03145808\n",
      "Iteration 43433, loss = 0.03145355\n",
      "Iteration 43434, loss = 0.03145501\n",
      "Iteration 43435, loss = 0.03144733\n",
      "Iteration 43436, loss = 0.03146198\n",
      "Iteration 43437, loss = 0.03146700\n",
      "Iteration 43438, loss = 0.03145458\n",
      "Iteration 43439, loss = 0.03145441\n",
      "Iteration 43440, loss = 0.03145880\n",
      "Iteration 43441, loss = 0.03145701\n",
      "Iteration 43442, loss = 0.03145817\n",
      "Iteration 43443, loss = 0.03145500\n",
      "Iteration 43444, loss = 0.03144592\n",
      "Iteration 43445, loss = 0.03144568\n",
      "Iteration 43446, loss = 0.03145090\n",
      "Iteration 43447, loss = 0.03144831\n",
      "Iteration 43448, loss = 0.03144685\n",
      "Iteration 43449, loss = 0.03144768\n",
      "Iteration 43450, loss = 0.03145658\n",
      "Iteration 43451, loss = 0.03145998\n",
      "Iteration 43452, loss = 0.03145319\n",
      "Iteration 43453, loss = 0.03144687\n",
      "Iteration 43454, loss = 0.03144646\n",
      "Iteration 43455, loss = 0.03144986\n",
      "Iteration 43456, loss = 0.03144136\n",
      "Iteration 43457, loss = 0.03144562\n",
      "Iteration 43458, loss = 0.03145058\n",
      "Iteration 43459, loss = 0.03145437\n",
      "Iteration 43460, loss = 0.03144847\n",
      "Iteration 43461, loss = 0.03144508\n",
      "Iteration 43462, loss = 0.03145431\n",
      "Iteration 43463, loss = 0.03145697\n",
      "Iteration 43464, loss = 0.03144836\n",
      "Iteration 43465, loss = 0.03144400\n",
      "Iteration 43466, loss = 0.03144578\n",
      "Iteration 43467, loss = 0.03144778\n",
      "Iteration 43468, loss = 0.03144079\n",
      "Iteration 43469, loss = 0.03143801\n",
      "Iteration 43470, loss = 0.03144157\n",
      "Iteration 43471, loss = 0.03144181\n",
      "Iteration 43472, loss = 0.03144682\n",
      "Iteration 43473, loss = 0.03144627\n",
      "Iteration 43474, loss = 0.03144298\n",
      "Iteration 43475, loss = 0.03144805\n",
      "Iteration 43476, loss = 0.03144807\n",
      "Iteration 43477, loss = 0.03143950\n",
      "Iteration 43478, loss = 0.03144764\n",
      "Iteration 43479, loss = 0.03144288\n",
      "Iteration 43480, loss = 0.03144101\n",
      "Iteration 43481, loss = 0.03144221\n",
      "Iteration 43482, loss = 0.03144500\n",
      "Iteration 43483, loss = 0.03145081\n",
      "Iteration 43484, loss = 0.03145012\n",
      "Iteration 43485, loss = 0.03144905\n",
      "Iteration 43486, loss = 0.03145065\n",
      "Iteration 43487, loss = 0.03143796\n",
      "Iteration 43488, loss = 0.03144229\n",
      "Iteration 43489, loss = 0.03144747\n",
      "Iteration 43490, loss = 0.03143430\n",
      "Iteration 43491, loss = 0.03144159\n",
      "Iteration 43492, loss = 0.03144809\n",
      "Iteration 43493, loss = 0.03145343\n",
      "Iteration 43494, loss = 0.03145489\n",
      "Iteration 43495, loss = 0.03144120\n",
      "Iteration 43496, loss = 0.03143822\n",
      "Iteration 43497, loss = 0.03144207\n",
      "Iteration 43498, loss = 0.03145480\n",
      "Iteration 43499, loss = 0.03145622\n",
      "Iteration 43500, loss = 0.03144478\n",
      "Iteration 43501, loss = 0.03143219\n",
      "Iteration 43502, loss = 0.03143676\n",
      "Iteration 43503, loss = 0.03144756\n",
      "Iteration 43504, loss = 0.03144596\n",
      "Iteration 43505, loss = 0.03144085\n",
      "Iteration 43506, loss = 0.03144075\n",
      "Iteration 43507, loss = 0.03144016\n",
      "Iteration 43508, loss = 0.03144033\n",
      "Iteration 43509, loss = 0.03143359\n",
      "Iteration 43510, loss = 0.03142947\n",
      "Iteration 43511, loss = 0.03143895\n",
      "Iteration 43512, loss = 0.03143106\n",
      "Iteration 43513, loss = 0.03142793\n",
      "Iteration 43514, loss = 0.03144110\n",
      "Iteration 43515, loss = 0.03144116\n",
      "Iteration 43516, loss = 0.03143415\n",
      "Iteration 43517, loss = 0.03143241\n",
      "Iteration 43518, loss = 0.03142723\n",
      "Iteration 43519, loss = 0.03142848\n",
      "Iteration 43520, loss = 0.03143305\n",
      "Iteration 43521, loss = 0.03142891\n",
      "Iteration 43522, loss = 0.03142428\n",
      "Iteration 43523, loss = 0.03142457\n",
      "Iteration 43524, loss = 0.03143319\n",
      "Iteration 43525, loss = 0.03143139\n",
      "Iteration 43526, loss = 0.03142835\n",
      "Iteration 43527, loss = 0.03142988\n",
      "Iteration 43528, loss = 0.03142456\n",
      "Iteration 43529, loss = 0.03143883\n",
      "Iteration 43530, loss = 0.03144342\n",
      "Iteration 43531, loss = 0.03144182\n",
      "Iteration 43532, loss = 0.03142634\n",
      "Iteration 43533, loss = 0.03143557\n",
      "Iteration 43534, loss = 0.03143698\n",
      "Iteration 43535, loss = 0.03144304\n",
      "Iteration 43536, loss = 0.03144599\n",
      "Iteration 43537, loss = 0.03143714\n",
      "Iteration 43538, loss = 0.03142730\n",
      "Iteration 43539, loss = 0.03143772\n",
      "Iteration 43540, loss = 0.03143658\n",
      "Iteration 43541, loss = 0.03143018\n",
      "Iteration 43542, loss = 0.03143061\n",
      "Iteration 43543, loss = 0.03143020\n",
      "Iteration 43544, loss = 0.03141682\n",
      "Iteration 43545, loss = 0.03141510\n",
      "Iteration 43546, loss = 0.03143147\n",
      "Iteration 43547, loss = 0.03144369\n",
      "Iteration 43548, loss = 0.03143820\n",
      "Iteration 43549, loss = 0.03142449\n",
      "Iteration 43550, loss = 0.03141882\n",
      "Iteration 43551, loss = 0.03141644\n",
      "Iteration 43552, loss = 0.03142703\n",
      "Iteration 43553, loss = 0.03142378\n",
      "Iteration 43554, loss = 0.03141841\n",
      "Iteration 43555, loss = 0.03142000\n",
      "Iteration 43556, loss = 0.03142131\n",
      "Iteration 43557, loss = 0.03142115\n",
      "Iteration 43558, loss = 0.03142074\n",
      "Iteration 43559, loss = 0.03143238\n",
      "Iteration 43560, loss = 0.03143134\n",
      "Iteration 43561, loss = 0.03141761\n",
      "Iteration 43562, loss = 0.03141277\n",
      "Iteration 43563, loss = 0.03142483\n",
      "Iteration 43564, loss = 0.03142532\n",
      "Iteration 43565, loss = 0.03141198\n",
      "Iteration 43566, loss = 0.03140426\n",
      "Iteration 43567, loss = 0.03140871\n",
      "Iteration 43568, loss = 0.03142033\n",
      "Iteration 43569, loss = 0.03140773\n",
      "Iteration 43570, loss = 0.03140362\n",
      "Iteration 43571, loss = 0.03141257\n",
      "Iteration 43572, loss = 0.03140989\n",
      "Iteration 43573, loss = 0.03140630\n",
      "Iteration 43574, loss = 0.03140257\n",
      "Iteration 43575, loss = 0.03140245\n",
      "Iteration 43576, loss = 0.03139938\n",
      "Iteration 43577, loss = 0.03140192\n",
      "Iteration 43578, loss = 0.03139953\n",
      "Iteration 43579, loss = 0.03140118\n",
      "Iteration 43580, loss = 0.03139893\n",
      "Iteration 43581, loss = 0.03140133\n",
      "Iteration 43582, loss = 0.03140254\n",
      "Iteration 43583, loss = 0.03140511\n",
      "Iteration 43584, loss = 0.03139644\n",
      "Iteration 43585, loss = 0.03139754\n",
      "Iteration 43586, loss = 0.03140321\n",
      "Iteration 43587, loss = 0.03140543\n",
      "Iteration 43588, loss = 0.03140110\n",
      "Iteration 43589, loss = 0.03139647\n",
      "Iteration 43590, loss = 0.03138881\n",
      "Iteration 43591, loss = 0.03140750\n",
      "Iteration 43592, loss = 0.03141319\n",
      "Iteration 43593, loss = 0.03140415\n",
      "Iteration 43594, loss = 0.03138905\n",
      "Iteration 43595, loss = 0.03139908\n",
      "Iteration 43596, loss = 0.03141114\n",
      "Iteration 43597, loss = 0.03140710\n",
      "Iteration 43598, loss = 0.03139457\n",
      "Iteration 43599, loss = 0.03139199\n",
      "Iteration 43600, loss = 0.03139519\n",
      "Iteration 43601, loss = 0.03139851\n",
      "Iteration 43602, loss = 0.03140194\n",
      "Iteration 43603, loss = 0.03140306\n",
      "Iteration 43604, loss = 0.03139326\n",
      "Iteration 43605, loss = 0.03139318\n",
      "Iteration 43606, loss = 0.03140134\n",
      "Iteration 43607, loss = 0.03140620\n",
      "Iteration 43608, loss = 0.03140636\n",
      "Iteration 43609, loss = 0.03139922\n",
      "Iteration 43610, loss = 0.03138339\n",
      "Iteration 43611, loss = 0.03140252\n",
      "Iteration 43612, loss = 0.03140534\n",
      "Iteration 43613, loss = 0.03140548\n",
      "Iteration 43614, loss = 0.03139945\n",
      "Iteration 43615, loss = 0.03139308\n",
      "Iteration 43616, loss = 0.03139506\n",
      "Iteration 43617, loss = 0.03139784\n",
      "Iteration 43618, loss = 0.03139351\n",
      "Iteration 43619, loss = 0.03138736\n",
      "Iteration 43620, loss = 0.03139959\n",
      "Iteration 43621, loss = 0.03140236\n",
      "Iteration 43622, loss = 0.03139336\n",
      "Iteration 43623, loss = 0.03138502\n",
      "Iteration 43624, loss = 0.03139097\n",
      "Iteration 43625, loss = 0.03138704\n",
      "Iteration 43626, loss = 0.03139494\n",
      "Iteration 43627, loss = 0.03139684\n",
      "Iteration 43628, loss = 0.03138872\n",
      "Iteration 43629, loss = 0.03139446\n",
      "Iteration 43630, loss = 0.03138583\n",
      "Iteration 43631, loss = 0.03139221\n",
      "Iteration 43632, loss = 0.03140150\n",
      "Iteration 43633, loss = 0.03139255\n",
      "Iteration 43634, loss = 0.03139478\n",
      "Iteration 43635, loss = 0.03138660\n",
      "Iteration 43636, loss = 0.03139688\n",
      "Iteration 43637, loss = 0.03140006\n",
      "Iteration 43638, loss = 0.03139468\n",
      "Iteration 43639, loss = 0.03140510\n",
      "Iteration 43640, loss = 0.03140464\n",
      "Iteration 43641, loss = 0.03140091\n",
      "Iteration 43642, loss = 0.03139158\n",
      "Iteration 43643, loss = 0.03139583\n",
      "Iteration 43644, loss = 0.03140169\n",
      "Iteration 43645, loss = 0.03140634\n",
      "Iteration 43646, loss = 0.03139759\n",
      "Iteration 43647, loss = 0.03138968\n",
      "Iteration 43648, loss = 0.03140298\n",
      "Iteration 43649, loss = 0.03140862\n",
      "Iteration 43650, loss = 0.03138980\n",
      "Iteration 43651, loss = 0.03138451\n",
      "Iteration 43652, loss = 0.03139171\n",
      "Iteration 43653, loss = 0.03139311\n",
      "Iteration 43654, loss = 0.03139126\n",
      "Iteration 43655, loss = 0.03138640\n",
      "Iteration 43656, loss = 0.03138132\n",
      "Iteration 43657, loss = 0.03137796\n",
      "Iteration 43658, loss = 0.03137437\n",
      "Iteration 43659, loss = 0.03137213\n",
      "Iteration 43660, loss = 0.03137510\n",
      "Iteration 43661, loss = 0.03137173\n",
      "Iteration 43662, loss = 0.03137165\n",
      "Iteration 43663, loss = 0.03136735\n",
      "Iteration 43664, loss = 0.03136357\n",
      "Iteration 43665, loss = 0.03137071\n",
      "Iteration 43666, loss = 0.03137211\n",
      "Iteration 43667, loss = 0.03136940\n",
      "Iteration 43668, loss = 0.03136873\n",
      "Iteration 43669, loss = 0.03137082\n",
      "Iteration 43670, loss = 0.03136666\n",
      "Iteration 43671, loss = 0.03137670\n",
      "Iteration 43672, loss = 0.03137188\n",
      "Iteration 43673, loss = 0.03136875\n",
      "Iteration 43674, loss = 0.03137998\n",
      "Iteration 43675, loss = 0.03137896\n",
      "Iteration 43676, loss = 0.03137902\n",
      "Iteration 43677, loss = 0.03137516\n",
      "Iteration 43678, loss = 0.03137064\n",
      "Iteration 43679, loss = 0.03136393\n",
      "Iteration 43680, loss = 0.03137023\n",
      "Iteration 43681, loss = 0.03137688\n",
      "Iteration 43682, loss = 0.03136865\n",
      "Iteration 43683, loss = 0.03136295\n",
      "Iteration 43684, loss = 0.03137147\n",
      "Iteration 43685, loss = 0.03137370\n",
      "Iteration 43686, loss = 0.03137081\n",
      "Iteration 43687, loss = 0.03135870\n",
      "Iteration 43688, loss = 0.03135833\n",
      "Iteration 43689, loss = 0.03136727\n",
      "Iteration 43690, loss = 0.03136771\n",
      "Iteration 43691, loss = 0.03136405\n",
      "Iteration 43692, loss = 0.03137029\n",
      "Iteration 43693, loss = 0.03137455\n",
      "Iteration 43694, loss = 0.03136884\n",
      "Iteration 43695, loss = 0.03136741\n",
      "Iteration 43696, loss = 0.03137557\n",
      "Iteration 43697, loss = 0.03137241\n",
      "Iteration 43698, loss = 0.03136321\n",
      "Iteration 43699, loss = 0.03136997\n",
      "Iteration 43700, loss = 0.03136950\n",
      "Iteration 43701, loss = 0.03136409\n",
      "Iteration 43702, loss = 0.03136201\n",
      "Iteration 43703, loss = 0.03136340\n",
      "Iteration 43704, loss = 0.03135396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 43705, loss = 0.03136922\n",
      "Iteration 43706, loss = 0.03137350\n",
      "Iteration 43707, loss = 0.03136187\n",
      "Iteration 43708, loss = 0.03135289\n",
      "Iteration 43709, loss = 0.03136147\n",
      "Iteration 43710, loss = 0.03136358\n",
      "Iteration 43711, loss = 0.03136102\n",
      "Iteration 43712, loss = 0.03135382\n",
      "Iteration 43713, loss = 0.03136531\n",
      "Iteration 43714, loss = 0.03137059\n",
      "Iteration 43715, loss = 0.03135805\n",
      "Iteration 43716, loss = 0.03135940\n",
      "Iteration 43717, loss = 0.03136549\n",
      "Iteration 43718, loss = 0.03136525\n",
      "Iteration 43719, loss = 0.03135977\n",
      "Iteration 43720, loss = 0.03135610\n",
      "Iteration 43721, loss = 0.03137311\n",
      "Iteration 43722, loss = 0.03137882\n",
      "Iteration 43723, loss = 0.03136734\n",
      "Iteration 43724, loss = 0.03136649\n",
      "Iteration 43725, loss = 0.03136483\n",
      "Iteration 43726, loss = 0.03137368\n",
      "Iteration 43727, loss = 0.03137530\n",
      "Iteration 43728, loss = 0.03137096\n",
      "Iteration 43729, loss = 0.03136364\n",
      "Iteration 43730, loss = 0.03137321\n",
      "Iteration 43731, loss = 0.03137810\n",
      "Iteration 43732, loss = 0.03136658\n",
      "Iteration 43733, loss = 0.03136641\n",
      "Iteration 43734, loss = 0.03136964\n",
      "Iteration 43735, loss = 0.03135344\n",
      "Iteration 43736, loss = 0.03135628\n",
      "Iteration 43737, loss = 0.03135492\n",
      "Iteration 43738, loss = 0.03136738\n",
      "Iteration 43739, loss = 0.03137493\n",
      "Iteration 43740, loss = 0.03136473\n",
      "Iteration 43741, loss = 0.03136180\n",
      "Iteration 43742, loss = 0.03134607\n",
      "Iteration 43743, loss = 0.03134871\n",
      "Iteration 43744, loss = 0.03136158\n",
      "Iteration 43745, loss = 0.03136965\n",
      "Iteration 43746, loss = 0.03136815\n",
      "Iteration 43747, loss = 0.03136737\n",
      "Iteration 43748, loss = 0.03135733\n",
      "Iteration 43749, loss = 0.03136129\n",
      "Iteration 43750, loss = 0.03136202\n",
      "Iteration 43751, loss = 0.03135622\n",
      "Iteration 43752, loss = 0.03135527\n",
      "Iteration 43753, loss = 0.03135563\n",
      "Iteration 43754, loss = 0.03135662\n",
      "Iteration 43755, loss = 0.03135932\n",
      "Iteration 43756, loss = 0.03135041\n",
      "Iteration 43757, loss = 0.03135322\n",
      "Iteration 43758, loss = 0.03135296\n",
      "Iteration 43759, loss = 0.03136093\n",
      "Iteration 43760, loss = 0.03135476\n",
      "Iteration 43761, loss = 0.03135048\n",
      "Iteration 43762, loss = 0.03134676\n",
      "Iteration 43763, loss = 0.03133521\n",
      "Iteration 43764, loss = 0.03134280\n",
      "Iteration 43765, loss = 0.03136159\n",
      "Iteration 43766, loss = 0.03135925\n",
      "Iteration 43767, loss = 0.03134159\n",
      "Iteration 43768, loss = 0.03133568\n",
      "Iteration 43769, loss = 0.03134122\n",
      "Iteration 43770, loss = 0.03133893\n",
      "Iteration 43771, loss = 0.03133329\n",
      "Iteration 43772, loss = 0.03133882\n",
      "Iteration 43773, loss = 0.03134250\n",
      "Iteration 43774, loss = 0.03133548\n",
      "Iteration 43775, loss = 0.03133809\n",
      "Iteration 43776, loss = 0.03134100\n",
      "Iteration 43777, loss = 0.03134196\n",
      "Iteration 43778, loss = 0.03134759\n",
      "Iteration 43779, loss = 0.03134340\n",
      "Iteration 43780, loss = 0.03133479\n",
      "Iteration 43781, loss = 0.03133153\n",
      "Iteration 43782, loss = 0.03133296\n",
      "Iteration 43783, loss = 0.03133725\n",
      "Iteration 43784, loss = 0.03133168\n",
      "Iteration 43785, loss = 0.03132577\n",
      "Iteration 43786, loss = 0.03133185\n",
      "Iteration 43787, loss = 0.03134013\n",
      "Iteration 43788, loss = 0.03133636\n",
      "Iteration 43789, loss = 0.03133197\n",
      "Iteration 43790, loss = 0.03132775\n",
      "Iteration 43791, loss = 0.03132651\n",
      "Iteration 43792, loss = 0.03132908\n",
      "Iteration 43793, loss = 0.03133290\n",
      "Iteration 43794, loss = 0.03132463\n",
      "Iteration 43795, loss = 0.03132857\n",
      "Iteration 43796, loss = 0.03133537\n",
      "Iteration 43797, loss = 0.03132733\n",
      "Iteration 43798, loss = 0.03132720\n",
      "Iteration 43799, loss = 0.03133211\n",
      "Iteration 43800, loss = 0.03133066\n",
      "Iteration 43801, loss = 0.03132291\n",
      "Iteration 43802, loss = 0.03132264\n",
      "Iteration 43803, loss = 0.03132724\n",
      "Iteration 43804, loss = 0.03132408\n",
      "Iteration 43805, loss = 0.03132466\n",
      "Iteration 43806, loss = 0.03132502\n",
      "Iteration 43807, loss = 0.03131740\n",
      "Iteration 43808, loss = 0.03132842\n",
      "Iteration 43809, loss = 0.03132981\n",
      "Iteration 43810, loss = 0.03132021\n",
      "Iteration 43811, loss = 0.03132087\n",
      "Iteration 43812, loss = 0.03132982\n",
      "Iteration 43813, loss = 0.03133111\n",
      "Iteration 43814, loss = 0.03132950\n",
      "Iteration 43815, loss = 0.03131468\n",
      "Iteration 43816, loss = 0.03133251\n",
      "Iteration 43817, loss = 0.03133967\n",
      "Iteration 43818, loss = 0.03133515\n",
      "Iteration 43819, loss = 0.03133226\n",
      "Iteration 43820, loss = 0.03132255\n",
      "Iteration 43821, loss = 0.03131199\n",
      "Iteration 43822, loss = 0.03132135\n",
      "Iteration 43823, loss = 0.03132658\n",
      "Iteration 43824, loss = 0.03133019\n",
      "Iteration 43825, loss = 0.03131636\n",
      "Iteration 43826, loss = 0.03131448\n",
      "Iteration 43827, loss = 0.03132465\n",
      "Iteration 43828, loss = 0.03132581\n",
      "Iteration 43829, loss = 0.03131786\n",
      "Iteration 43830, loss = 0.03131713\n",
      "Iteration 43831, loss = 0.03131882\n",
      "Iteration 43832, loss = 0.03131662\n",
      "Iteration 43833, loss = 0.03132865\n",
      "Iteration 43834, loss = 0.03132164\n",
      "Iteration 43835, loss = 0.03130918\n",
      "Iteration 43836, loss = 0.03131722\n",
      "Iteration 43837, loss = 0.03133143\n",
      "Iteration 43838, loss = 0.03132986\n",
      "Iteration 43839, loss = 0.03132223\n",
      "Iteration 43840, loss = 0.03131956\n",
      "Iteration 43841, loss = 0.03132165\n",
      "Iteration 43842, loss = 0.03131385\n",
      "Iteration 43843, loss = 0.03130746\n",
      "Iteration 43844, loss = 0.03131317\n",
      "Iteration 43845, loss = 0.03131462\n",
      "Iteration 43846, loss = 0.03130563\n",
      "Iteration 43847, loss = 0.03130639\n",
      "Iteration 43848, loss = 0.03130774\n",
      "Iteration 43849, loss = 0.03131438\n",
      "Iteration 43850, loss = 0.03131498\n",
      "Iteration 43851, loss = 0.03131100\n",
      "Iteration 43852, loss = 0.03131434\n",
      "Iteration 43853, loss = 0.03131523\n",
      "Iteration 43854, loss = 0.03130857\n",
      "Iteration 43855, loss = 0.03130919\n",
      "Iteration 43856, loss = 0.03130567\n",
      "Iteration 43857, loss = 0.03130890\n",
      "Iteration 43858, loss = 0.03130465\n",
      "Iteration 43859, loss = 0.03130164\n",
      "Iteration 43860, loss = 0.03129792\n",
      "Iteration 43861, loss = 0.03130550\n",
      "Iteration 43862, loss = 0.03130799\n",
      "Iteration 43863, loss = 0.03131512\n",
      "Iteration 43864, loss = 0.03131333\n",
      "Iteration 43865, loss = 0.03130395\n",
      "Iteration 43866, loss = 0.03130533\n",
      "Iteration 43867, loss = 0.03130645\n",
      "Iteration 43868, loss = 0.03130683\n",
      "Iteration 43869, loss = 0.03130731\n",
      "Iteration 43870, loss = 0.03129353\n",
      "Iteration 43871, loss = 0.03129815\n",
      "Iteration 43872, loss = 0.03131040\n",
      "Iteration 43873, loss = 0.03130918\n",
      "Iteration 43874, loss = 0.03129538\n",
      "Iteration 43875, loss = 0.03129446\n",
      "Iteration 43876, loss = 0.03130558\n",
      "Iteration 43877, loss = 0.03131084\n",
      "Iteration 43878, loss = 0.03129808\n",
      "Iteration 43879, loss = 0.03129393\n",
      "Iteration 43880, loss = 0.03130025\n",
      "Iteration 43881, loss = 0.03129755\n",
      "Iteration 43882, loss = 0.03130259\n",
      "Iteration 43883, loss = 0.03130289\n",
      "Iteration 43884, loss = 0.03130090\n",
      "Iteration 43885, loss = 0.03130177\n",
      "Iteration 43886, loss = 0.03129105\n",
      "Iteration 43887, loss = 0.03129194\n",
      "Iteration 43888, loss = 0.03130057\n",
      "Iteration 43889, loss = 0.03129846\n",
      "Iteration 43890, loss = 0.03129333\n",
      "Iteration 43891, loss = 0.03128659\n",
      "Iteration 43892, loss = 0.03129769\n",
      "Iteration 43893, loss = 0.03130158\n",
      "Iteration 43894, loss = 0.03130403\n",
      "Iteration 43895, loss = 0.03129148\n",
      "Iteration 43896, loss = 0.03128942\n",
      "Iteration 43897, loss = 0.03130263\n",
      "Iteration 43898, loss = 0.03129663\n",
      "Iteration 43899, loss = 0.03130522\n",
      "Iteration 43900, loss = 0.03129693\n",
      "Iteration 43901, loss = 0.03129230\n",
      "Iteration 43902, loss = 0.03130018\n",
      "Iteration 43903, loss = 0.03129440\n",
      "Iteration 43904, loss = 0.03129490\n",
      "Iteration 43905, loss = 0.03129712\n",
      "Iteration 43906, loss = 0.03129003\n",
      "Iteration 43907, loss = 0.03128605\n",
      "Iteration 43908, loss = 0.03129101\n",
      "Iteration 43909, loss = 0.03129561\n",
      "Iteration 43910, loss = 0.03129498\n",
      "Iteration 43911, loss = 0.03128325\n",
      "Iteration 43912, loss = 0.03129517\n",
      "Iteration 43913, loss = 0.03130029\n",
      "Iteration 43914, loss = 0.03128555\n",
      "Iteration 43915, loss = 0.03128582\n",
      "Iteration 43916, loss = 0.03129943\n",
      "Iteration 43917, loss = 0.03130134\n",
      "Iteration 43918, loss = 0.03129209\n",
      "Iteration 43919, loss = 0.03129264\n",
      "Iteration 43920, loss = 0.03129239\n",
      "Iteration 43921, loss = 0.03129788\n",
      "Iteration 43922, loss = 0.03129391\n",
      "Iteration 43923, loss = 0.03128485\n",
      "Iteration 43924, loss = 0.03128688\n",
      "Iteration 43925, loss = 0.03128263\n",
      "Iteration 43926, loss = 0.03129159\n",
      "Iteration 43927, loss = 0.03128215\n",
      "Iteration 43928, loss = 0.03128254\n",
      "Iteration 43929, loss = 0.03129051\n",
      "Iteration 43930, loss = 0.03128641\n",
      "Iteration 43931, loss = 0.03127944\n",
      "Iteration 43932, loss = 0.03127650\n",
      "Iteration 43933, loss = 0.03128740\n",
      "Iteration 43934, loss = 0.03128194\n",
      "Iteration 43935, loss = 0.03127198\n",
      "Iteration 43936, loss = 0.03127521\n",
      "Iteration 43937, loss = 0.03128620\n",
      "Iteration 43938, loss = 0.03129088\n",
      "Iteration 43939, loss = 0.03128654\n",
      "Iteration 43940, loss = 0.03128068\n",
      "Iteration 43941, loss = 0.03127490\n",
      "Iteration 43942, loss = 0.03127170\n",
      "Iteration 43943, loss = 0.03128676\n",
      "Iteration 43944, loss = 0.03128552\n",
      "Iteration 43945, loss = 0.03129047\n",
      "Iteration 43946, loss = 0.03128339\n",
      "Iteration 43947, loss = 0.03127745\n",
      "Iteration 43948, loss = 0.03127850\n",
      "Iteration 43949, loss = 0.03128723\n",
      "Iteration 43950, loss = 0.03128544\n",
      "Iteration 43951, loss = 0.03129506\n",
      "Iteration 43952, loss = 0.03128551\n",
      "Iteration 43953, loss = 0.03128069\n",
      "Iteration 43954, loss = 0.03129154\n",
      "Iteration 43955, loss = 0.03129517\n",
      "Iteration 43956, loss = 0.03129618\n",
      "Iteration 43957, loss = 0.03129269\n",
      "Iteration 43958, loss = 0.03129195\n",
      "Iteration 43959, loss = 0.03129424\n",
      "Iteration 43960, loss = 0.03128409\n",
      "Iteration 43961, loss = 0.03127211\n",
      "Iteration 43962, loss = 0.03128128\n",
      "Iteration 43963, loss = 0.03128268\n",
      "Iteration 43964, loss = 0.03127834\n",
      "Iteration 43965, loss = 0.03127920\n",
      "Iteration 43966, loss = 0.03128051\n",
      "Iteration 43967, loss = 0.03127199\n",
      "Iteration 43968, loss = 0.03127071\n",
      "Iteration 43969, loss = 0.03126949\n",
      "Iteration 43970, loss = 0.03127123\n",
      "Iteration 43971, loss = 0.03127087\n",
      "Iteration 43972, loss = 0.03127250\n",
      "Iteration 43973, loss = 0.03127551\n",
      "Iteration 43974, loss = 0.03126768\n",
      "Iteration 43975, loss = 0.03126050\n",
      "Iteration 43976, loss = 0.03126214\n",
      "Iteration 43977, loss = 0.03126058\n",
      "Iteration 43978, loss = 0.03126510\n",
      "Iteration 43979, loss = 0.03125972\n",
      "Iteration 43980, loss = 0.03125850\n",
      "Iteration 43981, loss = 0.03126355\n",
      "Iteration 43982, loss = 0.03126692\n",
      "Iteration 43983, loss = 0.03125995\n",
      "Iteration 43984, loss = 0.03126733\n",
      "Iteration 43985, loss = 0.03126695\n",
      "Iteration 43986, loss = 0.03126113\n",
      "Iteration 43987, loss = 0.03126570\n",
      "Iteration 43988, loss = 0.03126234\n",
      "Iteration 43989, loss = 0.03126365\n",
      "Iteration 43990, loss = 0.03127031\n",
      "Iteration 43991, loss = 0.03126448\n",
      "Iteration 43992, loss = 0.03126764\n",
      "Iteration 43993, loss = 0.03126650\n",
      "Iteration 43994, loss = 0.03125267\n",
      "Iteration 43995, loss = 0.03126034\n",
      "Iteration 43996, loss = 0.03126435\n",
      "Iteration 43997, loss = 0.03126285\n",
      "Iteration 43998, loss = 0.03125573\n",
      "Iteration 43999, loss = 0.03124871\n",
      "Iteration 44000, loss = 0.03124646\n",
      "Iteration 44001, loss = 0.03125396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 44002, loss = 0.03125043\n",
      "Iteration 44003, loss = 0.03125810\n",
      "Iteration 44004, loss = 0.03125363\n",
      "Iteration 44005, loss = 0.03126271\n",
      "Iteration 44006, loss = 0.03125995\n",
      "Iteration 44007, loss = 0.03125150\n",
      "Iteration 44008, loss = 0.03125837\n",
      "Iteration 44009, loss = 0.03125929\n",
      "Iteration 44010, loss = 0.03125500\n",
      "Iteration 44011, loss = 0.03124936\n",
      "Iteration 44012, loss = 0.03126201\n",
      "Iteration 44013, loss = 0.03126822\n",
      "Iteration 44014, loss = 0.03125823\n",
      "Iteration 44015, loss = 0.03124728\n",
      "Iteration 44016, loss = 0.03126609\n",
      "Iteration 44017, loss = 0.03127480\n",
      "Iteration 44018, loss = 0.03126510\n",
      "Iteration 44019, loss = 0.03126356\n",
      "Iteration 44020, loss = 0.03125267\n",
      "Iteration 44021, loss = 0.03126100\n",
      "Iteration 44022, loss = 0.03126603\n",
      "Iteration 44023, loss = 0.03126061\n",
      "Iteration 44024, loss = 0.03125140\n",
      "Iteration 44025, loss = 0.03125053\n",
      "Iteration 44026, loss = 0.03125272\n",
      "Iteration 44027, loss = 0.03125996\n",
      "Iteration 44028, loss = 0.03126225\n",
      "Iteration 44029, loss = 0.03125309\n",
      "Iteration 44030, loss = 0.03125149\n",
      "Iteration 44031, loss = 0.03125332\n",
      "Iteration 44032, loss = 0.03125387\n",
      "Iteration 44033, loss = 0.03124940\n",
      "Iteration 44034, loss = 0.03124248\n",
      "Iteration 44035, loss = 0.03124374\n",
      "Iteration 44036, loss = 0.03124766\n",
      "Iteration 44037, loss = 0.03124062\n",
      "Iteration 44038, loss = 0.03124500\n",
      "Iteration 44039, loss = 0.03125804\n",
      "Iteration 44040, loss = 0.03125759\n",
      "Iteration 44041, loss = 0.03124737\n",
      "Iteration 44042, loss = 0.03124716\n",
      "Iteration 44043, loss = 0.03124796\n",
      "Iteration 44044, loss = 0.03126563\n",
      "Iteration 44045, loss = 0.03126929\n",
      "Iteration 44046, loss = 0.03125954\n",
      "Iteration 44047, loss = 0.03124320\n",
      "Iteration 44048, loss = 0.03124163\n",
      "Iteration 44049, loss = 0.03125190\n",
      "Iteration 44050, loss = 0.03125913\n",
      "Iteration 44051, loss = 0.03125113\n",
      "Iteration 44052, loss = 0.03125095\n",
      "Iteration 44053, loss = 0.03125140\n",
      "Iteration 44054, loss = 0.03124873\n",
      "Iteration 44055, loss = 0.03124234\n",
      "Iteration 44056, loss = 0.03124898\n",
      "Iteration 44057, loss = 0.03124933\n",
      "Iteration 44058, loss = 0.03124382\n",
      "Iteration 44059, loss = 0.03124790\n",
      "Iteration 44060, loss = 0.03124178\n",
      "Iteration 44061, loss = 0.03123868\n",
      "Iteration 44062, loss = 0.03123381\n",
      "Iteration 44063, loss = 0.03124166\n",
      "Iteration 44064, loss = 0.03124231\n",
      "Iteration 44065, loss = 0.03123049\n",
      "Iteration 44066, loss = 0.03124098\n",
      "Iteration 44067, loss = 0.03124278\n",
      "Iteration 44068, loss = 0.03123612\n",
      "Iteration 44069, loss = 0.03123316\n",
      "Iteration 44070, loss = 0.03123690\n",
      "Iteration 44071, loss = 0.03123520\n",
      "Iteration 44072, loss = 0.03124069\n",
      "Iteration 44073, loss = 0.03123589\n",
      "Iteration 44074, loss = 0.03124010\n",
      "Iteration 44075, loss = 0.03124007\n",
      "Iteration 44076, loss = 0.03122833\n",
      "Iteration 44077, loss = 0.03123823\n",
      "Iteration 44078, loss = 0.03124985\n",
      "Iteration 44079, loss = 0.03124578\n",
      "Iteration 44080, loss = 0.03123375\n",
      "Iteration 44081, loss = 0.03122604\n",
      "Iteration 44082, loss = 0.03122586\n",
      "Iteration 44083, loss = 0.03122816\n",
      "Iteration 44084, loss = 0.03122940\n",
      "Iteration 44085, loss = 0.03122770\n",
      "Iteration 44086, loss = 0.03121822\n",
      "Iteration 44087, loss = 0.03122885\n",
      "Iteration 44088, loss = 0.03123128\n",
      "Iteration 44089, loss = 0.03123429\n",
      "Iteration 44090, loss = 0.03122070\n",
      "Iteration 44091, loss = 0.03123654\n",
      "Iteration 44092, loss = 0.03123916\n",
      "Iteration 44093, loss = 0.03123964\n",
      "Iteration 44094, loss = 0.03123128\n",
      "Iteration 44095, loss = 0.03124022\n",
      "Iteration 44096, loss = 0.03124750\n",
      "Iteration 44097, loss = 0.03123807\n",
      "Iteration 44098, loss = 0.03124198\n",
      "Iteration 44099, loss = 0.03124738\n",
      "Iteration 44100, loss = 0.03123510\n",
      "Iteration 44101, loss = 0.03122908\n",
      "Iteration 44102, loss = 0.03123544\n",
      "Iteration 44103, loss = 0.03123874\n",
      "Iteration 44104, loss = 0.03123568\n",
      "Iteration 44105, loss = 0.03123115\n",
      "Iteration 44106, loss = 0.03123883\n",
      "Iteration 44107, loss = 0.03123384\n",
      "Iteration 44108, loss = 0.03122470\n",
      "Iteration 44109, loss = 0.03121188\n",
      "Iteration 44110, loss = 0.03123921\n",
      "Iteration 44111, loss = 0.03124941\n",
      "Iteration 44112, loss = 0.03123502\n",
      "Iteration 44113, loss = 0.03122656\n",
      "Iteration 44114, loss = 0.03123044\n",
      "Iteration 44115, loss = 0.03124905\n",
      "Iteration 44116, loss = 0.03125173\n",
      "Iteration 44117, loss = 0.03123411\n",
      "Iteration 44118, loss = 0.03122423\n",
      "Iteration 44119, loss = 0.03122267\n",
      "Iteration 44120, loss = 0.03123017\n",
      "Iteration 44121, loss = 0.03122903\n",
      "Iteration 44122, loss = 0.03123047\n",
      "Iteration 44123, loss = 0.03122976\n",
      "Iteration 44124, loss = 0.03121820\n",
      "Iteration 44125, loss = 0.03122324\n",
      "Iteration 44126, loss = 0.03123667\n",
      "Iteration 44127, loss = 0.03123537\n",
      "Iteration 44128, loss = 0.03120879\n",
      "Iteration 44129, loss = 0.03121761\n",
      "Iteration 44130, loss = 0.03123126\n",
      "Iteration 44131, loss = 0.03123881\n",
      "Iteration 44132, loss = 0.03123178\n",
      "Iteration 44133, loss = 0.03123202\n",
      "Iteration 44134, loss = 0.03124270\n",
      "Iteration 44135, loss = 0.03123468\n",
      "Iteration 44136, loss = 0.03121114\n",
      "Iteration 44137, loss = 0.03121468\n",
      "Iteration 44138, loss = 0.03122365\n",
      "Iteration 44139, loss = 0.03120975\n",
      "Iteration 44140, loss = 0.03122485\n",
      "Iteration 44141, loss = 0.03123316\n",
      "Iteration 44142, loss = 0.03122458\n",
      "Iteration 44143, loss = 0.03122135\n",
      "Iteration 44144, loss = 0.03122143\n",
      "Iteration 44145, loss = 0.03121426\n",
      "Iteration 44146, loss = 0.03119960\n",
      "Iteration 44147, loss = 0.03120928\n",
      "Iteration 44148, loss = 0.03121165\n",
      "Iteration 44149, loss = 0.03120398\n",
      "Iteration 44150, loss = 0.03120281\n",
      "Iteration 44151, loss = 0.03120679\n",
      "Iteration 44152, loss = 0.03119967\n",
      "Iteration 44153, loss = 0.03119836\n",
      "Iteration 44154, loss = 0.03120067\n",
      "Iteration 44155, loss = 0.03121346\n",
      "Iteration 44156, loss = 0.03121103\n",
      "Iteration 44157, loss = 0.03120666\n",
      "Iteration 44158, loss = 0.03121300\n",
      "Iteration 44159, loss = 0.03120653\n",
      "Iteration 44160, loss = 0.03119326\n",
      "Iteration 44161, loss = 0.03121224\n",
      "Iteration 44162, loss = 0.03121830\n",
      "Iteration 44163, loss = 0.03120324\n",
      "Iteration 44164, loss = 0.03119370\n",
      "Iteration 44165, loss = 0.03120737\n",
      "Iteration 44166, loss = 0.03120726\n",
      "Iteration 44167, loss = 0.03120189\n",
      "Iteration 44168, loss = 0.03119732\n",
      "Iteration 44169, loss = 0.03120037\n",
      "Iteration 44170, loss = 0.03119256\n",
      "Iteration 44171, loss = 0.03119850\n",
      "Iteration 44172, loss = 0.03120561\n",
      "Iteration 44173, loss = 0.03119897\n",
      "Iteration 44174, loss = 0.03119383\n",
      "Iteration 44175, loss = 0.03119146\n",
      "Iteration 44176, loss = 0.03119335\n",
      "Iteration 44177, loss = 0.03118751\n",
      "Iteration 44178, loss = 0.03120099\n",
      "Iteration 44179, loss = 0.03120380\n",
      "Iteration 44180, loss = 0.03120335\n",
      "Iteration 44181, loss = 0.03119496\n",
      "Iteration 44182, loss = 0.03118779\n",
      "Iteration 44183, loss = 0.03119239\n",
      "Iteration 44184, loss = 0.03118715\n",
      "Iteration 44185, loss = 0.03119647\n",
      "Iteration 44186, loss = 0.03119713\n",
      "Iteration 44187, loss = 0.03119012\n",
      "Iteration 44188, loss = 0.03118627\n",
      "Iteration 44189, loss = 0.03118473\n",
      "Iteration 44190, loss = 0.03119327\n",
      "Iteration 44191, loss = 0.03118673\n",
      "Iteration 44192, loss = 0.03118468\n",
      "Iteration 44193, loss = 0.03118571\n",
      "Iteration 44194, loss = 0.03119528\n",
      "Iteration 44195, loss = 0.03119455\n",
      "Iteration 44196, loss = 0.03119728\n",
      "Iteration 44197, loss = 0.03119676\n",
      "Iteration 44198, loss = 0.03119368\n",
      "Iteration 44199, loss = 0.03119940\n",
      "Iteration 44200, loss = 0.03119899\n",
      "Iteration 44201, loss = 0.03119828\n",
      "Iteration 44202, loss = 0.03118693\n",
      "Iteration 44203, loss = 0.03118767\n",
      "Iteration 44204, loss = 0.03119282\n",
      "Iteration 44205, loss = 0.03117876\n",
      "Iteration 44206, loss = 0.03117917\n",
      "Iteration 44207, loss = 0.03118391\n",
      "Iteration 44208, loss = 0.03118291\n",
      "Iteration 44209, loss = 0.03117936\n",
      "Iteration 44210, loss = 0.03118390\n",
      "Iteration 44211, loss = 0.03119157\n",
      "Iteration 44212, loss = 0.03118632\n",
      "Iteration 44213, loss = 0.03118698\n",
      "Iteration 44214, loss = 0.03118490\n",
      "Iteration 44215, loss = 0.03118118\n",
      "Iteration 44216, loss = 0.03118556\n",
      "Iteration 44217, loss = 0.03119661\n",
      "Iteration 44218, loss = 0.03119014\n",
      "Iteration 44219, loss = 0.03117611\n",
      "Iteration 44220, loss = 0.03119169\n",
      "Iteration 44221, loss = 0.03119995\n",
      "Iteration 44222, loss = 0.03120543\n",
      "Iteration 44223, loss = 0.03119590\n",
      "Iteration 44224, loss = 0.03118927\n",
      "Iteration 44225, loss = 0.03120169\n",
      "Iteration 44226, loss = 0.03119343\n",
      "Iteration 44227, loss = 0.03118360\n",
      "Iteration 44228, loss = 0.03118776\n",
      "Iteration 44229, loss = 0.03119622\n",
      "Iteration 44230, loss = 0.03119213\n",
      "Iteration 44231, loss = 0.03119310\n",
      "Iteration 44232, loss = 0.03118471\n",
      "Iteration 44233, loss = 0.03118424\n",
      "Iteration 44234, loss = 0.03118433\n",
      "Iteration 44235, loss = 0.03118596\n",
      "Iteration 44236, loss = 0.03118123\n",
      "Iteration 44237, loss = 0.03117171\n",
      "Iteration 44238, loss = 0.03117755\n",
      "Iteration 44239, loss = 0.03118176\n",
      "Iteration 44240, loss = 0.03117724\n",
      "Iteration 44241, loss = 0.03116152\n",
      "Iteration 44242, loss = 0.03117756\n",
      "Iteration 44243, loss = 0.03118737\n",
      "Iteration 44244, loss = 0.03118958\n",
      "Iteration 44245, loss = 0.03118283\n",
      "Iteration 44246, loss = 0.03118387\n",
      "Iteration 44247, loss = 0.03117474\n",
      "Iteration 44248, loss = 0.03117399\n",
      "Iteration 44249, loss = 0.03117848\n",
      "Iteration 44250, loss = 0.03117629\n",
      "Iteration 44251, loss = 0.03117119\n",
      "Iteration 44252, loss = 0.03117786\n",
      "Iteration 44253, loss = 0.03117786\n",
      "Iteration 44254, loss = 0.03118093\n",
      "Iteration 44255, loss = 0.03116911\n",
      "Iteration 44256, loss = 0.03117173\n",
      "Iteration 44257, loss = 0.03117652\n",
      "Iteration 44258, loss = 0.03116812\n",
      "Iteration 44259, loss = 0.03116804\n",
      "Iteration 44260, loss = 0.03117137\n",
      "Iteration 44261, loss = 0.03116650\n",
      "Iteration 44262, loss = 0.03115912\n",
      "Iteration 44263, loss = 0.03116287\n",
      "Iteration 44264, loss = 0.03116857\n",
      "Iteration 44265, loss = 0.03116654\n",
      "Iteration 44266, loss = 0.03117441\n",
      "Iteration 44267, loss = 0.03116989\n",
      "Iteration 44268, loss = 0.03115969\n",
      "Iteration 44269, loss = 0.03115538\n",
      "Iteration 44270, loss = 0.03117316\n",
      "Iteration 44271, loss = 0.03117652\n",
      "Iteration 44272, loss = 0.03116936\n",
      "Iteration 44273, loss = 0.03115599\n",
      "Iteration 44274, loss = 0.03116032\n",
      "Iteration 44275, loss = 0.03117114\n",
      "Iteration 44276, loss = 0.03117589\n",
      "Iteration 44277, loss = 0.03116937\n",
      "Iteration 44278, loss = 0.03116878\n",
      "Iteration 44279, loss = 0.03116748\n",
      "Iteration 44280, loss = 0.03115850\n",
      "Iteration 44281, loss = 0.03116452\n",
      "Iteration 44282, loss = 0.03117330\n",
      "Iteration 44283, loss = 0.03116829\n",
      "Iteration 44284, loss = 0.03115401\n",
      "Iteration 44285, loss = 0.03116624\n",
      "Iteration 44286, loss = 0.03118519\n",
      "Iteration 44287, loss = 0.03118463\n",
      "Iteration 44288, loss = 0.03117670\n",
      "Iteration 44289, loss = 0.03116721\n",
      "Iteration 44290, loss = 0.03117306\n",
      "Iteration 44291, loss = 0.03117622\n",
      "Iteration 44292, loss = 0.03117143\n",
      "Iteration 44293, loss = 0.03116160\n",
      "Iteration 44294, loss = 0.03116106\n",
      "Iteration 44295, loss = 0.03116871\n",
      "Iteration 44296, loss = 0.03116351\n",
      "Iteration 44297, loss = 0.03116280\n",
      "Iteration 44298, loss = 0.03115465\n",
      "Iteration 44299, loss = 0.03117046\n",
      "Iteration 44300, loss = 0.03117249\n",
      "Iteration 44301, loss = 0.03116457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 44302, loss = 0.03114757\n",
      "Iteration 44303, loss = 0.03115292\n",
      "Iteration 44304, loss = 0.03116131\n",
      "Iteration 44305, loss = 0.03115939\n",
      "Iteration 44306, loss = 0.03116382\n",
      "Iteration 44307, loss = 0.03116092\n",
      "Iteration 44308, loss = 0.03115343\n",
      "Iteration 44309, loss = 0.03114864\n",
      "Iteration 44310, loss = 0.03115000\n",
      "Iteration 44311, loss = 0.03115033\n",
      "Iteration 44312, loss = 0.03115739\n",
      "Iteration 44313, loss = 0.03115163\n",
      "Iteration 44314, loss = 0.03114632\n",
      "Iteration 44315, loss = 0.03114288\n",
      "Iteration 44316, loss = 0.03115527\n",
      "Iteration 44317, loss = 0.03114879\n",
      "Iteration 44318, loss = 0.03114879\n",
      "Iteration 44319, loss = 0.03114154\n",
      "Iteration 44320, loss = 0.03115035\n",
      "Iteration 44321, loss = 0.03114293\n",
      "Iteration 44322, loss = 0.03113831\n",
      "Iteration 44323, loss = 0.03115379\n",
      "Iteration 44324, loss = 0.03115488\n",
      "Iteration 44325, loss = 0.03114837\n",
      "Iteration 44326, loss = 0.03113511\n",
      "Iteration 44327, loss = 0.03114159\n",
      "Iteration 44328, loss = 0.03113914\n",
      "Iteration 44329, loss = 0.03113465\n",
      "Iteration 44330, loss = 0.03113621\n",
      "Iteration 44331, loss = 0.03114370\n",
      "Iteration 44332, loss = 0.03113960\n",
      "Iteration 44333, loss = 0.03113777\n",
      "Iteration 44334, loss = 0.03113688\n",
      "Iteration 44335, loss = 0.03113727\n",
      "Iteration 44336, loss = 0.03113395\n",
      "Iteration 44337, loss = 0.03113734\n",
      "Iteration 44338, loss = 0.03114535\n",
      "Iteration 44339, loss = 0.03113863\n",
      "Iteration 44340, loss = 0.03113970\n",
      "Iteration 44341, loss = 0.03113729\n",
      "Iteration 44342, loss = 0.03113100\n",
      "Iteration 44343, loss = 0.03113060\n",
      "Iteration 44344, loss = 0.03113558\n",
      "Iteration 44345, loss = 0.03114130\n",
      "Iteration 44346, loss = 0.03114208\n",
      "Iteration 44347, loss = 0.03113656\n",
      "Iteration 44348, loss = 0.03113065\n",
      "Iteration 44349, loss = 0.03114435\n",
      "Iteration 44350, loss = 0.03114223\n",
      "Iteration 44351, loss = 0.03113804\n",
      "Iteration 44352, loss = 0.03113343\n",
      "Iteration 44353, loss = 0.03114251\n",
      "Iteration 44354, loss = 0.03115003\n",
      "Iteration 44355, loss = 0.03115661\n",
      "Iteration 44356, loss = 0.03115315\n",
      "Iteration 44357, loss = 0.03114497\n",
      "Iteration 44358, loss = 0.03113793\n",
      "Iteration 44359, loss = 0.03114143\n",
      "Iteration 44360, loss = 0.03115644\n",
      "Iteration 44361, loss = 0.03115389\n",
      "Iteration 44362, loss = 0.03113294\n",
      "Iteration 44363, loss = 0.03112554\n",
      "Iteration 44364, loss = 0.03113814\n",
      "Iteration 44365, loss = 0.03113756\n",
      "Iteration 44366, loss = 0.03113615\n",
      "Iteration 44367, loss = 0.03112275\n",
      "Iteration 44368, loss = 0.03113602\n",
      "Iteration 44369, loss = 0.03114066\n",
      "Iteration 44370, loss = 0.03113810\n",
      "Iteration 44371, loss = 0.03112926\n",
      "Iteration 44372, loss = 0.03112895\n",
      "Iteration 44373, loss = 0.03113046\n",
      "Iteration 44374, loss = 0.03113951\n",
      "Iteration 44375, loss = 0.03114688\n",
      "Iteration 44376, loss = 0.03113961\n",
      "Iteration 44377, loss = 0.03112772\n",
      "Iteration 44378, loss = 0.03112719\n",
      "Iteration 44379, loss = 0.03111930\n",
      "Iteration 44380, loss = 0.03113412\n",
      "Iteration 44381, loss = 0.03114096\n",
      "Iteration 44382, loss = 0.03113334\n",
      "Iteration 44383, loss = 0.03111762\n",
      "Iteration 44384, loss = 0.03112911\n",
      "Iteration 44385, loss = 0.03112945\n",
      "Iteration 44386, loss = 0.03112203\n",
      "Iteration 44387, loss = 0.03112359\n",
      "Iteration 44388, loss = 0.03112809\n",
      "Iteration 44389, loss = 0.03112662\n",
      "Iteration 44390, loss = 0.03111807\n",
      "Iteration 44391, loss = 0.03111887\n",
      "Iteration 44392, loss = 0.03113117\n",
      "Iteration 44393, loss = 0.03112950\n",
      "Iteration 44394, loss = 0.03112014\n",
      "Iteration 44395, loss = 0.03112384\n",
      "Iteration 44396, loss = 0.03113078\n",
      "Iteration 44397, loss = 0.03112438\n",
      "Iteration 44398, loss = 0.03112782\n",
      "Iteration 44399, loss = 0.03113373\n",
      "Iteration 44400, loss = 0.03112371\n",
      "Iteration 44401, loss = 0.03111869\n",
      "Iteration 44402, loss = 0.03111712\n",
      "Iteration 44403, loss = 0.03113129\n",
      "Iteration 44404, loss = 0.03112667\n",
      "Iteration 44405, loss = 0.03111870\n",
      "Iteration 44406, loss = 0.03112176\n",
      "Iteration 44407, loss = 0.03111860\n",
      "Iteration 44408, loss = 0.03112460\n",
      "Iteration 44409, loss = 0.03112744\n",
      "Iteration 44410, loss = 0.03112238\n",
      "Iteration 44411, loss = 0.03111447\n",
      "Iteration 44412, loss = 0.03111085\n",
      "Iteration 44413, loss = 0.03111415\n",
      "Iteration 44414, loss = 0.03110825\n",
      "Iteration 44415, loss = 0.03111304\n",
      "Iteration 44416, loss = 0.03111022\n",
      "Iteration 44417, loss = 0.03111158\n",
      "Iteration 44418, loss = 0.03110849\n",
      "Iteration 44419, loss = 0.03111364\n",
      "Iteration 44420, loss = 0.03111715\n",
      "Iteration 44421, loss = 0.03111361\n",
      "Iteration 44422, loss = 0.03111268\n",
      "Iteration 44423, loss = 0.03110810\n",
      "Iteration 44424, loss = 0.03110943\n",
      "Iteration 44425, loss = 0.03110894\n",
      "Iteration 44426, loss = 0.03111194\n",
      "Iteration 44427, loss = 0.03110978\n",
      "Iteration 44428, loss = 0.03110570\n",
      "Iteration 44429, loss = 0.03111384\n",
      "Iteration 44430, loss = 0.03111482\n",
      "Iteration 44431, loss = 0.03112164\n",
      "Iteration 44432, loss = 0.03111737\n",
      "Iteration 44433, loss = 0.03110925\n",
      "Iteration 44434, loss = 0.03111717\n",
      "Iteration 44435, loss = 0.03111465\n",
      "Iteration 44436, loss = 0.03110714\n",
      "Iteration 44437, loss = 0.03109867\n",
      "Iteration 44438, loss = 0.03110261\n",
      "Iteration 44439, loss = 0.03109951\n",
      "Iteration 44440, loss = 0.03110451\n",
      "Iteration 44441, loss = 0.03110020\n",
      "Iteration 44442, loss = 0.03109482\n",
      "Iteration 44443, loss = 0.03110372\n",
      "Iteration 44444, loss = 0.03111209\n",
      "Iteration 44445, loss = 0.03110407\n",
      "Iteration 44446, loss = 0.03110154\n",
      "Iteration 44447, loss = 0.03110354\n",
      "Iteration 44448, loss = 0.03111533\n",
      "Iteration 44449, loss = 0.03111726\n",
      "Iteration 44450, loss = 0.03111473\n",
      "Iteration 44451, loss = 0.03111750\n",
      "Iteration 44452, loss = 0.03111522\n",
      "Iteration 44453, loss = 0.03110783\n",
      "Iteration 44454, loss = 0.03110336\n",
      "Iteration 44455, loss = 0.03110291\n",
      "Iteration 44456, loss = 0.03109513\n",
      "Iteration 44457, loss = 0.03109519\n",
      "Iteration 44458, loss = 0.03110143\n",
      "Iteration 44459, loss = 0.03109984\n",
      "Iteration 44460, loss = 0.03109333\n",
      "Iteration 44461, loss = 0.03109722\n",
      "Iteration 44462, loss = 0.03109592\n",
      "Iteration 44463, loss = 0.03109439\n",
      "Iteration 44464, loss = 0.03109061\n",
      "Iteration 44465, loss = 0.03110079\n",
      "Iteration 44466, loss = 0.03109770\n",
      "Iteration 44467, loss = 0.03109039\n",
      "Iteration 44468, loss = 0.03109554\n",
      "Iteration 44469, loss = 0.03109664\n",
      "Iteration 44470, loss = 0.03109238\n",
      "Iteration 44471, loss = 0.03109236\n",
      "Iteration 44472, loss = 0.03109105\n",
      "Iteration 44473, loss = 0.03108533\n",
      "Iteration 44474, loss = 0.03108711\n",
      "Iteration 44475, loss = 0.03109032\n",
      "Iteration 44476, loss = 0.03108507\n",
      "Iteration 44477, loss = 0.03108372\n",
      "Iteration 44478, loss = 0.03109228\n",
      "Iteration 44479, loss = 0.03109030\n",
      "Iteration 44480, loss = 0.03108489\n",
      "Iteration 44481, loss = 0.03108973\n",
      "Iteration 44482, loss = 0.03108968\n",
      "Iteration 44483, loss = 0.03108676\n",
      "Iteration 44484, loss = 0.03108628\n",
      "Iteration 44485, loss = 0.03109052\n",
      "Iteration 44486, loss = 0.03109541\n",
      "Iteration 44487, loss = 0.03109908\n",
      "Iteration 44488, loss = 0.03109193\n",
      "Iteration 44489, loss = 0.03108658\n",
      "Iteration 44490, loss = 0.03109260\n",
      "Iteration 44491, loss = 0.03110323\n",
      "Iteration 44492, loss = 0.03109832\n",
      "Iteration 44493, loss = 0.03108549\n",
      "Iteration 44494, loss = 0.03109229\n",
      "Iteration 44495, loss = 0.03110081\n",
      "Iteration 44496, loss = 0.03110946\n",
      "Iteration 44497, loss = 0.03110618\n",
      "Iteration 44498, loss = 0.03109435\n",
      "Iteration 44499, loss = 0.03109033\n",
      "Iteration 44500, loss = 0.03109017\n",
      "Iteration 44501, loss = 0.03109265\n",
      "Iteration 44502, loss = 0.03108765\n",
      "Iteration 44503, loss = 0.03108942\n",
      "Iteration 44504, loss = 0.03108196\n",
      "Iteration 44505, loss = 0.03109230\n",
      "Iteration 44506, loss = 0.03109269\n",
      "Iteration 44507, loss = 0.03109584\n",
      "Iteration 44508, loss = 0.03109538\n",
      "Iteration 44509, loss = 0.03109133\n",
      "Iteration 44510, loss = 0.03109054\n",
      "Iteration 44511, loss = 0.03107461\n",
      "Iteration 44512, loss = 0.03108263\n",
      "Iteration 44513, loss = 0.03109360\n",
      "Iteration 44514, loss = 0.03108617\n",
      "Iteration 44515, loss = 0.03108401\n",
      "Iteration 44516, loss = 0.03108387\n",
      "Iteration 44517, loss = 0.03109341\n",
      "Iteration 44518, loss = 0.03109489\n",
      "Iteration 44519, loss = 0.03109163\n",
      "Iteration 44520, loss = 0.03108360\n",
      "Iteration 44521, loss = 0.03106925\n",
      "Iteration 44522, loss = 0.03108949\n",
      "Iteration 44523, loss = 0.03109871\n",
      "Iteration 44524, loss = 0.03108471\n",
      "Iteration 44525, loss = 0.03107236\n",
      "Iteration 44526, loss = 0.03108138\n",
      "Iteration 44527, loss = 0.03108092\n",
      "Iteration 44528, loss = 0.03107524\n",
      "Iteration 44529, loss = 0.03107091\n",
      "Iteration 44530, loss = 0.03108729\n",
      "Iteration 44531, loss = 0.03108364\n",
      "Iteration 44532, loss = 0.03107591\n",
      "Iteration 44533, loss = 0.03107982\n",
      "Iteration 44534, loss = 0.03109763\n",
      "Iteration 44535, loss = 0.03110407\n",
      "Iteration 44536, loss = 0.03110323\n",
      "Iteration 44537, loss = 0.03109517\n",
      "Iteration 44538, loss = 0.03108416\n",
      "Iteration 44539, loss = 0.03106958\n",
      "Iteration 44540, loss = 0.03107752\n",
      "Iteration 44541, loss = 0.03107438\n",
      "Iteration 44542, loss = 0.03107425\n",
      "Iteration 44543, loss = 0.03107477\n",
      "Iteration 44544, loss = 0.03107509\n",
      "Iteration 44545, loss = 0.03106798\n",
      "Iteration 44546, loss = 0.03107540\n",
      "Iteration 44547, loss = 0.03107496\n",
      "Iteration 44548, loss = 0.03106009\n",
      "Iteration 44549, loss = 0.03106323\n",
      "Iteration 44550, loss = 0.03107436\n",
      "Iteration 44551, loss = 0.03107574\n",
      "Iteration 44552, loss = 0.03107177\n",
      "Iteration 44553, loss = 0.03107376\n",
      "Iteration 44554, loss = 0.03107431\n",
      "Iteration 44555, loss = 0.03106408\n",
      "Iteration 44556, loss = 0.03106675\n",
      "Iteration 44557, loss = 0.03107057\n",
      "Iteration 44558, loss = 0.03106012\n",
      "Iteration 44559, loss = 0.03106059\n",
      "Iteration 44560, loss = 0.03107701\n",
      "Iteration 44561, loss = 0.03107707\n",
      "Iteration 44562, loss = 0.03106908\n",
      "Iteration 44563, loss = 0.03106728\n",
      "Iteration 44564, loss = 0.03106333\n",
      "Iteration 44565, loss = 0.03105982\n",
      "Iteration 44566, loss = 0.03106095\n",
      "Iteration 44567, loss = 0.03106641\n",
      "Iteration 44568, loss = 0.03106519\n",
      "Iteration 44569, loss = 0.03106047\n",
      "Iteration 44570, loss = 0.03106156\n",
      "Iteration 44571, loss = 0.03106119\n",
      "Iteration 44572, loss = 0.03105902\n",
      "Iteration 44573, loss = 0.03105082\n",
      "Iteration 44574, loss = 0.03105885\n",
      "Iteration 44575, loss = 0.03106737\n",
      "Iteration 44576, loss = 0.03106109\n",
      "Iteration 44577, loss = 0.03105767\n",
      "Iteration 44578, loss = 0.03106003\n",
      "Iteration 44579, loss = 0.03107274\n",
      "Iteration 44580, loss = 0.03106170\n",
      "Iteration 44581, loss = 0.03105255\n",
      "Iteration 44582, loss = 0.03105474\n",
      "Iteration 44583, loss = 0.03105609\n",
      "Iteration 44584, loss = 0.03106563\n",
      "Iteration 44585, loss = 0.03107053\n",
      "Iteration 44586, loss = 0.03106779\n",
      "Iteration 44587, loss = 0.03105388\n",
      "Iteration 44588, loss = 0.03104893\n",
      "Iteration 44589, loss = 0.03105752\n",
      "Iteration 44590, loss = 0.03104568\n",
      "Iteration 44591, loss = 0.03105830\n",
      "Iteration 44592, loss = 0.03107032\n",
      "Iteration 44593, loss = 0.03107253\n",
      "Iteration 44594, loss = 0.03107353\n",
      "Iteration 44595, loss = 0.03106285\n",
      "Iteration 44596, loss = 0.03105176\n",
      "Iteration 44597, loss = 0.03104225\n",
      "Iteration 44598, loss = 0.03105430\n",
      "Iteration 44599, loss = 0.03106348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 44600, loss = 0.03105684\n",
      "Iteration 44601, loss = 0.03104459\n",
      "Iteration 44602, loss = 0.03104737\n",
      "Iteration 44603, loss = 0.03104859\n",
      "Iteration 44604, loss = 0.03105500\n",
      "Iteration 44605, loss = 0.03105402\n",
      "Iteration 44606, loss = 0.03104602\n",
      "Iteration 44607, loss = 0.03105144\n",
      "Iteration 44608, loss = 0.03104843\n",
      "Iteration 44609, loss = 0.03106380\n",
      "Iteration 44610, loss = 0.03106354\n",
      "Iteration 44611, loss = 0.03104743\n",
      "Iteration 44612, loss = 0.03105715\n",
      "Iteration 44613, loss = 0.03106115\n",
      "Iteration 44614, loss = 0.03106511\n",
      "Iteration 44615, loss = 0.03106684\n",
      "Iteration 44616, loss = 0.03105349\n",
      "Iteration 44617, loss = 0.03105854\n",
      "Iteration 44618, loss = 0.03105909\n",
      "Iteration 44619, loss = 0.03107166\n",
      "Iteration 44620, loss = 0.03106564\n",
      "Iteration 44621, loss = 0.03105488\n",
      "Iteration 44622, loss = 0.03104753\n",
      "Iteration 44623, loss = 0.03104998\n",
      "Iteration 44624, loss = 0.03105051\n",
      "Iteration 44625, loss = 0.03104583\n",
      "Iteration 44626, loss = 0.03104069\n",
      "Iteration 44627, loss = 0.03104897\n",
      "Iteration 44628, loss = 0.03104552\n",
      "Iteration 44629, loss = 0.03103266\n",
      "Iteration 44630, loss = 0.03103490\n",
      "Iteration 44631, loss = 0.03103172\n",
      "Iteration 44632, loss = 0.03102775\n",
      "Iteration 44633, loss = 0.03102895\n",
      "Iteration 44634, loss = 0.03103390\n",
      "Iteration 44635, loss = 0.03103146\n",
      "Iteration 44636, loss = 0.03102947\n",
      "Iteration 44637, loss = 0.03103573\n",
      "Iteration 44638, loss = 0.03104246\n",
      "Iteration 44639, loss = 0.03102789\n",
      "Iteration 44640, loss = 0.03103391\n",
      "Iteration 44641, loss = 0.03103746\n",
      "Iteration 44642, loss = 0.03104210\n",
      "Iteration 44643, loss = 0.03103441\n",
      "Iteration 44644, loss = 0.03103011\n",
      "Iteration 44645, loss = 0.03103995\n",
      "Iteration 44646, loss = 0.03103873\n",
      "Iteration 44647, loss = 0.03104595\n",
      "Iteration 44648, loss = 0.03104701\n",
      "Iteration 44649, loss = 0.03103974\n",
      "Iteration 44650, loss = 0.03102626\n",
      "Iteration 44651, loss = 0.03103540\n",
      "Iteration 44652, loss = 0.03103535\n",
      "Iteration 44653, loss = 0.03102478\n",
      "Iteration 44654, loss = 0.03102584\n",
      "Iteration 44655, loss = 0.03102771\n",
      "Iteration 44656, loss = 0.03102462\n",
      "Iteration 44657, loss = 0.03102744\n",
      "Iteration 44658, loss = 0.03102315\n",
      "Iteration 44659, loss = 0.03102742\n",
      "Iteration 44660, loss = 0.03103401\n",
      "Iteration 44661, loss = 0.03102889\n",
      "Iteration 44662, loss = 0.03102575\n",
      "Iteration 44663, loss = 0.03102656\n",
      "Iteration 44664, loss = 0.03103002\n",
      "Iteration 44665, loss = 0.03103216\n",
      "Iteration 44666, loss = 0.03102514\n",
      "Iteration 44667, loss = 0.03102358\n",
      "Iteration 44668, loss = 0.03102118\n",
      "Iteration 44669, loss = 0.03101754\n",
      "Iteration 44670, loss = 0.03101729\n",
      "Iteration 44671, loss = 0.03102620\n",
      "Iteration 44672, loss = 0.03103634\n",
      "Iteration 44673, loss = 0.03103163\n",
      "Iteration 44674, loss = 0.03101878\n",
      "Iteration 44675, loss = 0.03101900\n",
      "Iteration 44676, loss = 0.03103301\n",
      "Iteration 44677, loss = 0.03103166\n",
      "Iteration 44678, loss = 0.03102961\n",
      "Iteration 44679, loss = 0.03102737\n",
      "Iteration 44680, loss = 0.03104657\n",
      "Iteration 44681, loss = 0.03105323\n",
      "Iteration 44682, loss = 0.03104492\n",
      "Iteration 44683, loss = 0.03103514\n",
      "Iteration 44684, loss = 0.03102592\n",
      "Iteration 44685, loss = 0.03103535\n",
      "Iteration 44686, loss = 0.03103874\n",
      "Iteration 44687, loss = 0.03102382\n",
      "Iteration 44688, loss = 0.03102282\n",
      "Iteration 44689, loss = 0.03103197\n",
      "Iteration 44690, loss = 0.03103775\n",
      "Iteration 44691, loss = 0.03104195\n",
      "Iteration 44692, loss = 0.03103800\n",
      "Iteration 44693, loss = 0.03102134\n",
      "Iteration 44694, loss = 0.03100424\n",
      "Iteration 44695, loss = 0.03102587\n",
      "Iteration 44696, loss = 0.03103155\n",
      "Iteration 44697, loss = 0.03103224\n",
      "Iteration 44698, loss = 0.03101843\n",
      "Iteration 44699, loss = 0.03102456\n",
      "Iteration 44700, loss = 0.03102379\n",
      "Iteration 44701, loss = 0.03103812\n",
      "Iteration 44702, loss = 0.03104131\n",
      "Iteration 44703, loss = 0.03102940\n",
      "Iteration 44704, loss = 0.03103229\n",
      "Iteration 44705, loss = 0.03103415\n",
      "Iteration 44706, loss = 0.03102419\n",
      "Iteration 44707, loss = 0.03102197\n",
      "Iteration 44708, loss = 0.03103439\n",
      "Iteration 44709, loss = 0.03102279\n",
      "Iteration 44710, loss = 0.03100575\n",
      "Iteration 44711, loss = 0.03101864\n",
      "Iteration 44712, loss = 0.03103021\n",
      "Iteration 44713, loss = 0.03102354\n",
      "Iteration 44714, loss = 0.03100686\n",
      "Iteration 44715, loss = 0.03101812\n",
      "Iteration 44716, loss = 0.03102574\n",
      "Iteration 44717, loss = 0.03101237\n",
      "Iteration 44718, loss = 0.03101573\n",
      "Iteration 44719, loss = 0.03101729\n",
      "Iteration 44720, loss = 0.03101074\n",
      "Iteration 44721, loss = 0.03100224\n",
      "Iteration 44722, loss = 0.03101957\n",
      "Iteration 44723, loss = 0.03103042\n",
      "Iteration 44724, loss = 0.03102207\n",
      "Iteration 44725, loss = 0.03101221\n",
      "Iteration 44726, loss = 0.03101040\n",
      "Iteration 44727, loss = 0.03101182\n",
      "Iteration 44728, loss = 0.03100833\n",
      "Iteration 44729, loss = 0.03100927\n",
      "Iteration 44730, loss = 0.03100884\n",
      "Iteration 44731, loss = 0.03100379\n",
      "Iteration 44732, loss = 0.03100532\n",
      "Iteration 44733, loss = 0.03101965\n",
      "Iteration 44734, loss = 0.03101534\n",
      "Iteration 44735, loss = 0.03099936\n",
      "Iteration 44736, loss = 0.03099888\n",
      "Iteration 44737, loss = 0.03100222\n",
      "Iteration 44738, loss = 0.03099430\n",
      "Iteration 44739, loss = 0.03099913\n",
      "Iteration 44740, loss = 0.03100307\n",
      "Iteration 44741, loss = 0.03099955\n",
      "Iteration 44742, loss = 0.03099576\n",
      "Iteration 44743, loss = 0.03098631\n",
      "Iteration 44744, loss = 0.03099344\n",
      "Iteration 44745, loss = 0.03099091\n",
      "Iteration 44746, loss = 0.03098794\n",
      "Iteration 44747, loss = 0.03099456\n",
      "Iteration 44748, loss = 0.03098890\n",
      "Iteration 44749, loss = 0.03098982\n",
      "Iteration 44750, loss = 0.03099564\n",
      "Iteration 44751, loss = 0.03099677\n",
      "Iteration 44752, loss = 0.03099028\n",
      "Iteration 44753, loss = 0.03098663\n",
      "Iteration 44754, loss = 0.03098677\n",
      "Iteration 44755, loss = 0.03099529\n",
      "Iteration 44756, loss = 0.03098933\n",
      "Iteration 44757, loss = 0.03099656\n",
      "Iteration 44758, loss = 0.03100260\n",
      "Iteration 44759, loss = 0.03099414\n",
      "Iteration 44760, loss = 0.03100169\n",
      "Iteration 44761, loss = 0.03100530\n",
      "Iteration 44762, loss = 0.03100365\n",
      "Iteration 44763, loss = 0.03099463\n",
      "Iteration 44764, loss = 0.03098791\n",
      "Iteration 44765, loss = 0.03100639\n",
      "Iteration 44766, loss = 0.03101319\n",
      "Iteration 44767, loss = 0.03100744\n",
      "Iteration 44768, loss = 0.03099387\n",
      "Iteration 44769, loss = 0.03098958\n",
      "Iteration 44770, loss = 0.03100643\n",
      "Iteration 44771, loss = 0.03100496\n",
      "Iteration 44772, loss = 0.03099403\n",
      "Iteration 44773, loss = 0.03098897\n",
      "Iteration 44774, loss = 0.03099153\n",
      "Iteration 44775, loss = 0.03100137\n",
      "Iteration 44776, loss = 0.03099808\n",
      "Iteration 44777, loss = 0.03099605\n",
      "Iteration 44778, loss = 0.03099168\n",
      "Iteration 44779, loss = 0.03098162\n",
      "Iteration 44780, loss = 0.03098097\n",
      "Iteration 44781, loss = 0.03097685\n",
      "Iteration 44782, loss = 0.03098163\n",
      "Iteration 44783, loss = 0.03097560\n",
      "Iteration 44784, loss = 0.03099057\n",
      "Iteration 44785, loss = 0.03099146\n",
      "Iteration 44786, loss = 0.03098665\n",
      "Iteration 44787, loss = 0.03097880\n",
      "Iteration 44788, loss = 0.03099057\n",
      "Iteration 44789, loss = 0.03099621\n",
      "Iteration 44790, loss = 0.03098301\n",
      "Iteration 44791, loss = 0.03098228\n",
      "Iteration 44792, loss = 0.03099065\n",
      "Iteration 44793, loss = 0.03097870\n",
      "Iteration 44794, loss = 0.03097628\n",
      "Iteration 44795, loss = 0.03099379\n",
      "Iteration 44796, loss = 0.03099654\n",
      "Iteration 44797, loss = 0.03099598\n",
      "Iteration 44798, loss = 0.03098937\n",
      "Iteration 44799, loss = 0.03098589\n",
      "Iteration 44800, loss = 0.03097584\n",
      "Iteration 44801, loss = 0.03098240\n",
      "Iteration 44802, loss = 0.03100636\n",
      "Iteration 44803, loss = 0.03100647\n",
      "Iteration 44804, loss = 0.03097820\n",
      "Iteration 44805, loss = 0.03098081\n",
      "Iteration 44806, loss = 0.03100319\n",
      "Iteration 44807, loss = 0.03100837\n",
      "Iteration 44808, loss = 0.03099603\n",
      "Iteration 44809, loss = 0.03100379\n",
      "Iteration 44810, loss = 0.03100460\n",
      "Iteration 44811, loss = 0.03099749\n",
      "Iteration 44812, loss = 0.03098536\n",
      "Iteration 44813, loss = 0.03098020\n",
      "Iteration 44814, loss = 0.03098449\n",
      "Iteration 44815, loss = 0.03096855\n",
      "Iteration 44816, loss = 0.03098210\n",
      "Iteration 44817, loss = 0.03099310\n",
      "Iteration 44818, loss = 0.03098393\n",
      "Iteration 44819, loss = 0.03097604\n",
      "Iteration 44820, loss = 0.03098071\n",
      "Iteration 44821, loss = 0.03098637\n",
      "Iteration 44822, loss = 0.03098194\n",
      "Iteration 44823, loss = 0.03097498\n",
      "Iteration 44824, loss = 0.03097603\n",
      "Iteration 44825, loss = 0.03096932\n",
      "Iteration 44826, loss = 0.03097031\n",
      "Iteration 44827, loss = 0.03097600\n",
      "Iteration 44828, loss = 0.03098030\n",
      "Iteration 44829, loss = 0.03097721\n",
      "Iteration 44830, loss = 0.03095734\n",
      "Iteration 44831, loss = 0.03097574\n",
      "Iteration 44832, loss = 0.03098181\n",
      "Iteration 44833, loss = 0.03097716\n",
      "Iteration 44834, loss = 0.03098296\n",
      "Iteration 44835, loss = 0.03097792\n",
      "Iteration 44836, loss = 0.03096846\n",
      "Iteration 44837, loss = 0.03098563\n",
      "Iteration 44838, loss = 0.03098872\n",
      "Iteration 44839, loss = 0.03097258\n",
      "Iteration 44840, loss = 0.03095888\n",
      "Iteration 44841, loss = 0.03097127\n",
      "Iteration 44842, loss = 0.03097035\n",
      "Iteration 44843, loss = 0.03095848\n",
      "Iteration 44844, loss = 0.03096693\n",
      "Iteration 44845, loss = 0.03096662\n",
      "Iteration 44846, loss = 0.03095266\n",
      "Iteration 44847, loss = 0.03096974\n",
      "Iteration 44848, loss = 0.03098059\n",
      "Iteration 44849, loss = 0.03097590\n",
      "Iteration 44850, loss = 0.03097136\n",
      "Iteration 44851, loss = 0.03096601\n",
      "Iteration 44852, loss = 0.03097404\n",
      "Iteration 44853, loss = 0.03098436\n",
      "Iteration 44854, loss = 0.03098497\n",
      "Iteration 44855, loss = 0.03096661\n",
      "Iteration 44856, loss = 0.03096395\n",
      "Iteration 44857, loss = 0.03097963\n",
      "Iteration 44858, loss = 0.03098061\n",
      "Iteration 44859, loss = 0.03097337\n",
      "Iteration 44860, loss = 0.03097318\n",
      "Iteration 44861, loss = 0.03097196\n",
      "Iteration 44862, loss = 0.03096783\n",
      "Iteration 44863, loss = 0.03095608\n",
      "Iteration 44864, loss = 0.03096142\n",
      "Iteration 44865, loss = 0.03097199\n",
      "Iteration 44866, loss = 0.03096621\n",
      "Iteration 44867, loss = 0.03095306\n",
      "Iteration 44868, loss = 0.03095705\n",
      "Iteration 44869, loss = 0.03096338\n",
      "Iteration 44870, loss = 0.03096413\n",
      "Iteration 44871, loss = 0.03096404\n",
      "Iteration 44872, loss = 0.03096529\n",
      "Iteration 44873, loss = 0.03096357\n",
      "Iteration 44874, loss = 0.03095035\n",
      "Iteration 44875, loss = 0.03095781\n",
      "Iteration 44876, loss = 0.03095931\n",
      "Iteration 44877, loss = 0.03095522\n",
      "Iteration 44878, loss = 0.03094803\n",
      "Iteration 44879, loss = 0.03095571\n",
      "Iteration 44880, loss = 0.03095671\n",
      "Iteration 44881, loss = 0.03094909\n",
      "Iteration 44882, loss = 0.03094736\n",
      "Iteration 44883, loss = 0.03095217\n",
      "Iteration 44884, loss = 0.03094734\n",
      "Iteration 44885, loss = 0.03095016\n",
      "Iteration 44886, loss = 0.03095430\n",
      "Iteration 44887, loss = 0.03094626\n",
      "Iteration 44888, loss = 0.03095012\n",
      "Iteration 44889, loss = 0.03094963\n",
      "Iteration 44890, loss = 0.03094884\n",
      "Iteration 44891, loss = 0.03095170\n",
      "Iteration 44892, loss = 0.03095580\n",
      "Iteration 44893, loss = 0.03095331\n",
      "Iteration 44894, loss = 0.03095682\n",
      "Iteration 44895, loss = 0.03096571\n",
      "Iteration 44896, loss = 0.03095886\n",
      "Iteration 44897, loss = 0.03094626\n",
      "Iteration 44898, loss = 0.03095963\n",
      "Iteration 44899, loss = 0.03096110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 44900, loss = 0.03095683\n",
      "Iteration 44901, loss = 0.03095621\n",
      "Iteration 44902, loss = 0.03095894\n",
      "Iteration 44903, loss = 0.03093810\n",
      "Iteration 44904, loss = 0.03094648\n",
      "Iteration 44905, loss = 0.03095713\n",
      "Iteration 44906, loss = 0.03095892\n",
      "Iteration 44907, loss = 0.03094509\n",
      "Iteration 44908, loss = 0.03093616\n",
      "Iteration 44909, loss = 0.03094615\n",
      "Iteration 44910, loss = 0.03095011\n",
      "Iteration 44911, loss = 0.03093823\n",
      "Iteration 44912, loss = 0.03093412\n",
      "Iteration 44913, loss = 0.03094143\n",
      "Iteration 44914, loss = 0.03094466\n",
      "Iteration 44915, loss = 0.03094405\n",
      "Iteration 44916, loss = 0.03094261\n",
      "Iteration 44917, loss = 0.03095466\n",
      "Iteration 44918, loss = 0.03095832\n",
      "Iteration 44919, loss = 0.03095999\n",
      "Iteration 44920, loss = 0.03094861\n",
      "Iteration 44921, loss = 0.03093721\n",
      "Iteration 44922, loss = 0.03093478\n",
      "Iteration 44923, loss = 0.03095145\n",
      "Iteration 44924, loss = 0.03096214\n",
      "Iteration 44925, loss = 0.03095571\n",
      "Iteration 44926, loss = 0.03094645\n",
      "Iteration 44927, loss = 0.03093178\n",
      "Iteration 44928, loss = 0.03093716\n",
      "Iteration 44929, loss = 0.03094247\n",
      "Iteration 44930, loss = 0.03092737\n",
      "Iteration 44931, loss = 0.03093293\n",
      "Iteration 44932, loss = 0.03094344\n",
      "Iteration 44933, loss = 0.03092856\n",
      "Iteration 44934, loss = 0.03093072\n",
      "Iteration 44935, loss = 0.03093842\n",
      "Iteration 44936, loss = 0.03093876\n",
      "Iteration 44937, loss = 0.03094624\n",
      "Iteration 44938, loss = 0.03094078\n",
      "Iteration 44939, loss = 0.03092803\n",
      "Iteration 44940, loss = 0.03093266\n",
      "Iteration 44941, loss = 0.03094340\n",
      "Iteration 44942, loss = 0.03094440\n",
      "Iteration 44943, loss = 0.03093282\n",
      "Iteration 44944, loss = 0.03093325\n",
      "Iteration 44945, loss = 0.03093875\n",
      "Iteration 44946, loss = 0.03093580\n",
      "Iteration 44947, loss = 0.03092109\n",
      "Iteration 44948, loss = 0.03091608\n",
      "Iteration 44949, loss = 0.03091978\n",
      "Iteration 44950, loss = 0.03092392\n",
      "Iteration 44951, loss = 0.03092013\n",
      "Iteration 44952, loss = 0.03091994\n",
      "Iteration 44953, loss = 0.03091838\n",
      "Iteration 44954, loss = 0.03092185\n",
      "Iteration 44955, loss = 0.03092074\n",
      "Iteration 44956, loss = 0.03091757\n",
      "Iteration 44957, loss = 0.03093051\n",
      "Iteration 44958, loss = 0.03093294\n",
      "Iteration 44959, loss = 0.03092157\n",
      "Iteration 44960, loss = 0.03094127\n",
      "Iteration 44961, loss = 0.03093911\n",
      "Iteration 44962, loss = 0.03093138\n",
      "Iteration 44963, loss = 0.03091628\n",
      "Iteration 44964, loss = 0.03092991\n",
      "Iteration 44965, loss = 0.03093342\n",
      "Iteration 44966, loss = 0.03093719\n",
      "Iteration 44967, loss = 0.03093394\n",
      "Iteration 44968, loss = 0.03093190\n",
      "Iteration 44969, loss = 0.03092259\n",
      "Iteration 44970, loss = 0.03092550\n",
      "Iteration 44971, loss = 0.03093835\n",
      "Iteration 44972, loss = 0.03093461\n",
      "Iteration 44973, loss = 0.03093526\n",
      "Iteration 44974, loss = 0.03092663\n",
      "Iteration 44975, loss = 0.03092842\n",
      "Iteration 44976, loss = 0.03092969\n",
      "Iteration 44977, loss = 0.03092831\n",
      "Iteration 44978, loss = 0.03091685\n",
      "Iteration 44979, loss = 0.03093723\n",
      "Iteration 44980, loss = 0.03094105\n",
      "Iteration 44981, loss = 0.03092186\n",
      "Iteration 44982, loss = 0.03091550\n",
      "Iteration 44983, loss = 0.03092313\n",
      "Iteration 44984, loss = 0.03092606\n",
      "Iteration 44985, loss = 0.03093410\n",
      "Iteration 44986, loss = 0.03093356\n",
      "Iteration 44987, loss = 0.03093158\n",
      "Iteration 44988, loss = 0.03092163\n",
      "Iteration 44989, loss = 0.03091734\n",
      "Iteration 44990, loss = 0.03091321\n",
      "Iteration 44991, loss = 0.03091611\n",
      "Iteration 44992, loss = 0.03091964\n",
      "Iteration 44993, loss = 0.03091618\n",
      "Iteration 44994, loss = 0.03090994\n",
      "Iteration 44995, loss = 0.03091635\n",
      "Iteration 44996, loss = 0.03092483\n",
      "Iteration 44997, loss = 0.03090933\n",
      "Iteration 44998, loss = 0.03091153\n",
      "Iteration 44999, loss = 0.03092289\n",
      "Iteration 45000, loss = 0.03091731\n",
      "Iteration 45001, loss = 0.03090716\n",
      "Iteration 45002, loss = 0.03090695\n",
      "Iteration 45003, loss = 0.03091756\n",
      "Iteration 45004, loss = 0.03092093\n",
      "Iteration 45005, loss = 0.03090848\n",
      "Iteration 45006, loss = 0.03092043\n",
      "Iteration 45007, loss = 0.03092567\n",
      "Iteration 45008, loss = 0.03091908\n",
      "Iteration 45009, loss = 0.03092001\n",
      "Iteration 45010, loss = 0.03092044\n",
      "Iteration 45011, loss = 0.03091059\n",
      "Iteration 45012, loss = 0.03090590\n",
      "Iteration 45013, loss = 0.03091352\n",
      "Iteration 45014, loss = 0.03092813\n",
      "Iteration 45015, loss = 0.03092204\n",
      "Iteration 45016, loss = 0.03090274\n",
      "Iteration 45017, loss = 0.03091355\n",
      "Iteration 45018, loss = 0.03090834\n",
      "Iteration 45019, loss = 0.03091245\n",
      "Iteration 45020, loss = 0.03091391\n",
      "Iteration 45021, loss = 0.03091618\n",
      "Iteration 45022, loss = 0.03091060\n",
      "Iteration 45023, loss = 0.03089914\n",
      "Iteration 45024, loss = 0.03091349\n",
      "Iteration 45025, loss = 0.03090830\n",
      "Iteration 45026, loss = 0.03091628\n",
      "Iteration 45027, loss = 0.03092052\n",
      "Iteration 45028, loss = 0.03091844\n",
      "Iteration 45029, loss = 0.03090170\n",
      "Iteration 45030, loss = 0.03089049\n",
      "Iteration 45031, loss = 0.03090409\n",
      "Iteration 45032, loss = 0.03090191\n",
      "Iteration 45033, loss = 0.03090860\n",
      "Iteration 45034, loss = 0.03090530\n",
      "Iteration 45035, loss = 0.03091221\n",
      "Iteration 45036, loss = 0.03090663\n",
      "Iteration 45037, loss = 0.03089190\n",
      "Iteration 45038, loss = 0.03090300\n",
      "Iteration 45039, loss = 0.03090932\n",
      "Iteration 45040, loss = 0.03090761\n",
      "Iteration 45041, loss = 0.03089693\n",
      "Iteration 45042, loss = 0.03089302\n",
      "Iteration 45043, loss = 0.03090455\n",
      "Iteration 45044, loss = 0.03090051\n",
      "Iteration 45045, loss = 0.03089655\n",
      "Iteration 45046, loss = 0.03089382\n",
      "Iteration 45047, loss = 0.03090435\n",
      "Iteration 45048, loss = 0.03091419\n",
      "Iteration 45049, loss = 0.03090780\n",
      "Iteration 45050, loss = 0.03088803\n",
      "Iteration 45051, loss = 0.03089831\n",
      "Iteration 45052, loss = 0.03090818\n",
      "Iteration 45053, loss = 0.03090936\n",
      "Iteration 45054, loss = 0.03090802\n",
      "Iteration 45055, loss = 0.03089658\n",
      "Iteration 45056, loss = 0.03088253\n",
      "Iteration 45057, loss = 0.03090715\n",
      "Iteration 45058, loss = 0.03090939\n",
      "Iteration 45059, loss = 0.03089525\n",
      "Iteration 45060, loss = 0.03089091\n",
      "Iteration 45061, loss = 0.03088003\n",
      "Iteration 45062, loss = 0.03088571\n",
      "Iteration 45063, loss = 0.03088773\n",
      "Iteration 45064, loss = 0.03088560\n",
      "Iteration 45065, loss = 0.03087942\n",
      "Iteration 45066, loss = 0.03088297\n",
      "Iteration 45067, loss = 0.03088364\n",
      "Iteration 45068, loss = 0.03088228\n",
      "Iteration 45069, loss = 0.03087726\n",
      "Iteration 45070, loss = 0.03088781\n",
      "Iteration 45071, loss = 0.03089281\n",
      "Iteration 45072, loss = 0.03088496\n",
      "Iteration 45073, loss = 0.03088734\n",
      "Iteration 45074, loss = 0.03089267\n",
      "Iteration 45075, loss = 0.03089079\n",
      "Iteration 45076, loss = 0.03089494\n",
      "Iteration 45077, loss = 0.03089108\n",
      "Iteration 45078, loss = 0.03088108\n",
      "Iteration 45079, loss = 0.03088751\n",
      "Iteration 45080, loss = 0.03089311\n",
      "Iteration 45081, loss = 0.03089118\n",
      "Iteration 45082, loss = 0.03088163\n",
      "Iteration 45083, loss = 0.03088448\n",
      "Iteration 45084, loss = 0.03088194\n",
      "Iteration 45085, loss = 0.03087804\n",
      "Iteration 45086, loss = 0.03087898\n",
      "Iteration 45087, loss = 0.03087704\n",
      "Iteration 45088, loss = 0.03088517\n",
      "Iteration 45089, loss = 0.03088810\n",
      "Iteration 45090, loss = 0.03088126\n",
      "Iteration 45091, loss = 0.03088945\n",
      "Iteration 45092, loss = 0.03088620\n",
      "Iteration 45093, loss = 0.03087526\n",
      "Iteration 45094, loss = 0.03086897\n",
      "Iteration 45095, loss = 0.03088186\n",
      "Iteration 45096, loss = 0.03087406\n",
      "Iteration 45097, loss = 0.03087519\n",
      "Iteration 45098, loss = 0.03087183\n",
      "Iteration 45099, loss = 0.03087792\n",
      "Iteration 45100, loss = 0.03087202\n",
      "Iteration 45101, loss = 0.03087740\n",
      "Iteration 45102, loss = 0.03088047\n",
      "Iteration 45103, loss = 0.03087373\n",
      "Iteration 45104, loss = 0.03087005\n",
      "Iteration 45105, loss = 0.03087169\n",
      "Iteration 45106, loss = 0.03087609\n",
      "Iteration 45107, loss = 0.03086871\n",
      "Iteration 45108, loss = 0.03086152\n",
      "Iteration 45109, loss = 0.03087322\n",
      "Iteration 45110, loss = 0.03087453\n",
      "Iteration 45111, loss = 0.03087798\n",
      "Iteration 45112, loss = 0.03087535\n",
      "Iteration 45113, loss = 0.03087477\n",
      "Iteration 45114, loss = 0.03087099\n",
      "Iteration 45115, loss = 0.03085977\n",
      "Iteration 45116, loss = 0.03088432\n",
      "Iteration 45117, loss = 0.03089408\n",
      "Iteration 45118, loss = 0.03088275\n",
      "Iteration 45119, loss = 0.03086246\n",
      "Iteration 45120, loss = 0.03088326\n",
      "Iteration 45121, loss = 0.03088898\n",
      "Iteration 45122, loss = 0.03089653\n",
      "Iteration 45123, loss = 0.03088678\n",
      "Iteration 45124, loss = 0.03089629\n",
      "Iteration 45125, loss = 0.03089692\n",
      "Iteration 45126, loss = 0.03089053\n",
      "Iteration 45127, loss = 0.03087625\n",
      "Iteration 45128, loss = 0.03086762\n",
      "Iteration 45129, loss = 0.03086347\n",
      "Iteration 45130, loss = 0.03086644\n",
      "Iteration 45131, loss = 0.03087544\n",
      "Iteration 45132, loss = 0.03087627\n",
      "Iteration 45133, loss = 0.03086686\n",
      "Iteration 45134, loss = 0.03086326\n",
      "Iteration 45135, loss = 0.03086400\n",
      "Iteration 45136, loss = 0.03086392\n",
      "Iteration 45137, loss = 0.03086844\n",
      "Iteration 45138, loss = 0.03085616\n",
      "Iteration 45139, loss = 0.03086096\n",
      "Iteration 45140, loss = 0.03086410\n",
      "Iteration 45141, loss = 0.03086143\n",
      "Iteration 45142, loss = 0.03085048\n",
      "Iteration 45143, loss = 0.03086380\n",
      "Iteration 45144, loss = 0.03086969\n",
      "Iteration 45145, loss = 0.03086955\n",
      "Iteration 45146, loss = 0.03086772\n",
      "Iteration 45147, loss = 0.03085978\n",
      "Iteration 45148, loss = 0.03085085\n",
      "Iteration 45149, loss = 0.03086258\n",
      "Iteration 45150, loss = 0.03087503\n",
      "Iteration 45151, loss = 0.03087483\n",
      "Iteration 45152, loss = 0.03086590\n",
      "Iteration 45153, loss = 0.03085614\n",
      "Iteration 45154, loss = 0.03085401\n",
      "Iteration 45155, loss = 0.03086510\n",
      "Iteration 45156, loss = 0.03087098\n",
      "Iteration 45157, loss = 0.03085867\n",
      "Iteration 45158, loss = 0.03085658\n",
      "Iteration 45159, loss = 0.03086478\n",
      "Iteration 45160, loss = 0.03086155\n",
      "Iteration 45161, loss = 0.03085257\n",
      "Iteration 45162, loss = 0.03085424\n",
      "Iteration 45163, loss = 0.03086071\n",
      "Iteration 45164, loss = 0.03086891\n",
      "Iteration 45165, loss = 0.03086631\n",
      "Iteration 45166, loss = 0.03085536\n",
      "Iteration 45167, loss = 0.03084497\n",
      "Iteration 45168, loss = 0.03085990\n",
      "Iteration 45169, loss = 0.03086853\n",
      "Iteration 45170, loss = 0.03086629\n",
      "Iteration 45171, loss = 0.03085528\n",
      "Iteration 45172, loss = 0.03084306\n",
      "Iteration 45173, loss = 0.03084743\n",
      "Iteration 45174, loss = 0.03084511\n",
      "Iteration 45175, loss = 0.03084214\n",
      "Iteration 45176, loss = 0.03084492\n",
      "Iteration 45177, loss = 0.03085121\n",
      "Iteration 45178, loss = 0.03084615\n",
      "Iteration 45179, loss = 0.03084368\n",
      "Iteration 45180, loss = 0.03084255\n",
      "Iteration 45181, loss = 0.03084378\n",
      "Iteration 45182, loss = 0.03084753\n",
      "Iteration 45183, loss = 0.03084620\n",
      "Iteration 45184, loss = 0.03084944\n",
      "Iteration 45185, loss = 0.03084774\n",
      "Iteration 45186, loss = 0.03084388\n",
      "Iteration 45187, loss = 0.03084775\n",
      "Iteration 45188, loss = 0.03084671\n",
      "Iteration 45189, loss = 0.03084420\n",
      "Iteration 45190, loss = 0.03084733\n",
      "Iteration 45191, loss = 0.03084929\n",
      "Iteration 45192, loss = 0.03083663\n",
      "Iteration 45193, loss = 0.03083668\n",
      "Iteration 45194, loss = 0.03084749\n",
      "Iteration 45195, loss = 0.03085116\n",
      "Iteration 45196, loss = 0.03084504\n",
      "Iteration 45197, loss = 0.03084685\n",
      "Iteration 45198, loss = 0.03083826\n",
      "Iteration 45199, loss = 0.03083861\n",
      "Iteration 45200, loss = 0.03084235\n",
      "Iteration 45201, loss = 0.03084580\n",
      "Iteration 45202, loss = 0.03084361\n",
      "Iteration 45203, loss = 0.03083867\n",
      "Iteration 45204, loss = 0.03083917\n",
      "Iteration 45205, loss = 0.03083549\n",
      "Iteration 45206, loss = 0.03082898\n",
      "Iteration 45207, loss = 0.03083361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 45208, loss = 0.03084272\n",
      "Iteration 45209, loss = 0.03084166\n",
      "Iteration 45210, loss = 0.03083900\n",
      "Iteration 45211, loss = 0.03084278\n",
      "Iteration 45212, loss = 0.03083894\n",
      "Iteration 45213, loss = 0.03083128\n",
      "Iteration 45214, loss = 0.03084390\n",
      "Iteration 45215, loss = 0.03084952\n",
      "Iteration 45216, loss = 0.03083609\n",
      "Iteration 45217, loss = 0.03082710\n",
      "Iteration 45218, loss = 0.03084430\n",
      "Iteration 45219, loss = 0.03084912\n",
      "Iteration 45220, loss = 0.03083496\n",
      "Iteration 45221, loss = 0.03082840\n",
      "Iteration 45222, loss = 0.03083229\n",
      "Iteration 45223, loss = 0.03083779\n",
      "Iteration 45224, loss = 0.03083412\n",
      "Iteration 45225, loss = 0.03082966\n",
      "Iteration 45226, loss = 0.03082650\n",
      "Iteration 45227, loss = 0.03082290\n",
      "Iteration 45228, loss = 0.03082660\n",
      "Iteration 45229, loss = 0.03081965\n",
      "Iteration 45230, loss = 0.03082194\n",
      "Iteration 45231, loss = 0.03082321\n",
      "Iteration 45232, loss = 0.03082929\n",
      "Iteration 45233, loss = 0.03082260\n",
      "Iteration 45234, loss = 0.03082004\n",
      "Iteration 45235, loss = 0.03082766\n",
      "Iteration 45236, loss = 0.03083214\n",
      "Iteration 45237, loss = 0.03082279\n",
      "Iteration 45238, loss = 0.03082438\n",
      "Iteration 45239, loss = 0.03083182\n",
      "Iteration 45240, loss = 0.03083099\n",
      "Iteration 45241, loss = 0.03082409\n",
      "Iteration 45242, loss = 0.03082114\n",
      "Iteration 45243, loss = 0.03082486\n",
      "Iteration 45244, loss = 0.03082947\n",
      "Iteration 45245, loss = 0.03082746\n",
      "Iteration 45246, loss = 0.03082484\n",
      "Iteration 45247, loss = 0.03082907\n",
      "Iteration 45248, loss = 0.03083819\n",
      "Iteration 45249, loss = 0.03083650\n",
      "Iteration 45250, loss = 0.03082825\n",
      "Iteration 45251, loss = 0.03081800\n",
      "Iteration 45252, loss = 0.03081590\n",
      "Iteration 45253, loss = 0.03082458\n",
      "Iteration 45254, loss = 0.03082296\n",
      "Iteration 45255, loss = 0.03082243\n",
      "Iteration 45256, loss = 0.03082351\n",
      "Iteration 45257, loss = 0.03083076\n",
      "Iteration 45258, loss = 0.03082028\n",
      "Iteration 45259, loss = 0.03082098\n",
      "Iteration 45260, loss = 0.03082602\n",
      "Iteration 45261, loss = 0.03081525\n",
      "Iteration 45262, loss = 0.03081237\n",
      "Iteration 45263, loss = 0.03081728\n",
      "Iteration 45264, loss = 0.03082171\n",
      "Iteration 45265, loss = 0.03081618\n",
      "Iteration 45266, loss = 0.03081201\n",
      "Iteration 45267, loss = 0.03081465\n",
      "Iteration 45268, loss = 0.03081317\n",
      "Iteration 45269, loss = 0.03080834\n",
      "Iteration 45270, loss = 0.03081525\n",
      "Iteration 45271, loss = 0.03082080\n",
      "Iteration 45272, loss = 0.03081409\n",
      "Iteration 45273, loss = 0.03081374\n",
      "Iteration 45274, loss = 0.03081319\n",
      "Iteration 45275, loss = 0.03080931\n",
      "Iteration 45276, loss = 0.03081437\n",
      "Iteration 45277, loss = 0.03081220\n",
      "Iteration 45278, loss = 0.03081436\n",
      "Iteration 45279, loss = 0.03081417\n",
      "Iteration 45280, loss = 0.03081630\n",
      "Iteration 45281, loss = 0.03081635\n",
      "Iteration 45282, loss = 0.03080354\n",
      "Iteration 45283, loss = 0.03080825\n",
      "Iteration 45284, loss = 0.03081875\n",
      "Iteration 45285, loss = 0.03081048\n",
      "Iteration 45286, loss = 0.03080234\n",
      "Iteration 45287, loss = 0.03080880\n",
      "Iteration 45288, loss = 0.03081163\n",
      "Iteration 45289, loss = 0.03080031\n",
      "Iteration 45290, loss = 0.03079971\n",
      "Iteration 45291, loss = 0.03080728\n",
      "Iteration 45292, loss = 0.03080434\n",
      "Iteration 45293, loss = 0.03080334\n",
      "Iteration 45294, loss = 0.03080323\n",
      "Iteration 45295, loss = 0.03080530\n",
      "Iteration 45296, loss = 0.03080826\n",
      "Iteration 45297, loss = 0.03080355\n",
      "Iteration 45298, loss = 0.03080034\n",
      "Iteration 45299, loss = 0.03080455\n",
      "Iteration 45300, loss = 0.03081542\n",
      "Iteration 45301, loss = 0.03081341\n",
      "Iteration 45302, loss = 0.03080906\n",
      "Iteration 45303, loss = 0.03080523\n",
      "Iteration 45304, loss = 0.03081653\n",
      "Iteration 45305, loss = 0.03082676\n",
      "Iteration 45306, loss = 0.03082568\n",
      "Iteration 45307, loss = 0.03081260\n",
      "Iteration 45308, loss = 0.03080046\n",
      "Iteration 45309, loss = 0.03080668\n",
      "Iteration 45310, loss = 0.03081190\n",
      "Iteration 45311, loss = 0.03080059\n",
      "Iteration 45312, loss = 0.03078975\n",
      "Iteration 45313, loss = 0.03079355\n",
      "Iteration 45314, loss = 0.03080845\n",
      "Iteration 45315, loss = 0.03080650\n",
      "Iteration 45316, loss = 0.03080262\n",
      "Iteration 45317, loss = 0.03079984\n",
      "Iteration 45318, loss = 0.03078993\n",
      "Iteration 45319, loss = 0.03079429\n",
      "Iteration 45320, loss = 0.03080059\n",
      "Iteration 45321, loss = 0.03080429\n",
      "Iteration 45322, loss = 0.03079790\n",
      "Iteration 45323, loss = 0.03079418\n",
      "Iteration 45324, loss = 0.03079413\n",
      "Iteration 45325, loss = 0.03080118\n",
      "Iteration 45326, loss = 0.03079615\n",
      "Iteration 45327, loss = 0.03079715\n",
      "Iteration 45328, loss = 0.03079977\n",
      "Iteration 45329, loss = 0.03080772\n",
      "Iteration 45330, loss = 0.03080379\n",
      "Iteration 45331, loss = 0.03079384\n",
      "Iteration 45332, loss = 0.03080152\n",
      "Iteration 45333, loss = 0.03079871\n",
      "Iteration 45334, loss = 0.03080172\n",
      "Iteration 45335, loss = 0.03080446\n",
      "Iteration 45336, loss = 0.03080645\n",
      "Iteration 45337, loss = 0.03079079\n",
      "Iteration 45338, loss = 0.03079445\n",
      "Iteration 45339, loss = 0.03079036\n",
      "Iteration 45340, loss = 0.03079248\n",
      "Iteration 45341, loss = 0.03078725\n",
      "Iteration 45342, loss = 0.03079808\n",
      "Iteration 45343, loss = 0.03080442\n",
      "Iteration 45344, loss = 0.03079153\n",
      "Iteration 45345, loss = 0.03080008\n",
      "Iteration 45346, loss = 0.03080364\n",
      "Iteration 45347, loss = 0.03080155\n",
      "Iteration 45348, loss = 0.03079832\n",
      "Iteration 45349, loss = 0.03080709\n",
      "Iteration 45350, loss = 0.03080250\n",
      "Iteration 45351, loss = 0.03079526\n",
      "Iteration 45352, loss = 0.03079817\n",
      "Iteration 45353, loss = 0.03079545\n",
      "Iteration 45354, loss = 0.03079205\n",
      "Iteration 45355, loss = 0.03077469\n",
      "Iteration 45356, loss = 0.03078215\n",
      "Iteration 45357, loss = 0.03078881\n",
      "Iteration 45358, loss = 0.03077962\n",
      "Iteration 45359, loss = 0.03078105\n",
      "Iteration 45360, loss = 0.03079192\n",
      "Iteration 45361, loss = 0.03078840\n",
      "Iteration 45362, loss = 0.03077775\n",
      "Iteration 45363, loss = 0.03077907\n",
      "Iteration 45364, loss = 0.03077557\n",
      "Iteration 45365, loss = 0.03077289\n",
      "Iteration 45366, loss = 0.03078219\n",
      "Iteration 45367, loss = 0.03078660\n",
      "Iteration 45368, loss = 0.03077540\n",
      "Iteration 45369, loss = 0.03076883\n",
      "Iteration 45370, loss = 0.03077733\n",
      "Iteration 45371, loss = 0.03078589\n",
      "Iteration 45372, loss = 0.03078294\n",
      "Iteration 45373, loss = 0.03077736\n",
      "Iteration 45374, loss = 0.03077753\n",
      "Iteration 45375, loss = 0.03078475\n",
      "Iteration 45376, loss = 0.03078199\n",
      "Iteration 45377, loss = 0.03078750\n",
      "Iteration 45378, loss = 0.03077941\n",
      "Iteration 45379, loss = 0.03077972\n",
      "Iteration 45380, loss = 0.03078857\n",
      "Iteration 45381, loss = 0.03078541\n",
      "Iteration 45382, loss = 0.03078036\n",
      "Iteration 45383, loss = 0.03078063\n",
      "Iteration 45384, loss = 0.03077496\n",
      "Iteration 45385, loss = 0.03077501\n",
      "Iteration 45386, loss = 0.03076405\n",
      "Iteration 45387, loss = 0.03077165\n",
      "Iteration 45388, loss = 0.03077546\n",
      "Iteration 45389, loss = 0.03076897\n",
      "Iteration 45390, loss = 0.03077125\n",
      "Iteration 45391, loss = 0.03077773\n",
      "Iteration 45392, loss = 0.03077669\n",
      "Iteration 45393, loss = 0.03077012\n",
      "Iteration 45394, loss = 0.03076821\n",
      "Iteration 45395, loss = 0.03077296\n",
      "Iteration 45396, loss = 0.03077191\n",
      "Iteration 45397, loss = 0.03076351\n",
      "Iteration 45398, loss = 0.03076971\n",
      "Iteration 45399, loss = 0.03077551\n",
      "Iteration 45400, loss = 0.03077538\n",
      "Iteration 45401, loss = 0.03077318\n",
      "Iteration 45402, loss = 0.03077084\n",
      "Iteration 45403, loss = 0.03076878\n",
      "Iteration 45404, loss = 0.03076227\n",
      "Iteration 45405, loss = 0.03076886\n",
      "Iteration 45406, loss = 0.03077171\n",
      "Iteration 45407, loss = 0.03076781\n",
      "Iteration 45408, loss = 0.03077598\n",
      "Iteration 45409, loss = 0.03077631\n",
      "Iteration 45410, loss = 0.03076751\n",
      "Iteration 45411, loss = 0.03076791\n",
      "Iteration 45412, loss = 0.03076258\n",
      "Iteration 45413, loss = 0.03075969\n",
      "Iteration 45414, loss = 0.03075437\n",
      "Iteration 45415, loss = 0.03076312\n",
      "Iteration 45416, loss = 0.03076730\n",
      "Iteration 45417, loss = 0.03076606\n",
      "Iteration 45418, loss = 0.03076769\n",
      "Iteration 45419, loss = 0.03075944\n",
      "Iteration 45420, loss = 0.03075980\n",
      "Iteration 45421, loss = 0.03075519\n",
      "Iteration 45422, loss = 0.03076109\n",
      "Iteration 45423, loss = 0.03076541\n",
      "Iteration 45424, loss = 0.03075789\n",
      "Iteration 45425, loss = 0.03076455\n",
      "Iteration 45426, loss = 0.03076535\n",
      "Iteration 45427, loss = 0.03076727\n",
      "Iteration 45428, loss = 0.03076421\n",
      "Iteration 45429, loss = 0.03077002\n",
      "Iteration 45430, loss = 0.03078161\n",
      "Iteration 45431, loss = 0.03078218\n",
      "Iteration 45432, loss = 0.03076912\n",
      "Iteration 45433, loss = 0.03077207\n",
      "Iteration 45434, loss = 0.03078515\n",
      "Iteration 45435, loss = 0.03078662\n",
      "Iteration 45436, loss = 0.03076677\n",
      "Iteration 45437, loss = 0.03077711\n",
      "Iteration 45438, loss = 0.03078011\n",
      "Iteration 45439, loss = 0.03078095\n",
      "Iteration 45440, loss = 0.03076866\n",
      "Iteration 45441, loss = 0.03076992\n",
      "Iteration 45442, loss = 0.03077156\n",
      "Iteration 45443, loss = 0.03077844\n",
      "Iteration 45444, loss = 0.03077909\n",
      "Iteration 45445, loss = 0.03077736\n",
      "Iteration 45446, loss = 0.03077428\n",
      "Iteration 45447, loss = 0.03075881\n",
      "Iteration 45448, loss = 0.03075945\n",
      "Iteration 45449, loss = 0.03075886\n",
      "Iteration 45450, loss = 0.03076991\n",
      "Iteration 45451, loss = 0.03075288\n",
      "Iteration 45452, loss = 0.03075970\n",
      "Iteration 45453, loss = 0.03076491\n",
      "Iteration 45454, loss = 0.03077125\n",
      "Iteration 45455, loss = 0.03076776\n",
      "Iteration 45456, loss = 0.03075471\n",
      "Iteration 45457, loss = 0.03074358\n",
      "Iteration 45458, loss = 0.03075926\n",
      "Iteration 45459, loss = 0.03076177\n",
      "Iteration 45460, loss = 0.03075718\n",
      "Iteration 45461, loss = 0.03075149\n",
      "Iteration 45462, loss = 0.03074779\n",
      "Iteration 45463, loss = 0.03075664\n",
      "Iteration 45464, loss = 0.03075923\n",
      "Iteration 45465, loss = 0.03075609\n",
      "Iteration 45466, loss = 0.03073759\n",
      "Iteration 45467, loss = 0.03074467\n",
      "Iteration 45468, loss = 0.03076792\n",
      "Iteration 45469, loss = 0.03076578\n",
      "Iteration 45470, loss = 0.03074243\n",
      "Iteration 45471, loss = 0.03075081\n",
      "Iteration 45472, loss = 0.03076595\n",
      "Iteration 45473, loss = 0.03076952\n",
      "Iteration 45474, loss = 0.03076316\n",
      "Iteration 45475, loss = 0.03075436\n",
      "Iteration 45476, loss = 0.03073591\n",
      "Iteration 45477, loss = 0.03074709\n",
      "Iteration 45478, loss = 0.03076757\n",
      "Iteration 45479, loss = 0.03076400\n",
      "Iteration 45480, loss = 0.03074810\n",
      "Iteration 45481, loss = 0.03073864\n",
      "Iteration 45482, loss = 0.03074492\n",
      "Iteration 45483, loss = 0.03074011\n",
      "Iteration 45484, loss = 0.03074190\n",
      "Iteration 45485, loss = 0.03073987\n",
      "Iteration 45486, loss = 0.03073897\n",
      "Iteration 45487, loss = 0.03073475\n",
      "Iteration 45488, loss = 0.03073700\n",
      "Iteration 45489, loss = 0.03073760\n",
      "Iteration 45490, loss = 0.03074165\n",
      "Iteration 45491, loss = 0.03073696\n",
      "Iteration 45492, loss = 0.03072816\n",
      "Iteration 45493, loss = 0.03073789\n",
      "Iteration 45494, loss = 0.03074162\n",
      "Iteration 45495, loss = 0.03073161\n",
      "Iteration 45496, loss = 0.03072873\n",
      "Iteration 45497, loss = 0.03073561\n",
      "Iteration 45498, loss = 0.03073848\n",
      "Iteration 45499, loss = 0.03073410\n",
      "Iteration 45500, loss = 0.03072949\n",
      "Iteration 45501, loss = 0.03073136\n",
      "Iteration 45502, loss = 0.03073093\n",
      "Iteration 45503, loss = 0.03072318\n",
      "Iteration 45504, loss = 0.03073018\n",
      "Iteration 45505, loss = 0.03072517\n",
      "Iteration 45506, loss = 0.03072312\n",
      "Iteration 45507, loss = 0.03073151\n",
      "Iteration 45508, loss = 0.03074082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 45509, loss = 0.03073875\n",
      "Iteration 45510, loss = 0.03073361\n",
      "Iteration 45511, loss = 0.03072428\n",
      "Iteration 45512, loss = 0.03073485\n",
      "Iteration 45513, loss = 0.03074354\n",
      "Iteration 45514, loss = 0.03074236\n",
      "Iteration 45515, loss = 0.03072780\n",
      "Iteration 45516, loss = 0.03073094\n",
      "Iteration 45517, loss = 0.03074636\n",
      "Iteration 45518, loss = 0.03075064\n",
      "Iteration 45519, loss = 0.03073908\n",
      "Iteration 45520, loss = 0.03072785\n",
      "Iteration 45521, loss = 0.03073063\n",
      "Iteration 45522, loss = 0.03073990\n",
      "Iteration 45523, loss = 0.03072523\n",
      "Iteration 45524, loss = 0.03073204\n",
      "Iteration 45525, loss = 0.03073673\n",
      "Iteration 45526, loss = 0.03074486\n",
      "Iteration 45527, loss = 0.03074053\n",
      "Iteration 45528, loss = 0.03074002\n",
      "Iteration 45529, loss = 0.03072943\n",
      "Iteration 45530, loss = 0.03072024\n",
      "Iteration 45531, loss = 0.03073261\n",
      "Iteration 45532, loss = 0.03074072\n",
      "Iteration 45533, loss = 0.03074125\n",
      "Iteration 45534, loss = 0.03073095\n",
      "Iteration 45535, loss = 0.03073189\n",
      "Iteration 45536, loss = 0.03072057\n",
      "Iteration 45537, loss = 0.03073190\n",
      "Iteration 45538, loss = 0.03074363\n",
      "Iteration 45539, loss = 0.03074287\n",
      "Iteration 45540, loss = 0.03072321\n",
      "Iteration 45541, loss = 0.03072230\n",
      "Iteration 45542, loss = 0.03073831\n",
      "Iteration 45543, loss = 0.03074742\n",
      "Iteration 45544, loss = 0.03073293\n",
      "Iteration 45545, loss = 0.03073159\n",
      "Iteration 45546, loss = 0.03072758\n",
      "Iteration 45547, loss = 0.03072934\n",
      "Iteration 45548, loss = 0.03071725\n",
      "Iteration 45549, loss = 0.03072602\n",
      "Iteration 45550, loss = 0.03073428\n",
      "Iteration 45551, loss = 0.03073171\n",
      "Iteration 45552, loss = 0.03073056\n",
      "Iteration 45553, loss = 0.03071816\n",
      "Iteration 45554, loss = 0.03072561\n",
      "Iteration 45555, loss = 0.03072449\n",
      "Iteration 45556, loss = 0.03070531\n",
      "Iteration 45557, loss = 0.03070806\n",
      "Iteration 45558, loss = 0.03070494\n",
      "Iteration 45559, loss = 0.03071546\n",
      "Iteration 45560, loss = 0.03071609\n",
      "Iteration 45561, loss = 0.03070827\n",
      "Iteration 45562, loss = 0.03070806\n",
      "Iteration 45563, loss = 0.03071548\n",
      "Iteration 45564, loss = 0.03071907\n",
      "Iteration 45565, loss = 0.03071101\n",
      "Iteration 45566, loss = 0.03073196\n",
      "Iteration 45567, loss = 0.03073831\n",
      "Iteration 45568, loss = 0.03071785\n",
      "Iteration 45569, loss = 0.03070709\n",
      "Iteration 45570, loss = 0.03073379\n",
      "Iteration 45571, loss = 0.03074980\n",
      "Iteration 45572, loss = 0.03074828\n",
      "Iteration 45573, loss = 0.03073898\n",
      "Iteration 45574, loss = 0.03072568\n",
      "Iteration 45575, loss = 0.03071075\n",
      "Iteration 45576, loss = 0.03072310\n",
      "Iteration 45577, loss = 0.03073284\n",
      "Iteration 45578, loss = 0.03072149\n",
      "Iteration 45579, loss = 0.03070412\n",
      "Iteration 45580, loss = 0.03071595\n",
      "Iteration 45581, loss = 0.03072773\n",
      "Iteration 45582, loss = 0.03072479\n",
      "Iteration 45583, loss = 0.03070711\n",
      "Iteration 45584, loss = 0.03069919\n",
      "Iteration 45585, loss = 0.03071122\n",
      "Iteration 45586, loss = 0.03072609\n",
      "Iteration 45587, loss = 0.03072607\n",
      "Iteration 45588, loss = 0.03070850\n",
      "Iteration 45589, loss = 0.03071183\n",
      "Iteration 45590, loss = 0.03071732\n",
      "Iteration 45591, loss = 0.03072253\n",
      "Iteration 45592, loss = 0.03072258\n",
      "Iteration 45593, loss = 0.03071310\n",
      "Iteration 45594, loss = 0.03069629\n",
      "Iteration 45595, loss = 0.03070863\n",
      "Iteration 45596, loss = 0.03072716\n",
      "Iteration 45597, loss = 0.03072855\n",
      "Iteration 45598, loss = 0.03071529\n",
      "Iteration 45599, loss = 0.03070523\n",
      "Iteration 45600, loss = 0.03070797\n",
      "Iteration 45601, loss = 0.03070027\n",
      "Iteration 45602, loss = 0.03069988\n",
      "Iteration 45603, loss = 0.03069498\n",
      "Iteration 45604, loss = 0.03069193\n",
      "Iteration 45605, loss = 0.03069092\n",
      "Iteration 45606, loss = 0.03069269\n",
      "Iteration 45607, loss = 0.03068764\n",
      "Iteration 45608, loss = 0.03070034\n",
      "Iteration 45609, loss = 0.03069638\n",
      "Iteration 45610, loss = 0.03069627\n",
      "Iteration 45611, loss = 0.03070158\n",
      "Iteration 45612, loss = 0.03070632\n",
      "Iteration 45613, loss = 0.03070341\n",
      "Iteration 45614, loss = 0.03069400\n",
      "Iteration 45615, loss = 0.03068409\n",
      "Iteration 45616, loss = 0.03068994\n",
      "Iteration 45617, loss = 0.03068938\n",
      "Iteration 45618, loss = 0.03068870\n",
      "Iteration 45619, loss = 0.03068537\n",
      "Iteration 45620, loss = 0.03068755\n",
      "Iteration 45621, loss = 0.03069111\n",
      "Iteration 45622, loss = 0.03069739\n",
      "Iteration 45623, loss = 0.03070341\n",
      "Iteration 45624, loss = 0.03069267\n",
      "Iteration 45625, loss = 0.03069918\n",
      "Iteration 45626, loss = 0.03070336\n",
      "Iteration 45627, loss = 0.03069926\n",
      "Iteration 45628, loss = 0.03069060\n",
      "Iteration 45629, loss = 0.03068586\n",
      "Iteration 45630, loss = 0.03068212\n",
      "Iteration 45631, loss = 0.03068045\n",
      "Iteration 45632, loss = 0.03068768\n",
      "Iteration 45633, loss = 0.03069043\n",
      "Iteration 45634, loss = 0.03069201\n",
      "Iteration 45635, loss = 0.03069369\n",
      "Iteration 45636, loss = 0.03069022\n",
      "Iteration 45637, loss = 0.03068283\n",
      "Iteration 45638, loss = 0.03067870\n",
      "Iteration 45639, loss = 0.03068215\n",
      "Iteration 45640, loss = 0.03067579\n",
      "Iteration 45641, loss = 0.03067773\n",
      "Iteration 45642, loss = 0.03067766\n",
      "Iteration 45643, loss = 0.03068473\n",
      "Iteration 45644, loss = 0.03068607\n",
      "Iteration 45645, loss = 0.03067597\n",
      "Iteration 45646, loss = 0.03068535\n",
      "Iteration 45647, loss = 0.03068586\n",
      "Iteration 45648, loss = 0.03068548\n",
      "Iteration 45649, loss = 0.03068815\n",
      "Iteration 45650, loss = 0.03067987\n",
      "Iteration 45651, loss = 0.03069226\n",
      "Iteration 45652, loss = 0.03070426\n",
      "Iteration 45653, loss = 0.03069656\n",
      "Iteration 45654, loss = 0.03069210\n",
      "Iteration 45655, loss = 0.03069035\n",
      "Iteration 45656, loss = 0.03069626\n",
      "Iteration 45657, loss = 0.03069696\n",
      "Iteration 45658, loss = 0.03069488\n",
      "Iteration 45659, loss = 0.03068206\n",
      "Iteration 45660, loss = 0.03067828\n",
      "Iteration 45661, loss = 0.03068711\n",
      "Iteration 45662, loss = 0.03068736\n",
      "Iteration 45663, loss = 0.03068581\n",
      "Iteration 45664, loss = 0.03067803\n",
      "Iteration 45665, loss = 0.03067170\n",
      "Iteration 45666, loss = 0.03068312\n",
      "Iteration 45667, loss = 0.03069612\n",
      "Iteration 45668, loss = 0.03069884\n",
      "Iteration 45669, loss = 0.03068689\n",
      "Iteration 45670, loss = 0.03067347\n",
      "Iteration 45671, loss = 0.03068108\n",
      "Iteration 45672, loss = 0.03069089\n",
      "Iteration 45673, loss = 0.03069673\n",
      "Iteration 45674, loss = 0.03069023\n",
      "Iteration 45675, loss = 0.03068362\n",
      "Iteration 45676, loss = 0.03067416\n",
      "Iteration 45677, loss = 0.03066978\n",
      "Iteration 45678, loss = 0.03067253\n",
      "Iteration 45679, loss = 0.03068377\n",
      "Iteration 45680, loss = 0.03068952\n",
      "Iteration 45681, loss = 0.03068229\n",
      "Iteration 45682, loss = 0.03066453\n",
      "Iteration 45683, loss = 0.03067886\n",
      "Iteration 45684, loss = 0.03068727\n",
      "Iteration 45685, loss = 0.03068740\n",
      "Iteration 45686, loss = 0.03068829\n",
      "Iteration 45687, loss = 0.03068167\n",
      "Iteration 45688, loss = 0.03067381\n",
      "Iteration 45689, loss = 0.03066318\n",
      "Iteration 45690, loss = 0.03067573\n",
      "Iteration 45691, loss = 0.03068505\n",
      "Iteration 45692, loss = 0.03069308\n",
      "Iteration 45693, loss = 0.03068257\n",
      "Iteration 45694, loss = 0.03066515\n",
      "Iteration 45695, loss = 0.03066908\n",
      "Iteration 45696, loss = 0.03068008\n",
      "Iteration 45697, loss = 0.03068341\n",
      "Iteration 45698, loss = 0.03067211\n",
      "Iteration 45699, loss = 0.03067474\n",
      "Iteration 45700, loss = 0.03066658\n",
      "Iteration 45701, loss = 0.03065819\n",
      "Iteration 45702, loss = 0.03066177\n",
      "Iteration 45703, loss = 0.03065727\n",
      "Iteration 45704, loss = 0.03065706\n",
      "Iteration 45705, loss = 0.03065927\n",
      "Iteration 45706, loss = 0.03066432\n",
      "Iteration 45707, loss = 0.03066640\n",
      "Iteration 45708, loss = 0.03066767\n",
      "Iteration 45709, loss = 0.03066423\n",
      "Iteration 45710, loss = 0.03066059\n",
      "Iteration 45711, loss = 0.03065222\n",
      "Iteration 45712, loss = 0.03067325\n",
      "Iteration 45713, loss = 0.03067803\n",
      "Iteration 45714, loss = 0.03066156\n",
      "Iteration 45715, loss = 0.03066271\n",
      "Iteration 45716, loss = 0.03067264\n",
      "Iteration 45717, loss = 0.03067165\n",
      "Iteration 45718, loss = 0.03066424\n",
      "Iteration 45719, loss = 0.03065923\n",
      "Iteration 45720, loss = 0.03066652\n",
      "Iteration 45721, loss = 0.03066643\n",
      "Iteration 45722, loss = 0.03066673\n",
      "Iteration 45723, loss = 0.03065938\n",
      "Iteration 45724, loss = 0.03066469\n",
      "Iteration 45725, loss = 0.03066584\n",
      "Iteration 45726, loss = 0.03067248\n",
      "Iteration 45727, loss = 0.03066240\n",
      "Iteration 45728, loss = 0.03066189\n",
      "Iteration 45729, loss = 0.03065997\n",
      "Iteration 45730, loss = 0.03065990\n",
      "Iteration 45731, loss = 0.03064254\n",
      "Iteration 45732, loss = 0.03066138\n",
      "Iteration 45733, loss = 0.03066494\n",
      "Iteration 45734, loss = 0.03065519\n",
      "Iteration 45735, loss = 0.03065370\n",
      "Iteration 45736, loss = 0.03066427\n",
      "Iteration 45737, loss = 0.03066680\n",
      "Iteration 45738, loss = 0.03065866\n",
      "Iteration 45739, loss = 0.03065205\n",
      "Iteration 45740, loss = 0.03065413\n",
      "Iteration 45741, loss = 0.03066227\n",
      "Iteration 45742, loss = 0.03066002\n",
      "Iteration 45743, loss = 0.03065291\n",
      "Iteration 45744, loss = 0.03065020\n",
      "Iteration 45745, loss = 0.03065303\n",
      "Iteration 45746, loss = 0.03066111\n",
      "Iteration 45747, loss = 0.03066775\n",
      "Iteration 45748, loss = 0.03066928\n",
      "Iteration 45749, loss = 0.03065934\n",
      "Iteration 45750, loss = 0.03064766\n",
      "Iteration 45751, loss = 0.03065561\n",
      "Iteration 45752, loss = 0.03065258\n",
      "Iteration 45753, loss = 0.03065845\n",
      "Iteration 45754, loss = 0.03065668\n",
      "Iteration 45755, loss = 0.03065704\n",
      "Iteration 45756, loss = 0.03066267\n",
      "Iteration 45757, loss = 0.03065928\n",
      "Iteration 45758, loss = 0.03065838\n",
      "Iteration 45759, loss = 0.03064664\n",
      "Iteration 45760, loss = 0.03065387\n",
      "Iteration 45761, loss = 0.03065865\n",
      "Iteration 45762, loss = 0.03063786\n",
      "Iteration 45763, loss = 0.03065110\n",
      "Iteration 45764, loss = 0.03066715\n",
      "Iteration 45765, loss = 0.03066489\n",
      "Iteration 45766, loss = 0.03064993\n",
      "Iteration 45767, loss = 0.03064244\n",
      "Iteration 45768, loss = 0.03064491\n",
      "Iteration 45769, loss = 0.03064502\n",
      "Iteration 45770, loss = 0.03064523\n",
      "Iteration 45771, loss = 0.03064303\n",
      "Iteration 45772, loss = 0.03063847\n",
      "Iteration 45773, loss = 0.03063144\n",
      "Iteration 45774, loss = 0.03064834\n",
      "Iteration 45775, loss = 0.03064148\n",
      "Iteration 45776, loss = 0.03064550\n",
      "Iteration 45777, loss = 0.03065143\n",
      "Iteration 45778, loss = 0.03064570\n",
      "Iteration 45779, loss = 0.03064682\n",
      "Iteration 45780, loss = 0.03064562\n",
      "Iteration 45781, loss = 0.03064730\n",
      "Iteration 45782, loss = 0.03064409\n",
      "Iteration 45783, loss = 0.03064582\n",
      "Iteration 45784, loss = 0.03065431\n",
      "Iteration 45785, loss = 0.03065265\n",
      "Iteration 45786, loss = 0.03065433\n",
      "Iteration 45787, loss = 0.03065077\n",
      "Iteration 45788, loss = 0.03063467\n",
      "Iteration 45789, loss = 0.03064934\n",
      "Iteration 45790, loss = 0.03065121\n",
      "Iteration 45791, loss = 0.03065865\n",
      "Iteration 45792, loss = 0.03066018\n",
      "Iteration 45793, loss = 0.03063949\n",
      "Iteration 45794, loss = 0.03064176\n",
      "Iteration 45795, loss = 0.03065992\n",
      "Iteration 45796, loss = 0.03066821\n",
      "Iteration 45797, loss = 0.03067350\n",
      "Iteration 45798, loss = 0.03066972\n",
      "Iteration 45799, loss = 0.03065901\n",
      "Iteration 45800, loss = 0.03064191\n",
      "Iteration 45801, loss = 0.03064811\n",
      "Iteration 45802, loss = 0.03064816\n",
      "Iteration 45803, loss = 0.03064496\n",
      "Iteration 45804, loss = 0.03064445\n",
      "Iteration 45805, loss = 0.03062226\n",
      "Iteration 45806, loss = 0.03062716\n",
      "Iteration 45807, loss = 0.03063035\n",
      "Iteration 45808, loss = 0.03062832\n",
      "Iteration 45809, loss = 0.03062663\n",
      "Iteration 45810, loss = 0.03062824\n",
      "Iteration 45811, loss = 0.03061933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 45812, loss = 0.03062816\n",
      "Iteration 45813, loss = 0.03063002\n",
      "Iteration 45814, loss = 0.03063190\n",
      "Iteration 45815, loss = 0.03062543\n",
      "Iteration 45816, loss = 0.03061776\n",
      "Iteration 45817, loss = 0.03062758\n",
      "Iteration 45818, loss = 0.03062796\n",
      "Iteration 45819, loss = 0.03063376\n",
      "Iteration 45820, loss = 0.03062096\n",
      "Iteration 45821, loss = 0.03062575\n",
      "Iteration 45822, loss = 0.03063700\n",
      "Iteration 45823, loss = 0.03062836\n",
      "Iteration 45824, loss = 0.03062080\n",
      "Iteration 45825, loss = 0.03063374\n",
      "Iteration 45826, loss = 0.03063234\n",
      "Iteration 45827, loss = 0.03062721\n",
      "Iteration 45828, loss = 0.03060881\n",
      "Iteration 45829, loss = 0.03062075\n",
      "Iteration 45830, loss = 0.03062991\n",
      "Iteration 45831, loss = 0.03062564\n",
      "Iteration 45832, loss = 0.03061763\n",
      "Iteration 45833, loss = 0.03061380\n",
      "Iteration 45834, loss = 0.03061971\n",
      "Iteration 45835, loss = 0.03062029\n",
      "Iteration 45836, loss = 0.03062034\n",
      "Iteration 45837, loss = 0.03062081\n",
      "Iteration 45838, loss = 0.03061953\n",
      "Iteration 45839, loss = 0.03061227\n",
      "Iteration 45840, loss = 0.03061235\n",
      "Iteration 45841, loss = 0.03062091\n",
      "Iteration 45842, loss = 0.03061435\n",
      "Iteration 45843, loss = 0.03061309\n",
      "Iteration 45844, loss = 0.03061773\n",
      "Iteration 45845, loss = 0.03061673\n",
      "Iteration 45846, loss = 0.03060919\n",
      "Iteration 45847, loss = 0.03061052\n",
      "Iteration 45848, loss = 0.03060965\n",
      "Iteration 45849, loss = 0.03060418\n",
      "Iteration 45850, loss = 0.03061184\n",
      "Iteration 45851, loss = 0.03061206\n",
      "Iteration 45852, loss = 0.03061008\n",
      "Iteration 45853, loss = 0.03060579\n",
      "Iteration 45854, loss = 0.03060980\n",
      "Iteration 45855, loss = 0.03061506\n",
      "Iteration 45856, loss = 0.03060693\n",
      "Iteration 45857, loss = 0.03061084\n",
      "Iteration 45858, loss = 0.03061541\n",
      "Iteration 45859, loss = 0.03060934\n",
      "Iteration 45860, loss = 0.03060211\n",
      "Iteration 45861, loss = 0.03060588\n",
      "Iteration 45862, loss = 0.03060580\n",
      "Iteration 45863, loss = 0.03059922\n",
      "Iteration 45864, loss = 0.03060331\n",
      "Iteration 45865, loss = 0.03061496\n",
      "Iteration 45866, loss = 0.03060862\n",
      "Iteration 45867, loss = 0.03061211\n",
      "Iteration 45868, loss = 0.03061691\n",
      "Iteration 45869, loss = 0.03060980\n",
      "Iteration 45870, loss = 0.03060013\n",
      "Iteration 45871, loss = 0.03060748\n",
      "Iteration 45872, loss = 0.03061545\n",
      "Iteration 45873, loss = 0.03060847\n",
      "Iteration 45874, loss = 0.03059593\n",
      "Iteration 45875, loss = 0.03060212\n",
      "Iteration 45876, loss = 0.03059679\n",
      "Iteration 45877, loss = 0.03059828\n",
      "Iteration 45878, loss = 0.03059782\n",
      "Iteration 45879, loss = 0.03060326\n",
      "Iteration 45880, loss = 0.03060450\n",
      "Iteration 45881, loss = 0.03060443\n",
      "Iteration 45882, loss = 0.03060094\n",
      "Iteration 45883, loss = 0.03059788\n",
      "Iteration 45884, loss = 0.03060834\n",
      "Iteration 45885, loss = 0.03060748\n",
      "Iteration 45886, loss = 0.03058905\n",
      "Iteration 45887, loss = 0.03061107\n",
      "Iteration 45888, loss = 0.03062193\n",
      "Iteration 45889, loss = 0.03061053\n",
      "Iteration 45890, loss = 0.03061288\n",
      "Iteration 45891, loss = 0.03061525\n",
      "Iteration 45892, loss = 0.03060947\n",
      "Iteration 45893, loss = 0.03059943\n",
      "Iteration 45894, loss = 0.03060547\n",
      "Iteration 45895, loss = 0.03062629\n",
      "Iteration 45896, loss = 0.03062946\n",
      "Iteration 45897, loss = 0.03060564\n",
      "Iteration 45898, loss = 0.03059821\n",
      "Iteration 45899, loss = 0.03061787\n",
      "Iteration 45900, loss = 0.03063132\n",
      "Iteration 45901, loss = 0.03062940\n",
      "Iteration 45902, loss = 0.03062579\n",
      "Iteration 45903, loss = 0.03062415\n",
      "Iteration 45904, loss = 0.03060905\n",
      "Iteration 45905, loss = 0.03060325\n",
      "Iteration 45906, loss = 0.03059250\n",
      "Iteration 45907, loss = 0.03060520\n",
      "Iteration 45908, loss = 0.03060211\n",
      "Iteration 45909, loss = 0.03059474\n",
      "Iteration 45910, loss = 0.03059279\n",
      "Iteration 45911, loss = 0.03059753\n",
      "Iteration 45912, loss = 0.03059574\n",
      "Iteration 45913, loss = 0.03059007\n",
      "Iteration 45914, loss = 0.03059284\n",
      "Iteration 45915, loss = 0.03059740\n",
      "Iteration 45916, loss = 0.03059185\n",
      "Iteration 45917, loss = 0.03058650\n",
      "Iteration 45918, loss = 0.03058893\n",
      "Iteration 45919, loss = 0.03059206\n",
      "Iteration 45920, loss = 0.03058145\n",
      "Iteration 45921, loss = 0.03058785\n",
      "Iteration 45922, loss = 0.03059789\n",
      "Iteration 45923, loss = 0.03058865\n",
      "Iteration 45924, loss = 0.03058557\n",
      "Iteration 45925, loss = 0.03058701\n",
      "Iteration 45926, loss = 0.03059932\n",
      "Iteration 45927, loss = 0.03059221\n",
      "Iteration 45928, loss = 0.03058705\n",
      "Iteration 45929, loss = 0.03059638\n",
      "Iteration 45930, loss = 0.03060014\n",
      "Iteration 45931, loss = 0.03058851\n",
      "Iteration 45932, loss = 0.03058170\n",
      "Iteration 45933, loss = 0.03059109\n",
      "Iteration 45934, loss = 0.03059474\n",
      "Iteration 45935, loss = 0.03060459\n",
      "Iteration 45936, loss = 0.03058906\n",
      "Iteration 45937, loss = 0.03057716\n",
      "Iteration 45938, loss = 0.03058468\n",
      "Iteration 45939, loss = 0.03058367\n",
      "Iteration 45940, loss = 0.03057677\n",
      "Iteration 45941, loss = 0.03058171\n",
      "Iteration 45942, loss = 0.03058197\n",
      "Iteration 45943, loss = 0.03058095\n",
      "Iteration 45944, loss = 0.03058220\n",
      "Iteration 45945, loss = 0.03057382\n",
      "Iteration 45946, loss = 0.03057188\n",
      "Iteration 45947, loss = 0.03057761\n",
      "Iteration 45948, loss = 0.03057325\n",
      "Iteration 45949, loss = 0.03056681\n",
      "Iteration 45950, loss = 0.03057870\n",
      "Iteration 45951, loss = 0.03057209\n",
      "Iteration 45952, loss = 0.03057774\n",
      "Iteration 45953, loss = 0.03057997\n",
      "Iteration 45954, loss = 0.03057611\n",
      "Iteration 45955, loss = 0.03057352\n",
      "Iteration 45956, loss = 0.03058024\n",
      "Iteration 45957, loss = 0.03058229\n",
      "Iteration 45958, loss = 0.03057163\n",
      "Iteration 45959, loss = 0.03057119\n",
      "Iteration 45960, loss = 0.03058001\n",
      "Iteration 45961, loss = 0.03058091\n",
      "Iteration 45962, loss = 0.03057393\n",
      "Iteration 45963, loss = 0.03057506\n",
      "Iteration 45964, loss = 0.03057075\n",
      "Iteration 45965, loss = 0.03058013\n",
      "Iteration 45966, loss = 0.03058205\n",
      "Iteration 45967, loss = 0.03057579\n",
      "Iteration 45968, loss = 0.03057511\n",
      "Iteration 45969, loss = 0.03056959\n",
      "Iteration 45970, loss = 0.03057575\n",
      "Iteration 45971, loss = 0.03058225\n",
      "Iteration 45972, loss = 0.03057092\n",
      "Iteration 45973, loss = 0.03057587\n",
      "Iteration 45974, loss = 0.03057610\n",
      "Iteration 45975, loss = 0.03057980\n",
      "Iteration 45976, loss = 0.03057699\n",
      "Iteration 45977, loss = 0.03056818\n",
      "Iteration 45978, loss = 0.03056792\n",
      "Iteration 45979, loss = 0.03056053\n",
      "Iteration 45980, loss = 0.03057177\n",
      "Iteration 45981, loss = 0.03058368\n",
      "Iteration 45982, loss = 0.03057787\n",
      "Iteration 45983, loss = 0.03057377\n",
      "Iteration 45984, loss = 0.03056042\n",
      "Iteration 45985, loss = 0.03056122\n",
      "Iteration 45986, loss = 0.03056797\n",
      "Iteration 45987, loss = 0.03056905\n",
      "Iteration 45988, loss = 0.03056490\n",
      "Iteration 45989, loss = 0.03055445\n",
      "Iteration 45990, loss = 0.03057262\n",
      "Iteration 45991, loss = 0.03058000\n",
      "Iteration 45992, loss = 0.03057012\n",
      "Iteration 45993, loss = 0.03055761\n",
      "Iteration 45994, loss = 0.03057300\n",
      "Iteration 45995, loss = 0.03058387\n",
      "Iteration 45996, loss = 0.03057978\n",
      "Iteration 45997, loss = 0.03056370\n",
      "Iteration 45998, loss = 0.03056799\n",
      "Iteration 45999, loss = 0.03057332\n",
      "Iteration 46000, loss = 0.03058499\n",
      "Iteration 46001, loss = 0.03057875\n",
      "Iteration 46002, loss = 0.03056999\n",
      "Iteration 46003, loss = 0.03055850\n",
      "Iteration 46004, loss = 0.03056317\n",
      "Iteration 46005, loss = 0.03056604\n",
      "Iteration 46006, loss = 0.03056492\n",
      "Iteration 46007, loss = 0.03055917\n",
      "Iteration 46008, loss = 0.03056081\n",
      "Iteration 46009, loss = 0.03056827\n",
      "Iteration 46010, loss = 0.03056248\n",
      "Iteration 46011, loss = 0.03056263\n",
      "Iteration 46012, loss = 0.03055839\n",
      "Iteration 46013, loss = 0.03055460\n",
      "Iteration 46014, loss = 0.03056117\n",
      "Iteration 46015, loss = 0.03055436\n",
      "Iteration 46016, loss = 0.03054970\n",
      "Iteration 46017, loss = 0.03055228\n",
      "Iteration 46018, loss = 0.03055432\n",
      "Iteration 46019, loss = 0.03055191\n",
      "Iteration 46020, loss = 0.03055616\n",
      "Iteration 46021, loss = 0.03055292\n",
      "Iteration 46022, loss = 0.03055356\n",
      "Iteration 46023, loss = 0.03054877\n",
      "Iteration 46024, loss = 0.03055265\n",
      "Iteration 46025, loss = 0.03054942\n",
      "Iteration 46026, loss = 0.03055005\n",
      "Iteration 46027, loss = 0.03054670\n",
      "Iteration 46028, loss = 0.03053897\n",
      "Iteration 46029, loss = 0.03054095\n",
      "Iteration 46030, loss = 0.03053559\n",
      "Iteration 46031, loss = 0.03054778\n",
      "Iteration 46032, loss = 0.03054127\n",
      "Iteration 46033, loss = 0.03054756\n",
      "Iteration 46034, loss = 0.03054990\n",
      "Iteration 46035, loss = 0.03054915\n",
      "Iteration 46036, loss = 0.03054762\n",
      "Iteration 46037, loss = 0.03054377\n",
      "Iteration 46038, loss = 0.03054702\n",
      "Iteration 46039, loss = 0.03054506\n",
      "Iteration 46040, loss = 0.03054054\n",
      "Iteration 46041, loss = 0.03054271\n",
      "Iteration 46042, loss = 0.03054660\n",
      "Iteration 46043, loss = 0.03054820\n",
      "Iteration 46044, loss = 0.03055387\n",
      "Iteration 46045, loss = 0.03054906\n",
      "Iteration 46046, loss = 0.03053654\n",
      "Iteration 46047, loss = 0.03054879\n",
      "Iteration 46048, loss = 0.03054567\n",
      "Iteration 46049, loss = 0.03053769\n",
      "Iteration 46050, loss = 0.03054558\n",
      "Iteration 46051, loss = 0.03054827\n",
      "Iteration 46052, loss = 0.03054072\n",
      "Iteration 46053, loss = 0.03054219\n",
      "Iteration 46054, loss = 0.03053482\n",
      "Iteration 46055, loss = 0.03054085\n",
      "Iteration 46056, loss = 0.03055200\n",
      "Iteration 46057, loss = 0.03053602\n",
      "Iteration 46058, loss = 0.03054396\n",
      "Iteration 46059, loss = 0.03055544\n",
      "Iteration 46060, loss = 0.03055316\n",
      "Iteration 46061, loss = 0.03054496\n",
      "Iteration 46062, loss = 0.03053671\n",
      "Iteration 46063, loss = 0.03054406\n",
      "Iteration 46064, loss = 0.03054871\n",
      "Iteration 46065, loss = 0.03054600\n",
      "Iteration 46066, loss = 0.03053662\n",
      "Iteration 46067, loss = 0.03053922\n",
      "Iteration 46068, loss = 0.03054245\n",
      "Iteration 46069, loss = 0.03054825\n",
      "Iteration 46070, loss = 0.03055066\n",
      "Iteration 46071, loss = 0.03053885\n",
      "Iteration 46072, loss = 0.03053421\n",
      "Iteration 46073, loss = 0.03054279\n",
      "Iteration 46074, loss = 0.03053700\n",
      "Iteration 46075, loss = 0.03053943\n",
      "Iteration 46076, loss = 0.03054758\n",
      "Iteration 46077, loss = 0.03054294\n",
      "Iteration 46078, loss = 0.03053262\n",
      "Iteration 46079, loss = 0.03053855\n",
      "Iteration 46080, loss = 0.03053231\n",
      "Iteration 46081, loss = 0.03053919\n",
      "Iteration 46082, loss = 0.03054648\n",
      "Iteration 46083, loss = 0.03054158\n",
      "Iteration 46084, loss = 0.03053448\n",
      "Iteration 46085, loss = 0.03053650\n",
      "Iteration 46086, loss = 0.03054925\n",
      "Iteration 46087, loss = 0.03054939\n",
      "Iteration 46088, loss = 0.03053752\n",
      "Iteration 46089, loss = 0.03053216\n",
      "Iteration 46090, loss = 0.03054309\n",
      "Iteration 46091, loss = 0.03054510\n",
      "Iteration 46092, loss = 0.03052666\n",
      "Iteration 46093, loss = 0.03052679\n",
      "Iteration 46094, loss = 0.03054282\n",
      "Iteration 46095, loss = 0.03053871\n",
      "Iteration 46096, loss = 0.03054059\n",
      "Iteration 46097, loss = 0.03053931\n",
      "Iteration 46098, loss = 0.03053451\n",
      "Iteration 46099, loss = 0.03053038\n",
      "Iteration 46100, loss = 0.03052655\n",
      "Iteration 46101, loss = 0.03052129\n",
      "Iteration 46102, loss = 0.03051837\n",
      "Iteration 46103, loss = 0.03051474\n",
      "Iteration 46104, loss = 0.03052714\n",
      "Iteration 46105, loss = 0.03052702\n",
      "Iteration 46106, loss = 0.03053279\n",
      "Iteration 46107, loss = 0.03053445\n",
      "Iteration 46108, loss = 0.03053348\n",
      "Iteration 46109, loss = 0.03052083\n",
      "Iteration 46110, loss = 0.03053389\n",
      "Iteration 46111, loss = 0.03054358\n",
      "Iteration 46112, loss = 0.03053796\n",
      "Iteration 46113, loss = 0.03051632\n",
      "Iteration 46114, loss = 0.03053785\n",
      "Iteration 46115, loss = 0.03055274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 46116, loss = 0.03055892\n",
      "Iteration 46117, loss = 0.03055353\n",
      "Iteration 46118, loss = 0.03053880\n",
      "Iteration 46119, loss = 0.03052981\n",
      "Iteration 46120, loss = 0.03053471\n",
      "Iteration 46121, loss = 0.03055420\n",
      "Iteration 46122, loss = 0.03054545\n",
      "Iteration 46123, loss = 0.03053010\n",
      "Iteration 46124, loss = 0.03052578\n",
      "Iteration 46125, loss = 0.03053944\n",
      "Iteration 46126, loss = 0.03054107\n",
      "Iteration 46127, loss = 0.03053096\n",
      "Iteration 46128, loss = 0.03051597\n",
      "Iteration 46129, loss = 0.03052237\n",
      "Iteration 46130, loss = 0.03053155\n",
      "Iteration 46131, loss = 0.03052197\n",
      "Iteration 46132, loss = 0.03051238\n",
      "Iteration 46133, loss = 0.03050817\n",
      "Iteration 46134, loss = 0.03051044\n",
      "Iteration 46135, loss = 0.03051097\n",
      "Iteration 46136, loss = 0.03051090\n",
      "Iteration 46137, loss = 0.03051677\n",
      "Iteration 46138, loss = 0.03051245\n",
      "Iteration 46139, loss = 0.03050604\n",
      "Iteration 46140, loss = 0.03051496\n",
      "Iteration 46141, loss = 0.03051604\n",
      "Iteration 46142, loss = 0.03051365\n",
      "Iteration 46143, loss = 0.03050879\n",
      "Iteration 46144, loss = 0.03051511\n",
      "Iteration 46145, loss = 0.03052288\n",
      "Iteration 46146, loss = 0.03050970\n",
      "Iteration 46147, loss = 0.03051490\n",
      "Iteration 46148, loss = 0.03052296\n",
      "Iteration 46149, loss = 0.03051652\n",
      "Iteration 46150, loss = 0.03052034\n",
      "Iteration 46151, loss = 0.03051557\n",
      "Iteration 46152, loss = 0.03050335\n",
      "Iteration 46153, loss = 0.03051855\n",
      "Iteration 46154, loss = 0.03051941\n",
      "Iteration 46155, loss = 0.03050923\n",
      "Iteration 46156, loss = 0.03050694\n",
      "Iteration 46157, loss = 0.03051042\n",
      "Iteration 46158, loss = 0.03051090\n",
      "Iteration 46159, loss = 0.03051532\n",
      "Iteration 46160, loss = 0.03051854\n",
      "Iteration 46161, loss = 0.03050852\n",
      "Iteration 46162, loss = 0.03050144\n",
      "Iteration 46163, loss = 0.03050277\n",
      "Iteration 46164, loss = 0.03049380\n",
      "Iteration 46165, loss = 0.03050430\n",
      "Iteration 46166, loss = 0.03050812\n",
      "Iteration 46167, loss = 0.03049913\n",
      "Iteration 46168, loss = 0.03050092\n",
      "Iteration 46169, loss = 0.03051453\n",
      "Iteration 46170, loss = 0.03051263\n",
      "Iteration 46171, loss = 0.03050666\n",
      "Iteration 46172, loss = 0.03050852\n",
      "Iteration 46173, loss = 0.03051672\n",
      "Iteration 46174, loss = 0.03051496\n",
      "Iteration 46175, loss = 0.03050470\n",
      "Iteration 46176, loss = 0.03050550\n",
      "Iteration 46177, loss = 0.03051158\n",
      "Iteration 46178, loss = 0.03051482\n",
      "Iteration 46179, loss = 0.03051203\n",
      "Iteration 46180, loss = 0.03049872\n",
      "Iteration 46181, loss = 0.03050800\n",
      "Iteration 46182, loss = 0.03050825\n",
      "Iteration 46183, loss = 0.03049873\n",
      "Iteration 46184, loss = 0.03049585\n",
      "Iteration 46185, loss = 0.03049781\n",
      "Iteration 46186, loss = 0.03050725\n",
      "Iteration 46187, loss = 0.03050869\n",
      "Iteration 46188, loss = 0.03050454\n",
      "Iteration 46189, loss = 0.03049158\n",
      "Iteration 46190, loss = 0.03048950\n",
      "Iteration 46191, loss = 0.03049568\n",
      "Iteration 46192, loss = 0.03048427\n",
      "Iteration 46193, loss = 0.03049235\n",
      "Iteration 46194, loss = 0.03049914\n",
      "Iteration 46195, loss = 0.03049417\n",
      "Iteration 46196, loss = 0.03050411\n",
      "Iteration 46197, loss = 0.03050555\n",
      "Iteration 46198, loss = 0.03049830\n",
      "Iteration 46199, loss = 0.03048206\n",
      "Iteration 46200, loss = 0.03050652\n",
      "Iteration 46201, loss = 0.03051628\n",
      "Iteration 46202, loss = 0.03050606\n",
      "Iteration 46203, loss = 0.03048677\n",
      "Iteration 46204, loss = 0.03049456\n",
      "Iteration 46205, loss = 0.03050208\n",
      "Iteration 46206, loss = 0.03050472\n",
      "Iteration 46207, loss = 0.03050399\n",
      "Iteration 46208, loss = 0.03050528\n",
      "Iteration 46209, loss = 0.03050788\n",
      "Iteration 46210, loss = 0.03049736\n",
      "Iteration 46211, loss = 0.03048167\n",
      "Iteration 46212, loss = 0.03048752\n",
      "Iteration 46213, loss = 0.03049349\n",
      "Iteration 46214, loss = 0.03049039\n",
      "Iteration 46215, loss = 0.03047522\n",
      "Iteration 46216, loss = 0.03049118\n",
      "Iteration 46217, loss = 0.03049422\n",
      "Iteration 46218, loss = 0.03048157\n",
      "Iteration 46219, loss = 0.03047599\n",
      "Iteration 46220, loss = 0.03049011\n",
      "Iteration 46221, loss = 0.03048656\n",
      "Iteration 46222, loss = 0.03048044\n",
      "Iteration 46223, loss = 0.03049784\n",
      "Iteration 46224, loss = 0.03049924\n",
      "Iteration 46225, loss = 0.03050060\n",
      "Iteration 46226, loss = 0.03051050\n",
      "Iteration 46227, loss = 0.03050920\n",
      "Iteration 46228, loss = 0.03049434\n",
      "Iteration 46229, loss = 0.03048796\n",
      "Iteration 46230, loss = 0.03049883\n",
      "Iteration 46231, loss = 0.03050709\n",
      "Iteration 46232, loss = 0.03049047\n",
      "Iteration 46233, loss = 0.03048321\n",
      "Iteration 46234, loss = 0.03048357\n",
      "Iteration 46235, loss = 0.03049240\n",
      "Iteration 46236, loss = 0.03049187\n",
      "Iteration 46237, loss = 0.03049117\n",
      "Iteration 46238, loss = 0.03048861\n",
      "Iteration 46239, loss = 0.03048551\n",
      "Iteration 46240, loss = 0.03047652\n",
      "Iteration 46241, loss = 0.03048398\n",
      "Iteration 46242, loss = 0.03048690\n",
      "Iteration 46243, loss = 0.03048537\n",
      "Iteration 46244, loss = 0.03047954\n",
      "Iteration 46245, loss = 0.03047089\n",
      "Iteration 46246, loss = 0.03047639\n",
      "Iteration 46247, loss = 0.03048298\n",
      "Iteration 46248, loss = 0.03048184\n",
      "Iteration 46249, loss = 0.03048262\n",
      "Iteration 46250, loss = 0.03046683\n",
      "Iteration 46251, loss = 0.03046625\n",
      "Iteration 46252, loss = 0.03047594\n",
      "Iteration 46253, loss = 0.03046332\n",
      "Iteration 46254, loss = 0.03047177\n",
      "Iteration 46255, loss = 0.03047885\n",
      "Iteration 46256, loss = 0.03046944\n",
      "Iteration 46257, loss = 0.03047078\n",
      "Iteration 46258, loss = 0.03047203\n",
      "Iteration 46259, loss = 0.03047775\n",
      "Iteration 46260, loss = 0.03047431\n",
      "Iteration 46261, loss = 0.03046688\n",
      "Iteration 46262, loss = 0.03047102\n",
      "Iteration 46263, loss = 0.03048352\n",
      "Iteration 46264, loss = 0.03048834\n",
      "Iteration 46265, loss = 0.03047878\n",
      "Iteration 46266, loss = 0.03048200\n",
      "Iteration 46267, loss = 0.03048738\n",
      "Iteration 46268, loss = 0.03049318\n",
      "Iteration 46269, loss = 0.03048190\n",
      "Iteration 46270, loss = 0.03046818\n",
      "Iteration 46271, loss = 0.03046929\n",
      "Iteration 46272, loss = 0.03047147\n",
      "Iteration 46273, loss = 0.03045908\n",
      "Iteration 46274, loss = 0.03046697\n",
      "Iteration 46275, loss = 0.03047700\n",
      "Iteration 46276, loss = 0.03047862\n",
      "Iteration 46277, loss = 0.03047358\n",
      "Iteration 46278, loss = 0.03047349\n",
      "Iteration 46279, loss = 0.03046923\n",
      "Iteration 46280, loss = 0.03045461\n",
      "Iteration 46281, loss = 0.03047252\n",
      "Iteration 46282, loss = 0.03047654\n",
      "Iteration 46283, loss = 0.03046401\n",
      "Iteration 46284, loss = 0.03045842\n",
      "Iteration 46285, loss = 0.03046274\n",
      "Iteration 46286, loss = 0.03047122\n",
      "Iteration 46287, loss = 0.03046236\n",
      "Iteration 46288, loss = 0.03046092\n",
      "Iteration 46289, loss = 0.03046027\n",
      "Iteration 46290, loss = 0.03046140\n",
      "Iteration 46291, loss = 0.03045485\n",
      "Iteration 46292, loss = 0.03044854\n",
      "Iteration 46293, loss = 0.03045666\n",
      "Iteration 46294, loss = 0.03045184\n",
      "Iteration 46295, loss = 0.03045905\n",
      "Iteration 46296, loss = 0.03046554\n",
      "Iteration 46297, loss = 0.03046044\n",
      "Iteration 46298, loss = 0.03045439\n",
      "Iteration 46299, loss = 0.03044802\n",
      "Iteration 46300, loss = 0.03044343\n",
      "Iteration 46301, loss = 0.03044236\n",
      "Iteration 46302, loss = 0.03044339\n",
      "Iteration 46303, loss = 0.03044951\n",
      "Iteration 46304, loss = 0.03044704\n",
      "Iteration 46305, loss = 0.03043989\n",
      "Iteration 46306, loss = 0.03044602\n",
      "Iteration 46307, loss = 0.03044639\n",
      "Iteration 46308, loss = 0.03044374\n",
      "Iteration 46309, loss = 0.03044524\n",
      "Iteration 46310, loss = 0.03043888\n",
      "Iteration 46311, loss = 0.03043765\n",
      "Iteration 46312, loss = 0.03044317\n",
      "Iteration 46313, loss = 0.03044101\n",
      "Iteration 46314, loss = 0.03045031\n",
      "Iteration 46315, loss = 0.03045379\n",
      "Iteration 46316, loss = 0.03045117\n",
      "Iteration 46317, loss = 0.03044942\n",
      "Iteration 46318, loss = 0.03045706\n",
      "Iteration 46319, loss = 0.03045166\n",
      "Iteration 46320, loss = 0.03043732\n",
      "Iteration 46321, loss = 0.03044105\n",
      "Iteration 46322, loss = 0.03045152\n",
      "Iteration 46323, loss = 0.03044780\n",
      "Iteration 46324, loss = 0.03042827\n",
      "Iteration 46325, loss = 0.03043539\n",
      "Iteration 46326, loss = 0.03044493\n",
      "Iteration 46327, loss = 0.03043701\n",
      "Iteration 46328, loss = 0.03042678\n",
      "Iteration 46329, loss = 0.03044164\n",
      "Iteration 46330, loss = 0.03044016\n",
      "Iteration 46331, loss = 0.03043412\n",
      "Iteration 46332, loss = 0.03044582\n",
      "Iteration 46333, loss = 0.03044793\n",
      "Iteration 46334, loss = 0.03044928\n",
      "Iteration 46335, loss = 0.03045884\n",
      "Iteration 46336, loss = 0.03045563\n",
      "Iteration 46337, loss = 0.03043874\n",
      "Iteration 46338, loss = 0.03043077\n",
      "Iteration 46339, loss = 0.03043934\n",
      "Iteration 46340, loss = 0.03044224\n",
      "Iteration 46341, loss = 0.03042622\n",
      "Iteration 46342, loss = 0.03043103\n",
      "Iteration 46343, loss = 0.03044420\n",
      "Iteration 46344, loss = 0.03044108\n",
      "Iteration 46345, loss = 0.03043379\n",
      "Iteration 46346, loss = 0.03043218\n",
      "Iteration 46347, loss = 0.03043792\n",
      "Iteration 46348, loss = 0.03043110\n",
      "Iteration 46349, loss = 0.03043557\n",
      "Iteration 46350, loss = 0.03043903\n",
      "Iteration 46351, loss = 0.03043261\n",
      "Iteration 46352, loss = 0.03043299\n",
      "Iteration 46353, loss = 0.03043786\n",
      "Iteration 46354, loss = 0.03043082\n",
      "Iteration 46355, loss = 0.03043005\n",
      "Iteration 46356, loss = 0.03042459\n",
      "Iteration 46357, loss = 0.03042089\n",
      "Iteration 46358, loss = 0.03041904\n",
      "Iteration 46359, loss = 0.03042505\n",
      "Iteration 46360, loss = 0.03042821\n",
      "Iteration 46361, loss = 0.03042384\n",
      "Iteration 46362, loss = 0.03042847\n",
      "Iteration 46363, loss = 0.03042341\n",
      "Iteration 46364, loss = 0.03041676\n",
      "Iteration 46365, loss = 0.03041419\n",
      "Iteration 46366, loss = 0.03041562\n",
      "Iteration 46367, loss = 0.03042002\n",
      "Iteration 46368, loss = 0.03042363\n",
      "Iteration 46369, loss = 0.03041553\n",
      "Iteration 46370, loss = 0.03042085\n",
      "Iteration 46371, loss = 0.03042094\n",
      "Iteration 46372, loss = 0.03041119\n",
      "Iteration 46373, loss = 0.03042132\n",
      "Iteration 46374, loss = 0.03042791\n",
      "Iteration 46375, loss = 0.03042411\n",
      "Iteration 46376, loss = 0.03042145\n",
      "Iteration 46377, loss = 0.03041785\n",
      "Iteration 46378, loss = 0.03041033\n",
      "Iteration 46379, loss = 0.03041885\n",
      "Iteration 46380, loss = 0.03042104\n",
      "Iteration 46381, loss = 0.03041942\n",
      "Iteration 46382, loss = 0.03041703\n",
      "Iteration 46383, loss = 0.03041203\n",
      "Iteration 46384, loss = 0.03041991\n",
      "Iteration 46385, loss = 0.03042082\n",
      "Iteration 46386, loss = 0.03041409\n",
      "Iteration 46387, loss = 0.03041405\n",
      "Iteration 46388, loss = 0.03041666\n",
      "Iteration 46389, loss = 0.03042240\n",
      "Iteration 46390, loss = 0.03042106\n",
      "Iteration 46391, loss = 0.03041239\n",
      "Iteration 46392, loss = 0.03040738\n",
      "Iteration 46393, loss = 0.03041201\n",
      "Iteration 46394, loss = 0.03040730\n",
      "Iteration 46395, loss = 0.03040836\n",
      "Iteration 46396, loss = 0.03042441\n",
      "Iteration 46397, loss = 0.03042154\n",
      "Iteration 46398, loss = 0.03042010\n",
      "Iteration 46399, loss = 0.03042681\n",
      "Iteration 46400, loss = 0.03042726\n",
      "Iteration 46401, loss = 0.03041473\n",
      "Iteration 46402, loss = 0.03040388\n",
      "Iteration 46403, loss = 0.03041033\n",
      "Iteration 46404, loss = 0.03040491\n",
      "Iteration 46405, loss = 0.03040034\n",
      "Iteration 46406, loss = 0.03040883\n",
      "Iteration 46407, loss = 0.03041694\n",
      "Iteration 46408, loss = 0.03040874\n",
      "Iteration 46409, loss = 0.03040883\n",
      "Iteration 46410, loss = 0.03041471\n",
      "Iteration 46411, loss = 0.03041808\n",
      "Iteration 46412, loss = 0.03040853\n",
      "Iteration 46413, loss = 0.03040639\n",
      "Iteration 46414, loss = 0.03041426\n",
      "Iteration 46415, loss = 0.03040416\n",
      "Iteration 46416, loss = 0.03039950\n",
      "Iteration 46417, loss = 0.03041069\n",
      "Iteration 46418, loss = 0.03041862\n",
      "Iteration 46419, loss = 0.03041688\n",
      "Iteration 46420, loss = 0.03041394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 46421, loss = 0.03041177\n",
      "Iteration 46422, loss = 0.03040509\n",
      "Iteration 46423, loss = 0.03039513\n",
      "Iteration 46424, loss = 0.03040906\n",
      "Iteration 46425, loss = 0.03041791\n",
      "Iteration 46426, loss = 0.03040318\n",
      "Iteration 46427, loss = 0.03039562\n",
      "Iteration 46428, loss = 0.03039838\n",
      "Iteration 46429, loss = 0.03039444\n",
      "Iteration 46430, loss = 0.03040279\n",
      "Iteration 46431, loss = 0.03040032\n",
      "Iteration 46432, loss = 0.03041399\n",
      "Iteration 46433, loss = 0.03041467\n",
      "Iteration 46434, loss = 0.03039228\n",
      "Iteration 46435, loss = 0.03040967\n",
      "Iteration 46436, loss = 0.03042778\n",
      "Iteration 46437, loss = 0.03043180\n",
      "Iteration 46438, loss = 0.03042110\n",
      "Iteration 46439, loss = 0.03041661\n",
      "Iteration 46440, loss = 0.03041007\n",
      "Iteration 46441, loss = 0.03041084\n",
      "Iteration 46442, loss = 0.03041991\n",
      "Iteration 46443, loss = 0.03041116\n",
      "Iteration 46444, loss = 0.03039270\n",
      "Iteration 46445, loss = 0.03039446\n",
      "Iteration 46446, loss = 0.03039296\n",
      "Iteration 46447, loss = 0.03040080\n",
      "Iteration 46448, loss = 0.03040180\n",
      "Iteration 46449, loss = 0.03040230\n",
      "Iteration 46450, loss = 0.03039668\n",
      "Iteration 46451, loss = 0.03039368\n",
      "Iteration 46452, loss = 0.03040211\n",
      "Iteration 46453, loss = 0.03040282\n",
      "Iteration 46454, loss = 0.03039663\n",
      "Iteration 46455, loss = 0.03039285\n",
      "Iteration 46456, loss = 0.03039972\n",
      "Iteration 46457, loss = 0.03040095\n",
      "Iteration 46458, loss = 0.03039838\n",
      "Iteration 46459, loss = 0.03039602\n",
      "Iteration 46460, loss = 0.03038316\n",
      "Iteration 46461, loss = 0.03038631\n",
      "Iteration 46462, loss = 0.03039887\n",
      "Iteration 46463, loss = 0.03039498\n",
      "Iteration 46464, loss = 0.03037770\n",
      "Iteration 46465, loss = 0.03038170\n",
      "Iteration 46466, loss = 0.03038697\n",
      "Iteration 46467, loss = 0.03037892\n",
      "Iteration 46468, loss = 0.03039037\n",
      "Iteration 46469, loss = 0.03039345\n",
      "Iteration 46470, loss = 0.03039671\n",
      "Iteration 46471, loss = 0.03039622\n",
      "Iteration 46472, loss = 0.03038144\n",
      "Iteration 46473, loss = 0.03037686\n",
      "Iteration 46474, loss = 0.03038440\n",
      "Iteration 46475, loss = 0.03038336\n",
      "Iteration 46476, loss = 0.03038196\n",
      "Iteration 46477, loss = 0.03037692\n",
      "Iteration 46478, loss = 0.03038577\n",
      "Iteration 46479, loss = 0.03038558\n",
      "Iteration 46480, loss = 0.03037726\n",
      "Iteration 46481, loss = 0.03037618\n",
      "Iteration 46482, loss = 0.03039462\n",
      "Iteration 46483, loss = 0.03039906\n",
      "Iteration 46484, loss = 0.03039407\n",
      "Iteration 46485, loss = 0.03037554\n",
      "Iteration 46486, loss = 0.03037933\n",
      "Iteration 46487, loss = 0.03039275\n",
      "Iteration 46488, loss = 0.03039236\n",
      "Iteration 46489, loss = 0.03040019\n",
      "Iteration 46490, loss = 0.03039484\n",
      "Iteration 46491, loss = 0.03037949\n",
      "Iteration 46492, loss = 0.03038448\n",
      "Iteration 46493, loss = 0.03039092\n",
      "Iteration 46494, loss = 0.03038335\n",
      "Iteration 46495, loss = 0.03037544\n",
      "Iteration 46496, loss = 0.03037620\n",
      "Iteration 46497, loss = 0.03038640\n",
      "Iteration 46498, loss = 0.03039264\n",
      "Iteration 46499, loss = 0.03038489\n",
      "Iteration 46500, loss = 0.03037723\n",
      "Iteration 46501, loss = 0.03037247\n",
      "Iteration 46502, loss = 0.03038448\n",
      "Iteration 46503, loss = 0.03038495\n",
      "Iteration 46504, loss = 0.03037280\n",
      "Iteration 46505, loss = 0.03037565\n",
      "Iteration 46506, loss = 0.03037652\n",
      "Iteration 46507, loss = 0.03037722\n",
      "Iteration 46508, loss = 0.03037727\n",
      "Iteration 46509, loss = 0.03037937\n",
      "Iteration 46510, loss = 0.03037309\n",
      "Iteration 46511, loss = 0.03037113\n",
      "Iteration 46512, loss = 0.03037169\n",
      "Iteration 46513, loss = 0.03037133\n",
      "Iteration 46514, loss = 0.03037393\n",
      "Iteration 46515, loss = 0.03036780\n",
      "Iteration 46516, loss = 0.03037462\n",
      "Iteration 46517, loss = 0.03038321\n",
      "Iteration 46518, loss = 0.03038449\n",
      "Iteration 46519, loss = 0.03037430\n",
      "Iteration 46520, loss = 0.03038109\n",
      "Iteration 46521, loss = 0.03037912\n",
      "Iteration 46522, loss = 0.03037681\n",
      "Iteration 46523, loss = 0.03037710\n",
      "Iteration 46524, loss = 0.03036613\n",
      "Iteration 46525, loss = 0.03036530\n",
      "Iteration 46526, loss = 0.03037750\n",
      "Iteration 46527, loss = 0.03037149\n",
      "Iteration 46528, loss = 0.03036159\n",
      "Iteration 46529, loss = 0.03036166\n",
      "Iteration 46530, loss = 0.03036917\n",
      "Iteration 46531, loss = 0.03037415\n",
      "Iteration 46532, loss = 0.03037068\n",
      "Iteration 46533, loss = 0.03036229\n",
      "Iteration 46534, loss = 0.03035639\n",
      "Iteration 46535, loss = 0.03036040\n",
      "Iteration 46536, loss = 0.03035258\n",
      "Iteration 46537, loss = 0.03036465\n",
      "Iteration 46538, loss = 0.03037030\n",
      "Iteration 46539, loss = 0.03036773\n",
      "Iteration 46540, loss = 0.03036588\n",
      "Iteration 46541, loss = 0.03037774\n",
      "Iteration 46542, loss = 0.03038427\n",
      "Iteration 46543, loss = 0.03037456\n",
      "Iteration 46544, loss = 0.03036644\n",
      "Iteration 46545, loss = 0.03036401\n",
      "Iteration 46546, loss = 0.03036271\n",
      "Iteration 46547, loss = 0.03036527\n",
      "Iteration 46548, loss = 0.03036722\n",
      "Iteration 46549, loss = 0.03035981\n",
      "Iteration 46550, loss = 0.03036119\n",
      "Iteration 46551, loss = 0.03036505\n",
      "Iteration 46552, loss = 0.03035660\n",
      "Iteration 46553, loss = 0.03035203\n",
      "Iteration 46554, loss = 0.03036060\n",
      "Iteration 46555, loss = 0.03036119\n",
      "Iteration 46556, loss = 0.03035415\n",
      "Iteration 46557, loss = 0.03034388\n",
      "Iteration 46558, loss = 0.03036269\n",
      "Iteration 46559, loss = 0.03036500\n",
      "Iteration 46560, loss = 0.03036138\n",
      "Iteration 46561, loss = 0.03035693\n",
      "Iteration 46562, loss = 0.03033823\n",
      "Iteration 46563, loss = 0.03036560\n",
      "Iteration 46564, loss = 0.03038319\n",
      "Iteration 46565, loss = 0.03037644\n",
      "Iteration 46566, loss = 0.03035214\n",
      "Iteration 46567, loss = 0.03034615\n",
      "Iteration 46568, loss = 0.03036066\n",
      "Iteration 46569, loss = 0.03036397\n",
      "Iteration 46570, loss = 0.03035481\n",
      "Iteration 46571, loss = 0.03035236\n",
      "Iteration 46572, loss = 0.03035830\n",
      "Iteration 46573, loss = 0.03035632\n",
      "Iteration 46574, loss = 0.03035027\n",
      "Iteration 46575, loss = 0.03034894\n",
      "Iteration 46576, loss = 0.03034829\n",
      "Iteration 46577, loss = 0.03034875\n",
      "Iteration 46578, loss = 0.03033815\n",
      "Iteration 46579, loss = 0.03035634\n",
      "Iteration 46580, loss = 0.03036460\n",
      "Iteration 46581, loss = 0.03035348\n",
      "Iteration 46582, loss = 0.03033828\n",
      "Iteration 46583, loss = 0.03034997\n",
      "Iteration 46584, loss = 0.03035081\n",
      "Iteration 46585, loss = 0.03033734\n",
      "Iteration 46586, loss = 0.03034986\n",
      "Iteration 46587, loss = 0.03035394\n",
      "Iteration 46588, loss = 0.03035379\n",
      "Iteration 46589, loss = 0.03034986\n",
      "Iteration 46590, loss = 0.03034875\n",
      "Iteration 46591, loss = 0.03033803\n",
      "Iteration 46592, loss = 0.03034075\n",
      "Iteration 46593, loss = 0.03035222\n",
      "Iteration 46594, loss = 0.03035089\n",
      "Iteration 46595, loss = 0.03033269\n",
      "Iteration 46596, loss = 0.03034477\n",
      "Iteration 46597, loss = 0.03035055\n",
      "Iteration 46598, loss = 0.03036268\n",
      "Iteration 46599, loss = 0.03035829\n",
      "Iteration 46600, loss = 0.03035096\n",
      "Iteration 46601, loss = 0.03035106\n",
      "Iteration 46602, loss = 0.03033902\n",
      "Iteration 46603, loss = 0.03032986\n",
      "Iteration 46604, loss = 0.03034372\n",
      "Iteration 46605, loss = 0.03035376\n",
      "Iteration 46606, loss = 0.03035240\n",
      "Iteration 46607, loss = 0.03034703\n",
      "Iteration 46608, loss = 0.03033897\n",
      "Iteration 46609, loss = 0.03033119\n",
      "Iteration 46610, loss = 0.03034402\n",
      "Iteration 46611, loss = 0.03034771\n",
      "Iteration 46612, loss = 0.03034356\n",
      "Iteration 46613, loss = 0.03034007\n",
      "Iteration 46614, loss = 0.03033204\n",
      "Iteration 46615, loss = 0.03034042\n",
      "Iteration 46616, loss = 0.03034416\n",
      "Iteration 46617, loss = 0.03035335\n",
      "Iteration 46618, loss = 0.03034238\n",
      "Iteration 46619, loss = 0.03033833\n",
      "Iteration 46620, loss = 0.03034763\n",
      "Iteration 46621, loss = 0.03035446\n",
      "Iteration 46622, loss = 0.03035919\n",
      "Iteration 46623, loss = 0.03035315\n",
      "Iteration 46624, loss = 0.03033678\n",
      "Iteration 46625, loss = 0.03034956\n",
      "Iteration 46626, loss = 0.03035571\n",
      "Iteration 46627, loss = 0.03035327\n",
      "Iteration 46628, loss = 0.03034667\n",
      "Iteration 46629, loss = 0.03033476\n",
      "Iteration 46630, loss = 0.03034586\n",
      "Iteration 46631, loss = 0.03034632\n",
      "Iteration 46632, loss = 0.03036100\n",
      "Iteration 46633, loss = 0.03037216\n",
      "Iteration 46634, loss = 0.03036841\n",
      "Iteration 46635, loss = 0.03035719\n",
      "Iteration 46636, loss = 0.03033825\n",
      "Iteration 46637, loss = 0.03033600\n",
      "Iteration 46638, loss = 0.03033855\n",
      "Iteration 46639, loss = 0.03034041\n",
      "Iteration 46640, loss = 0.03034037\n",
      "Iteration 46641, loss = 0.03033102\n",
      "Iteration 46642, loss = 0.03032587\n",
      "Iteration 46643, loss = 0.03032743\n",
      "Iteration 46644, loss = 0.03032804\n",
      "Iteration 46645, loss = 0.03033233\n",
      "Iteration 46646, loss = 0.03032686\n",
      "Iteration 46647, loss = 0.03032363\n",
      "Iteration 46648, loss = 0.03031730\n",
      "Iteration 46649, loss = 0.03031665\n",
      "Iteration 46650, loss = 0.03031519\n",
      "Iteration 46651, loss = 0.03031562\n",
      "Iteration 46652, loss = 0.03032596\n",
      "Iteration 46653, loss = 0.03032460\n",
      "Iteration 46654, loss = 0.03031679\n",
      "Iteration 46655, loss = 0.03032291\n",
      "Iteration 46656, loss = 0.03032622\n",
      "Iteration 46657, loss = 0.03031845\n",
      "Iteration 46658, loss = 0.03031414\n",
      "Iteration 46659, loss = 0.03031300\n",
      "Iteration 46660, loss = 0.03031184\n",
      "Iteration 46661, loss = 0.03031344\n",
      "Iteration 46662, loss = 0.03031225\n",
      "Iteration 46663, loss = 0.03031190\n",
      "Iteration 46664, loss = 0.03031177\n",
      "Iteration 46665, loss = 0.03032332\n",
      "Iteration 46666, loss = 0.03031673\n",
      "Iteration 46667, loss = 0.03031635\n",
      "Iteration 46668, loss = 0.03032868\n",
      "Iteration 46669, loss = 0.03032718\n",
      "Iteration 46670, loss = 0.03033857\n",
      "Iteration 46671, loss = 0.03033762\n",
      "Iteration 46672, loss = 0.03033031\n",
      "Iteration 46673, loss = 0.03033427\n",
      "Iteration 46674, loss = 0.03032761\n",
      "Iteration 46675, loss = 0.03031922\n",
      "Iteration 46676, loss = 0.03031307\n",
      "Iteration 46677, loss = 0.03031548\n",
      "Iteration 46678, loss = 0.03031742\n",
      "Iteration 46679, loss = 0.03032594\n",
      "Iteration 46680, loss = 0.03032976\n",
      "Iteration 46681, loss = 0.03032436\n",
      "Iteration 46682, loss = 0.03031824\n",
      "Iteration 46683, loss = 0.03030691\n",
      "Iteration 46684, loss = 0.03031221\n",
      "Iteration 46685, loss = 0.03032070\n",
      "Iteration 46686, loss = 0.03031825\n",
      "Iteration 46687, loss = 0.03030894\n",
      "Iteration 46688, loss = 0.03031021\n",
      "Iteration 46689, loss = 0.03031726\n",
      "Iteration 46690, loss = 0.03032412\n",
      "Iteration 46691, loss = 0.03031741\n",
      "Iteration 46692, loss = 0.03030966\n",
      "Iteration 46693, loss = 0.03031310\n",
      "Iteration 46694, loss = 0.03031237\n",
      "Iteration 46695, loss = 0.03032404\n",
      "Iteration 46696, loss = 0.03031825\n",
      "Iteration 46697, loss = 0.03030452\n",
      "Iteration 46698, loss = 0.03031593\n",
      "Iteration 46699, loss = 0.03031714\n",
      "Iteration 46700, loss = 0.03030946\n",
      "Iteration 46701, loss = 0.03030728\n",
      "Iteration 46702, loss = 0.03031015\n",
      "Iteration 46703, loss = 0.03031346\n",
      "Iteration 46704, loss = 0.03032161\n",
      "Iteration 46705, loss = 0.03031861\n",
      "Iteration 46706, loss = 0.03031323\n",
      "Iteration 46707, loss = 0.03029752\n",
      "Iteration 46708, loss = 0.03031349\n",
      "Iteration 46709, loss = 0.03032754\n",
      "Iteration 46710, loss = 0.03032431\n",
      "Iteration 46711, loss = 0.03031163\n",
      "Iteration 46712, loss = 0.03029791\n",
      "Iteration 46713, loss = 0.03030212\n",
      "Iteration 46714, loss = 0.03029967\n",
      "Iteration 46715, loss = 0.03029597\n",
      "Iteration 46716, loss = 0.03029106\n",
      "Iteration 46717, loss = 0.03029900\n",
      "Iteration 46718, loss = 0.03029208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 46719, loss = 0.03029825\n",
      "Iteration 46720, loss = 0.03029721\n",
      "Iteration 46721, loss = 0.03029690\n",
      "Iteration 46722, loss = 0.03029603\n",
      "Iteration 46723, loss = 0.03029778\n",
      "Iteration 46724, loss = 0.03029908\n",
      "Iteration 46725, loss = 0.03029121\n",
      "Iteration 46726, loss = 0.03028864\n",
      "Iteration 46727, loss = 0.03029219\n",
      "Iteration 46728, loss = 0.03030754\n",
      "Iteration 46729, loss = 0.03030711\n",
      "Iteration 46730, loss = 0.03029046\n",
      "Iteration 46731, loss = 0.03030084\n",
      "Iteration 46732, loss = 0.03029804\n",
      "Iteration 46733, loss = 0.03030397\n",
      "Iteration 46734, loss = 0.03030559\n",
      "Iteration 46735, loss = 0.03029642\n",
      "Iteration 46736, loss = 0.03029469\n",
      "Iteration 46737, loss = 0.03030142\n",
      "Iteration 46738, loss = 0.03031520\n",
      "Iteration 46739, loss = 0.03030605\n",
      "Iteration 46740, loss = 0.03030891\n",
      "Iteration 46741, loss = 0.03031870\n",
      "Iteration 46742, loss = 0.03032191\n",
      "Iteration 46743, loss = 0.03031357\n",
      "Iteration 46744, loss = 0.03030459\n",
      "Iteration 46745, loss = 0.03030669\n",
      "Iteration 46746, loss = 0.03031884\n",
      "Iteration 46747, loss = 0.03030938\n",
      "Iteration 46748, loss = 0.03030969\n",
      "Iteration 46749, loss = 0.03030562\n",
      "Iteration 46750, loss = 0.03029442\n",
      "Iteration 46751, loss = 0.03029812\n",
      "Iteration 46752, loss = 0.03029626\n",
      "Iteration 46753, loss = 0.03028508\n",
      "Iteration 46754, loss = 0.03027967\n",
      "Iteration 46755, loss = 0.03028561\n",
      "Iteration 46756, loss = 0.03029128\n",
      "Iteration 46757, loss = 0.03029010\n",
      "Iteration 46758, loss = 0.03028002\n",
      "Iteration 46759, loss = 0.03027266\n",
      "Iteration 46760, loss = 0.03028594\n",
      "Iteration 46761, loss = 0.03029332\n",
      "Iteration 46762, loss = 0.03029165\n",
      "Iteration 46763, loss = 0.03028898\n",
      "Iteration 46764, loss = 0.03027761\n",
      "Iteration 46765, loss = 0.03027976\n",
      "Iteration 46766, loss = 0.03028677\n",
      "Iteration 46767, loss = 0.03027360\n",
      "Iteration 46768, loss = 0.03027213\n",
      "Iteration 46769, loss = 0.03028454\n",
      "Iteration 46770, loss = 0.03029459\n",
      "Iteration 46771, loss = 0.03028245\n",
      "Iteration 46772, loss = 0.03027258\n",
      "Iteration 46773, loss = 0.03029488\n",
      "Iteration 46774, loss = 0.03030450\n",
      "Iteration 46775, loss = 0.03030718\n",
      "Iteration 46776, loss = 0.03029817\n",
      "Iteration 46777, loss = 0.03029070\n",
      "Iteration 46778, loss = 0.03027941\n",
      "Iteration 46779, loss = 0.03027577\n",
      "Iteration 46780, loss = 0.03028519\n",
      "Iteration 46781, loss = 0.03028374\n",
      "Iteration 46782, loss = 0.03027220\n",
      "Iteration 46783, loss = 0.03027255\n",
      "Iteration 46784, loss = 0.03027882\n",
      "Iteration 46785, loss = 0.03028109\n",
      "Iteration 46786, loss = 0.03027751\n",
      "Iteration 46787, loss = 0.03027760\n",
      "Iteration 46788, loss = 0.03027729\n",
      "Iteration 46789, loss = 0.03027897\n",
      "Iteration 46790, loss = 0.03027287\n",
      "Iteration 46791, loss = 0.03026484\n",
      "Iteration 46792, loss = 0.03027942\n",
      "Iteration 46793, loss = 0.03027469\n",
      "Iteration 46794, loss = 0.03025938\n",
      "Iteration 46795, loss = 0.03027022\n",
      "Iteration 46796, loss = 0.03028223\n",
      "Iteration 46797, loss = 0.03028727\n",
      "Iteration 46798, loss = 0.03028219\n",
      "Iteration 46799, loss = 0.03028237\n",
      "Iteration 46800, loss = 0.03027547\n",
      "Iteration 46801, loss = 0.03026916\n",
      "Iteration 46802, loss = 0.03027802\n",
      "Iteration 46803, loss = 0.03027741\n",
      "Iteration 46804, loss = 0.03028586\n",
      "Iteration 46805, loss = 0.03028596\n",
      "Iteration 46806, loss = 0.03028151\n",
      "Iteration 46807, loss = 0.03028496\n",
      "Iteration 46808, loss = 0.03028063\n",
      "Iteration 46809, loss = 0.03027446\n",
      "Iteration 46810, loss = 0.03026595\n",
      "Iteration 46811, loss = 0.03026578\n",
      "Iteration 46812, loss = 0.03027637\n",
      "Iteration 46813, loss = 0.03028188\n",
      "Iteration 46814, loss = 0.03027934\n",
      "Iteration 46815, loss = 0.03026183\n",
      "Iteration 46816, loss = 0.03027442\n",
      "Iteration 46817, loss = 0.03028022\n",
      "Iteration 46818, loss = 0.03028137\n",
      "Iteration 46819, loss = 0.03028125\n",
      "Iteration 46820, loss = 0.03027247\n",
      "Iteration 46821, loss = 0.03025532\n",
      "Iteration 46822, loss = 0.03026829\n",
      "Iteration 46823, loss = 0.03026083\n",
      "Iteration 46824, loss = 0.03026322\n",
      "Iteration 46825, loss = 0.03026746\n",
      "Iteration 46826, loss = 0.03025985\n",
      "Iteration 46827, loss = 0.03025767\n",
      "Iteration 46828, loss = 0.03026943\n",
      "Iteration 46829, loss = 0.03026975\n",
      "Iteration 46830, loss = 0.03025553\n",
      "Iteration 46831, loss = 0.03025734\n",
      "Iteration 46832, loss = 0.03026321\n",
      "Iteration 46833, loss = 0.03026152\n",
      "Iteration 46834, loss = 0.03025522\n",
      "Iteration 46835, loss = 0.03025632\n",
      "Iteration 46836, loss = 0.03025745\n",
      "Iteration 46837, loss = 0.03026014\n",
      "Iteration 46838, loss = 0.03026768\n",
      "Iteration 46839, loss = 0.03026500\n",
      "Iteration 46840, loss = 0.03026246\n",
      "Iteration 46841, loss = 0.03025493\n",
      "Iteration 46842, loss = 0.03024910\n",
      "Iteration 46843, loss = 0.03025539\n",
      "Iteration 46844, loss = 0.03025648\n",
      "Iteration 46845, loss = 0.03025178\n",
      "Iteration 46846, loss = 0.03025046\n",
      "Iteration 46847, loss = 0.03025781\n",
      "Iteration 46848, loss = 0.03026611\n",
      "Iteration 46849, loss = 0.03026251\n",
      "Iteration 46850, loss = 0.03025425\n",
      "Iteration 46851, loss = 0.03025502\n",
      "Iteration 46852, loss = 0.03026980\n",
      "Iteration 46853, loss = 0.03027365\n",
      "Iteration 46854, loss = 0.03026452\n",
      "Iteration 46855, loss = 0.03024657\n",
      "Iteration 46856, loss = 0.03024261\n",
      "Iteration 46857, loss = 0.03024626\n",
      "Iteration 46858, loss = 0.03023878\n",
      "Iteration 46859, loss = 0.03023832\n",
      "Iteration 46860, loss = 0.03024227\n",
      "Iteration 46861, loss = 0.03024965\n",
      "Iteration 46862, loss = 0.03024001\n",
      "Iteration 46863, loss = 0.03024009\n",
      "Iteration 46864, loss = 0.03024556\n",
      "Iteration 46865, loss = 0.03024741\n",
      "Iteration 46866, loss = 0.03024692\n",
      "Iteration 46867, loss = 0.03023621\n",
      "Iteration 46868, loss = 0.03024848\n",
      "Iteration 46869, loss = 0.03025439\n",
      "Iteration 46870, loss = 0.03023934\n",
      "Iteration 46871, loss = 0.03024757\n",
      "Iteration 46872, loss = 0.03026431\n",
      "Iteration 46873, loss = 0.03027136\n",
      "Iteration 46874, loss = 0.03026210\n",
      "Iteration 46875, loss = 0.03025382\n",
      "Iteration 46876, loss = 0.03025885\n",
      "Iteration 46877, loss = 0.03025736\n",
      "Iteration 46878, loss = 0.03026882\n",
      "Iteration 46879, loss = 0.03026238\n",
      "Iteration 46880, loss = 0.03025819\n",
      "Iteration 46881, loss = 0.03024720\n",
      "Iteration 46882, loss = 0.03025642\n",
      "Iteration 46883, loss = 0.03025652\n",
      "Iteration 46884, loss = 0.03024852\n",
      "Iteration 46885, loss = 0.03023423\n",
      "Iteration 46886, loss = 0.03025310\n",
      "Iteration 46887, loss = 0.03026317\n",
      "Iteration 46888, loss = 0.03025310\n",
      "Iteration 46889, loss = 0.03024759\n",
      "Iteration 46890, loss = 0.03024254\n",
      "Iteration 46891, loss = 0.03025287\n",
      "Iteration 46892, loss = 0.03025285\n",
      "Iteration 46893, loss = 0.03024949\n",
      "Iteration 46894, loss = 0.03023949\n",
      "Iteration 46895, loss = 0.03023078\n",
      "Iteration 46896, loss = 0.03023780\n",
      "Iteration 46897, loss = 0.03023991\n",
      "Iteration 46898, loss = 0.03023271\n",
      "Iteration 46899, loss = 0.03023650\n",
      "Iteration 46900, loss = 0.03023961\n",
      "Iteration 46901, loss = 0.03023978\n",
      "Iteration 46902, loss = 0.03024838\n",
      "Iteration 46903, loss = 0.03024699\n",
      "Iteration 46904, loss = 0.03024492\n",
      "Iteration 46905, loss = 0.03023748\n",
      "Iteration 46906, loss = 0.03022253\n",
      "Iteration 46907, loss = 0.03024807\n",
      "Iteration 46908, loss = 0.03025548\n",
      "Iteration 46909, loss = 0.03024503\n",
      "Iteration 46910, loss = 0.03022574\n",
      "Iteration 46911, loss = 0.03023575\n",
      "Iteration 46912, loss = 0.03024238\n",
      "Iteration 46913, loss = 0.03024524\n",
      "Iteration 46914, loss = 0.03023722\n",
      "Iteration 46915, loss = 0.03022376\n",
      "Iteration 46916, loss = 0.03023427\n",
      "Iteration 46917, loss = 0.03023999\n",
      "Iteration 46918, loss = 0.03022467\n",
      "Iteration 46919, loss = 0.03022712\n",
      "Iteration 46920, loss = 0.03023746\n",
      "Iteration 46921, loss = 0.03023579\n",
      "Iteration 46922, loss = 0.03022922\n",
      "Iteration 46923, loss = 0.03022683\n",
      "Iteration 46924, loss = 0.03023489\n",
      "Iteration 46925, loss = 0.03023429\n",
      "Iteration 46926, loss = 0.03023111\n",
      "Iteration 46927, loss = 0.03022764\n",
      "Iteration 46928, loss = 0.03023152\n",
      "Iteration 46929, loss = 0.03022721\n",
      "Iteration 46930, loss = 0.03022232\n",
      "Iteration 46931, loss = 0.03022667\n",
      "Iteration 46932, loss = 0.03022829\n",
      "Iteration 46933, loss = 0.03023448\n",
      "Iteration 46934, loss = 0.03023166\n",
      "Iteration 46935, loss = 0.03022316\n",
      "Iteration 46936, loss = 0.03022881\n",
      "Iteration 46937, loss = 0.03022595\n",
      "Iteration 46938, loss = 0.03022118\n",
      "Iteration 46939, loss = 0.03021654\n",
      "Iteration 46940, loss = 0.03021278\n",
      "Iteration 46941, loss = 0.03023129\n",
      "Iteration 46942, loss = 0.03022840\n",
      "Iteration 46943, loss = 0.03022656\n",
      "Iteration 46944, loss = 0.03022824\n",
      "Iteration 46945, loss = 0.03023151\n",
      "Iteration 46946, loss = 0.03022203\n",
      "Iteration 46947, loss = 0.03023241\n",
      "Iteration 46948, loss = 0.03024473\n",
      "Iteration 46949, loss = 0.03023659\n",
      "Iteration 46950, loss = 0.03021815\n",
      "Iteration 46951, loss = 0.03021824\n",
      "Iteration 46952, loss = 0.03021224\n",
      "Iteration 46953, loss = 0.03022193\n",
      "Iteration 46954, loss = 0.03021876\n",
      "Iteration 46955, loss = 0.03022285\n",
      "Iteration 46956, loss = 0.03022520\n",
      "Iteration 46957, loss = 0.03022183\n",
      "Iteration 46958, loss = 0.03020776\n",
      "Iteration 46959, loss = 0.03020619\n",
      "Iteration 46960, loss = 0.03021523\n",
      "Iteration 46961, loss = 0.03021109\n",
      "Iteration 46962, loss = 0.03021066\n",
      "Iteration 46963, loss = 0.03020752\n",
      "Iteration 46964, loss = 0.03021387\n",
      "Iteration 46965, loss = 0.03020746\n",
      "Iteration 46966, loss = 0.03020421\n",
      "Iteration 46967, loss = 0.03020979\n",
      "Iteration 46968, loss = 0.03021547\n",
      "Iteration 46969, loss = 0.03022043\n",
      "Iteration 46970, loss = 0.03021031\n",
      "Iteration 46971, loss = 0.03021508\n",
      "Iteration 46972, loss = 0.03021365\n",
      "Iteration 46973, loss = 0.03021240\n",
      "Iteration 46974, loss = 0.03021424\n",
      "Iteration 46975, loss = 0.03022150\n",
      "Iteration 46976, loss = 0.03022643\n",
      "Iteration 46977, loss = 0.03023075\n",
      "Iteration 46978, loss = 0.03022240\n",
      "Iteration 46979, loss = 0.03020968\n",
      "Iteration 46980, loss = 0.03020601\n",
      "Iteration 46981, loss = 0.03022578\n",
      "Iteration 46982, loss = 0.03023554\n",
      "Iteration 46983, loss = 0.03022790\n",
      "Iteration 46984, loss = 0.03021009\n",
      "Iteration 46985, loss = 0.03021402\n",
      "Iteration 46986, loss = 0.03020618\n",
      "Iteration 46987, loss = 0.03019780\n",
      "Iteration 46988, loss = 0.03020994\n",
      "Iteration 46989, loss = 0.03021986\n",
      "Iteration 46990, loss = 0.03021858\n",
      "Iteration 46991, loss = 0.03020268\n",
      "Iteration 46992, loss = 0.03021222\n",
      "Iteration 46993, loss = 0.03021587\n",
      "Iteration 46994, loss = 0.03021124\n",
      "Iteration 46995, loss = 0.03021479\n",
      "Iteration 46996, loss = 0.03020917\n",
      "Iteration 46997, loss = 0.03020462\n",
      "Iteration 46998, loss = 0.03019928\n",
      "Iteration 46999, loss = 0.03020628\n",
      "Iteration 47000, loss = 0.03021437\n",
      "Iteration 47001, loss = 0.03020972\n",
      "Iteration 47002, loss = 0.03020489\n",
      "Iteration 47003, loss = 0.03020295\n",
      "Iteration 47004, loss = 0.03021752\n",
      "Iteration 47005, loss = 0.03022169\n",
      "Iteration 47006, loss = 0.03020908\n",
      "Iteration 47007, loss = 0.03019510\n",
      "Iteration 47008, loss = 0.03019704\n",
      "Iteration 47009, loss = 0.03018856\n",
      "Iteration 47010, loss = 0.03019270\n",
      "Iteration 47011, loss = 0.03018901\n",
      "Iteration 47012, loss = 0.03019000\n",
      "Iteration 47013, loss = 0.03020897\n",
      "Iteration 47014, loss = 0.03020817\n",
      "Iteration 47015, loss = 0.03019190\n",
      "Iteration 47016, loss = 0.03018750\n",
      "Iteration 47017, loss = 0.03018763\n",
      "Iteration 47018, loss = 0.03019780\n",
      "Iteration 47019, loss = 0.03019507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 47020, loss = 0.03019282\n",
      "Iteration 47021, loss = 0.03019166\n",
      "Iteration 47022, loss = 0.03018955\n",
      "Iteration 47023, loss = 0.03019010\n",
      "Iteration 47024, loss = 0.03018876\n",
      "Iteration 47025, loss = 0.03018731\n",
      "Iteration 47026, loss = 0.03019106\n",
      "Iteration 47027, loss = 0.03018904\n",
      "Iteration 47028, loss = 0.03019110\n",
      "Iteration 47029, loss = 0.03018747\n",
      "Iteration 47030, loss = 0.03018927\n",
      "Iteration 47031, loss = 0.03019907\n",
      "Iteration 47032, loss = 0.03019112\n",
      "Iteration 47033, loss = 0.03017603\n",
      "Iteration 47034, loss = 0.03019919\n",
      "Iteration 47035, loss = 0.03019815\n",
      "Iteration 47036, loss = 0.03018906\n",
      "Iteration 47037, loss = 0.03018847\n",
      "Iteration 47038, loss = 0.03019438\n",
      "Iteration 47039, loss = 0.03019210\n",
      "Iteration 47040, loss = 0.03019548\n",
      "Iteration 47041, loss = 0.03019752\n",
      "Iteration 47042, loss = 0.03019621\n",
      "Iteration 47043, loss = 0.03019207\n",
      "Iteration 47044, loss = 0.03018771\n",
      "Iteration 47045, loss = 0.03018536\n",
      "Iteration 47046, loss = 0.03017821\n",
      "Iteration 47047, loss = 0.03019503\n",
      "Iteration 47048, loss = 0.03019629\n",
      "Iteration 47049, loss = 0.03018890\n",
      "Iteration 47050, loss = 0.03018271\n",
      "Iteration 47051, loss = 0.03017884\n",
      "Iteration 47052, loss = 0.03019877\n",
      "Iteration 47053, loss = 0.03020732\n",
      "Iteration 47054, loss = 0.03020205\n",
      "Iteration 47055, loss = 0.03018760\n",
      "Iteration 47056, loss = 0.03018051\n",
      "Iteration 47057, loss = 0.03018683\n",
      "Iteration 47058, loss = 0.03018543\n",
      "Iteration 47059, loss = 0.03019035\n",
      "Iteration 47060, loss = 0.03018560\n",
      "Iteration 47061, loss = 0.03018556\n",
      "Iteration 47062, loss = 0.03019291\n",
      "Iteration 47063, loss = 0.03018760\n",
      "Iteration 47064, loss = 0.03017333\n",
      "Iteration 47065, loss = 0.03017390\n",
      "Iteration 47066, loss = 0.03017685\n",
      "Iteration 47067, loss = 0.03016773\n",
      "Iteration 47068, loss = 0.03017257\n",
      "Iteration 47069, loss = 0.03018047\n",
      "Iteration 47070, loss = 0.03018409\n",
      "Iteration 47071, loss = 0.03017186\n",
      "Iteration 47072, loss = 0.03017459\n",
      "Iteration 47073, loss = 0.03018723\n",
      "Iteration 47074, loss = 0.03018589\n",
      "Iteration 47075, loss = 0.03018004\n",
      "Iteration 47076, loss = 0.03017398\n",
      "Iteration 47077, loss = 0.03017104\n",
      "Iteration 47078, loss = 0.03018616\n",
      "Iteration 47079, loss = 0.03018391\n",
      "Iteration 47080, loss = 0.03018716\n",
      "Iteration 47081, loss = 0.03018043\n",
      "Iteration 47082, loss = 0.03018620\n",
      "Iteration 47083, loss = 0.03019131\n",
      "Iteration 47084, loss = 0.03018374\n",
      "Iteration 47085, loss = 0.03017741\n",
      "Iteration 47086, loss = 0.03017413\n",
      "Iteration 47087, loss = 0.03018430\n",
      "Iteration 47088, loss = 0.03018437\n",
      "Iteration 47089, loss = 0.03018311\n",
      "Iteration 47090, loss = 0.03016923\n",
      "Iteration 47091, loss = 0.03017293\n",
      "Iteration 47092, loss = 0.03017507\n",
      "Iteration 47093, loss = 0.03019124\n",
      "Iteration 47094, loss = 0.03019323\n",
      "Iteration 47095, loss = 0.03019604\n",
      "Iteration 47096, loss = 0.03019503\n",
      "Iteration 47097, loss = 0.03018372\n",
      "Iteration 47098, loss = 0.03016579\n",
      "Iteration 47099, loss = 0.03016750\n",
      "Iteration 47100, loss = 0.03018497\n",
      "Iteration 47101, loss = 0.03018090\n",
      "Iteration 47102, loss = 0.03017118\n",
      "Iteration 47103, loss = 0.03017045\n",
      "Iteration 47104, loss = 0.03016622\n",
      "Iteration 47105, loss = 0.03016906\n",
      "Iteration 47106, loss = 0.03017480\n",
      "Iteration 47107, loss = 0.03017660\n",
      "Iteration 47108, loss = 0.03017124\n",
      "Iteration 47109, loss = 0.03015846\n",
      "Iteration 47110, loss = 0.03015658\n",
      "Iteration 47111, loss = 0.03016929\n",
      "Iteration 47112, loss = 0.03016800\n",
      "Iteration 47113, loss = 0.03016438\n",
      "Iteration 47114, loss = 0.03015995\n",
      "Iteration 47115, loss = 0.03015414\n",
      "Iteration 47116, loss = 0.03016044\n",
      "Iteration 47117, loss = 0.03017400\n",
      "Iteration 47118, loss = 0.03017628\n",
      "Iteration 47119, loss = 0.03017070\n",
      "Iteration 47120, loss = 0.03015767\n",
      "Iteration 47121, loss = 0.03015459\n",
      "Iteration 47122, loss = 0.03015174\n",
      "Iteration 47123, loss = 0.03016516\n",
      "Iteration 47124, loss = 0.03017142\n",
      "Iteration 47125, loss = 0.03016143\n",
      "Iteration 47126, loss = 0.03015230\n",
      "Iteration 47127, loss = 0.03015776\n",
      "Iteration 47128, loss = 0.03015797\n",
      "Iteration 47129, loss = 0.03015476\n",
      "Iteration 47130, loss = 0.03015580\n",
      "Iteration 47131, loss = 0.03016083\n",
      "Iteration 47132, loss = 0.03015728\n",
      "Iteration 47133, loss = 0.03015036\n",
      "Iteration 47134, loss = 0.03015244\n",
      "Iteration 47135, loss = 0.03015339\n",
      "Iteration 47136, loss = 0.03014581\n",
      "Iteration 47137, loss = 0.03014587\n",
      "Iteration 47138, loss = 0.03014572\n",
      "Iteration 47139, loss = 0.03014534\n",
      "Iteration 47140, loss = 0.03014732\n",
      "Iteration 47141, loss = 0.03014417\n",
      "Iteration 47142, loss = 0.03014455\n",
      "Iteration 47143, loss = 0.03014868\n",
      "Iteration 47144, loss = 0.03014866\n",
      "Iteration 47145, loss = 0.03014494\n",
      "Iteration 47146, loss = 0.03014592\n",
      "Iteration 47147, loss = 0.03013867\n",
      "Iteration 47148, loss = 0.03015075\n",
      "Iteration 47149, loss = 0.03015258\n",
      "Iteration 47150, loss = 0.03014759\n",
      "Iteration 47151, loss = 0.03014416\n",
      "Iteration 47152, loss = 0.03014645\n",
      "Iteration 47153, loss = 0.03014319\n",
      "Iteration 47154, loss = 0.03014173\n",
      "Iteration 47155, loss = 0.03013918\n",
      "Iteration 47156, loss = 0.03013514\n",
      "Iteration 47157, loss = 0.03013941\n",
      "Iteration 47158, loss = 0.03014773\n",
      "Iteration 47159, loss = 0.03014719\n",
      "Iteration 47160, loss = 0.03013452\n",
      "Iteration 47161, loss = 0.03013725\n",
      "Iteration 47162, loss = 0.03015496\n",
      "Iteration 47163, loss = 0.03015768\n",
      "Iteration 47164, loss = 0.03014569\n",
      "Iteration 47165, loss = 0.03013637\n",
      "Iteration 47166, loss = 0.03014813\n",
      "Iteration 47167, loss = 0.03016041\n",
      "Iteration 47168, loss = 0.03015871\n",
      "Iteration 47169, loss = 0.03014338\n",
      "Iteration 47170, loss = 0.03014685\n",
      "Iteration 47171, loss = 0.03015046\n",
      "Iteration 47172, loss = 0.03015219\n",
      "Iteration 47173, loss = 0.03014557\n",
      "Iteration 47174, loss = 0.03014772\n",
      "Iteration 47175, loss = 0.03014380\n",
      "Iteration 47176, loss = 0.03015879\n",
      "Iteration 47177, loss = 0.03016385\n",
      "Iteration 47178, loss = 0.03015999\n",
      "Iteration 47179, loss = 0.03014592\n",
      "Iteration 47180, loss = 0.03012625\n",
      "Iteration 47181, loss = 0.03014011\n",
      "Iteration 47182, loss = 0.03013716\n",
      "Iteration 47183, loss = 0.03012858\n",
      "Iteration 47184, loss = 0.03012635\n",
      "Iteration 47185, loss = 0.03014019\n",
      "Iteration 47186, loss = 0.03014247\n",
      "Iteration 47187, loss = 0.03014596\n",
      "Iteration 47188, loss = 0.03013986\n",
      "Iteration 47189, loss = 0.03013762\n",
      "Iteration 47190, loss = 0.03014078\n",
      "Iteration 47191, loss = 0.03013396\n",
      "Iteration 47192, loss = 0.03014151\n",
      "Iteration 47193, loss = 0.03015139\n",
      "Iteration 47194, loss = 0.03014400\n",
      "Iteration 47195, loss = 0.03013946\n",
      "Iteration 47196, loss = 0.03013649\n",
      "Iteration 47197, loss = 0.03014533\n",
      "Iteration 47198, loss = 0.03015657\n",
      "Iteration 47199, loss = 0.03015250\n",
      "Iteration 47200, loss = 0.03013140\n",
      "Iteration 47201, loss = 0.03012824\n",
      "Iteration 47202, loss = 0.03015462\n",
      "Iteration 47203, loss = 0.03016074\n",
      "Iteration 47204, loss = 0.03015195\n",
      "Iteration 47205, loss = 0.03012745\n",
      "Iteration 47206, loss = 0.03013118\n",
      "Iteration 47207, loss = 0.03014563\n",
      "Iteration 47208, loss = 0.03013879\n",
      "Iteration 47209, loss = 0.03014657\n",
      "Iteration 47210, loss = 0.03015173\n",
      "Iteration 47211, loss = 0.03015078\n",
      "Iteration 47212, loss = 0.03013523\n",
      "Iteration 47213, loss = 0.03012345\n",
      "Iteration 47214, loss = 0.03013544\n",
      "Iteration 47215, loss = 0.03013740\n",
      "Iteration 47216, loss = 0.03012803\n",
      "Iteration 47217, loss = 0.03012871\n",
      "Iteration 47218, loss = 0.03013678\n",
      "Iteration 47219, loss = 0.03012817\n",
      "Iteration 47220, loss = 0.03012228\n",
      "Iteration 47221, loss = 0.03011605\n",
      "Iteration 47222, loss = 0.03011890\n",
      "Iteration 47223, loss = 0.03012036\n",
      "Iteration 47224, loss = 0.03011300\n",
      "Iteration 47225, loss = 0.03012430\n",
      "Iteration 47226, loss = 0.03012511\n",
      "Iteration 47227, loss = 0.03013159\n",
      "Iteration 47228, loss = 0.03013581\n",
      "Iteration 47229, loss = 0.03012906\n",
      "Iteration 47230, loss = 0.03012362\n",
      "Iteration 47231, loss = 0.03012768\n",
      "Iteration 47232, loss = 0.03014135\n",
      "Iteration 47233, loss = 0.03014249\n",
      "Iteration 47234, loss = 0.03012482\n",
      "Iteration 47235, loss = 0.03012344\n",
      "Iteration 47236, loss = 0.03012905\n",
      "Iteration 47237, loss = 0.03013163\n",
      "Iteration 47238, loss = 0.03012296\n",
      "Iteration 47239, loss = 0.03012255\n",
      "Iteration 47240, loss = 0.03012515\n",
      "Iteration 47241, loss = 0.03013092\n",
      "Iteration 47242, loss = 0.03012380\n",
      "Iteration 47243, loss = 0.03011350\n",
      "Iteration 47244, loss = 0.03010132\n",
      "Iteration 47245, loss = 0.03012177\n",
      "Iteration 47246, loss = 0.03013301\n",
      "Iteration 47247, loss = 0.03012693\n",
      "Iteration 47248, loss = 0.03011978\n",
      "Iteration 47249, loss = 0.03010125\n",
      "Iteration 47250, loss = 0.03010747\n",
      "Iteration 47251, loss = 0.03011944\n",
      "Iteration 47252, loss = 0.03011335\n",
      "Iteration 47253, loss = 0.03010400\n",
      "Iteration 47254, loss = 0.03010908\n",
      "Iteration 47255, loss = 0.03011163\n",
      "Iteration 47256, loss = 0.03010698\n",
      "Iteration 47257, loss = 0.03009913\n",
      "Iteration 47258, loss = 0.03011521\n",
      "Iteration 47259, loss = 0.03011466\n",
      "Iteration 47260, loss = 0.03010144\n",
      "Iteration 47261, loss = 0.03011068\n",
      "Iteration 47262, loss = 0.03011500\n",
      "Iteration 47263, loss = 0.03011233\n",
      "Iteration 47264, loss = 0.03011424\n",
      "Iteration 47265, loss = 0.03010583\n",
      "Iteration 47266, loss = 0.03010777\n",
      "Iteration 47267, loss = 0.03011503\n",
      "Iteration 47268, loss = 0.03011271\n",
      "Iteration 47269, loss = 0.03010001\n",
      "Iteration 47270, loss = 0.03011021\n",
      "Iteration 47271, loss = 0.03011685\n",
      "Iteration 47272, loss = 0.03011558\n",
      "Iteration 47273, loss = 0.03010758\n",
      "Iteration 47274, loss = 0.03011069\n",
      "Iteration 47275, loss = 0.03010354\n",
      "Iteration 47276, loss = 0.03012811\n",
      "Iteration 47277, loss = 0.03012951\n",
      "Iteration 47278, loss = 0.03012021\n",
      "Iteration 47279, loss = 0.03010956\n",
      "Iteration 47280, loss = 0.03010475\n",
      "Iteration 47281, loss = 0.03011202\n",
      "Iteration 47282, loss = 0.03010847\n",
      "Iteration 47283, loss = 0.03009394\n",
      "Iteration 47284, loss = 0.03010038\n",
      "Iteration 47285, loss = 0.03010895\n",
      "Iteration 47286, loss = 0.03011346\n",
      "Iteration 47287, loss = 0.03010926\n",
      "Iteration 47288, loss = 0.03009953\n",
      "Iteration 47289, loss = 0.03009601\n",
      "Iteration 47290, loss = 0.03009450\n",
      "Iteration 47291, loss = 0.03009547\n",
      "Iteration 47292, loss = 0.03010676\n",
      "Iteration 47293, loss = 0.03010568\n",
      "Iteration 47294, loss = 0.03010263\n",
      "Iteration 47295, loss = 0.03010802\n",
      "Iteration 47296, loss = 0.03009847\n",
      "Iteration 47297, loss = 0.03009447\n",
      "Iteration 47298, loss = 0.03009943\n",
      "Iteration 47299, loss = 0.03010258\n",
      "Iteration 47300, loss = 0.03010310\n",
      "Iteration 47301, loss = 0.03010879\n",
      "Iteration 47302, loss = 0.03010307\n",
      "Iteration 47303, loss = 0.03009478\n",
      "Iteration 47304, loss = 0.03008958\n",
      "Iteration 47305, loss = 0.03010168\n",
      "Iteration 47306, loss = 0.03009515\n",
      "Iteration 47307, loss = 0.03009749\n",
      "Iteration 47308, loss = 0.03009706\n",
      "Iteration 47309, loss = 0.03009657\n",
      "Iteration 47310, loss = 0.03009235\n",
      "Iteration 47311, loss = 0.03009056\n",
      "Iteration 47312, loss = 0.03008842\n",
      "Iteration 47313, loss = 0.03007907\n",
      "Iteration 47314, loss = 0.03008571\n",
      "Iteration 47315, loss = 0.03008507\n",
      "Iteration 47316, loss = 0.03008513\n",
      "Iteration 47317, loss = 0.03007511\n",
      "Iteration 47318, loss = 0.03009202\n",
      "Iteration 47319, loss = 0.03009222\n",
      "Iteration 47320, loss = 0.03008592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 47321, loss = 0.03008971\n",
      "Iteration 47322, loss = 0.03009650\n",
      "Iteration 47323, loss = 0.03008964\n",
      "Iteration 47324, loss = 0.03007975\n",
      "Iteration 47325, loss = 0.03009566\n",
      "Iteration 47326, loss = 0.03009598\n",
      "Iteration 47327, loss = 0.03008270\n",
      "Iteration 47328, loss = 0.03008274\n",
      "Iteration 47329, loss = 0.03009116\n",
      "Iteration 47330, loss = 0.03009267\n",
      "Iteration 47331, loss = 0.03009058\n",
      "Iteration 47332, loss = 0.03009443\n",
      "Iteration 47333, loss = 0.03009256\n",
      "Iteration 47334, loss = 0.03007551\n",
      "Iteration 47335, loss = 0.03009144\n",
      "Iteration 47336, loss = 0.03010322\n",
      "Iteration 47337, loss = 0.03009315\n",
      "Iteration 47338, loss = 0.03007413\n",
      "Iteration 47339, loss = 0.03007909\n",
      "Iteration 47340, loss = 0.03009116\n",
      "Iteration 47341, loss = 0.03009373\n",
      "Iteration 47342, loss = 0.03009507\n",
      "Iteration 47343, loss = 0.03009656\n",
      "Iteration 47344, loss = 0.03009141\n",
      "Iteration 47345, loss = 0.03008634\n",
      "Iteration 47346, loss = 0.03008370\n",
      "Iteration 47347, loss = 0.03007562\n",
      "Iteration 47348, loss = 0.03008243\n",
      "Iteration 47349, loss = 0.03009022\n",
      "Iteration 47350, loss = 0.03008706\n",
      "Iteration 47351, loss = 0.03008468\n",
      "Iteration 47352, loss = 0.03007821\n",
      "Iteration 47353, loss = 0.03007679\n",
      "Iteration 47354, loss = 0.03007617\n",
      "Iteration 47355, loss = 0.03007628\n",
      "Iteration 47356, loss = 0.03007029\n",
      "Iteration 47357, loss = 0.03006883\n",
      "Iteration 47358, loss = 0.03007712\n",
      "Iteration 47359, loss = 0.03007846\n",
      "Iteration 47360, loss = 0.03007366\n",
      "Iteration 47361, loss = 0.03006894\n",
      "Iteration 47362, loss = 0.03006372\n",
      "Iteration 47363, loss = 0.03007889\n",
      "Iteration 47364, loss = 0.03007643\n",
      "Iteration 47365, loss = 0.03007243\n",
      "Iteration 47366, loss = 0.03006633\n",
      "Iteration 47367, loss = 0.03007580\n",
      "Iteration 47368, loss = 0.03006704\n",
      "Iteration 47369, loss = 0.03006741\n",
      "Iteration 47370, loss = 0.03007700\n",
      "Iteration 47371, loss = 0.03007757\n",
      "Iteration 47372, loss = 0.03006801\n",
      "Iteration 47373, loss = 0.03006178\n",
      "Iteration 47374, loss = 0.03006079\n",
      "Iteration 47375, loss = 0.03006456\n",
      "Iteration 47376, loss = 0.03005908\n",
      "Iteration 47377, loss = 0.03006893\n",
      "Iteration 47378, loss = 0.03007205\n",
      "Iteration 47379, loss = 0.03006345\n",
      "Iteration 47380, loss = 0.03006157\n",
      "Iteration 47381, loss = 0.03006264\n",
      "Iteration 47382, loss = 0.03005927\n",
      "Iteration 47383, loss = 0.03006523\n",
      "Iteration 47384, loss = 0.03006355\n",
      "Iteration 47385, loss = 0.03006440\n",
      "Iteration 47386, loss = 0.03006591\n",
      "Iteration 47387, loss = 0.03007055\n",
      "Iteration 47388, loss = 0.03007020\n",
      "Iteration 47389, loss = 0.03006316\n",
      "Iteration 47390, loss = 0.03005190\n",
      "Iteration 47391, loss = 0.03006708\n",
      "Iteration 47392, loss = 0.03006936\n",
      "Iteration 47393, loss = 0.03005965\n",
      "Iteration 47394, loss = 0.03006565\n",
      "Iteration 47395, loss = 0.03006909\n",
      "Iteration 47396, loss = 0.03005857\n",
      "Iteration 47397, loss = 0.03006545\n",
      "Iteration 47398, loss = 0.03006820\n",
      "Iteration 47399, loss = 0.03006504\n",
      "Iteration 47400, loss = 0.03006521\n",
      "Iteration 47401, loss = 0.03005997\n",
      "Iteration 47402, loss = 0.03006490\n",
      "Iteration 47403, loss = 0.03006369\n",
      "Iteration 47404, loss = 0.03005887\n",
      "Iteration 47405, loss = 0.03005515\n",
      "Iteration 47406, loss = 0.03005073\n",
      "Iteration 47407, loss = 0.03006551\n",
      "Iteration 47408, loss = 0.03007341\n",
      "Iteration 47409, loss = 0.03005904\n",
      "Iteration 47410, loss = 0.03005052\n",
      "Iteration 47411, loss = 0.03006822\n",
      "Iteration 47412, loss = 0.03007486\n",
      "Iteration 47413, loss = 0.03007342\n",
      "Iteration 47414, loss = 0.03006283\n",
      "Iteration 47415, loss = 0.03004695\n",
      "Iteration 47416, loss = 0.03006046\n",
      "Iteration 47417, loss = 0.03007000\n",
      "Iteration 47418, loss = 0.03005406\n",
      "Iteration 47419, loss = 0.03004149\n",
      "Iteration 47420, loss = 0.03005429\n",
      "Iteration 47421, loss = 0.03007209\n",
      "Iteration 47422, loss = 0.03006608\n",
      "Iteration 47423, loss = 0.03006105\n",
      "Iteration 47424, loss = 0.03006051\n",
      "Iteration 47425, loss = 0.03004941\n",
      "Iteration 47426, loss = 0.03006055\n",
      "Iteration 47427, loss = 0.03006139\n",
      "Iteration 47428, loss = 0.03007316\n",
      "Iteration 47429, loss = 0.03007011\n",
      "Iteration 47430, loss = 0.03004378\n",
      "Iteration 47431, loss = 0.03005258\n",
      "Iteration 47432, loss = 0.03006727\n",
      "Iteration 47433, loss = 0.03006807\n",
      "Iteration 47434, loss = 0.03006097\n",
      "Iteration 47435, loss = 0.03006304\n",
      "Iteration 47436, loss = 0.03006167\n",
      "Iteration 47437, loss = 0.03005472\n",
      "Iteration 47438, loss = 0.03005478\n",
      "Iteration 47439, loss = 0.03006547\n",
      "Iteration 47440, loss = 0.03006863\n",
      "Iteration 47441, loss = 0.03005144\n",
      "Iteration 47442, loss = 0.03006188\n",
      "Iteration 47443, loss = 0.03006618\n",
      "Iteration 47444, loss = 0.03005950\n",
      "Iteration 47445, loss = 0.03006866\n",
      "Iteration 47446, loss = 0.03007767\n",
      "Iteration 47447, loss = 0.03007253\n",
      "Iteration 47448, loss = 0.03005790\n",
      "Iteration 47449, loss = 0.03004422\n",
      "Iteration 47450, loss = 0.03004381\n",
      "Iteration 47451, loss = 0.03005481\n",
      "Iteration 47452, loss = 0.03004663\n",
      "Iteration 47453, loss = 0.03003372\n",
      "Iteration 47454, loss = 0.03004375\n",
      "Iteration 47455, loss = 0.03004846\n",
      "Iteration 47456, loss = 0.03004371\n",
      "Iteration 47457, loss = 0.03003292\n",
      "Iteration 47458, loss = 0.03003870\n",
      "Iteration 47459, loss = 0.03003757\n",
      "Iteration 47460, loss = 0.03003330\n",
      "Iteration 47461, loss = 0.03003029\n",
      "Iteration 47462, loss = 0.03003301\n",
      "Iteration 47463, loss = 0.03004028\n",
      "Iteration 47464, loss = 0.03003115\n",
      "Iteration 47465, loss = 0.03002918\n",
      "Iteration 47466, loss = 0.03004146\n",
      "Iteration 47467, loss = 0.03004641\n",
      "Iteration 47468, loss = 0.03004529\n",
      "Iteration 47469, loss = 0.03003409\n",
      "Iteration 47470, loss = 0.03004384\n",
      "Iteration 47471, loss = 0.03004136\n",
      "Iteration 47472, loss = 0.03002841\n",
      "Iteration 47473, loss = 0.03003884\n",
      "Iteration 47474, loss = 0.03004119\n",
      "Iteration 47475, loss = 0.03003186\n",
      "Iteration 47476, loss = 0.03003014\n",
      "Iteration 47477, loss = 0.03002573\n",
      "Iteration 47478, loss = 0.03003201\n",
      "Iteration 47479, loss = 0.03004190\n",
      "Iteration 47480, loss = 0.03004084\n",
      "Iteration 47481, loss = 0.03002773\n",
      "Iteration 47482, loss = 0.03002968\n",
      "Iteration 47483, loss = 0.03003245\n",
      "Iteration 47484, loss = 0.03004035\n",
      "Iteration 47485, loss = 0.03003766\n",
      "Iteration 47486, loss = 0.03002936\n",
      "Iteration 47487, loss = 0.03001949\n",
      "Iteration 47488, loss = 0.03003309\n",
      "Iteration 47489, loss = 0.03003889\n",
      "Iteration 47490, loss = 0.03004150\n",
      "Iteration 47491, loss = 0.03003371\n",
      "Iteration 47492, loss = 0.03002039\n",
      "Iteration 47493, loss = 0.03002190\n",
      "Iteration 47494, loss = 0.03003199\n",
      "Iteration 47495, loss = 0.03002666\n",
      "Iteration 47496, loss = 0.03001495\n",
      "Iteration 47497, loss = 0.03002308\n",
      "Iteration 47498, loss = 0.03002145\n",
      "Iteration 47499, loss = 0.03001744\n",
      "Iteration 47500, loss = 0.03002543\n",
      "Iteration 47501, loss = 0.03003160\n",
      "Iteration 47502, loss = 0.03002589\n",
      "Iteration 47503, loss = 0.03002118\n",
      "Iteration 47504, loss = 0.03003188\n",
      "Iteration 47505, loss = 0.03003683\n",
      "Iteration 47506, loss = 0.03003274\n",
      "Iteration 47507, loss = 0.03002387\n",
      "Iteration 47508, loss = 0.03001403\n",
      "Iteration 47509, loss = 0.03003091\n",
      "Iteration 47510, loss = 0.03004265\n",
      "Iteration 47511, loss = 0.03003439\n",
      "Iteration 47512, loss = 0.03002227\n",
      "Iteration 47513, loss = 0.03002767\n",
      "Iteration 47514, loss = 0.03004042\n",
      "Iteration 47515, loss = 0.03004806\n",
      "Iteration 47516, loss = 0.03004322\n",
      "Iteration 47517, loss = 0.03003244\n",
      "Iteration 47518, loss = 0.03001248\n",
      "Iteration 47519, loss = 0.03002422\n",
      "Iteration 47520, loss = 0.03003964\n",
      "Iteration 47521, loss = 0.03004510\n",
      "Iteration 47522, loss = 0.03003208\n",
      "Iteration 47523, loss = 0.03000413\n",
      "Iteration 47524, loss = 0.03001826\n",
      "Iteration 47525, loss = 0.03002225\n",
      "Iteration 47526, loss = 0.03002415\n",
      "Iteration 47527, loss = 0.03002351\n",
      "Iteration 47528, loss = 0.03001939\n",
      "Iteration 47529, loss = 0.03001192\n",
      "Iteration 47530, loss = 0.03001087\n",
      "Iteration 47531, loss = 0.03002082\n",
      "Iteration 47532, loss = 0.03002323\n",
      "Iteration 47533, loss = 0.03001394\n",
      "Iteration 47534, loss = 0.03001247\n",
      "Iteration 47535, loss = 0.03002884\n",
      "Iteration 47536, loss = 0.03003461\n",
      "Iteration 47537, loss = 0.03002977\n",
      "Iteration 47538, loss = 0.03002139\n",
      "Iteration 47539, loss = 0.03000743\n",
      "Iteration 47540, loss = 0.03001135\n",
      "Iteration 47541, loss = 0.03003666\n",
      "Iteration 47542, loss = 0.03003665\n",
      "Iteration 47543, loss = 0.03001785\n",
      "Iteration 47544, loss = 0.03000737\n",
      "Iteration 47545, loss = 0.03002469\n",
      "Iteration 47546, loss = 0.03003617\n",
      "Iteration 47547, loss = 0.03003490\n",
      "Iteration 47548, loss = 0.03003843\n",
      "Iteration 47549, loss = 0.03002474\n",
      "Iteration 47550, loss = 0.03003533\n",
      "Iteration 47551, loss = 0.03004553\n",
      "Iteration 47552, loss = 0.03004113\n",
      "Iteration 47553, loss = 0.03002812\n",
      "Iteration 47554, loss = 0.03001388\n",
      "Iteration 47555, loss = 0.03001786\n",
      "Iteration 47556, loss = 0.03001730\n",
      "Iteration 47557, loss = 0.03000698\n",
      "Iteration 47558, loss = 0.03001535\n",
      "Iteration 47559, loss = 0.03002125\n",
      "Iteration 47560, loss = 0.03001763\n",
      "Iteration 47561, loss = 0.03001045\n",
      "Iteration 47562, loss = 0.03000480\n",
      "Iteration 47563, loss = 0.03000582\n",
      "Iteration 47564, loss = 0.03001297\n",
      "Iteration 47565, loss = 0.03000392\n",
      "Iteration 47566, loss = 0.03000817\n",
      "Iteration 47567, loss = 0.03000588\n",
      "Iteration 47568, loss = 0.03000120\n",
      "Iteration 47569, loss = 0.03000778\n",
      "Iteration 47570, loss = 0.03000934\n",
      "Iteration 47571, loss = 0.03001444\n",
      "Iteration 47572, loss = 0.03000973\n",
      "Iteration 47573, loss = 0.03001478\n",
      "Iteration 47574, loss = 0.03000865\n",
      "Iteration 47575, loss = 0.03000511\n",
      "Iteration 47576, loss = 0.03001471\n",
      "Iteration 47577, loss = 0.03001377\n",
      "Iteration 47578, loss = 0.03000163\n",
      "Iteration 47579, loss = 0.02999758\n",
      "Iteration 47580, loss = 0.02998961\n",
      "Iteration 47581, loss = 0.03000741\n",
      "Iteration 47582, loss = 0.03000627\n",
      "Iteration 47583, loss = 0.03000678\n",
      "Iteration 47584, loss = 0.03000020\n",
      "Iteration 47585, loss = 0.02998318\n",
      "Iteration 47586, loss = 0.02999263\n",
      "Iteration 47587, loss = 0.02999610\n",
      "Iteration 47588, loss = 0.02998514\n",
      "Iteration 47589, loss = 0.02998428\n",
      "Iteration 47590, loss = 0.02998456\n",
      "Iteration 47591, loss = 0.02998776\n",
      "Iteration 47592, loss = 0.02998903\n",
      "Iteration 47593, loss = 0.02998374\n",
      "Iteration 47594, loss = 0.02998765\n",
      "Iteration 47595, loss = 0.02999217\n",
      "Iteration 47596, loss = 0.02998366\n",
      "Iteration 47597, loss = 0.02998539\n",
      "Iteration 47598, loss = 0.02999544\n",
      "Iteration 47599, loss = 0.02999518\n",
      "Iteration 47600, loss = 0.02999902\n",
      "Iteration 47601, loss = 0.02998598\n",
      "Iteration 47602, loss = 0.02999402\n",
      "Iteration 47603, loss = 0.03000071\n",
      "Iteration 47604, loss = 0.02999234\n",
      "Iteration 47605, loss = 0.02998034\n",
      "Iteration 47606, loss = 0.02998589\n",
      "Iteration 47607, loss = 0.02999030\n",
      "Iteration 47608, loss = 0.02999693\n",
      "Iteration 47609, loss = 0.02999973\n",
      "Iteration 47610, loss = 0.02998838\n",
      "Iteration 47611, loss = 0.02997673\n",
      "Iteration 47612, loss = 0.02999143\n",
      "Iteration 47613, loss = 0.03001300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 47614, loss = 0.03001326\n",
      "Iteration 47615, loss = 0.02999373\n",
      "Iteration 47616, loss = 0.02997816\n",
      "Iteration 47617, loss = 0.02999235\n",
      "Iteration 47618, loss = 0.02999998\n",
      "Iteration 47619, loss = 0.02999665\n",
      "Iteration 47620, loss = 0.02998328\n",
      "Iteration 47621, loss = 0.02997413\n",
      "Iteration 47622, loss = 0.02998110\n",
      "Iteration 47623, loss = 0.02998403\n",
      "Iteration 47624, loss = 0.02996947\n",
      "Iteration 47625, loss = 0.02997868\n",
      "Iteration 47626, loss = 0.02998398\n",
      "Iteration 47627, loss = 0.02998390\n",
      "Iteration 47628, loss = 0.02998782\n",
      "Iteration 47629, loss = 0.02999223\n",
      "Iteration 47630, loss = 0.02998968\n",
      "Iteration 47631, loss = 0.02999093\n",
      "Iteration 47632, loss = 0.02998247\n",
      "Iteration 47633, loss = 0.02998279\n",
      "Iteration 47634, loss = 0.02999291\n",
      "Iteration 47635, loss = 0.02999210\n",
      "Iteration 47636, loss = 0.02996970\n",
      "Iteration 47637, loss = 0.02999055\n",
      "Iteration 47638, loss = 0.03000033\n",
      "Iteration 47639, loss = 0.03000621\n",
      "Iteration 47640, loss = 0.03000711\n",
      "Iteration 47641, loss = 0.02999372\n",
      "Iteration 47642, loss = 0.02997697\n",
      "Iteration 47643, loss = 0.02997878\n",
      "Iteration 47644, loss = 0.03000287\n",
      "Iteration 47645, loss = 0.03000721\n",
      "Iteration 47646, loss = 0.02998706\n",
      "Iteration 47647, loss = 0.02997380\n",
      "Iteration 47648, loss = 0.02998758\n",
      "Iteration 47649, loss = 0.02998725\n",
      "Iteration 47650, loss = 0.02997583\n",
      "Iteration 47651, loss = 0.02996777\n",
      "Iteration 47652, loss = 0.02997952\n",
      "Iteration 47653, loss = 0.02998315\n",
      "Iteration 47654, loss = 0.02996556\n",
      "Iteration 47655, loss = 0.02996704\n",
      "Iteration 47656, loss = 0.02996957\n",
      "Iteration 47657, loss = 0.02996934\n",
      "Iteration 47658, loss = 0.02996437\n",
      "Iteration 47659, loss = 0.02996382\n",
      "Iteration 47660, loss = 0.02997089\n",
      "Iteration 47661, loss = 0.02996298\n",
      "Iteration 47662, loss = 0.02996038\n",
      "Iteration 47663, loss = 0.02996618\n",
      "Iteration 47664, loss = 0.02997269\n",
      "Iteration 47665, loss = 0.02997045\n",
      "Iteration 47666, loss = 0.02996009\n",
      "Iteration 47667, loss = 0.02995823\n",
      "Iteration 47668, loss = 0.02996560\n",
      "Iteration 47669, loss = 0.02996645\n",
      "Iteration 47670, loss = 0.02996089\n",
      "Iteration 47671, loss = 0.02996483\n",
      "Iteration 47672, loss = 0.02997163\n",
      "Iteration 47673, loss = 0.02997577\n",
      "Iteration 47674, loss = 0.02997087\n",
      "Iteration 47675, loss = 0.02996299\n",
      "Iteration 47676, loss = 0.02996934\n",
      "Iteration 47677, loss = 0.02996699\n",
      "Iteration 47678, loss = 0.02996583\n",
      "Iteration 47679, loss = 0.02996315\n",
      "Iteration 47680, loss = 0.02997223\n",
      "Iteration 47681, loss = 0.02997779\n",
      "Iteration 47682, loss = 0.02998309\n",
      "Iteration 47683, loss = 0.02997831\n",
      "Iteration 47684, loss = 0.02997032\n",
      "Iteration 47685, loss = 0.02996394\n",
      "Iteration 47686, loss = 0.02996519\n",
      "Iteration 47687, loss = 0.02997995\n",
      "Iteration 47688, loss = 0.02998567\n",
      "Iteration 47689, loss = 0.02997309\n",
      "Iteration 47690, loss = 0.02995191\n",
      "Iteration 47691, loss = 0.02996549\n",
      "Iteration 47692, loss = 0.02996852\n",
      "Iteration 47693, loss = 0.02996948\n",
      "Iteration 47694, loss = 0.02996820\n",
      "Iteration 47695, loss = 0.02997901\n",
      "Iteration 47696, loss = 0.02997623\n",
      "Iteration 47697, loss = 0.02996497\n",
      "Iteration 47698, loss = 0.02994785\n",
      "Iteration 47699, loss = 0.02996463\n",
      "Iteration 47700, loss = 0.02997609\n",
      "Iteration 47701, loss = 0.02997106\n",
      "Iteration 47702, loss = 0.02996432\n",
      "Iteration 47703, loss = 0.02995403\n",
      "Iteration 47704, loss = 0.02994164\n",
      "Iteration 47705, loss = 0.02995544\n",
      "Iteration 47706, loss = 0.02996071\n",
      "Iteration 47707, loss = 0.02996075\n",
      "Iteration 47708, loss = 0.02995086\n",
      "Iteration 47709, loss = 0.02995153\n",
      "Iteration 47710, loss = 0.02995398\n",
      "Iteration 47711, loss = 0.02995810\n",
      "Iteration 47712, loss = 0.02995862\n",
      "Iteration 47713, loss = 0.02994456\n",
      "Iteration 47714, loss = 0.02995447\n",
      "Iteration 47715, loss = 0.02995502\n",
      "Iteration 47716, loss = 0.02996417\n",
      "Iteration 47717, loss = 0.02994987\n",
      "Iteration 47718, loss = 0.02994538\n",
      "Iteration 47719, loss = 0.02994825\n",
      "Iteration 47720, loss = 0.02995178\n",
      "Iteration 47721, loss = 0.02995108\n",
      "Iteration 47722, loss = 0.02995292\n",
      "Iteration 47723, loss = 0.02995845\n",
      "Iteration 47724, loss = 0.02994400\n",
      "Iteration 47725, loss = 0.02994095\n",
      "Iteration 47726, loss = 0.02994910\n",
      "Iteration 47727, loss = 0.02994563\n",
      "Iteration 47728, loss = 0.02994322\n",
      "Iteration 47729, loss = 0.02993692\n",
      "Iteration 47730, loss = 0.02994479\n",
      "Iteration 47731, loss = 0.02994856\n",
      "Iteration 47732, loss = 0.02995055\n",
      "Iteration 47733, loss = 0.02995437\n",
      "Iteration 47734, loss = 0.02995205\n",
      "Iteration 47735, loss = 0.02995216\n",
      "Iteration 47736, loss = 0.02994256\n",
      "Iteration 47737, loss = 0.02993607\n",
      "Iteration 47738, loss = 0.02994536\n",
      "Iteration 47739, loss = 0.02994763\n",
      "Iteration 47740, loss = 0.02994601\n",
      "Iteration 47741, loss = 0.02993501\n",
      "Iteration 47742, loss = 0.02994588\n",
      "Iteration 47743, loss = 0.02994650\n",
      "Iteration 47744, loss = 0.02995807\n",
      "Iteration 47745, loss = 0.02996002\n",
      "Iteration 47746, loss = 0.02994990\n",
      "Iteration 47747, loss = 0.02993690\n",
      "Iteration 47748, loss = 0.02994931\n",
      "Iteration 47749, loss = 0.02995679\n",
      "Iteration 47750, loss = 0.02995771\n",
      "Iteration 47751, loss = 0.02993892\n",
      "Iteration 47752, loss = 0.02993208\n",
      "Iteration 47753, loss = 0.02994167\n",
      "Iteration 47754, loss = 0.02993545\n",
      "Iteration 47755, loss = 0.02993359\n",
      "Iteration 47756, loss = 0.02993196\n",
      "Iteration 47757, loss = 0.02994864\n",
      "Iteration 47758, loss = 0.02994261\n",
      "Iteration 47759, loss = 0.02994230\n",
      "Iteration 47760, loss = 0.02993473\n",
      "Iteration 47761, loss = 0.02994790\n",
      "Iteration 47762, loss = 0.02994770\n",
      "Iteration 47763, loss = 0.02995173\n",
      "Iteration 47764, loss = 0.02994774\n",
      "Iteration 47765, loss = 0.02994280\n",
      "Iteration 47766, loss = 0.02993245\n",
      "Iteration 47767, loss = 0.02993926\n",
      "Iteration 47768, loss = 0.02994384\n",
      "Iteration 47769, loss = 0.02993698\n",
      "Iteration 47770, loss = 0.02993090\n",
      "Iteration 47771, loss = 0.02992538\n",
      "Iteration 47772, loss = 0.02992990\n",
      "Iteration 47773, loss = 0.02993122\n",
      "Iteration 47774, loss = 0.02993189\n",
      "Iteration 47775, loss = 0.02992219\n",
      "Iteration 47776, loss = 0.02993873\n",
      "Iteration 47777, loss = 0.02994113\n",
      "Iteration 47778, loss = 0.02992377\n",
      "Iteration 47779, loss = 0.02992297\n",
      "Iteration 47780, loss = 0.02993441\n",
      "Iteration 47781, loss = 0.02992438\n",
      "Iteration 47782, loss = 0.02992470\n",
      "Iteration 47783, loss = 0.02992582\n",
      "Iteration 47784, loss = 0.02992612\n",
      "Iteration 47785, loss = 0.02993078\n",
      "Iteration 47786, loss = 0.02992226\n",
      "Iteration 47787, loss = 0.02991360\n",
      "Iteration 47788, loss = 0.02993108\n",
      "Iteration 47789, loss = 0.02993505\n",
      "Iteration 47790, loss = 0.02992626\n",
      "Iteration 47791, loss = 0.02991628\n",
      "Iteration 47792, loss = 0.02992509\n",
      "Iteration 47793, loss = 0.02992022\n",
      "Iteration 47794, loss = 0.02991169\n",
      "Iteration 47795, loss = 0.02991902\n",
      "Iteration 47796, loss = 0.02993116\n",
      "Iteration 47797, loss = 0.02993291\n",
      "Iteration 47798, loss = 0.02992786\n",
      "Iteration 47799, loss = 0.02991719\n",
      "Iteration 47800, loss = 0.02991566\n",
      "Iteration 47801, loss = 0.02991783\n",
      "Iteration 47802, loss = 0.02991902\n",
      "Iteration 47803, loss = 0.02990401\n",
      "Iteration 47804, loss = 0.02991860\n",
      "Iteration 47805, loss = 0.02992662\n",
      "Iteration 47806, loss = 0.02993238\n",
      "Iteration 47807, loss = 0.02993201\n",
      "Iteration 47808, loss = 0.02992344\n",
      "Iteration 47809, loss = 0.02991984\n",
      "Iteration 47810, loss = 0.02992395\n",
      "Iteration 47811, loss = 0.02992802\n",
      "Iteration 47812, loss = 0.02992693\n",
      "Iteration 47813, loss = 0.02991843\n",
      "Iteration 47814, loss = 0.02991405\n",
      "Iteration 47815, loss = 0.02991630\n",
      "Iteration 47816, loss = 0.02992608\n",
      "Iteration 47817, loss = 0.02992176\n",
      "Iteration 47818, loss = 0.02991362\n",
      "Iteration 47819, loss = 0.02991978\n",
      "Iteration 47820, loss = 0.02992581\n",
      "Iteration 47821, loss = 0.02992522\n",
      "Iteration 47822, loss = 0.02991920\n",
      "Iteration 47823, loss = 0.02992642\n",
      "Iteration 47824, loss = 0.02991851\n",
      "Iteration 47825, loss = 0.02993581\n",
      "Iteration 47826, loss = 0.02993883\n",
      "Iteration 47827, loss = 0.02992516\n",
      "Iteration 47828, loss = 0.02991409\n",
      "Iteration 47829, loss = 0.02991867\n",
      "Iteration 47830, loss = 0.02992374\n",
      "Iteration 47831, loss = 0.02992721\n",
      "Iteration 47832, loss = 0.02991260\n",
      "Iteration 47833, loss = 0.02990797\n",
      "Iteration 47834, loss = 0.02990927\n",
      "Iteration 47835, loss = 0.02991863\n",
      "Iteration 47836, loss = 0.02991019\n",
      "Iteration 47837, loss = 0.02990166\n",
      "Iteration 47838, loss = 0.02990451\n",
      "Iteration 47839, loss = 0.02990913\n",
      "Iteration 47840, loss = 0.02989879\n",
      "Iteration 47841, loss = 0.02990342\n",
      "Iteration 47842, loss = 0.02991244\n",
      "Iteration 47843, loss = 0.02990675\n",
      "Iteration 47844, loss = 0.02990177\n",
      "Iteration 47845, loss = 0.02989923\n",
      "Iteration 47846, loss = 0.02990833\n",
      "Iteration 47847, loss = 0.02990621\n",
      "Iteration 47848, loss = 0.02989575\n",
      "Iteration 47849, loss = 0.02989220\n",
      "Iteration 47850, loss = 0.02990549\n",
      "Iteration 47851, loss = 0.02990769\n",
      "Iteration 47852, loss = 0.02989528\n",
      "Iteration 47853, loss = 0.02989170\n",
      "Iteration 47854, loss = 0.02990166\n",
      "Iteration 47855, loss = 0.02990502\n",
      "Iteration 47856, loss = 0.02991507\n",
      "Iteration 47857, loss = 0.02990921\n",
      "Iteration 47858, loss = 0.02991191\n",
      "Iteration 47859, loss = 0.02991750\n",
      "Iteration 47860, loss = 0.02990453\n",
      "Iteration 47861, loss = 0.02991212\n",
      "Iteration 47862, loss = 0.02992039\n",
      "Iteration 47863, loss = 0.02991482\n",
      "Iteration 47864, loss = 0.02990615\n",
      "Iteration 47865, loss = 0.02990484\n",
      "Iteration 47866, loss = 0.02991477\n",
      "Iteration 47867, loss = 0.02990423\n",
      "Iteration 47868, loss = 0.02989549\n",
      "Iteration 47869, loss = 0.02990492\n",
      "Iteration 47870, loss = 0.02990551\n",
      "Iteration 47871, loss = 0.02989724\n",
      "Iteration 47872, loss = 0.02990198\n",
      "Iteration 47873, loss = 0.02990193\n",
      "Iteration 47874, loss = 0.02990193\n",
      "Iteration 47875, loss = 0.02990132\n",
      "Iteration 47876, loss = 0.02990862\n",
      "Iteration 47877, loss = 0.02990200\n",
      "Iteration 47878, loss = 0.02990836\n",
      "Iteration 47879, loss = 0.02991357\n",
      "Iteration 47880, loss = 0.02990683\n",
      "Iteration 47881, loss = 0.02990406\n",
      "Iteration 47882, loss = 0.02991595\n",
      "Iteration 47883, loss = 0.02990512\n",
      "Iteration 47884, loss = 0.02989543\n",
      "Iteration 47885, loss = 0.02990380\n",
      "Iteration 47886, loss = 0.02990638\n",
      "Iteration 47887, loss = 0.02989479\n",
      "Iteration 47888, loss = 0.02988316\n",
      "Iteration 47889, loss = 0.02989285\n",
      "Iteration 47890, loss = 0.02989680\n",
      "Iteration 47891, loss = 0.02989193\n",
      "Iteration 47892, loss = 0.02987905\n",
      "Iteration 47893, loss = 0.02989520\n",
      "Iteration 47894, loss = 0.02990117\n",
      "Iteration 47895, loss = 0.02989563\n",
      "Iteration 47896, loss = 0.02987895\n",
      "Iteration 47897, loss = 0.02989339\n",
      "Iteration 47898, loss = 0.02990486\n",
      "Iteration 47899, loss = 0.02990478\n",
      "Iteration 47900, loss = 0.02989809\n",
      "Iteration 47901, loss = 0.02989331\n",
      "Iteration 47902, loss = 0.02988300\n",
      "Iteration 47903, loss = 0.02988910\n",
      "Iteration 47904, loss = 0.02989506\n",
      "Iteration 47905, loss = 0.02988773\n",
      "Iteration 47906, loss = 0.02989394\n",
      "Iteration 47907, loss = 0.02989354\n",
      "Iteration 47908, loss = 0.02989042\n",
      "Iteration 47909, loss = 0.02989075\n",
      "Iteration 47910, loss = 0.02989848\n",
      "Iteration 47911, loss = 0.02988887\n",
      "Iteration 47912, loss = 0.02988052\n",
      "Iteration 47913, loss = 0.02988410\n",
      "Iteration 47914, loss = 0.02989031\n",
      "Iteration 47915, loss = 0.02989152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 47916, loss = 0.02988797\n",
      "Iteration 47917, loss = 0.02988211\n",
      "Iteration 47918, loss = 0.02987068\n",
      "Iteration 47919, loss = 0.02987688\n",
      "Iteration 47920, loss = 0.02988365\n",
      "Iteration 47921, loss = 0.02987372\n",
      "Iteration 47922, loss = 0.02986625\n",
      "Iteration 47923, loss = 0.02987007\n",
      "Iteration 47924, loss = 0.02987221\n",
      "Iteration 47925, loss = 0.02986547\n",
      "Iteration 47926, loss = 0.02986888\n",
      "Iteration 47927, loss = 0.02987381\n",
      "Iteration 47928, loss = 0.02987014\n",
      "Iteration 47929, loss = 0.02986928\n",
      "Iteration 47930, loss = 0.02986915\n",
      "Iteration 47931, loss = 0.02987856\n",
      "Iteration 47932, loss = 0.02987788\n",
      "Iteration 47933, loss = 0.02986999\n",
      "Iteration 47934, loss = 0.02987263\n",
      "Iteration 47935, loss = 0.02988025\n",
      "Iteration 47936, loss = 0.02987723\n",
      "Iteration 47937, loss = 0.02988085\n",
      "Iteration 47938, loss = 0.02987956\n",
      "Iteration 47939, loss = 0.02986790\n",
      "Iteration 47940, loss = 0.02987542\n",
      "Iteration 47941, loss = 0.02988174\n",
      "Iteration 47942, loss = 0.02986926\n",
      "Iteration 47943, loss = 0.02987832\n",
      "Iteration 47944, loss = 0.02988067\n",
      "Iteration 47945, loss = 0.02987536\n",
      "Iteration 47946, loss = 0.02987660\n",
      "Iteration 47947, loss = 0.02987455\n",
      "Iteration 47948, loss = 0.02988220\n",
      "Iteration 47949, loss = 0.02987954\n",
      "Iteration 47950, loss = 0.02987878\n",
      "Iteration 47951, loss = 0.02986341\n",
      "Iteration 47952, loss = 0.02987403\n",
      "Iteration 47953, loss = 0.02987974\n",
      "Iteration 47954, loss = 0.02987807\n",
      "Iteration 47955, loss = 0.02987653\n",
      "Iteration 47956, loss = 0.02986698\n",
      "Iteration 47957, loss = 0.02986331\n",
      "Iteration 47958, loss = 0.02986885\n",
      "Iteration 47959, loss = 0.02987707\n",
      "Iteration 47960, loss = 0.02988051\n",
      "Iteration 47961, loss = 0.02987522\n",
      "Iteration 47962, loss = 0.02986508\n",
      "Iteration 47963, loss = 0.02986419\n",
      "Iteration 47964, loss = 0.02986582\n",
      "Iteration 47965, loss = 0.02986438\n",
      "Iteration 47966, loss = 0.02984879\n",
      "Iteration 47967, loss = 0.02986888\n",
      "Iteration 47968, loss = 0.02987531\n",
      "Iteration 47969, loss = 0.02987007\n",
      "Iteration 47970, loss = 0.02985702\n",
      "Iteration 47971, loss = 0.02986302\n",
      "Iteration 47972, loss = 0.02986955\n",
      "Iteration 47973, loss = 0.02988598\n",
      "Iteration 47974, loss = 0.02988449\n",
      "Iteration 47975, loss = 0.02986984\n",
      "Iteration 47976, loss = 0.02985746\n",
      "Iteration 47977, loss = 0.02987549\n",
      "Iteration 47978, loss = 0.02988221\n",
      "Iteration 47979, loss = 0.02989496\n",
      "Iteration 47980, loss = 0.02988855\n",
      "Iteration 47981, loss = 0.02988915\n",
      "Iteration 47982, loss = 0.02989118\n",
      "Iteration 47983, loss = 0.02988105\n",
      "Iteration 47984, loss = 0.02986951\n",
      "Iteration 47985, loss = 0.02986626\n",
      "Iteration 47986, loss = 0.02987769\n",
      "Iteration 47987, loss = 0.02988344\n",
      "Iteration 47988, loss = 0.02986804\n",
      "Iteration 47989, loss = 0.02985316\n",
      "Iteration 47990, loss = 0.02986646\n",
      "Iteration 47991, loss = 0.02986462\n",
      "Iteration 47992, loss = 0.02985446\n",
      "Iteration 47993, loss = 0.02984897\n",
      "Iteration 47994, loss = 0.02984379\n",
      "Iteration 47995, loss = 0.02986988\n",
      "Iteration 47996, loss = 0.02987720\n",
      "Iteration 47997, loss = 0.02986601\n",
      "Iteration 47998, loss = 0.02984201\n",
      "Iteration 47999, loss = 0.02985287\n",
      "Iteration 48000, loss = 0.02986311\n",
      "Iteration 48001, loss = 0.02986816\n",
      "Iteration 48002, loss = 0.02985682\n",
      "Iteration 48003, loss = 0.02984863\n",
      "Iteration 48004, loss = 0.02984849\n",
      "Iteration 48005, loss = 0.02984434\n",
      "Iteration 48006, loss = 0.02984451\n",
      "Iteration 48007, loss = 0.02985135\n",
      "Iteration 48008, loss = 0.02984332\n",
      "Iteration 48009, loss = 0.02984093\n",
      "Iteration 48010, loss = 0.02984256\n",
      "Iteration 48011, loss = 0.02984729\n",
      "Iteration 48012, loss = 0.02984697\n",
      "Iteration 48013, loss = 0.02985642\n",
      "Iteration 48014, loss = 0.02985116\n",
      "Iteration 48015, loss = 0.02984246\n",
      "Iteration 48016, loss = 0.02983744\n",
      "Iteration 48017, loss = 0.02984225\n",
      "Iteration 48018, loss = 0.02984518\n",
      "Iteration 48019, loss = 0.02984581\n",
      "Iteration 48020, loss = 0.02984166\n",
      "Iteration 48021, loss = 0.02983666\n",
      "Iteration 48022, loss = 0.02983455\n",
      "Iteration 48023, loss = 0.02983537\n",
      "Iteration 48024, loss = 0.02983998\n",
      "Iteration 48025, loss = 0.02983558\n",
      "Iteration 48026, loss = 0.02983440\n",
      "Iteration 48027, loss = 0.02982787\n",
      "Iteration 48028, loss = 0.02984737\n",
      "Iteration 48029, loss = 0.02984597\n",
      "Iteration 48030, loss = 0.02983838\n",
      "Iteration 48031, loss = 0.02983625\n",
      "Iteration 48032, loss = 0.02983853\n",
      "Iteration 48033, loss = 0.02984123\n",
      "Iteration 48034, loss = 0.02984442\n",
      "Iteration 48035, loss = 0.02984871\n",
      "Iteration 48036, loss = 0.02984066\n",
      "Iteration 48037, loss = 0.02983698\n",
      "Iteration 48038, loss = 0.02983606\n",
      "Iteration 48039, loss = 0.02983467\n",
      "Iteration 48040, loss = 0.02984712\n",
      "Iteration 48041, loss = 0.02984688\n",
      "Iteration 48042, loss = 0.02984084\n",
      "Iteration 48043, loss = 0.02984337\n",
      "Iteration 48044, loss = 0.02983440\n",
      "Iteration 48045, loss = 0.02983903\n",
      "Iteration 48046, loss = 0.02983411\n",
      "Iteration 48047, loss = 0.02982538\n",
      "Iteration 48048, loss = 0.02983065\n",
      "Iteration 48049, loss = 0.02983751\n",
      "Iteration 48050, loss = 0.02983599\n",
      "Iteration 48051, loss = 0.02983494\n",
      "Iteration 48052, loss = 0.02982935\n",
      "Iteration 48053, loss = 0.02983908\n",
      "Iteration 48054, loss = 0.02984293\n",
      "Iteration 48055, loss = 0.02984382\n",
      "Iteration 48056, loss = 0.02983179\n",
      "Iteration 48057, loss = 0.02983851\n",
      "Iteration 48058, loss = 0.02985044\n",
      "Iteration 48059, loss = 0.02985011\n",
      "Iteration 48060, loss = 0.02983808\n",
      "Iteration 48061, loss = 0.02984096\n",
      "Iteration 48062, loss = 0.02982569\n",
      "Iteration 48063, loss = 0.02983476\n",
      "Iteration 48064, loss = 0.02984696\n",
      "Iteration 48065, loss = 0.02985277\n",
      "Iteration 48066, loss = 0.02984784\n",
      "Iteration 48067, loss = 0.02984100\n",
      "Iteration 48068, loss = 0.02984590\n",
      "Iteration 48069, loss = 0.02983955\n",
      "Iteration 48070, loss = 0.02982946\n",
      "Iteration 48071, loss = 0.02982774\n",
      "Iteration 48072, loss = 0.02983048\n",
      "Iteration 48073, loss = 0.02983672\n",
      "Iteration 48074, loss = 0.02983185\n",
      "Iteration 48075, loss = 0.02981873\n",
      "Iteration 48076, loss = 0.02982706\n",
      "Iteration 48077, loss = 0.02983630\n",
      "Iteration 48078, loss = 0.02983864\n",
      "Iteration 48079, loss = 0.02983957\n",
      "Iteration 48080, loss = 0.02983584\n",
      "Iteration 48081, loss = 0.02982204\n",
      "Iteration 48082, loss = 0.02984095\n",
      "Iteration 48083, loss = 0.02984349\n",
      "Iteration 48084, loss = 0.02982463\n",
      "Iteration 48085, loss = 0.02982639\n",
      "Iteration 48086, loss = 0.02983414\n",
      "Iteration 48087, loss = 0.02984260\n",
      "Iteration 48088, loss = 0.02984328\n",
      "Iteration 48089, loss = 0.02983658\n",
      "Iteration 48090, loss = 0.02982127\n",
      "Iteration 48091, loss = 0.02981910\n",
      "Iteration 48092, loss = 0.02982409\n",
      "Iteration 48093, loss = 0.02983015\n",
      "Iteration 48094, loss = 0.02981848\n",
      "Iteration 48095, loss = 0.02981863\n",
      "Iteration 48096, loss = 0.02982224\n",
      "Iteration 48097, loss = 0.02981289\n",
      "Iteration 48098, loss = 0.02983016\n",
      "Iteration 48099, loss = 0.02983147\n",
      "Iteration 48100, loss = 0.02981419\n",
      "Iteration 48101, loss = 0.02980442\n",
      "Iteration 48102, loss = 0.02981708\n",
      "Iteration 48103, loss = 0.02981910\n",
      "Iteration 48104, loss = 0.02981293\n",
      "Iteration 48105, loss = 0.02981262\n",
      "Iteration 48106, loss = 0.02981821\n",
      "Iteration 48107, loss = 0.02980967\n",
      "Iteration 48108, loss = 0.02981083\n",
      "Iteration 48109, loss = 0.02982011\n",
      "Iteration 48110, loss = 0.02981786\n",
      "Iteration 48111, loss = 0.02981055\n",
      "Iteration 48112, loss = 0.02980649\n",
      "Iteration 48113, loss = 0.02981154\n",
      "Iteration 48114, loss = 0.02980811\n",
      "Iteration 48115, loss = 0.02980683\n",
      "Iteration 48116, loss = 0.02980989\n",
      "Iteration 48117, loss = 0.02980772\n",
      "Iteration 48118, loss = 0.02981293\n",
      "Iteration 48119, loss = 0.02981545\n",
      "Iteration 48120, loss = 0.02981259\n",
      "Iteration 48121, loss = 0.02980582\n",
      "Iteration 48122, loss = 0.02982115\n",
      "Iteration 48123, loss = 0.02982697\n",
      "Iteration 48124, loss = 0.02981962\n",
      "Iteration 48125, loss = 0.02980267\n",
      "Iteration 48126, loss = 0.02981228\n",
      "Iteration 48127, loss = 0.02981877\n",
      "Iteration 48128, loss = 0.02981515\n",
      "Iteration 48129, loss = 0.02981873\n",
      "Iteration 48130, loss = 0.02981570\n",
      "Iteration 48131, loss = 0.02980685\n",
      "Iteration 48132, loss = 0.02980922\n",
      "Iteration 48133, loss = 0.02981615\n",
      "Iteration 48134, loss = 0.02981220\n",
      "Iteration 48135, loss = 0.02980421\n",
      "Iteration 48136, loss = 0.02980510\n",
      "Iteration 48137, loss = 0.02981342\n",
      "Iteration 48138, loss = 0.02980321\n",
      "Iteration 48139, loss = 0.02981780\n",
      "Iteration 48140, loss = 0.02982046\n",
      "Iteration 48141, loss = 0.02981855\n",
      "Iteration 48142, loss = 0.02981206\n",
      "Iteration 48143, loss = 0.02980147\n",
      "Iteration 48144, loss = 0.02981408\n",
      "Iteration 48145, loss = 0.02981704\n",
      "Iteration 48146, loss = 0.02981083\n",
      "Iteration 48147, loss = 0.02980266\n",
      "Iteration 48148, loss = 0.02980625\n",
      "Iteration 48149, loss = 0.02981113\n",
      "Iteration 48150, loss = 0.02981680\n",
      "Iteration 48151, loss = 0.02981136\n",
      "Iteration 48152, loss = 0.02980750\n",
      "Iteration 48153, loss = 0.02980213\n",
      "Iteration 48154, loss = 0.02979706\n",
      "Iteration 48155, loss = 0.02979301\n",
      "Iteration 48156, loss = 0.02980158\n",
      "Iteration 48157, loss = 0.02980738\n",
      "Iteration 48158, loss = 0.02980454\n",
      "Iteration 48159, loss = 0.02979915\n",
      "Iteration 48160, loss = 0.02979883\n",
      "Iteration 48161, loss = 0.02979301\n",
      "Iteration 48162, loss = 0.02979230\n",
      "Iteration 48163, loss = 0.02979492\n",
      "Iteration 48164, loss = 0.02979964\n",
      "Iteration 48165, loss = 0.02979403\n",
      "Iteration 48166, loss = 0.02979409\n",
      "Iteration 48167, loss = 0.02979301\n",
      "Iteration 48168, loss = 0.02979697\n",
      "Iteration 48169, loss = 0.02979027\n",
      "Iteration 48170, loss = 0.02979368\n",
      "Iteration 48171, loss = 0.02979721\n",
      "Iteration 48172, loss = 0.02978664\n",
      "Iteration 48173, loss = 0.02979846\n",
      "Iteration 48174, loss = 0.02979915\n",
      "Iteration 48175, loss = 0.02978286\n",
      "Iteration 48176, loss = 0.02978446\n",
      "Iteration 48177, loss = 0.02980112\n",
      "Iteration 48178, loss = 0.02980747\n",
      "Iteration 48179, loss = 0.02980297\n",
      "Iteration 48180, loss = 0.02978655\n",
      "Iteration 48181, loss = 0.02978399\n",
      "Iteration 48182, loss = 0.02978061\n",
      "Iteration 48183, loss = 0.02979515\n",
      "Iteration 48184, loss = 0.02979882\n",
      "Iteration 48185, loss = 0.02979555\n",
      "Iteration 48186, loss = 0.02977852\n",
      "Iteration 48187, loss = 0.02978589\n",
      "Iteration 48188, loss = 0.02979927\n",
      "Iteration 48189, loss = 0.02980579\n",
      "Iteration 48190, loss = 0.02979972\n",
      "Iteration 48191, loss = 0.02980725\n",
      "Iteration 48192, loss = 0.02980954\n",
      "Iteration 48193, loss = 0.02979819\n",
      "Iteration 48194, loss = 0.02978146\n",
      "Iteration 48195, loss = 0.02977972\n",
      "Iteration 48196, loss = 0.02978780\n",
      "Iteration 48197, loss = 0.02978179\n",
      "Iteration 48198, loss = 0.02978048\n",
      "Iteration 48199, loss = 0.02978188\n",
      "Iteration 48200, loss = 0.02978398\n",
      "Iteration 48201, loss = 0.02978503\n",
      "Iteration 48202, loss = 0.02977662\n",
      "Iteration 48203, loss = 0.02977611\n",
      "Iteration 48204, loss = 0.02977707\n",
      "Iteration 48205, loss = 0.02977082\n",
      "Iteration 48206, loss = 0.02977489\n",
      "Iteration 48207, loss = 0.02977954\n",
      "Iteration 48208, loss = 0.02977381\n",
      "Iteration 48209, loss = 0.02977995\n",
      "Iteration 48210, loss = 0.02977693\n",
      "Iteration 48211, loss = 0.02977271\n",
      "Iteration 48212, loss = 0.02976720\n",
      "Iteration 48213, loss = 0.02976671\n",
      "Iteration 48214, loss = 0.02977514\n",
      "Iteration 48215, loss = 0.02978084\n",
      "Iteration 48216, loss = 0.02977479\n",
      "Iteration 48217, loss = 0.02976589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 48218, loss = 0.02977873\n",
      "Iteration 48219, loss = 0.02977408\n",
      "Iteration 48220, loss = 0.02976825\n",
      "Iteration 48221, loss = 0.02976773\n",
      "Iteration 48222, loss = 0.02977074\n",
      "Iteration 48223, loss = 0.02976678\n",
      "Iteration 48224, loss = 0.02977860\n",
      "Iteration 48225, loss = 0.02977377\n",
      "Iteration 48226, loss = 0.02976296\n",
      "Iteration 48227, loss = 0.02976644\n",
      "Iteration 48228, loss = 0.02976307\n",
      "Iteration 48229, loss = 0.02975706\n",
      "Iteration 48230, loss = 0.02976543\n",
      "Iteration 48231, loss = 0.02977615\n",
      "Iteration 48232, loss = 0.02976735\n",
      "Iteration 48233, loss = 0.02976166\n",
      "Iteration 48234, loss = 0.02976335\n",
      "Iteration 48235, loss = 0.02975945\n",
      "Iteration 48236, loss = 0.02976954\n",
      "Iteration 48237, loss = 0.02976655\n",
      "Iteration 48238, loss = 0.02975867\n",
      "Iteration 48239, loss = 0.02976950\n",
      "Iteration 48240, loss = 0.02976731\n",
      "Iteration 48241, loss = 0.02976848\n",
      "Iteration 48242, loss = 0.02977377\n",
      "Iteration 48243, loss = 0.02976157\n",
      "Iteration 48244, loss = 0.02977553\n",
      "Iteration 48245, loss = 0.02978568\n",
      "Iteration 48246, loss = 0.02977818\n",
      "Iteration 48247, loss = 0.02977169\n",
      "Iteration 48248, loss = 0.02976252\n",
      "Iteration 48249, loss = 0.02976597\n",
      "Iteration 48250, loss = 0.02977850\n",
      "Iteration 48251, loss = 0.02977555\n",
      "Iteration 48252, loss = 0.02975957\n",
      "Iteration 48253, loss = 0.02976196\n",
      "Iteration 48254, loss = 0.02975404\n",
      "Iteration 48255, loss = 0.02976620\n",
      "Iteration 48256, loss = 0.02977235\n",
      "Iteration 48257, loss = 0.02976583\n",
      "Iteration 48258, loss = 0.02976921\n",
      "Iteration 48259, loss = 0.02977276\n",
      "Iteration 48260, loss = 0.02977246\n",
      "Iteration 48261, loss = 0.02976979\n",
      "Iteration 48262, loss = 0.02976655\n",
      "Iteration 48263, loss = 0.02976589\n",
      "Iteration 48264, loss = 0.02975628\n",
      "Iteration 48265, loss = 0.02975312\n",
      "Iteration 48266, loss = 0.02974869\n",
      "Iteration 48267, loss = 0.02975583\n",
      "Iteration 48268, loss = 0.02975062\n",
      "Iteration 48269, loss = 0.02975413\n",
      "Iteration 48270, loss = 0.02975423\n",
      "Iteration 48271, loss = 0.02975117\n",
      "Iteration 48272, loss = 0.02975811\n",
      "Iteration 48273, loss = 0.02975804\n",
      "Iteration 48274, loss = 0.02975000\n",
      "Iteration 48275, loss = 0.02974329\n",
      "Iteration 48276, loss = 0.02975267\n",
      "Iteration 48277, loss = 0.02975604\n",
      "Iteration 48278, loss = 0.02974542\n",
      "Iteration 48279, loss = 0.02975063\n",
      "Iteration 48280, loss = 0.02976095\n",
      "Iteration 48281, loss = 0.02976133\n",
      "Iteration 48282, loss = 0.02975638\n",
      "Iteration 48283, loss = 0.02976417\n",
      "Iteration 48284, loss = 0.02976115\n",
      "Iteration 48285, loss = 0.02974928\n",
      "Iteration 48286, loss = 0.02977433\n",
      "Iteration 48287, loss = 0.02977464\n",
      "Iteration 48288, loss = 0.02976022\n",
      "Iteration 48289, loss = 0.02974606\n",
      "Iteration 48290, loss = 0.02975805\n",
      "Iteration 48291, loss = 0.02976246\n",
      "Iteration 48292, loss = 0.02976767\n",
      "Iteration 48293, loss = 0.02976968\n",
      "Iteration 48294, loss = 0.02976579\n",
      "Iteration 48295, loss = 0.02975736\n",
      "Iteration 48296, loss = 0.02976008\n",
      "Iteration 48297, loss = 0.02975560\n",
      "Iteration 48298, loss = 0.02974042\n",
      "Iteration 48299, loss = 0.02975478\n",
      "Iteration 48300, loss = 0.02975609\n",
      "Iteration 48301, loss = 0.02975755\n",
      "Iteration 48302, loss = 0.02975354\n",
      "Iteration 48303, loss = 0.02974833\n",
      "Iteration 48304, loss = 0.02975362\n",
      "Iteration 48305, loss = 0.02974912\n",
      "Iteration 48306, loss = 0.02975279\n",
      "Iteration 48307, loss = 0.02974375\n",
      "Iteration 48308, loss = 0.02974298\n",
      "Iteration 48309, loss = 0.02974259\n",
      "Iteration 48310, loss = 0.02974228\n",
      "Iteration 48311, loss = 0.02973479\n",
      "Iteration 48312, loss = 0.02974264\n",
      "Iteration 48313, loss = 0.02974693\n",
      "Iteration 48314, loss = 0.02974263\n",
      "Iteration 48315, loss = 0.02974044\n",
      "Iteration 48316, loss = 0.02973985\n",
      "Iteration 48317, loss = 0.02973082\n",
      "Iteration 48318, loss = 0.02974214\n",
      "Iteration 48319, loss = 0.02974314\n",
      "Iteration 48320, loss = 0.02973066\n",
      "Iteration 48321, loss = 0.02973305\n",
      "Iteration 48322, loss = 0.02974139\n",
      "Iteration 48323, loss = 0.02974031\n",
      "Iteration 48324, loss = 0.02974022\n",
      "Iteration 48325, loss = 0.02973697\n",
      "Iteration 48326, loss = 0.02973012\n",
      "Iteration 48327, loss = 0.02973954\n",
      "Iteration 48328, loss = 0.02974242\n",
      "Iteration 48329, loss = 0.02973409\n",
      "Iteration 48330, loss = 0.02972316\n",
      "Iteration 48331, loss = 0.02972856\n",
      "Iteration 48332, loss = 0.02973823\n",
      "Iteration 48333, loss = 0.02973526\n",
      "Iteration 48334, loss = 0.02973468\n",
      "Iteration 48335, loss = 0.02972996\n",
      "Iteration 48336, loss = 0.02973321\n",
      "Iteration 48337, loss = 0.02973961\n",
      "Iteration 48338, loss = 0.02972442\n",
      "Iteration 48339, loss = 0.02973344\n",
      "Iteration 48340, loss = 0.02973980\n",
      "Iteration 48341, loss = 0.02973036\n",
      "Iteration 48342, loss = 0.02974156\n",
      "Iteration 48343, loss = 0.02973857\n",
      "Iteration 48344, loss = 0.02974922\n",
      "Iteration 48345, loss = 0.02975241\n",
      "Iteration 48346, loss = 0.02973667\n",
      "Iteration 48347, loss = 0.02972834\n",
      "Iteration 48348, loss = 0.02974013\n",
      "Iteration 48349, loss = 0.02973408\n",
      "Iteration 48350, loss = 0.02972960\n",
      "Iteration 48351, loss = 0.02973193\n",
      "Iteration 48352, loss = 0.02972476\n",
      "Iteration 48353, loss = 0.02972280\n",
      "Iteration 48354, loss = 0.02971744\n",
      "Iteration 48355, loss = 0.02972217\n",
      "Iteration 48356, loss = 0.02972604\n",
      "Iteration 48357, loss = 0.02972527\n",
      "Iteration 48358, loss = 0.02972374\n",
      "Iteration 48359, loss = 0.02972362\n",
      "Iteration 48360, loss = 0.02971585\n",
      "Iteration 48361, loss = 0.02972290\n",
      "Iteration 48362, loss = 0.02972710\n",
      "Iteration 48363, loss = 0.02972466\n",
      "Iteration 48364, loss = 0.02972874\n",
      "Iteration 48365, loss = 0.02972186\n",
      "Iteration 48366, loss = 0.02973606\n",
      "Iteration 48367, loss = 0.02974043\n",
      "Iteration 48368, loss = 0.02973466\n",
      "Iteration 48369, loss = 0.02972842\n",
      "Iteration 48370, loss = 0.02972352\n",
      "Iteration 48371, loss = 0.02973389\n",
      "Iteration 48372, loss = 0.02973384\n",
      "Iteration 48373, loss = 0.02972811\n",
      "Iteration 48374, loss = 0.02971532\n",
      "Iteration 48375, loss = 0.02972934\n",
      "Iteration 48376, loss = 0.02973529\n",
      "Iteration 48377, loss = 0.02973560\n",
      "Iteration 48378, loss = 0.02973922\n",
      "Iteration 48379, loss = 0.02973391\n",
      "Iteration 48380, loss = 0.02971528\n",
      "Iteration 48381, loss = 0.02973020\n",
      "Iteration 48382, loss = 0.02974003\n",
      "Iteration 48383, loss = 0.02972888\n",
      "Iteration 48384, loss = 0.02972684\n",
      "Iteration 48385, loss = 0.02971633\n",
      "Iteration 48386, loss = 0.02972285\n",
      "Iteration 48387, loss = 0.02972565\n",
      "Iteration 48388, loss = 0.02972193\n",
      "Iteration 48389, loss = 0.02971974\n",
      "Iteration 48390, loss = 0.02972549\n",
      "Iteration 48391, loss = 0.02972658\n",
      "Iteration 48392, loss = 0.02972396\n",
      "Iteration 48393, loss = 0.02971334\n",
      "Iteration 48394, loss = 0.02971517\n",
      "Iteration 48395, loss = 0.02971344\n",
      "Iteration 48396, loss = 0.02970449\n",
      "Iteration 48397, loss = 0.02972017\n",
      "Iteration 48398, loss = 0.02972671\n",
      "Iteration 48399, loss = 0.02971978\n",
      "Iteration 48400, loss = 0.02971808\n",
      "Iteration 48401, loss = 0.02971621\n",
      "Iteration 48402, loss = 0.02971564\n",
      "Iteration 48403, loss = 0.02972661\n",
      "Iteration 48404, loss = 0.02972101\n",
      "Iteration 48405, loss = 0.02970811\n",
      "Iteration 48406, loss = 0.02971668\n",
      "Iteration 48407, loss = 0.02972424\n",
      "Iteration 48408, loss = 0.02971088\n",
      "Iteration 48409, loss = 0.02970758\n",
      "Iteration 48410, loss = 0.02971020\n",
      "Iteration 48411, loss = 0.02970892\n",
      "Iteration 48412, loss = 0.02970921\n",
      "Iteration 48413, loss = 0.02972245\n",
      "Iteration 48414, loss = 0.02972486\n",
      "Iteration 48415, loss = 0.02971499\n",
      "Iteration 48416, loss = 0.02971673\n",
      "Iteration 48417, loss = 0.02971243\n",
      "Iteration 48418, loss = 0.02970534\n",
      "Iteration 48419, loss = 0.02972044\n",
      "Iteration 48420, loss = 0.02971832\n",
      "Iteration 48421, loss = 0.02971000\n",
      "Iteration 48422, loss = 0.02969914\n",
      "Iteration 48423, loss = 0.02971262\n",
      "Iteration 48424, loss = 0.02971309\n",
      "Iteration 48425, loss = 0.02970741\n",
      "Iteration 48426, loss = 0.02971311\n",
      "Iteration 48427, loss = 0.02971302\n",
      "Iteration 48428, loss = 0.02971739\n",
      "Iteration 48429, loss = 0.02970922\n",
      "Iteration 48430, loss = 0.02969747\n",
      "Iteration 48431, loss = 0.02969924\n",
      "Iteration 48432, loss = 0.02969558\n",
      "Iteration 48433, loss = 0.02969919\n",
      "Iteration 48434, loss = 0.02969768\n",
      "Iteration 48435, loss = 0.02969312\n",
      "Iteration 48436, loss = 0.02969509\n",
      "Iteration 48437, loss = 0.02969461\n",
      "Iteration 48438, loss = 0.02969603\n",
      "Iteration 48439, loss = 0.02969900\n",
      "Iteration 48440, loss = 0.02969789\n",
      "Iteration 48441, loss = 0.02969419\n",
      "Iteration 48442, loss = 0.02970836\n",
      "Iteration 48443, loss = 0.02970145\n",
      "Iteration 48444, loss = 0.02969115\n",
      "Iteration 48445, loss = 0.02970315\n",
      "Iteration 48446, loss = 0.02970557\n",
      "Iteration 48447, loss = 0.02970814\n",
      "Iteration 48448, loss = 0.02970597\n",
      "Iteration 48449, loss = 0.02971258\n",
      "Iteration 48450, loss = 0.02970811\n",
      "Iteration 48451, loss = 0.02968784\n",
      "Iteration 48452, loss = 0.02969788\n",
      "Iteration 48453, loss = 0.02970935\n",
      "Iteration 48454, loss = 0.02969525\n",
      "Iteration 48455, loss = 0.02968437\n",
      "Iteration 48456, loss = 0.02969528\n",
      "Iteration 48457, loss = 0.02969753\n",
      "Iteration 48458, loss = 0.02969293\n",
      "Iteration 48459, loss = 0.02968247\n",
      "Iteration 48460, loss = 0.02968971\n",
      "Iteration 48461, loss = 0.02968671\n",
      "Iteration 48462, loss = 0.02968182\n",
      "Iteration 48463, loss = 0.02968972\n",
      "Iteration 48464, loss = 0.02969156\n",
      "Iteration 48465, loss = 0.02968231\n",
      "Iteration 48466, loss = 0.02970401\n",
      "Iteration 48467, loss = 0.02970022\n",
      "Iteration 48468, loss = 0.02968355\n",
      "Iteration 48469, loss = 0.02969361\n",
      "Iteration 48470, loss = 0.02969261\n",
      "Iteration 48471, loss = 0.02968978\n",
      "Iteration 48472, loss = 0.02970210\n",
      "Iteration 48473, loss = 0.02969486\n",
      "Iteration 48474, loss = 0.02969563\n",
      "Iteration 48475, loss = 0.02970149\n",
      "Iteration 48476, loss = 0.02969336\n",
      "Iteration 48477, loss = 0.02968300\n",
      "Iteration 48478, loss = 0.02968684\n",
      "Iteration 48479, loss = 0.02969584\n",
      "Iteration 48480, loss = 0.02969844\n",
      "Iteration 48481, loss = 0.02969673\n",
      "Iteration 48482, loss = 0.02968683\n",
      "Iteration 48483, loss = 0.02967868\n",
      "Iteration 48484, loss = 0.02969482\n",
      "Iteration 48485, loss = 0.02970093\n",
      "Iteration 48486, loss = 0.02970449\n",
      "Iteration 48487, loss = 0.02969701\n",
      "Iteration 48488, loss = 0.02968327\n",
      "Iteration 48489, loss = 0.02968386\n",
      "Iteration 48490, loss = 0.02968343\n",
      "Iteration 48491, loss = 0.02969422\n",
      "Iteration 48492, loss = 0.02968505\n",
      "Iteration 48493, loss = 0.02967965\n",
      "Iteration 48494, loss = 0.02968536\n",
      "Iteration 48495, loss = 0.02969045\n",
      "Iteration 48496, loss = 0.02969321\n",
      "Iteration 48497, loss = 0.02969472\n",
      "Iteration 48498, loss = 0.02968147\n",
      "Iteration 48499, loss = 0.02967989\n",
      "Iteration 48500, loss = 0.02969335\n",
      "Iteration 48501, loss = 0.02970219\n",
      "Iteration 48502, loss = 0.02968418\n",
      "Iteration 48503, loss = 0.02968852\n",
      "Iteration 48504, loss = 0.02969349\n",
      "Iteration 48505, loss = 0.02970318\n",
      "Iteration 48506, loss = 0.02970642\n",
      "Iteration 48507, loss = 0.02969542\n",
      "Iteration 48508, loss = 0.02967414\n",
      "Iteration 48509, loss = 0.02967696\n",
      "Iteration 48510, loss = 0.02969520\n",
      "Iteration 48511, loss = 0.02969725\n",
      "Iteration 48512, loss = 0.02969920\n",
      "Iteration 48513, loss = 0.02968293\n",
      "Iteration 48514, loss = 0.02967535\n",
      "Iteration 48515, loss = 0.02968565\n",
      "Iteration 48516, loss = 0.02969439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 48517, loss = 0.02969353\n",
      "Iteration 48518, loss = 0.02968502\n",
      "Iteration 48519, loss = 0.02967416\n",
      "Iteration 48520, loss = 0.02967463\n",
      "Iteration 48521, loss = 0.02968704\n",
      "Iteration 48522, loss = 0.02968336\n",
      "Iteration 48523, loss = 0.02966115\n",
      "Iteration 48524, loss = 0.02967401\n",
      "Iteration 48525, loss = 0.02968505\n",
      "Iteration 48526, loss = 0.02968080\n",
      "Iteration 48527, loss = 0.02967598\n",
      "Iteration 48528, loss = 0.02967880\n",
      "Iteration 48529, loss = 0.02966619\n",
      "Iteration 48530, loss = 0.02966140\n",
      "Iteration 48531, loss = 0.02968233\n",
      "Iteration 48532, loss = 0.02969415\n",
      "Iteration 48533, loss = 0.02968657\n",
      "Iteration 48534, loss = 0.02966571\n",
      "Iteration 48535, loss = 0.02966412\n",
      "Iteration 48536, loss = 0.02969404\n",
      "Iteration 48537, loss = 0.02970429\n",
      "Iteration 48538, loss = 0.02969841\n",
      "Iteration 48539, loss = 0.02968367\n",
      "Iteration 48540, loss = 0.02969386\n",
      "Iteration 48541, loss = 0.02969311\n",
      "Iteration 48542, loss = 0.02968559\n",
      "Iteration 48543, loss = 0.02967802\n",
      "Iteration 48544, loss = 0.02967924\n",
      "Iteration 48545, loss = 0.02966819\n",
      "Iteration 48546, loss = 0.02966414\n",
      "Iteration 48547, loss = 0.02967781\n",
      "Iteration 48548, loss = 0.02967769\n",
      "Iteration 48549, loss = 0.02966790\n",
      "Iteration 48550, loss = 0.02967068\n",
      "Iteration 48551, loss = 0.02967502\n",
      "Iteration 48552, loss = 0.02967442\n",
      "Iteration 48553, loss = 0.02967018\n",
      "Iteration 48554, loss = 0.02966046\n",
      "Iteration 48555, loss = 0.02965244\n",
      "Iteration 48556, loss = 0.02966199\n",
      "Iteration 48557, loss = 0.02965596\n",
      "Iteration 48558, loss = 0.02965652\n",
      "Iteration 48559, loss = 0.02965948\n",
      "Iteration 48560, loss = 0.02965871\n",
      "Iteration 48561, loss = 0.02966417\n",
      "Iteration 48562, loss = 0.02966765\n",
      "Iteration 48563, loss = 0.02966914\n",
      "Iteration 48564, loss = 0.02966623\n",
      "Iteration 48565, loss = 0.02966452\n",
      "Iteration 48566, loss = 0.02965580\n",
      "Iteration 48567, loss = 0.02966813\n",
      "Iteration 48568, loss = 0.02966807\n",
      "Iteration 48569, loss = 0.02966370\n",
      "Iteration 48570, loss = 0.02964897\n",
      "Iteration 48571, loss = 0.02965292\n",
      "Iteration 48572, loss = 0.02966391\n",
      "Iteration 48573, loss = 0.02967182\n",
      "Iteration 48574, loss = 0.02966773\n",
      "Iteration 48575, loss = 0.02965883\n",
      "Iteration 48576, loss = 0.02964330\n",
      "Iteration 48577, loss = 0.02965819\n",
      "Iteration 48578, loss = 0.02966500\n",
      "Iteration 48579, loss = 0.02966155\n",
      "Iteration 48580, loss = 0.02964905\n",
      "Iteration 48581, loss = 0.02964982\n",
      "Iteration 48582, loss = 0.02965721\n",
      "Iteration 48583, loss = 0.02966000\n",
      "Iteration 48584, loss = 0.02965985\n",
      "Iteration 48585, loss = 0.02965588\n",
      "Iteration 48586, loss = 0.02964715\n",
      "Iteration 48587, loss = 0.02965188\n",
      "Iteration 48588, loss = 0.02964127\n",
      "Iteration 48589, loss = 0.02964750\n",
      "Iteration 48590, loss = 0.02965378\n",
      "Iteration 48591, loss = 0.02965073\n",
      "Iteration 48592, loss = 0.02964244\n",
      "Iteration 48593, loss = 0.02964520\n",
      "Iteration 48594, loss = 0.02964244\n",
      "Iteration 48595, loss = 0.02963770\n",
      "Iteration 48596, loss = 0.02964906\n",
      "Iteration 48597, loss = 0.02965509\n",
      "Iteration 48598, loss = 0.02964825\n",
      "Iteration 48599, loss = 0.02964441\n",
      "Iteration 48600, loss = 0.02964022\n",
      "Iteration 48601, loss = 0.02964201\n",
      "Iteration 48602, loss = 0.02964811\n",
      "Iteration 48603, loss = 0.02964745\n",
      "Iteration 48604, loss = 0.02965011\n",
      "Iteration 48605, loss = 0.02964369\n",
      "Iteration 48606, loss = 0.02966008\n",
      "Iteration 48607, loss = 0.02965740\n",
      "Iteration 48608, loss = 0.02964891\n",
      "Iteration 48609, loss = 0.02964545\n",
      "Iteration 48610, loss = 0.02965331\n",
      "Iteration 48611, loss = 0.02965713\n",
      "Iteration 48612, loss = 0.02965437\n",
      "Iteration 48613, loss = 0.02964219\n",
      "Iteration 48614, loss = 0.02963633\n",
      "Iteration 48615, loss = 0.02963579\n",
      "Iteration 48616, loss = 0.02963493\n",
      "Iteration 48617, loss = 0.02963929\n",
      "Iteration 48618, loss = 0.02964835\n",
      "Iteration 48619, loss = 0.02964592\n",
      "Iteration 48620, loss = 0.02964407\n",
      "Iteration 48621, loss = 0.02964965\n",
      "Iteration 48622, loss = 0.02964629\n",
      "Iteration 48623, loss = 0.02963837\n",
      "Iteration 48624, loss = 0.02963673\n",
      "Iteration 48625, loss = 0.02963827\n",
      "Iteration 48626, loss = 0.02962738\n",
      "Iteration 48627, loss = 0.02963746\n",
      "Iteration 48628, loss = 0.02963570\n",
      "Iteration 48629, loss = 0.02964043\n",
      "Iteration 48630, loss = 0.02963281\n",
      "Iteration 48631, loss = 0.02963106\n",
      "Iteration 48632, loss = 0.02964392\n",
      "Iteration 48633, loss = 0.02964969\n",
      "Iteration 48634, loss = 0.02964647\n",
      "Iteration 48635, loss = 0.02963107\n",
      "Iteration 48636, loss = 0.02963769\n",
      "Iteration 48637, loss = 0.02964646\n",
      "Iteration 48638, loss = 0.02964501\n",
      "Iteration 48639, loss = 0.02964779\n",
      "Iteration 48640, loss = 0.02964632\n",
      "Iteration 48641, loss = 0.02964601\n",
      "Iteration 48642, loss = 0.02966725\n",
      "Iteration 48643, loss = 0.02967224\n",
      "Iteration 48644, loss = 0.02966206\n",
      "Iteration 48645, loss = 0.02963608\n",
      "Iteration 48646, loss = 0.02964002\n",
      "Iteration 48647, loss = 0.02966177\n",
      "Iteration 48648, loss = 0.02965637\n",
      "Iteration 48649, loss = 0.02962483\n",
      "Iteration 48650, loss = 0.02962859\n",
      "Iteration 48651, loss = 0.02964242\n",
      "Iteration 48652, loss = 0.02965041\n",
      "Iteration 48653, loss = 0.02964765\n",
      "Iteration 48654, loss = 0.02963591\n",
      "Iteration 48655, loss = 0.02964774\n",
      "Iteration 48656, loss = 0.02964292\n",
      "Iteration 48657, loss = 0.02962284\n",
      "Iteration 48658, loss = 0.02962953\n",
      "Iteration 48659, loss = 0.02964299\n",
      "Iteration 48660, loss = 0.02964146\n",
      "Iteration 48661, loss = 0.02963142\n",
      "Iteration 48662, loss = 0.02962374\n",
      "Iteration 48663, loss = 0.02963258\n",
      "Iteration 48664, loss = 0.02963641\n",
      "Iteration 48665, loss = 0.02962465\n",
      "Iteration 48666, loss = 0.02961858\n",
      "Iteration 48667, loss = 0.02963580\n",
      "Iteration 48668, loss = 0.02963244\n",
      "Iteration 48669, loss = 0.02964451\n",
      "Iteration 48670, loss = 0.02964303\n",
      "Iteration 48671, loss = 0.02964415\n",
      "Iteration 48672, loss = 0.02963693\n",
      "Iteration 48673, loss = 0.02963688\n",
      "Iteration 48674, loss = 0.02963770\n",
      "Iteration 48675, loss = 0.02963082\n",
      "Iteration 48676, loss = 0.02963586\n",
      "Iteration 48677, loss = 0.02963602\n",
      "Iteration 48678, loss = 0.02963364\n",
      "Iteration 48679, loss = 0.02962417\n",
      "Iteration 48680, loss = 0.02962812\n",
      "Iteration 48681, loss = 0.02961963\n",
      "Iteration 48682, loss = 0.02961683\n",
      "Iteration 48683, loss = 0.02962000\n",
      "Iteration 48684, loss = 0.02962351\n",
      "Iteration 48685, loss = 0.02961648\n",
      "Iteration 48686, loss = 0.02960663\n",
      "Iteration 48687, loss = 0.02961153\n",
      "Iteration 48688, loss = 0.02960697\n",
      "Iteration 48689, loss = 0.02960199\n",
      "Iteration 48690, loss = 0.02961126\n",
      "Iteration 48691, loss = 0.02960877\n",
      "Iteration 48692, loss = 0.02961441\n",
      "Iteration 48693, loss = 0.02961529\n",
      "Iteration 48694, loss = 0.02961053\n",
      "Iteration 48695, loss = 0.02960509\n",
      "Iteration 48696, loss = 0.02960601\n",
      "Iteration 48697, loss = 0.02961804\n",
      "Iteration 48698, loss = 0.02962163\n",
      "Iteration 48699, loss = 0.02960042\n",
      "Iteration 48700, loss = 0.02960919\n",
      "Iteration 48701, loss = 0.02962048\n",
      "Iteration 48702, loss = 0.02962810\n",
      "Iteration 48703, loss = 0.02962880\n",
      "Iteration 48704, loss = 0.02961838\n",
      "Iteration 48705, loss = 0.02961010\n",
      "Iteration 48706, loss = 0.02961407\n",
      "Iteration 48707, loss = 0.02961266\n",
      "Iteration 48708, loss = 0.02961715\n",
      "Iteration 48709, loss = 0.02960338\n",
      "Iteration 48710, loss = 0.02960688\n",
      "Iteration 48711, loss = 0.02961752\n",
      "Iteration 48712, loss = 0.02961077\n",
      "Iteration 48713, loss = 0.02960875\n",
      "Iteration 48714, loss = 0.02961639\n",
      "Iteration 48715, loss = 0.02960936\n",
      "Iteration 48716, loss = 0.02959767\n",
      "Iteration 48717, loss = 0.02960607\n",
      "Iteration 48718, loss = 0.02961236\n",
      "Iteration 48719, loss = 0.02961442\n",
      "Iteration 48720, loss = 0.02960584\n",
      "Iteration 48721, loss = 0.02961056\n",
      "Iteration 48722, loss = 0.02960955\n",
      "Iteration 48723, loss = 0.02961759\n",
      "Iteration 48724, loss = 0.02961189\n",
      "Iteration 48725, loss = 0.02962039\n",
      "Iteration 48726, loss = 0.02961382\n",
      "Iteration 48727, loss = 0.02961175\n",
      "Iteration 48728, loss = 0.02960700\n",
      "Iteration 48729, loss = 0.02960610\n",
      "Iteration 48730, loss = 0.02960549\n",
      "Iteration 48731, loss = 0.02960462\n",
      "Iteration 48732, loss = 0.02960020\n",
      "Iteration 48733, loss = 0.02959608\n",
      "Iteration 48734, loss = 0.02958703\n",
      "Iteration 48735, loss = 0.02959820\n",
      "Iteration 48736, loss = 0.02959948\n",
      "Iteration 48737, loss = 0.02959306\n",
      "Iteration 48738, loss = 0.02959303\n",
      "Iteration 48739, loss = 0.02960287\n",
      "Iteration 48740, loss = 0.02960080\n",
      "Iteration 48741, loss = 0.02958927\n",
      "Iteration 48742, loss = 0.02959337\n",
      "Iteration 48743, loss = 0.02960248\n",
      "Iteration 48744, loss = 0.02960268\n",
      "Iteration 48745, loss = 0.02960234\n",
      "Iteration 48746, loss = 0.02959071\n",
      "Iteration 48747, loss = 0.02958910\n",
      "Iteration 48748, loss = 0.02959051\n",
      "Iteration 48749, loss = 0.02959748\n",
      "Iteration 48750, loss = 0.02960070\n",
      "Iteration 48751, loss = 0.02960336\n",
      "Iteration 48752, loss = 0.02959532\n",
      "Iteration 48753, loss = 0.02958474\n",
      "Iteration 48754, loss = 0.02959395\n",
      "Iteration 48755, loss = 0.02959921\n",
      "Iteration 48756, loss = 0.02958343\n",
      "Iteration 48757, loss = 0.02958988\n",
      "Iteration 48758, loss = 0.02960425\n",
      "Iteration 48759, loss = 0.02960652\n",
      "Iteration 48760, loss = 0.02959545\n",
      "Iteration 48761, loss = 0.02960396\n",
      "Iteration 48762, loss = 0.02960538\n",
      "Iteration 48763, loss = 0.02959023\n",
      "Iteration 48764, loss = 0.02959281\n",
      "Iteration 48765, loss = 0.02961180\n",
      "Iteration 48766, loss = 0.02961673\n",
      "Iteration 48767, loss = 0.02960166\n",
      "Iteration 48768, loss = 0.02960915\n",
      "Iteration 48769, loss = 0.02961208\n",
      "Iteration 48770, loss = 0.02960873\n",
      "Iteration 48771, loss = 0.02960376\n",
      "Iteration 48772, loss = 0.02961258\n",
      "Iteration 48773, loss = 0.02961053\n",
      "Iteration 48774, loss = 0.02958794\n",
      "Iteration 48775, loss = 0.02958896\n",
      "Iteration 48776, loss = 0.02959635\n",
      "Iteration 48777, loss = 0.02960347\n",
      "Iteration 48778, loss = 0.02959286\n",
      "Iteration 48779, loss = 0.02958565\n",
      "Iteration 48780, loss = 0.02958155\n",
      "Iteration 48781, loss = 0.02959945\n",
      "Iteration 48782, loss = 0.02960285\n",
      "Iteration 48783, loss = 0.02959531\n",
      "Iteration 48784, loss = 0.02959539\n",
      "Iteration 48785, loss = 0.02957745\n",
      "Iteration 48786, loss = 0.02957751\n",
      "Iteration 48787, loss = 0.02957595\n",
      "Iteration 48788, loss = 0.02957744\n",
      "Iteration 48789, loss = 0.02958152\n",
      "Iteration 48790, loss = 0.02957253\n",
      "Iteration 48791, loss = 0.02957596\n",
      "Iteration 48792, loss = 0.02958017\n",
      "Iteration 48793, loss = 0.02958058\n",
      "Iteration 48794, loss = 0.02958350\n",
      "Iteration 48795, loss = 0.02958037\n",
      "Iteration 48796, loss = 0.02957103\n",
      "Iteration 48797, loss = 0.02957744\n",
      "Iteration 48798, loss = 0.02957866\n",
      "Iteration 48799, loss = 0.02958501\n",
      "Iteration 48800, loss = 0.02957935\n",
      "Iteration 48801, loss = 0.02956162\n",
      "Iteration 48802, loss = 0.02957607\n",
      "Iteration 48803, loss = 0.02957665\n",
      "Iteration 48804, loss = 0.02958247\n",
      "Iteration 48805, loss = 0.02956825\n",
      "Iteration 48806, loss = 0.02958091\n",
      "Iteration 48807, loss = 0.02959454\n",
      "Iteration 48808, loss = 0.02959306\n",
      "Iteration 48809, loss = 0.02959174\n",
      "Iteration 48810, loss = 0.02958575\n",
      "Iteration 48811, loss = 0.02957981\n",
      "Iteration 48812, loss = 0.02958408\n",
      "Iteration 48813, loss = 0.02958795\n",
      "Iteration 48814, loss = 0.02958622\n",
      "Iteration 48815, loss = 0.02957379\n",
      "Iteration 48816, loss = 0.02956444\n",
      "Iteration 48817, loss = 0.02958286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 48818, loss = 0.02959045\n",
      "Iteration 48819, loss = 0.02957381\n",
      "Iteration 48820, loss = 0.02956425\n",
      "Iteration 48821, loss = 0.02957657\n",
      "Iteration 48822, loss = 0.02958458\n",
      "Iteration 48823, loss = 0.02958140\n",
      "Iteration 48824, loss = 0.02956724\n",
      "Iteration 48825, loss = 0.02956627\n",
      "Iteration 48826, loss = 0.02956251\n",
      "Iteration 48827, loss = 0.02956405\n",
      "Iteration 48828, loss = 0.02956645\n",
      "Iteration 48829, loss = 0.02957113\n",
      "Iteration 48830, loss = 0.02956812\n",
      "Iteration 48831, loss = 0.02956438\n",
      "Iteration 48832, loss = 0.02955921\n",
      "Iteration 48833, loss = 0.02956136\n",
      "Iteration 48834, loss = 0.02956246\n",
      "Iteration 48835, loss = 0.02956984\n",
      "Iteration 48836, loss = 0.02956862\n",
      "Iteration 48837, loss = 0.02956414\n",
      "Iteration 48838, loss = 0.02956824\n",
      "Iteration 48839, loss = 0.02956567\n",
      "Iteration 48840, loss = 0.02956085\n",
      "Iteration 48841, loss = 0.02955982\n",
      "Iteration 48842, loss = 0.02955455\n",
      "Iteration 48843, loss = 0.02955443\n",
      "Iteration 48844, loss = 0.02956189\n",
      "Iteration 48845, loss = 0.02956020\n",
      "Iteration 48846, loss = 0.02955141\n",
      "Iteration 48847, loss = 0.02955576\n",
      "Iteration 48848, loss = 0.02955843\n",
      "Iteration 48849, loss = 0.02956232\n",
      "Iteration 48850, loss = 0.02955543\n",
      "Iteration 48851, loss = 0.02956070\n",
      "Iteration 48852, loss = 0.02956408\n",
      "Iteration 48853, loss = 0.02956976\n",
      "Iteration 48854, loss = 0.02956130\n",
      "Iteration 48855, loss = 0.02955763\n",
      "Iteration 48856, loss = 0.02956228\n",
      "Iteration 48857, loss = 0.02956140\n",
      "Iteration 48858, loss = 0.02955581\n",
      "Iteration 48859, loss = 0.02955975\n",
      "Iteration 48860, loss = 0.02955462\n",
      "Iteration 48861, loss = 0.02956525\n",
      "Iteration 48862, loss = 0.02956621\n",
      "Iteration 48863, loss = 0.02955989\n",
      "Iteration 48864, loss = 0.02954890\n",
      "Iteration 48865, loss = 0.02955407\n",
      "Iteration 48866, loss = 0.02954959\n",
      "Iteration 48867, loss = 0.02954637\n",
      "Iteration 48868, loss = 0.02955654\n",
      "Iteration 48869, loss = 0.02955192\n",
      "Iteration 48870, loss = 0.02954468\n",
      "Iteration 48871, loss = 0.02954106\n",
      "Iteration 48872, loss = 0.02955779\n",
      "Iteration 48873, loss = 0.02955708\n",
      "Iteration 48874, loss = 0.02954641\n",
      "Iteration 48875, loss = 0.02954914\n",
      "Iteration 48876, loss = 0.02955506\n",
      "Iteration 48877, loss = 0.02956068\n",
      "Iteration 48878, loss = 0.02956205\n",
      "Iteration 48879, loss = 0.02955155\n",
      "Iteration 48880, loss = 0.02954379\n",
      "Iteration 48881, loss = 0.02954086\n",
      "Iteration 48882, loss = 0.02954328\n",
      "Iteration 48883, loss = 0.02954080\n",
      "Iteration 48884, loss = 0.02953845\n",
      "Iteration 48885, loss = 0.02954751\n",
      "Iteration 48886, loss = 0.02954582\n",
      "Iteration 48887, loss = 0.02953531\n",
      "Iteration 48888, loss = 0.02954223\n",
      "Iteration 48889, loss = 0.02954765\n",
      "Iteration 48890, loss = 0.02954350\n",
      "Iteration 48891, loss = 0.02954264\n",
      "Iteration 48892, loss = 0.02954063\n",
      "Iteration 48893, loss = 0.02954049\n",
      "Iteration 48894, loss = 0.02954009\n",
      "Iteration 48895, loss = 0.02953341\n",
      "Iteration 48896, loss = 0.02953945\n",
      "Iteration 48897, loss = 0.02953164\n",
      "Iteration 48898, loss = 0.02953800\n",
      "Iteration 48899, loss = 0.02954201\n",
      "Iteration 48900, loss = 0.02954508\n",
      "Iteration 48901, loss = 0.02954719\n",
      "Iteration 48902, loss = 0.02954710\n",
      "Iteration 48903, loss = 0.02953677\n",
      "Iteration 48904, loss = 0.02953279\n",
      "Iteration 48905, loss = 0.02953211\n",
      "Iteration 48906, loss = 0.02953401\n",
      "Iteration 48907, loss = 0.02953057\n",
      "Iteration 48908, loss = 0.02953304\n",
      "Iteration 48909, loss = 0.02953823\n",
      "Iteration 48910, loss = 0.02953394\n",
      "Iteration 48911, loss = 0.02953054\n",
      "Iteration 48912, loss = 0.02953352\n",
      "Iteration 48913, loss = 0.02953841\n",
      "Iteration 48914, loss = 0.02953100\n",
      "Iteration 48915, loss = 0.02952989\n",
      "Iteration 48916, loss = 0.02954096\n",
      "Iteration 48917, loss = 0.02954467\n",
      "Iteration 48918, loss = 0.02953270\n",
      "Iteration 48919, loss = 0.02953247\n",
      "Iteration 48920, loss = 0.02953367\n",
      "Iteration 48921, loss = 0.02953428\n",
      "Iteration 48922, loss = 0.02953398\n",
      "Iteration 48923, loss = 0.02953569\n",
      "Iteration 48924, loss = 0.02952524\n",
      "Iteration 48925, loss = 0.02953160\n",
      "Iteration 48926, loss = 0.02954776\n",
      "Iteration 48927, loss = 0.02954245\n",
      "Iteration 48928, loss = 0.02952018\n",
      "Iteration 48929, loss = 0.02952753\n",
      "Iteration 48930, loss = 0.02953595\n",
      "Iteration 48931, loss = 0.02954131\n",
      "Iteration 48932, loss = 0.02954313\n",
      "Iteration 48933, loss = 0.02953944\n",
      "Iteration 48934, loss = 0.02953308\n",
      "Iteration 48935, loss = 0.02952516\n",
      "Iteration 48936, loss = 0.02952395\n",
      "Iteration 48937, loss = 0.02953018\n",
      "Iteration 48938, loss = 0.02952601\n",
      "Iteration 48939, loss = 0.02951743\n",
      "Iteration 48940, loss = 0.02952728\n",
      "Iteration 48941, loss = 0.02952539\n",
      "Iteration 48942, loss = 0.02951733\n",
      "Iteration 48943, loss = 0.02952361\n",
      "Iteration 48944, loss = 0.02952153\n",
      "Iteration 48945, loss = 0.02952382\n",
      "Iteration 48946, loss = 0.02952593\n",
      "Iteration 48947, loss = 0.02951985\n",
      "Iteration 48948, loss = 0.02953511\n",
      "Iteration 48949, loss = 0.02953909\n",
      "Iteration 48950, loss = 0.02953659\n",
      "Iteration 48951, loss = 0.02952703\n",
      "Iteration 48952, loss = 0.02952179\n",
      "Iteration 48953, loss = 0.02953198\n",
      "Iteration 48954, loss = 0.02952927\n",
      "Iteration 48955, loss = 0.02952296\n",
      "Iteration 48956, loss = 0.02952494\n",
      "Iteration 48957, loss = 0.02952792\n",
      "Iteration 48958, loss = 0.02952329\n",
      "Iteration 48959, loss = 0.02951650\n",
      "Iteration 48960, loss = 0.02951111\n",
      "Iteration 48961, loss = 0.02952279\n",
      "Iteration 48962, loss = 0.02952566\n",
      "Iteration 48963, loss = 0.02951947\n",
      "Iteration 48964, loss = 0.02951272\n",
      "Iteration 48965, loss = 0.02951035\n",
      "Iteration 48966, loss = 0.02950775\n",
      "Iteration 48967, loss = 0.02951548\n",
      "Iteration 48968, loss = 0.02950833\n",
      "Iteration 48969, loss = 0.02952479\n",
      "Iteration 48970, loss = 0.02952204\n",
      "Iteration 48971, loss = 0.02951731\n",
      "Iteration 48972, loss = 0.02951546\n",
      "Iteration 48973, loss = 0.02951606\n",
      "Iteration 48974, loss = 0.02951787\n",
      "Iteration 48975, loss = 0.02952777\n",
      "Iteration 48976, loss = 0.02952102\n",
      "Iteration 48977, loss = 0.02951485\n",
      "Iteration 48978, loss = 0.02952649\n",
      "Iteration 48979, loss = 0.02953124\n",
      "Iteration 48980, loss = 0.02952820\n",
      "Iteration 48981, loss = 0.02951892\n",
      "Iteration 48982, loss = 0.02952175\n",
      "Iteration 48983, loss = 0.02953827\n",
      "Iteration 48984, loss = 0.02953987\n",
      "Iteration 48985, loss = 0.02952665\n",
      "Iteration 48986, loss = 0.02951741\n",
      "Iteration 48987, loss = 0.02951651\n",
      "Iteration 48988, loss = 0.02951042\n",
      "Iteration 48989, loss = 0.02951646\n",
      "Iteration 48990, loss = 0.02952784\n",
      "Iteration 48991, loss = 0.02952251\n",
      "Iteration 48992, loss = 0.02952197\n",
      "Iteration 48993, loss = 0.02951173\n",
      "Iteration 48994, loss = 0.02951571\n",
      "Iteration 48995, loss = 0.02952236\n",
      "Iteration 48996, loss = 0.02953840\n",
      "Iteration 48997, loss = 0.02952841\n",
      "Iteration 48998, loss = 0.02951063\n",
      "Iteration 48999, loss = 0.02951099\n",
      "Iteration 49000, loss = 0.02951826\n",
      "Iteration 49001, loss = 0.02950538\n",
      "Iteration 49002, loss = 0.02950145\n",
      "Iteration 49003, loss = 0.02949513\n",
      "Iteration 49004, loss = 0.02950379\n",
      "Iteration 49005, loss = 0.02950307\n",
      "Iteration 49006, loss = 0.02950038\n",
      "Iteration 49007, loss = 0.02950834\n",
      "Iteration 49008, loss = 0.02950807\n",
      "Iteration 49009, loss = 0.02950596\n",
      "Iteration 49010, loss = 0.02950269\n",
      "Iteration 49011, loss = 0.02950279\n",
      "Iteration 49012, loss = 0.02950414\n",
      "Iteration 49013, loss = 0.02949859\n",
      "Iteration 49014, loss = 0.02949953\n",
      "Iteration 49015, loss = 0.02949274\n",
      "Iteration 49016, loss = 0.02949856\n",
      "Iteration 49017, loss = 0.02950611\n",
      "Iteration 49018, loss = 0.02950913\n",
      "Iteration 49019, loss = 0.02950459\n",
      "Iteration 49020, loss = 0.02951150\n",
      "Iteration 49021, loss = 0.02950770\n",
      "Iteration 49022, loss = 0.02951416\n",
      "Iteration 49023, loss = 0.02952209\n",
      "Iteration 49024, loss = 0.02950279\n",
      "Iteration 49025, loss = 0.02949860\n",
      "Iteration 49026, loss = 0.02951321\n",
      "Iteration 49027, loss = 0.02951149\n",
      "Iteration 49028, loss = 0.02950092\n",
      "Iteration 49029, loss = 0.02949676\n",
      "Iteration 49030, loss = 0.02949798\n",
      "Iteration 49031, loss = 0.02950011\n",
      "Iteration 49032, loss = 0.02950127\n",
      "Iteration 49033, loss = 0.02950318\n",
      "Iteration 49034, loss = 0.02951040\n",
      "Iteration 49035, loss = 0.02949969\n",
      "Iteration 49036, loss = 0.02949902\n",
      "Iteration 49037, loss = 0.02949677\n",
      "Iteration 49038, loss = 0.02950662\n",
      "Iteration 49039, loss = 0.02950993\n",
      "Iteration 49040, loss = 0.02949432\n",
      "Iteration 49041, loss = 0.02949454\n",
      "Iteration 49042, loss = 0.02948994\n",
      "Iteration 49043, loss = 0.02949801\n",
      "Iteration 49044, loss = 0.02950802\n",
      "Iteration 49045, loss = 0.02950137\n",
      "Iteration 49046, loss = 0.02949284\n",
      "Iteration 49047, loss = 0.02949397\n",
      "Iteration 49048, loss = 0.02949504\n",
      "Iteration 49049, loss = 0.02949253\n",
      "Iteration 49050, loss = 0.02950108\n",
      "Iteration 49051, loss = 0.02949879\n",
      "Iteration 49052, loss = 0.02948298\n",
      "Iteration 49053, loss = 0.02949174\n",
      "Iteration 49054, loss = 0.02950058\n",
      "Iteration 49055, loss = 0.02948864\n",
      "Iteration 49056, loss = 0.02949415\n",
      "Iteration 49057, loss = 0.02950065\n",
      "Iteration 49058, loss = 0.02950252\n",
      "Iteration 49059, loss = 0.02949942\n",
      "Iteration 49060, loss = 0.02948722\n",
      "Iteration 49061, loss = 0.02948665\n",
      "Iteration 49062, loss = 0.02949668\n",
      "Iteration 49063, loss = 0.02949370\n",
      "Iteration 49064, loss = 0.02947617\n",
      "Iteration 49065, loss = 0.02948598\n",
      "Iteration 49066, loss = 0.02948771\n",
      "Iteration 49067, loss = 0.02949040\n",
      "Iteration 49068, loss = 0.02948661\n",
      "Iteration 49069, loss = 0.02948196\n",
      "Iteration 49070, loss = 0.02948000\n",
      "Iteration 49071, loss = 0.02949356\n",
      "Iteration 49072, loss = 0.02949872\n",
      "Iteration 49073, loss = 0.02948698\n",
      "Iteration 49074, loss = 0.02947308\n",
      "Iteration 49075, loss = 0.02949496\n",
      "Iteration 49076, loss = 0.02949542\n",
      "Iteration 49077, loss = 0.02949178\n",
      "Iteration 49078, loss = 0.02948280\n",
      "Iteration 49079, loss = 0.02947614\n",
      "Iteration 49080, loss = 0.02949138\n",
      "Iteration 49081, loss = 0.02949425\n",
      "Iteration 49082, loss = 0.02949121\n",
      "Iteration 49083, loss = 0.02948207\n",
      "Iteration 49084, loss = 0.02947870\n",
      "Iteration 49085, loss = 0.02948813\n",
      "Iteration 49086, loss = 0.02948488\n",
      "Iteration 49087, loss = 0.02949042\n",
      "Iteration 49088, loss = 0.02948408\n",
      "Iteration 49089, loss = 0.02948504\n",
      "Iteration 49090, loss = 0.02948332\n",
      "Iteration 49091, loss = 0.02948293\n",
      "Iteration 49092, loss = 0.02947461\n",
      "Iteration 49093, loss = 0.02946739\n",
      "Iteration 49094, loss = 0.02948746\n",
      "Iteration 49095, loss = 0.02949405\n",
      "Iteration 49096, loss = 0.02948824\n",
      "Iteration 49097, loss = 0.02947031\n",
      "Iteration 49098, loss = 0.02947602\n",
      "Iteration 49099, loss = 0.02948256\n",
      "Iteration 49100, loss = 0.02949153\n",
      "Iteration 49101, loss = 0.02949888\n",
      "Iteration 49102, loss = 0.02949735\n",
      "Iteration 49103, loss = 0.02948585\n",
      "Iteration 49104, loss = 0.02949169\n",
      "Iteration 49105, loss = 0.02948690\n",
      "Iteration 49106, loss = 0.02948757\n",
      "Iteration 49107, loss = 0.02948820\n",
      "Iteration 49108, loss = 0.02947418\n",
      "Iteration 49109, loss = 0.02947015\n",
      "Iteration 49110, loss = 0.02947383\n",
      "Iteration 49111, loss = 0.02947621\n",
      "Iteration 49112, loss = 0.02947243\n",
      "Iteration 49113, loss = 0.02947248\n",
      "Iteration 49114, loss = 0.02947589\n",
      "Iteration 49115, loss = 0.02947021\n",
      "Iteration 49116, loss = 0.02945612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 49117, loss = 0.02947748\n",
      "Iteration 49118, loss = 0.02949212\n",
      "Iteration 49119, loss = 0.02948951\n",
      "Iteration 49120, loss = 0.02947747\n",
      "Iteration 49121, loss = 0.02948067\n",
      "Iteration 49122, loss = 0.02947712\n",
      "Iteration 49123, loss = 0.02948130\n",
      "Iteration 49124, loss = 0.02949185\n",
      "Iteration 49125, loss = 0.02949750\n",
      "Iteration 49126, loss = 0.02948564\n",
      "Iteration 49127, loss = 0.02946334\n",
      "Iteration 49128, loss = 0.02947364\n",
      "Iteration 49129, loss = 0.02949149\n",
      "Iteration 49130, loss = 0.02949392\n",
      "Iteration 49131, loss = 0.02947917\n",
      "Iteration 49132, loss = 0.02947046\n",
      "Iteration 49133, loss = 0.02946615\n",
      "Iteration 49134, loss = 0.02946793\n",
      "Iteration 49135, loss = 0.02946652\n",
      "Iteration 49136, loss = 0.02945838\n",
      "Iteration 49137, loss = 0.02945909\n",
      "Iteration 49138, loss = 0.02944697\n",
      "Iteration 49139, loss = 0.02946097\n",
      "Iteration 49140, loss = 0.02946391\n",
      "Iteration 49141, loss = 0.02946133\n",
      "Iteration 49142, loss = 0.02945374\n",
      "Iteration 49143, loss = 0.02945975\n",
      "Iteration 49144, loss = 0.02946288\n",
      "Iteration 49145, loss = 0.02946420\n",
      "Iteration 49146, loss = 0.02946409\n",
      "Iteration 49147, loss = 0.02947068\n",
      "Iteration 49148, loss = 0.02946287\n",
      "Iteration 49149, loss = 0.02945814\n",
      "Iteration 49150, loss = 0.02946672\n",
      "Iteration 49151, loss = 0.02946658\n",
      "Iteration 49152, loss = 0.02947073\n",
      "Iteration 49153, loss = 0.02946936\n",
      "Iteration 49154, loss = 0.02946889\n",
      "Iteration 49155, loss = 0.02946432\n",
      "Iteration 49156, loss = 0.02946439\n",
      "Iteration 49157, loss = 0.02946355\n",
      "Iteration 49158, loss = 0.02946358\n",
      "Iteration 49159, loss = 0.02946121\n",
      "Iteration 49160, loss = 0.02944547\n",
      "Iteration 49161, loss = 0.02944889\n",
      "Iteration 49162, loss = 0.02944687\n",
      "Iteration 49163, loss = 0.02944854\n",
      "Iteration 49164, loss = 0.02944753\n",
      "Iteration 49165, loss = 0.02945439\n",
      "Iteration 49166, loss = 0.02945325\n",
      "Iteration 49167, loss = 0.02944579\n",
      "Iteration 49168, loss = 0.02944506\n",
      "Iteration 49169, loss = 0.02944362\n",
      "Iteration 49170, loss = 0.02944747\n",
      "Iteration 49171, loss = 0.02945036\n",
      "Iteration 49172, loss = 0.02944731\n",
      "Iteration 49173, loss = 0.02944808\n",
      "Iteration 49174, loss = 0.02944575\n",
      "Iteration 49175, loss = 0.02945238\n",
      "Iteration 49176, loss = 0.02946469\n",
      "Iteration 49177, loss = 0.02945704\n",
      "Iteration 49178, loss = 0.02946298\n",
      "Iteration 49179, loss = 0.02946568\n",
      "Iteration 49180, loss = 0.02945499\n",
      "Iteration 49181, loss = 0.02945421\n",
      "Iteration 49182, loss = 0.02945760\n",
      "Iteration 49183, loss = 0.02945201\n",
      "Iteration 49184, loss = 0.02943572\n",
      "Iteration 49185, loss = 0.02945011\n",
      "Iteration 49186, loss = 0.02944506\n",
      "Iteration 49187, loss = 0.02945411\n",
      "Iteration 49188, loss = 0.02945434\n",
      "Iteration 49189, loss = 0.02945467\n",
      "Iteration 49190, loss = 0.02945133\n",
      "Iteration 49191, loss = 0.02945503\n",
      "Iteration 49192, loss = 0.02945320\n",
      "Iteration 49193, loss = 0.02944248\n",
      "Iteration 49194, loss = 0.02943582\n",
      "Iteration 49195, loss = 0.02943472\n",
      "Iteration 49196, loss = 0.02943367\n",
      "Iteration 49197, loss = 0.02944035\n",
      "Iteration 49198, loss = 0.02944408\n",
      "Iteration 49199, loss = 0.02942901\n",
      "Iteration 49200, loss = 0.02944038\n",
      "Iteration 49201, loss = 0.02944727\n",
      "Iteration 49202, loss = 0.02944883\n",
      "Iteration 49203, loss = 0.02944564\n",
      "Iteration 49204, loss = 0.02942893\n",
      "Iteration 49205, loss = 0.02944642\n",
      "Iteration 49206, loss = 0.02945560\n",
      "Iteration 49207, loss = 0.02944579\n",
      "Iteration 49208, loss = 0.02943473\n",
      "Iteration 49209, loss = 0.02944469\n",
      "Iteration 49210, loss = 0.02944949\n",
      "Iteration 49211, loss = 0.02944890\n",
      "Iteration 49212, loss = 0.02944338\n",
      "Iteration 49213, loss = 0.02943456\n",
      "Iteration 49214, loss = 0.02943712\n",
      "Iteration 49215, loss = 0.02944913\n",
      "Iteration 49216, loss = 0.02944765\n",
      "Iteration 49217, loss = 0.02944485\n",
      "Iteration 49218, loss = 0.02943123\n",
      "Iteration 49219, loss = 0.02943569\n",
      "Iteration 49220, loss = 0.02943128\n",
      "Iteration 49221, loss = 0.02943399\n",
      "Iteration 49222, loss = 0.02942717\n",
      "Iteration 49223, loss = 0.02943106\n",
      "Iteration 49224, loss = 0.02943925\n",
      "Iteration 49225, loss = 0.02943307\n",
      "Iteration 49226, loss = 0.02944221\n",
      "Iteration 49227, loss = 0.02944954\n",
      "Iteration 49228, loss = 0.02944106\n",
      "Iteration 49229, loss = 0.02944673\n",
      "Iteration 49230, loss = 0.02944976\n",
      "Iteration 49231, loss = 0.02944305\n",
      "Iteration 49232, loss = 0.02942725\n",
      "Iteration 49233, loss = 0.02943143\n",
      "Iteration 49234, loss = 0.02943914\n",
      "Iteration 49235, loss = 0.02943274\n",
      "Iteration 49236, loss = 0.02941978\n",
      "Iteration 49237, loss = 0.02943026\n",
      "Iteration 49238, loss = 0.02943826\n",
      "Iteration 49239, loss = 0.02943261\n",
      "Iteration 49240, loss = 0.02942575\n",
      "Iteration 49241, loss = 0.02942103\n",
      "Iteration 49242, loss = 0.02941914\n",
      "Iteration 49243, loss = 0.02942130\n",
      "Iteration 49244, loss = 0.02941769\n",
      "Iteration 49245, loss = 0.02941991\n",
      "Iteration 49246, loss = 0.02941445\n",
      "Iteration 49247, loss = 0.02942143\n",
      "Iteration 49248, loss = 0.02942269\n",
      "Iteration 49249, loss = 0.02941646\n",
      "Iteration 49250, loss = 0.02941384\n",
      "Iteration 49251, loss = 0.02941482\n",
      "Iteration 49252, loss = 0.02941731\n",
      "Iteration 49253, loss = 0.02942431\n",
      "Iteration 49254, loss = 0.02942018\n",
      "Iteration 49255, loss = 0.02941012\n",
      "Iteration 49256, loss = 0.02941163\n",
      "Iteration 49257, loss = 0.02941045\n",
      "Iteration 49258, loss = 0.02940734\n",
      "Iteration 49259, loss = 0.02941962\n",
      "Iteration 49260, loss = 0.02941850\n",
      "Iteration 49261, loss = 0.02941100\n",
      "Iteration 49262, loss = 0.02941507\n",
      "Iteration 49263, loss = 0.02941360\n",
      "Iteration 49264, loss = 0.02941144\n",
      "Iteration 49265, loss = 0.02941074\n",
      "Iteration 49266, loss = 0.02941289\n",
      "Iteration 49267, loss = 0.02941838\n",
      "Iteration 49268, loss = 0.02941676\n",
      "Iteration 49269, loss = 0.02941618\n",
      "Iteration 49270, loss = 0.02940749\n",
      "Iteration 49271, loss = 0.02940741\n",
      "Iteration 49272, loss = 0.02940859\n",
      "Iteration 49273, loss = 0.02940967\n",
      "Iteration 49274, loss = 0.02941325\n",
      "Iteration 49275, loss = 0.02940976\n",
      "Iteration 49276, loss = 0.02940825\n",
      "Iteration 49277, loss = 0.02941048\n",
      "Iteration 49278, loss = 0.02942303\n",
      "Iteration 49279, loss = 0.02941767\n",
      "Iteration 49280, loss = 0.02941167\n",
      "Iteration 49281, loss = 0.02941871\n",
      "Iteration 49282, loss = 0.02942613\n",
      "Iteration 49283, loss = 0.02942286\n",
      "Iteration 49284, loss = 0.02942278\n",
      "Iteration 49285, loss = 0.02941928\n",
      "Iteration 49286, loss = 0.02942583\n",
      "Iteration 49287, loss = 0.02942786\n",
      "Iteration 49288, loss = 0.02942148\n",
      "Iteration 49289, loss = 0.02939856\n",
      "Iteration 49290, loss = 0.02941110\n",
      "Iteration 49291, loss = 0.02942419\n",
      "Iteration 49292, loss = 0.02941830\n",
      "Iteration 49293, loss = 0.02941499\n",
      "Iteration 49294, loss = 0.02941455\n",
      "Iteration 49295, loss = 0.02940710\n",
      "Iteration 49296, loss = 0.02939752\n",
      "Iteration 49297, loss = 0.02939467\n",
      "Iteration 49298, loss = 0.02940504\n",
      "Iteration 49299, loss = 0.02940315\n",
      "Iteration 49300, loss = 0.02940588\n",
      "Iteration 49301, loss = 0.02940799\n",
      "Iteration 49302, loss = 0.02940541\n",
      "Iteration 49303, loss = 0.02941039\n",
      "Iteration 49304, loss = 0.02940559\n",
      "Iteration 49305, loss = 0.02940758\n",
      "Iteration 49306, loss = 0.02940622\n",
      "Iteration 49307, loss = 0.02939809\n",
      "Iteration 49308, loss = 0.02939921\n",
      "Iteration 49309, loss = 0.02939964\n",
      "Iteration 49310, loss = 0.02939012\n",
      "Iteration 49311, loss = 0.02939836\n",
      "Iteration 49312, loss = 0.02941357\n",
      "Iteration 49313, loss = 0.02941768\n",
      "Iteration 49314, loss = 0.02941254\n",
      "Iteration 49315, loss = 0.02941306\n",
      "Iteration 49316, loss = 0.02940286\n",
      "Iteration 49317, loss = 0.02939486\n",
      "Iteration 49318, loss = 0.02941156\n",
      "Iteration 49319, loss = 0.02941656\n",
      "Iteration 49320, loss = 0.02941224\n",
      "Iteration 49321, loss = 0.02938915\n",
      "Iteration 49322, loss = 0.02941516\n",
      "Iteration 49323, loss = 0.02942539\n",
      "Iteration 49324, loss = 0.02941741\n",
      "Iteration 49325, loss = 0.02942133\n",
      "Iteration 49326, loss = 0.02943305\n",
      "Iteration 49327, loss = 0.02943445\n",
      "Iteration 49328, loss = 0.02942424\n",
      "Iteration 49329, loss = 0.02940335\n",
      "Iteration 49330, loss = 0.02939895\n",
      "Iteration 49331, loss = 0.02941949\n",
      "Iteration 49332, loss = 0.02941446\n",
      "Iteration 49333, loss = 0.02939318\n",
      "Iteration 49334, loss = 0.02939144\n",
      "Iteration 49335, loss = 0.02939395\n",
      "Iteration 49336, loss = 0.02939560\n",
      "Iteration 49337, loss = 0.02938656\n",
      "Iteration 49338, loss = 0.02938775\n",
      "Iteration 49339, loss = 0.02940056\n",
      "Iteration 49340, loss = 0.02939500\n",
      "Iteration 49341, loss = 0.02938956\n",
      "Iteration 49342, loss = 0.02938605\n",
      "Iteration 49343, loss = 0.02938662\n",
      "Iteration 49344, loss = 0.02938681\n",
      "Iteration 49345, loss = 0.02938342\n",
      "Iteration 49346, loss = 0.02938864\n",
      "Iteration 49347, loss = 0.02938955\n",
      "Iteration 49348, loss = 0.02938242\n",
      "Iteration 49349, loss = 0.02939457\n",
      "Iteration 49350, loss = 0.02939254\n",
      "Iteration 49351, loss = 0.02939475\n",
      "Iteration 49352, loss = 0.02939802\n",
      "Iteration 49353, loss = 0.02938391\n",
      "Iteration 49354, loss = 0.02937776\n",
      "Iteration 49355, loss = 0.02939913\n",
      "Iteration 49356, loss = 0.02939197\n",
      "Iteration 49357, loss = 0.02937971\n",
      "Iteration 49358, loss = 0.02938311\n",
      "Iteration 49359, loss = 0.02937995\n",
      "Iteration 49360, loss = 0.02937655\n",
      "Iteration 49361, loss = 0.02937364\n",
      "Iteration 49362, loss = 0.02937571\n",
      "Iteration 49363, loss = 0.02937964\n",
      "Iteration 49364, loss = 0.02937866\n",
      "Iteration 49365, loss = 0.02937539\n",
      "Iteration 49366, loss = 0.02937703\n",
      "Iteration 49367, loss = 0.02937551\n",
      "Iteration 49368, loss = 0.02937959\n",
      "Iteration 49369, loss = 0.02937267\n",
      "Iteration 49370, loss = 0.02938068\n",
      "Iteration 49371, loss = 0.02939256\n",
      "Iteration 49372, loss = 0.02938228\n",
      "Iteration 49373, loss = 0.02937669\n",
      "Iteration 49374, loss = 0.02937496\n",
      "Iteration 49375, loss = 0.02937958\n",
      "Iteration 49376, loss = 0.02938867\n",
      "Iteration 49377, loss = 0.02938324\n",
      "Iteration 49378, loss = 0.02936986\n",
      "Iteration 49379, loss = 0.02938120\n",
      "Iteration 49380, loss = 0.02938766\n",
      "Iteration 49381, loss = 0.02939091\n",
      "Iteration 49382, loss = 0.02938164\n",
      "Iteration 49383, loss = 0.02937122\n",
      "Iteration 49384, loss = 0.02938782\n",
      "Iteration 49385, loss = 0.02938393\n",
      "Iteration 49386, loss = 0.02937323\n",
      "Iteration 49387, loss = 0.02937550\n",
      "Iteration 49388, loss = 0.02938424\n",
      "Iteration 49389, loss = 0.02938107\n",
      "Iteration 49390, loss = 0.02938998\n",
      "Iteration 49391, loss = 0.02938739\n",
      "Iteration 49392, loss = 0.02938039\n",
      "Iteration 49393, loss = 0.02938566\n",
      "Iteration 49394, loss = 0.02938300\n",
      "Iteration 49395, loss = 0.02937936\n",
      "Iteration 49396, loss = 0.02938169\n",
      "Iteration 49397, loss = 0.02938033\n",
      "Iteration 49398, loss = 0.02937079\n",
      "Iteration 49399, loss = 0.02937907\n",
      "Iteration 49400, loss = 0.02938263\n",
      "Iteration 49401, loss = 0.02938816\n",
      "Iteration 49402, loss = 0.02938578\n",
      "Iteration 49403, loss = 0.02937716\n",
      "Iteration 49404, loss = 0.02936934\n",
      "Iteration 49405, loss = 0.02938242\n",
      "Iteration 49406, loss = 0.02938501\n",
      "Iteration 49407, loss = 0.02938930\n",
      "Iteration 49408, loss = 0.02937252\n",
      "Iteration 49409, loss = 0.02937027\n",
      "Iteration 49410, loss = 0.02938156\n",
      "Iteration 49411, loss = 0.02937734\n",
      "Iteration 49412, loss = 0.02937640\n",
      "Iteration 49413, loss = 0.02936742\n",
      "Iteration 49414, loss = 0.02938117\n",
      "Iteration 49415, loss = 0.02938179\n",
      "Iteration 49416, loss = 0.02936556\n",
      "Iteration 49417, loss = 0.02937131\n",
      "Iteration 49418, loss = 0.02938474\n",
      "Iteration 49419, loss = 0.02938054\n",
      "Iteration 49420, loss = 0.02937280\n",
      "Iteration 49421, loss = 0.02938788\n",
      "Iteration 49422, loss = 0.02938784\n",
      "Iteration 49423, loss = 0.02936800\n",
      "Iteration 49424, loss = 0.02937055\n",
      "Iteration 49425, loss = 0.02938520\n",
      "Iteration 49426, loss = 0.02938131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 49427, loss = 0.02935831\n",
      "Iteration 49428, loss = 0.02936820\n",
      "Iteration 49429, loss = 0.02936915\n",
      "Iteration 49430, loss = 0.02937220\n",
      "Iteration 49431, loss = 0.02937732\n",
      "Iteration 49432, loss = 0.02938249\n",
      "Iteration 49433, loss = 0.02936891\n",
      "Iteration 49434, loss = 0.02936550\n",
      "Iteration 49435, loss = 0.02936774\n",
      "Iteration 49436, loss = 0.02938040\n",
      "Iteration 49437, loss = 0.02938333\n",
      "Iteration 49438, loss = 0.02937667\n",
      "Iteration 49439, loss = 0.02936185\n",
      "Iteration 49440, loss = 0.02935930\n",
      "Iteration 49441, loss = 0.02937151\n",
      "Iteration 49442, loss = 0.02938323\n",
      "Iteration 49443, loss = 0.02937650\n",
      "Iteration 49444, loss = 0.02936653\n",
      "Iteration 49445, loss = 0.02935706\n",
      "Iteration 49446, loss = 0.02935899\n",
      "Iteration 49447, loss = 0.02935874\n",
      "Iteration 49448, loss = 0.02936515\n",
      "Iteration 49449, loss = 0.02936096\n",
      "Iteration 49450, loss = 0.02936099\n",
      "Iteration 49451, loss = 0.02935918\n",
      "Iteration 49452, loss = 0.02935697\n",
      "Iteration 49453, loss = 0.02935007\n",
      "Iteration 49454, loss = 0.02934404\n",
      "Iteration 49455, loss = 0.02934776\n",
      "Iteration 49456, loss = 0.02934977\n",
      "Iteration 49457, loss = 0.02934053\n",
      "Iteration 49458, loss = 0.02935222\n",
      "Iteration 49459, loss = 0.02935722\n",
      "Iteration 49460, loss = 0.02935040\n",
      "Iteration 49461, loss = 0.02933875\n",
      "Iteration 49462, loss = 0.02933993\n",
      "Iteration 49463, loss = 0.02934498\n",
      "Iteration 49464, loss = 0.02933661\n",
      "Iteration 49465, loss = 0.02934767\n",
      "Iteration 49466, loss = 0.02935153\n",
      "Iteration 49467, loss = 0.02934871\n",
      "Iteration 49468, loss = 0.02935057\n",
      "Iteration 49469, loss = 0.02935094\n",
      "Iteration 49470, loss = 0.02934689\n",
      "Iteration 49471, loss = 0.02935000\n",
      "Iteration 49472, loss = 0.02935419\n",
      "Iteration 49473, loss = 0.02934825\n",
      "Iteration 49474, loss = 0.02934724\n",
      "Iteration 49475, loss = 0.02935023\n",
      "Iteration 49476, loss = 0.02935583\n",
      "Iteration 49477, loss = 0.02935640\n",
      "Iteration 49478, loss = 0.02934996\n",
      "Iteration 49479, loss = 0.02935498\n",
      "Iteration 49480, loss = 0.02935751\n",
      "Iteration 49481, loss = 0.02937298\n",
      "Iteration 49482, loss = 0.02936483\n",
      "Iteration 49483, loss = 0.02933807\n",
      "Iteration 49484, loss = 0.02935819\n",
      "Iteration 49485, loss = 0.02937374\n",
      "Iteration 49486, loss = 0.02936705\n",
      "Iteration 49487, loss = 0.02934613\n",
      "Iteration 49488, loss = 0.02935440\n",
      "Iteration 49489, loss = 0.02936255\n",
      "Iteration 49490, loss = 0.02936714\n",
      "Iteration 49491, loss = 0.02936597\n",
      "Iteration 49492, loss = 0.02934873\n",
      "Iteration 49493, loss = 0.02933453\n",
      "Iteration 49494, loss = 0.02934733\n",
      "Iteration 49495, loss = 0.02936053\n",
      "Iteration 49496, loss = 0.02936423\n",
      "Iteration 49497, loss = 0.02934680\n",
      "Iteration 49498, loss = 0.02933082\n",
      "Iteration 49499, loss = 0.02934393\n",
      "Iteration 49500, loss = 0.02934776\n",
      "Iteration 49501, loss = 0.02935226\n",
      "Iteration 49502, loss = 0.02934932\n",
      "Iteration 49503, loss = 0.02935634\n",
      "Iteration 49504, loss = 0.02935713\n",
      "Iteration 49505, loss = 0.02933859\n",
      "Iteration 49506, loss = 0.02933760\n",
      "Iteration 49507, loss = 0.02937310\n",
      "Iteration 49508, loss = 0.02938465\n",
      "Iteration 49509, loss = 0.02937002\n",
      "Iteration 49510, loss = 0.02934910\n",
      "Iteration 49511, loss = 0.02936042\n",
      "Iteration 49512, loss = 0.02936086\n",
      "Iteration 49513, loss = 0.02935584\n",
      "Iteration 49514, loss = 0.02936924\n",
      "Iteration 49515, loss = 0.02936726\n",
      "Iteration 49516, loss = 0.02935033\n",
      "Iteration 49517, loss = 0.02935953\n",
      "Iteration 49518, loss = 0.02936252\n",
      "Iteration 49519, loss = 0.02937025\n",
      "Iteration 49520, loss = 0.02935815\n",
      "Iteration 49521, loss = 0.02935870\n",
      "Iteration 49522, loss = 0.02935566\n",
      "Iteration 49523, loss = 0.02934869\n",
      "Iteration 49524, loss = 0.02933315\n",
      "Iteration 49525, loss = 0.02934016\n",
      "Iteration 49526, loss = 0.02934015\n",
      "Iteration 49527, loss = 0.02933701\n",
      "Iteration 49528, loss = 0.02932706\n",
      "Iteration 49529, loss = 0.02933848\n",
      "Iteration 49530, loss = 0.02934101\n",
      "Iteration 49531, loss = 0.02932328\n",
      "Iteration 49532, loss = 0.02932467\n",
      "Iteration 49533, loss = 0.02932923\n",
      "Iteration 49534, loss = 0.02933264\n",
      "Iteration 49535, loss = 0.02932335\n",
      "Iteration 49536, loss = 0.02931771\n",
      "Iteration 49537, loss = 0.02932300\n",
      "Iteration 49538, loss = 0.02931873\n",
      "Iteration 49539, loss = 0.02931887\n",
      "Iteration 49540, loss = 0.02931638\n",
      "Iteration 49541, loss = 0.02932440\n",
      "Iteration 49542, loss = 0.02933112\n",
      "Iteration 49543, loss = 0.02932360\n",
      "Iteration 49544, loss = 0.02932207\n",
      "Iteration 49545, loss = 0.02932338\n",
      "Iteration 49546, loss = 0.02932935\n",
      "Iteration 49547, loss = 0.02932585\n",
      "Iteration 49548, loss = 0.02932255\n",
      "Iteration 49549, loss = 0.02932405\n",
      "Iteration 49550, loss = 0.02933294\n",
      "Iteration 49551, loss = 0.02934005\n",
      "Iteration 49552, loss = 0.02933696\n",
      "Iteration 49553, loss = 0.02933075\n",
      "Iteration 49554, loss = 0.02932571\n",
      "Iteration 49555, loss = 0.02931724\n",
      "Iteration 49556, loss = 0.02934104\n",
      "Iteration 49557, loss = 0.02934988\n",
      "Iteration 49558, loss = 0.02933488\n",
      "Iteration 49559, loss = 0.02932039\n",
      "Iteration 49560, loss = 0.02931537\n",
      "Iteration 49561, loss = 0.02933461\n",
      "Iteration 49562, loss = 0.02934411\n",
      "Iteration 49563, loss = 0.02933839\n",
      "Iteration 49564, loss = 0.02932092\n",
      "Iteration 49565, loss = 0.02931770\n",
      "Iteration 49566, loss = 0.02932237\n",
      "Iteration 49567, loss = 0.02933151\n",
      "Iteration 49568, loss = 0.02932821\n",
      "Iteration 49569, loss = 0.02932306\n",
      "Iteration 49570, loss = 0.02931418\n",
      "Iteration 49571, loss = 0.02932082\n",
      "Iteration 49572, loss = 0.02931636\n",
      "Iteration 49573, loss = 0.02931825\n",
      "Iteration 49574, loss = 0.02933119\n",
      "Iteration 49575, loss = 0.02932867\n",
      "Iteration 49576, loss = 0.02931037\n",
      "Iteration 49577, loss = 0.02931371\n",
      "Iteration 49578, loss = 0.02932043\n",
      "Iteration 49579, loss = 0.02931737\n",
      "Iteration 49580, loss = 0.02931959\n",
      "Iteration 49581, loss = 0.02931407\n",
      "Iteration 49582, loss = 0.02931477\n",
      "Iteration 49583, loss = 0.02931615\n",
      "Iteration 49584, loss = 0.02930906\n",
      "Iteration 49585, loss = 0.02932355\n",
      "Iteration 49586, loss = 0.02933273\n",
      "Iteration 49587, loss = 0.02932130\n",
      "Iteration 49588, loss = 0.02930647\n",
      "Iteration 49589, loss = 0.02931272\n",
      "Iteration 49590, loss = 0.02931804\n",
      "Iteration 49591, loss = 0.02931699\n",
      "Iteration 49592, loss = 0.02931828\n",
      "Iteration 49593, loss = 0.02930792\n",
      "Iteration 49594, loss = 0.02930331\n",
      "Iteration 49595, loss = 0.02932194\n",
      "Iteration 49596, loss = 0.02932294\n",
      "Iteration 49597, loss = 0.02932469\n",
      "Iteration 49598, loss = 0.02931665\n",
      "Iteration 49599, loss = 0.02930295\n",
      "Iteration 49600, loss = 0.02931336\n",
      "Iteration 49601, loss = 0.02931425\n",
      "Iteration 49602, loss = 0.02930920\n",
      "Iteration 49603, loss = 0.02930191\n",
      "Iteration 49604, loss = 0.02931101\n",
      "Iteration 49605, loss = 0.02932423\n",
      "Iteration 49606, loss = 0.02932326\n",
      "Iteration 49607, loss = 0.02930473\n",
      "Iteration 49608, loss = 0.02930908\n",
      "Iteration 49609, loss = 0.02932250\n",
      "Iteration 49610, loss = 0.02932367\n",
      "Iteration 49611, loss = 0.02932293\n",
      "Iteration 49612, loss = 0.02931455\n",
      "Iteration 49613, loss = 0.02930273\n",
      "Iteration 49614, loss = 0.02930662\n",
      "Iteration 49615, loss = 0.02931358\n",
      "Iteration 49616, loss = 0.02930626\n",
      "Iteration 49617, loss = 0.02930297\n",
      "Iteration 49618, loss = 0.02931363\n",
      "Iteration 49619, loss = 0.02932194\n",
      "Iteration 49620, loss = 0.02932628\n",
      "Iteration 49621, loss = 0.02931837\n",
      "Iteration 49622, loss = 0.02930687\n",
      "Iteration 49623, loss = 0.02928565\n",
      "Iteration 49624, loss = 0.02930697\n",
      "Iteration 49625, loss = 0.02931594\n",
      "Iteration 49626, loss = 0.02930985\n",
      "Iteration 49627, loss = 0.02929628\n",
      "Iteration 49628, loss = 0.02928322\n",
      "Iteration 49629, loss = 0.02928602\n",
      "Iteration 49630, loss = 0.02929276\n",
      "Iteration 49631, loss = 0.02929697\n",
      "Iteration 49632, loss = 0.02929020\n",
      "Iteration 49633, loss = 0.02928882\n",
      "Iteration 49634, loss = 0.02928779\n",
      "Iteration 49635, loss = 0.02929543\n",
      "Iteration 49636, loss = 0.02929211\n",
      "Iteration 49637, loss = 0.02928627\n",
      "Iteration 49638, loss = 0.02929076\n",
      "Iteration 49639, loss = 0.02927746\n",
      "Iteration 49640, loss = 0.02929501\n",
      "Iteration 49641, loss = 0.02929777\n",
      "Iteration 49642, loss = 0.02930190\n",
      "Iteration 49643, loss = 0.02929826\n",
      "Iteration 49644, loss = 0.02929397\n",
      "Iteration 49645, loss = 0.02929339\n",
      "Iteration 49646, loss = 0.02929354\n",
      "Iteration 49647, loss = 0.02929146\n",
      "Iteration 49648, loss = 0.02928535\n",
      "Iteration 49649, loss = 0.02928021\n",
      "Iteration 49650, loss = 0.02928355\n",
      "Iteration 49651, loss = 0.02928383\n",
      "Iteration 49652, loss = 0.02927659\n",
      "Iteration 49653, loss = 0.02928073\n",
      "Iteration 49654, loss = 0.02928415\n",
      "Iteration 49655, loss = 0.02928846\n",
      "Iteration 49656, loss = 0.02928338\n",
      "Iteration 49657, loss = 0.02927806\n",
      "Iteration 49658, loss = 0.02927423\n",
      "Iteration 49659, loss = 0.02928656\n",
      "Iteration 49660, loss = 0.02928091\n",
      "Iteration 49661, loss = 0.02927746\n",
      "Iteration 49662, loss = 0.02927715\n",
      "Iteration 49663, loss = 0.02927082\n",
      "Iteration 49664, loss = 0.02927394\n",
      "Iteration 49665, loss = 0.02927438\n",
      "Iteration 49666, loss = 0.02926953\n",
      "Iteration 49667, loss = 0.02927658\n",
      "Iteration 49668, loss = 0.02927690\n",
      "Iteration 49669, loss = 0.02927305\n",
      "Iteration 49670, loss = 0.02927482\n",
      "Iteration 49671, loss = 0.02927772\n",
      "Iteration 49672, loss = 0.02927860\n",
      "Iteration 49673, loss = 0.02926806\n",
      "Iteration 49674, loss = 0.02927426\n",
      "Iteration 49675, loss = 0.02928149\n",
      "Iteration 49676, loss = 0.02928194\n",
      "Iteration 49677, loss = 0.02927883\n",
      "Iteration 49678, loss = 0.02927629\n",
      "Iteration 49679, loss = 0.02927387\n",
      "Iteration 49680, loss = 0.02927743\n",
      "Iteration 49681, loss = 0.02927431\n",
      "Iteration 49682, loss = 0.02926946\n",
      "Iteration 49683, loss = 0.02927670\n",
      "Iteration 49684, loss = 0.02927199\n",
      "Iteration 49685, loss = 0.02926860\n",
      "Iteration 49686, loss = 0.02927466\n",
      "Iteration 49687, loss = 0.02926993\n",
      "Iteration 49688, loss = 0.02926698\n",
      "Iteration 49689, loss = 0.02926654\n",
      "Iteration 49690, loss = 0.02927223\n",
      "Iteration 49691, loss = 0.02926780\n",
      "Iteration 49692, loss = 0.02927185\n",
      "Iteration 49693, loss = 0.02926968\n",
      "Iteration 49694, loss = 0.02927165\n",
      "Iteration 49695, loss = 0.02927085\n",
      "Iteration 49696, loss = 0.02926526\n",
      "Iteration 49697, loss = 0.02926600\n",
      "Iteration 49698, loss = 0.02926757\n",
      "Iteration 49699, loss = 0.02925992\n",
      "Iteration 49700, loss = 0.02926335\n",
      "Iteration 49701, loss = 0.02926554\n",
      "Iteration 49702, loss = 0.02927417\n",
      "Iteration 49703, loss = 0.02926880\n",
      "Iteration 49704, loss = 0.02926142\n",
      "Iteration 49705, loss = 0.02928262\n",
      "Iteration 49706, loss = 0.02928318\n",
      "Iteration 49707, loss = 0.02928506\n",
      "Iteration 49708, loss = 0.02928094\n",
      "Iteration 49709, loss = 0.02928186\n",
      "Iteration 49710, loss = 0.02928717\n",
      "Iteration 49711, loss = 0.02928933\n",
      "Iteration 49712, loss = 0.02927620\n",
      "Iteration 49713, loss = 0.02925843\n",
      "Iteration 49714, loss = 0.02926724\n",
      "Iteration 49715, loss = 0.02926850\n",
      "Iteration 49716, loss = 0.02925904\n",
      "Iteration 49717, loss = 0.02926520\n",
      "Iteration 49718, loss = 0.02927336\n",
      "Iteration 49719, loss = 0.02927438\n",
      "Iteration 49720, loss = 0.02927338\n",
      "Iteration 49721, loss = 0.02926791\n",
      "Iteration 49722, loss = 0.02926061\n",
      "Iteration 49723, loss = 0.02925747\n",
      "Iteration 49724, loss = 0.02926353\n",
      "Iteration 49725, loss = 0.02925648\n",
      "Iteration 49726, loss = 0.02925475\n",
      "Iteration 49727, loss = 0.02925555\n",
      "Iteration 49728, loss = 0.02925893\n",
      "Iteration 49729, loss = 0.02925864\n",
      "Iteration 49730, loss = 0.02925119\n",
      "Iteration 49731, loss = 0.02926145\n",
      "Iteration 49732, loss = 0.02926474\n",
      "Iteration 49733, loss = 0.02926097\n",
      "Iteration 49734, loss = 0.02925303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 49735, loss = 0.02925576\n",
      "Iteration 49736, loss = 0.02925971\n",
      "Iteration 49737, loss = 0.02925261\n",
      "Iteration 49738, loss = 0.02925980\n",
      "Iteration 49739, loss = 0.02926298\n",
      "Iteration 49740, loss = 0.02926785\n",
      "Iteration 49741, loss = 0.02926279\n",
      "Iteration 49742, loss = 0.02926853\n",
      "Iteration 49743, loss = 0.02926582\n",
      "Iteration 49744, loss = 0.02924905\n",
      "Iteration 49745, loss = 0.02926233\n",
      "Iteration 49746, loss = 0.02927833\n",
      "Iteration 49747, loss = 0.02927752\n",
      "Iteration 49748, loss = 0.02925399\n",
      "Iteration 49749, loss = 0.02925741\n",
      "Iteration 49750, loss = 0.02925576\n",
      "Iteration 49751, loss = 0.02926819\n",
      "Iteration 49752, loss = 0.02926677\n",
      "Iteration 49753, loss = 0.02926714\n",
      "Iteration 49754, loss = 0.02924893\n",
      "Iteration 49755, loss = 0.02925582\n",
      "Iteration 49756, loss = 0.02926942\n",
      "Iteration 49757, loss = 0.02926040\n",
      "Iteration 49758, loss = 0.02926736\n",
      "Iteration 49759, loss = 0.02927013\n",
      "Iteration 49760, loss = 0.02926586\n",
      "Iteration 49761, loss = 0.02925868\n",
      "Iteration 49762, loss = 0.02924725\n",
      "Iteration 49763, loss = 0.02926333\n",
      "Iteration 49764, loss = 0.02926911\n",
      "Iteration 49765, loss = 0.02927242\n",
      "Iteration 49766, loss = 0.02925152\n",
      "Iteration 49767, loss = 0.02925229\n",
      "Iteration 49768, loss = 0.02926539\n",
      "Iteration 49769, loss = 0.02926637\n",
      "Iteration 49770, loss = 0.02926313\n",
      "Iteration 49771, loss = 0.02925709\n",
      "Iteration 49772, loss = 0.02924127\n",
      "Iteration 49773, loss = 0.02925175\n",
      "Iteration 49774, loss = 0.02926059\n",
      "Iteration 49775, loss = 0.02925730\n",
      "Iteration 49776, loss = 0.02924021\n",
      "Iteration 49777, loss = 0.02924583\n",
      "Iteration 49778, loss = 0.02926259\n",
      "Iteration 49779, loss = 0.02926721\n",
      "Iteration 49780, loss = 0.02926162\n",
      "Iteration 49781, loss = 0.02925264\n",
      "Iteration 49782, loss = 0.02924395\n",
      "Iteration 49783, loss = 0.02924095\n",
      "Iteration 49784, loss = 0.02924128\n",
      "Iteration 49785, loss = 0.02924461\n",
      "Iteration 49786, loss = 0.02924425\n",
      "Iteration 49787, loss = 0.02924132\n",
      "Iteration 49788, loss = 0.02924132\n",
      "Iteration 49789, loss = 0.02926276\n",
      "Iteration 49790, loss = 0.02926350\n",
      "Iteration 49791, loss = 0.02924828\n",
      "Iteration 49792, loss = 0.02924595\n",
      "Iteration 49793, loss = 0.02925341\n",
      "Iteration 49794, loss = 0.02924770\n",
      "Iteration 49795, loss = 0.02924051\n",
      "Iteration 49796, loss = 0.02922918\n",
      "Iteration 49797, loss = 0.02924101\n",
      "Iteration 49798, loss = 0.02923990\n",
      "Iteration 49799, loss = 0.02923753\n",
      "Iteration 49800, loss = 0.02923558\n",
      "Iteration 49801, loss = 0.02922982\n",
      "Iteration 49802, loss = 0.02925532\n",
      "Iteration 49803, loss = 0.02925785\n",
      "Iteration 49804, loss = 0.02923910\n",
      "Iteration 49805, loss = 0.02923344\n",
      "Iteration 49806, loss = 0.02925264\n",
      "Iteration 49807, loss = 0.02925696\n",
      "Iteration 49808, loss = 0.02924665\n",
      "Iteration 49809, loss = 0.02923948\n",
      "Iteration 49810, loss = 0.02923957\n",
      "Iteration 49811, loss = 0.02923503\n",
      "Iteration 49812, loss = 0.02922848\n",
      "Iteration 49813, loss = 0.02923671\n",
      "Iteration 49814, loss = 0.02924264\n",
      "Iteration 49815, loss = 0.02923260\n",
      "Iteration 49816, loss = 0.02923392\n",
      "Iteration 49817, loss = 0.02924034\n",
      "Iteration 49818, loss = 0.02923693\n",
      "Iteration 49819, loss = 0.02922938\n",
      "Iteration 49820, loss = 0.02921732\n",
      "Iteration 49821, loss = 0.02922668\n",
      "Iteration 49822, loss = 0.02922827\n",
      "Iteration 49823, loss = 0.02922282\n",
      "Iteration 49824, loss = 0.02922093\n",
      "Iteration 49825, loss = 0.02922801\n",
      "Iteration 49826, loss = 0.02922858\n",
      "Iteration 49827, loss = 0.02922707\n",
      "Iteration 49828, loss = 0.02922559\n",
      "Iteration 49829, loss = 0.02923844\n",
      "Iteration 49830, loss = 0.02923704\n",
      "Iteration 49831, loss = 0.02923013\n",
      "Iteration 49832, loss = 0.02923088\n",
      "Iteration 49833, loss = 0.02923393\n",
      "Iteration 49834, loss = 0.02922292\n",
      "Iteration 49835, loss = 0.02923110\n",
      "Iteration 49836, loss = 0.02923425\n",
      "Iteration 49837, loss = 0.02923173\n",
      "Iteration 49838, loss = 0.02922757\n",
      "Iteration 49839, loss = 0.02923869\n",
      "Iteration 49840, loss = 0.02923364\n",
      "Iteration 49841, loss = 0.02922020\n",
      "Iteration 49842, loss = 0.02922404\n",
      "Iteration 49843, loss = 0.02921536\n",
      "Iteration 49844, loss = 0.02921379\n",
      "Iteration 49845, loss = 0.02921477\n",
      "Iteration 49846, loss = 0.02921404\n",
      "Iteration 49847, loss = 0.02921498\n",
      "Iteration 49848, loss = 0.02921713\n",
      "Iteration 49849, loss = 0.02921490\n",
      "Iteration 49850, loss = 0.02921259\n",
      "Iteration 49851, loss = 0.02921248\n",
      "Iteration 49852, loss = 0.02921875\n",
      "Iteration 49853, loss = 0.02921444\n",
      "Iteration 49854, loss = 0.02921665\n",
      "Iteration 49855, loss = 0.02922306\n",
      "Iteration 49856, loss = 0.02922893\n",
      "Iteration 49857, loss = 0.02922304\n",
      "Iteration 49858, loss = 0.02921446\n",
      "Iteration 49859, loss = 0.02921527\n",
      "Iteration 49860, loss = 0.02921471\n",
      "Iteration 49861, loss = 0.02921348\n",
      "Iteration 49862, loss = 0.02921059\n",
      "Iteration 49863, loss = 0.02921072\n",
      "Iteration 49864, loss = 0.02920932\n",
      "Iteration 49865, loss = 0.02921093\n",
      "Iteration 49866, loss = 0.02921193\n",
      "Iteration 49867, loss = 0.02919965\n",
      "Iteration 49868, loss = 0.02920621\n",
      "Iteration 49869, loss = 0.02920535\n",
      "Iteration 49870, loss = 0.02920980\n",
      "Iteration 49871, loss = 0.02920162\n",
      "Iteration 49872, loss = 0.02920862\n",
      "Iteration 49873, loss = 0.02921708\n",
      "Iteration 49874, loss = 0.02920529\n",
      "Iteration 49875, loss = 0.02919656\n",
      "Iteration 49876, loss = 0.02921767\n",
      "Iteration 49877, loss = 0.02921626\n",
      "Iteration 49878, loss = 0.02920765\n",
      "Iteration 49879, loss = 0.02920059\n",
      "Iteration 49880, loss = 0.02919795\n",
      "Iteration 49881, loss = 0.02920143\n",
      "Iteration 49882, loss = 0.02920363\n",
      "Iteration 49883, loss = 0.02919989\n",
      "Iteration 49884, loss = 0.02920088\n",
      "Iteration 49885, loss = 0.02919832\n",
      "Iteration 49886, loss = 0.02919361\n",
      "Iteration 49887, loss = 0.02920797\n",
      "Iteration 49888, loss = 0.02920536\n",
      "Iteration 49889, loss = 0.02920094\n",
      "Iteration 49890, loss = 0.02920239\n",
      "Iteration 49891, loss = 0.02919653\n",
      "Iteration 49892, loss = 0.02920550\n",
      "Iteration 49893, loss = 0.02919987\n",
      "Iteration 49894, loss = 0.02920085\n",
      "Iteration 49895, loss = 0.02920591\n",
      "Iteration 49896, loss = 0.02919945\n",
      "Iteration 49897, loss = 0.02919941\n",
      "Iteration 49898, loss = 0.02920006\n",
      "Iteration 49899, loss = 0.02919467\n",
      "Iteration 49900, loss = 0.02919674\n",
      "Iteration 49901, loss = 0.02919762\n",
      "Iteration 49902, loss = 0.02919537\n",
      "Iteration 49903, loss = 0.02919800\n",
      "Iteration 49904, loss = 0.02919708\n",
      "Iteration 49905, loss = 0.02920305\n",
      "Iteration 49906, loss = 0.02920616\n",
      "Iteration 49907, loss = 0.02919922\n",
      "Iteration 49908, loss = 0.02919705\n",
      "Iteration 49909, loss = 0.02920177\n",
      "Iteration 49910, loss = 0.02920530\n",
      "Iteration 49911, loss = 0.02920811\n",
      "Iteration 49912, loss = 0.02920017\n",
      "Iteration 49913, loss = 0.02919948\n",
      "Iteration 49914, loss = 0.02919794\n",
      "Iteration 49915, loss = 0.02920180\n",
      "Iteration 49916, loss = 0.02918755\n",
      "Iteration 49917, loss = 0.02920089\n",
      "Iteration 49918, loss = 0.02920397\n",
      "Iteration 49919, loss = 0.02920153\n",
      "Iteration 49920, loss = 0.02918465\n",
      "Iteration 49921, loss = 0.02919699\n",
      "Iteration 49922, loss = 0.02920158\n",
      "Iteration 49923, loss = 0.02918980\n",
      "Iteration 49924, loss = 0.02920404\n",
      "Iteration 49925, loss = 0.02920702\n",
      "Iteration 49926, loss = 0.02920734\n",
      "Iteration 49927, loss = 0.02920684\n",
      "Iteration 49928, loss = 0.02919700\n",
      "Iteration 49929, loss = 0.02920255\n",
      "Iteration 49930, loss = 0.02920345\n",
      "Iteration 49931, loss = 0.02920846\n",
      "Iteration 49932, loss = 0.02920481\n",
      "Iteration 49933, loss = 0.02919726\n",
      "Iteration 49934, loss = 0.02920336\n",
      "Iteration 49935, loss = 0.02919765\n",
      "Iteration 49936, loss = 0.02921094\n",
      "Iteration 49937, loss = 0.02920855\n",
      "Iteration 49938, loss = 0.02919289\n",
      "Iteration 49939, loss = 0.02918693\n",
      "Iteration 49940, loss = 0.02920032\n",
      "Iteration 49941, loss = 0.02919767\n",
      "Iteration 49942, loss = 0.02918300\n",
      "Iteration 49943, loss = 0.02918582\n",
      "Iteration 49944, loss = 0.02920620\n",
      "Iteration 49945, loss = 0.02921079\n",
      "Iteration 49946, loss = 0.02920091\n",
      "Iteration 49947, loss = 0.02919738\n",
      "Iteration 49948, loss = 0.02919646\n",
      "Iteration 49949, loss = 0.02921188\n",
      "Iteration 49950, loss = 0.02921490\n",
      "Iteration 49951, loss = 0.02920932\n",
      "Iteration 49952, loss = 0.02919460\n",
      "Iteration 49953, loss = 0.02918526\n",
      "Iteration 49954, loss = 0.02919621\n",
      "Iteration 49955, loss = 0.02920594\n",
      "Iteration 49956, loss = 0.02919854\n",
      "Iteration 49957, loss = 0.02918016\n",
      "Iteration 49958, loss = 0.02918012\n",
      "Iteration 49959, loss = 0.02919114\n",
      "Iteration 49960, loss = 0.02918648\n",
      "Iteration 49961, loss = 0.02917418\n",
      "Iteration 49962, loss = 0.02918637\n",
      "Iteration 49963, loss = 0.02919086\n",
      "Iteration 49964, loss = 0.02918027\n",
      "Iteration 49965, loss = 0.02919371\n",
      "Iteration 49966, loss = 0.02918992\n",
      "Iteration 49967, loss = 0.02917597\n",
      "Iteration 49968, loss = 0.02918115\n",
      "Iteration 49969, loss = 0.02919021\n",
      "Iteration 49970, loss = 0.02918408\n",
      "Iteration 49971, loss = 0.02917365\n",
      "Iteration 49972, loss = 0.02917377\n",
      "Iteration 49973, loss = 0.02917758\n",
      "Iteration 49974, loss = 0.02916558\n",
      "Iteration 49975, loss = 0.02916241\n",
      "Iteration 49976, loss = 0.02917419\n",
      "Iteration 49977, loss = 0.02916648\n",
      "Iteration 49978, loss = 0.02917209\n",
      "Iteration 49979, loss = 0.02917919\n",
      "Iteration 49980, loss = 0.02917072\n",
      "Iteration 49981, loss = 0.02916240\n",
      "Iteration 49982, loss = 0.02917885\n",
      "Iteration 49983, loss = 0.02918036\n",
      "Iteration 49984, loss = 0.02916983\n",
      "Iteration 49985, loss = 0.02916739\n",
      "Iteration 49986, loss = 0.02916813\n",
      "Iteration 49987, loss = 0.02917165\n",
      "Iteration 49988, loss = 0.02916810\n",
      "Iteration 49989, loss = 0.02918436\n",
      "Iteration 49990, loss = 0.02918150\n",
      "Iteration 49991, loss = 0.02916823\n",
      "Iteration 49992, loss = 0.02917447\n",
      "Iteration 49993, loss = 0.02918314\n",
      "Iteration 49994, loss = 0.02918434\n",
      "Iteration 49995, loss = 0.02917578\n",
      "Iteration 49996, loss = 0.02916053\n",
      "Iteration 49997, loss = 0.02917778\n",
      "Iteration 49998, loss = 0.02918815\n",
      "Iteration 49999, loss = 0.02918161\n",
      "Iteration 50000, loss = 0.02916717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\victo\\.virtualenv\\base\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_iter=50000, momentum=0.9,\n",
       "              n_iter_no_change=50000, nesterovs_momentum=True, power_t=0.5,\n",
       "              random_state=None, shuffle=True, solver='adam', tol=1e-12,\n",
       "              validation_fraction=0.1, verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if use_tunned:\n",
    "    mlp = clf\n",
    "else:\n",
    "    mlp = load_model()\n",
    "\n",
    "mlp.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.99\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99       100\n",
      "           1       0.98      1.00      0.99       100\n",
      "\n",
      "    accuracy                           0.99       200\n",
      "   macro avg       0.99      0.99      0.99       200\n",
      "weighted avg       0.99      0.99      0.99       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy = {mlp.score(x_train, y_train)}')\n",
    "predicted = mlp.predict(x_train)\n",
    "print(classification_report(y_train, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        36\n",
      "           1       1.00      1.00      1.00        44\n",
      "\n",
      "    accuracy                           1.00        80\n",
      "   macro avg       1.00      1.00      1.00        80\n",
      "weighted avg       1.00      1.00      1.00        80\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mlp.score(x_test, y_test)\n",
    "predicted = mlp.predict(x_test)\n",
    "print(classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46.051241471987616"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(mlp.coefs_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate hyperplane with noisy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\victo\\Google Drive\\Mestrado\\Redes Neurais\\Trabalhos\\Artigo_03\\code\\notebooks\\utils.py:45: UserWarning: No contour levels were found within the data range.\n",
      "  plt.contour(x1, x2, z, levels=[0], colors=('cyan',), linewidths=(2.5,))\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set()\n",
    "sns.scatterplot(x='x1', y='x2', hue='class', data=data_train)\n",
    "plt.title('Distribuição de Classes em Meia Lua - Treino')    \n",
    "plot_plain_separator(mlp, x_train, save=False, grid_range=(-1, 2))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\victo\\Google Drive\\Mestrado\\Redes Neurais\\Trabalhos\\Artigo_03\\code\\notebooks\\utils.py:45: UserWarning: No contour levels were found within the data range.\n",
      "  plt.contour(x1, x2, z, levels=[0], colors=('cyan',), linewidths=(2.5,))\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set()\n",
    "sns.scatterplot(x='x1', y='x2', hue='class', data=data_test)\n",
    "plt.title('Distribuição de Classes em Meia Lua - Treino')    \n",
    "plot_plain_separator(mlp, x_train, save=False, grid_range=(-1, 2))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run relabeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run training on relabeled dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate hyperplane with relabeled dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
